  Sprint 2: OCP Training Data Preparation

  Tasks:
  1. Extract training sequences from ontology chains:
  # Convert 173K chains â†’ training sequences
  ./.venv/bin/python -m src.lvm.prepare_ocp_sequences \
      --ontology-chains artifacts/ontology_chains/*.jsonl \
      --output artifacts/lvm/ocp_training.npz \
      --min-chain-length 3 \
      --max-chain-length 15
  2. Generate vectors for all sequences:
    - Use GTR-T5 embeddings (already computed for 9.5K)
    - Batch encode remaining concepts from 173K chains
    - Store in NPZ: [N_sequences, max_length, 768]
  3. Create train/val/test splits:
    - 70% train / 15% val / 15% test
    - Stratify by ontology source (SWO, GO, DBpedia) and chain length

  Exit criteria:
  - artifacts/lvm/ocp_training.npz contains ~800K training examples
  - All vectors pre-computed (no runtime embedding overhead)
  - Splits balanced across ontology sources

[Director] Comments:
Why not starty with our current 10k ingested concepts? 
I thought the vector database was populated when the vecRAG was populated as part of the injestion phase? (This is concerning; I was expecing CPESH + TMD 768+16 to already be in a vector database)
  Exit criteria: COMMENTS
  - artifacts/lvm/ocp_training.npz contains ~800K training examples >>. Shouldnt that be 10k?
  - All vectors pre-computed (no runtime embedding overhead) >>. Agree
  - Splits balanced across ontology sources >>. Agree

Update 1:
  Sprint 2: Prepare OCP Training Data (FROM CURRENT 9.5K)

  Task 1: Extract training sequences from INGESTED concepts
  # File: src/lvm/extract_ocp_sequences.py

  def extract_sequences_from_current_data():
      """
      Extract training sequences from 9,477 already-ingested concepts.
      No need to wait for 173K - use what we have NOW.
      """

      # Query Postgres for all ingested concepts with their relationships
      conn = psycopg2.connect("dbname=lnsp")
      cur = conn.cursor()

      cur.execute("""
          SELECT 
              e.cpe_id,
              e.concept_text,
              e.relations_text,
              e.tmd_lane,
              v.concept_vec,
              v.tmd_dense
          FROM cpe_entry e
          JOIN cpe_vectors v USING (cpe_id)
          WHERE e.validation_status = 'passed'
          ORDER BY e.created_at;
      """)

      concepts = cur.fetchall()

      # Build sequences from relationships in Neo4j
      sequences = []
      for concept in concepts:
          # Get ordered chain from Neo4j (if part of ontology)
          chain = neo4j_get_chain(concept['cpe_id'])

          if len(chain) >= 3:
              # Create training pairs: predict next from context
              vectors = [c['concept_vec'] for c in chain]
              for i in range(2, len(vectors)):
                  sequences.append({
                      'context': vectors[:i],
                      'target': vectors[i],
                      'tmd': chain[0]['tmd_dense'],
                  })

      return sequences  # Expected: ~45K sequences from 9.5K concepts

  Task 2: Export to NPZ format
  # Create training NPZ from current database
  ./.venv/bin/python -m src.lvm.extract_ocp_sequences \
      --output artifacts/lvm/ocp_training_9k.npz \
      --min-chain-length 3 \
      --max-chain-length 15

  Task 3: Train/Val/Test split
  # Split: 70/15/15
  # Expected sizes:
  # - Train: ~31.5K sequences
  # - Val: ~6.75K sequences
  # - Test: ~6.75K sequences

  Exit Criteria (CORRECTED):
  - âœ… artifacts/lvm/ocp_training_9k.npz contains ~45K training examples (not 800K)
  - âœ… All vectors pre-computed (loaded from cpe_vectors table)
  - âœ… Splits balanced across ontology sources (SWO, GO, DBpedia)

  ## Vector Dimensions (CLARIFICATION):
  - **Training inputs**: 768D concept vectors from GTR-T5 (from `cpe_vectors.concept_vec`)
  - **Optional conditioning**: 16D TMD encoding (from `cpe_vectors.tmd_dense`)
  - **Total per token**: 768D or 784D (768+16) depending on TMD usage
  - **Note**: Faiss/vecRAG uses 784D fused vectors for retrieval; LVM training can use either 768D or 784D

---

## TASK ASSIGNMENTS

### Task 2.1: [Architect] Design OCP Sequence Extraction Algorithm
**Owner:** [Architect]
**Priority:** Critical Path
**Estimated Time:** 3-4 hours

**Objective:**
Design and implement algorithm to extract ~45K training sequences from 9,477 ingested ontology concepts using Postgres + Neo4j chain traversal.

**Technical Specification:**
```python
# File: src/lvm/extract_ocp_sequences.py

import psycopg2
import numpy as np
from neo4j import GraphDatabase

def extract_sequences_from_current_data(
    min_chain_length=3,
    max_chain_length=15,
    output_path="artifacts/lvm/ocp_training_9k.npz"
):
    """
    Extract autoregressive training sequences from ingested ontology chains.

    Data Flow:
    1. Query Postgres for all valid CPE entries with vectors
    2. For each concept, query Neo4j for its ontology chain
    3. Build contextâ†’target pairs: predict vec[i] from vec[0:i]
    4. Filter by chain length (3-15 concepts)
    5. Export to NPZ with train/val/test splits

    Expected Output:
    - ~45K training sequences from 9.5K concepts
    - Each sequence: (context_vecs, target_vec, tmd_dense, metadata)
    - 70/15/15 train/val/test split, stratified by source
    """

    # Step 1: Load all concepts with vectors from Postgres
    conn = psycopg2.connect("dbname=lnsp user=postgres")
    cur = conn.cursor()

    query = """
        SELECT
            e.cpe_id,
            e.concept_text,
            e.source_type,
            e.tmd_lane,
            e.tmd_bits,
            v.concept_vec,
            v.tmd_dense,
            v.fused_vec
        FROM cpe_entry e
        JOIN cpe_vectors v USING (cpe_id)
        WHERE e.validation_status = 'passed'
          AND v.concept_vec IS NOT NULL
        ORDER BY e.source_type, e.created_at;
    """

    cur.execute(query)
    concepts = {
        row[0]: {
            'cpe_id': row[0],
            'text': row[1],
            'source': row[2],
            'tmd_lane': row[3],
            'tmd_bits': row[4],
            'concept_vec': np.array(row[5]),
            'tmd_dense': np.array(row[6]),
            'fused_vec': np.array(row[7])
        }
        for row in cur.fetchall()
    }

    print(f"Loaded {len(concepts)} concepts from Postgres")

    # Step 2: Query Neo4j for ontology chains
    neo4j_driver = GraphDatabase.driver(
        "bolt://localhost:7687",
        auth=("neo4j", "password")
    )

    sequences = []

    with neo4j_driver.session() as session:
        # Find all ontology chains (paths with IS_A, PART_OF relations)
        result = session.run("""
            MATCH path = (start:Concept)-[:IS_A|PART_OF*1..15]->(end:Concept)
            WHERE start.source IN ['swo', 'go', 'dbpedia']
            RETURN [node in nodes(path) | node.cpe_id] as chain_ids,
                   length(path) as chain_len
        """)

        for record in result:
            chain_ids = record['chain_ids']
            chain_len = record['chain_len']

            # Filter by length
            if chain_len < min_chain_length or chain_len > max_chain_length:
                continue

            # Get vectors for all concepts in chain
            chain_vecs = []
            valid_chain = True
            for cpe_id in chain_ids:
                if cpe_id not in concepts:
                    valid_chain = False
                    break
                chain_vecs.append(concepts[cpe_id])

            if not valid_chain:
                continue

            # Create training pairs: predict vec[i] from vec[0:i]
            for i in range(2, len(chain_vecs)):
                context = [c['concept_vec'] for c in chain_vecs[:i]]
                target = chain_vecs[i]['concept_vec']
                tmd = chain_vecs[0]['tmd_dense']  # Use root TMD
                source = chain_vecs[0]['source']

                sequences.append({
                    'context': np.array(context),  # [i, 768]
                    'target': target,              # [768]
                    'tmd': tmd,                    # [16]
                    'source': source,              # 'swo', 'go', or 'dbpedia'
                    'chain_length': len(chain_vecs),
                    'context_length': i,
                })

    print(f"Extracted {len(sequences)} training sequences")

    # Step 3: Train/val/test split (70/15/15) stratified by source
    from sklearn.model_selection import train_test_split

    sources = [s['source'] for s in sequences]

    train_idx, temp_idx = train_test_split(
        range(len(sequences)),
        test_size=0.3,
        stratify=sources,
        random_state=42
    )

    val_idx, test_idx = train_test_split(
        temp_idx,
        test_size=0.5,
        stratify=[sources[i] for i in temp_idx],
        random_state=42
    )

    # Step 4: Export to NPZ
    np.savez_compressed(
        output_path,
        train_context=[sequences[i]['context'] for i in train_idx],
        train_target=[sequences[i]['target'] for i in train_idx],
        train_tmd=[sequences[i]['tmd'] for i in train_idx],
        val_context=[sequences[i]['context'] for i in val_idx],
        val_target=[sequences[i]['target'] for i in val_idx],
        val_tmd=[sequences[i]['tmd'] for i in val_idx],
        test_context=[sequences[i]['context'] for i in test_idx],
        test_target=[sequences[i]['target'] for i in test_idx],
        test_tmd=[sequences[i]['tmd'] for i in test_idx],
        metadata={
            'total_sequences': len(sequences),
            'train_size': len(train_idx),
            'val_size': len(val_idx),
            'test_size': len(test_idx),
            'source_counts': {
                'swo': sum(1 for s in sequences if s['source'] == 'swo'),
                'go': sum(1 for s in sequences if s['source'] == 'go'),
                'dbpedia': sum(1 for s in sequences if s['source'] == 'dbpedia'),
            }
        }
    )

    print(f"Saved to {output_path}")
    print(f"  Train: {len(train_idx)} sequences")
    print(f"  Val: {len(val_idx)} sequences")
    print(f"  Test: {len(test_idx)} sequences")

    return sequences, train_idx, val_idx, test_idx
```

**Implementation Steps:**
1. Connect to Postgres and load all 9,477 concepts with vectors
2. Connect to Neo4j and query all ontology chains (IS_A, PART_OF paths)
3. For each chain of length 3-15, create contextâ†’target training pairs
4. Apply 70/15/15 train/val/test split stratified by source
5. Export to NPZ with all splits and metadata

**Deliverables:**
- `src/lvm/extract_ocp_sequences.py` implementation
- `artifacts/lvm/ocp_training_9k.npz` with ~45K sequences
- Documentation: Algorithm explanation, data format specification
- Validation script: `tests/test_ocp_sequences.py`

**Exit Criteria:**
- âœ… ~45K sequences extracted (Â±10% acceptable)
- âœ… All sequences have valid context, target, and TMD vectors
- âœ… Train/val/test split is 70/15/15 (Â±2%)
- âœ… Source distribution balanced across splits

---

### Task 2.2: [Programmer] Implement Sequence Builder and NPZ Export
**Owner:** [Programmer]
**Priority:** Critical Path
**Estimated Time:** 2-3 hours

**Objective:**
Implement robust sequence builder with error handling, validation, and efficient NPZ export for training pipeline.

**Technical Specification:**
```python
# File: src/lvm/sequence_builder.py

import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Tuple

@dataclass
class TrainingSequence:
    """
    Single training example for autoregressive LVM training.

    Attributes:
    - context: List of 768D vectors (length 2-14)
    - target: 768D vector to predict
    - tmd: 16D TMD metadata vector
    - metadata: Dict with source, chain_length, etc.
    """
    context: np.ndarray  # [context_len, 768]
    target: np.ndarray   # [768]
    tmd: np.ndarray      # [16]
    metadata: Dict

    def validate(self) -> bool:
        """
        Validate sequence has correct shapes and no NaN/Inf values.
        """
        # Check shapes
        if self.context.ndim != 2 or self.context.shape[1] != 768:
            return False
        if self.target.shape != (768,):
            return False
        if self.tmd.shape != (16,):
            return False

        # Check for NaN/Inf
        if np.any(np.isnan(self.context)) or np.any(np.isinf(self.context)):
            return False
        if np.any(np.isnan(self.target)) or np.any(np.isinf(self.target)):
            return False
        if np.any(np.isnan(self.tmd)) or np.any(np.isinf(self.tmd)):
            return False

        # Check context length
        if len(self.context) < 2 or len(self.context) > 14:
            return False

        return True


class SequenceDataset:
    """
    Container for training sequences with efficient batching and export.
    """

    def __init__(self, sequences: List[TrainingSequence]):
        self.sequences = [s for s in sequences if s.validate()]
        print(f"Validated {len(self.sequences)}/{len(sequences)} sequences")

    def export_to_npz(
        self,
        output_path: str,
        train_idx: List[int],
        val_idx: List[int],
        test_idx: List[int]
    ):
        """
        Export sequences to NPZ with train/val/test splits.

        NPZ Format:
        - Ragged arrays: Use object dtype to store variable-length contexts
        - train_context: Array of [context_len, 768] arrays
        - train_target: Array of [768] vectors
        - train_tmd: Array of [16] vectors
        """

        def extract_split(indices):
            return (
                np.array([self.sequences[i].context for i in indices], dtype=object),
                np.array([self.sequences[i].target for i in indices]),
                np.array([self.sequences[i].tmd for i in indices])
            )

        train_ctx, train_tgt, train_tmd = extract_split(train_idx)
        val_ctx, val_tgt, val_tmd = extract_split(val_idx)
        test_ctx, test_tgt, test_tmd = extract_split(test_idx)

        # Compute statistics
        context_lengths = [len(self.sequences[i].context) for i in train_idx]
        sources = [self.sequences[i].metadata['source'] for i in train_idx]

        np.savez_compressed(
            output_path,
            train_context=train_ctx,
            train_target=train_tgt,
            train_tmd=train_tmd,
            val_context=val_ctx,
            val_target=val_tgt,
            val_tmd=val_tmd,
            test_context=test_ctx,
            test_target=test_tgt,
            test_tmd=test_tmd,
            metadata=np.array([{
                'total_sequences': len(self.sequences),
                'train_size': len(train_idx),
                'val_size': len(val_idx),
                'test_size': len(test_idx),
                'mean_context_length': np.mean(context_lengths),
                'max_context_length': np.max(context_lengths),
                'source_distribution': {
                    src: sum(1 for s in sources if s == src)
                    for src in set(sources)
                }
            }], dtype=object)[0]
        )

        print(f"Exported to {output_path}")
        print(f"  Train: {len(train_idx)} sequences")
        print(f"  Val: {len(val_idx)} sequences")
        print(f"  Test: {len(test_idx)} sequences")
        print(f"  Mean context length: {np.mean(context_lengths):.2f}")
```

**Implementation Steps:**
1. Create `TrainingSequence` dataclass with validation
2. Implement `SequenceDataset` container with NPZ export
3. Handle ragged arrays (variable-length contexts) using object dtype
4. Add comprehensive error handling and validation
5. Test with small sample before full export

**Deliverables:**
- `src/lvm/sequence_builder.py` implementation
- Unit tests: `tests/test_sequence_builder.py`
- Example usage script: `examples/build_sequences.py`
- Documentation: Data format and validation rules

**Exit Criteria:**
- âœ… All sequences pass validation (no NaN/Inf, correct shapes)
- âœ… NPZ export handles variable-length contexts correctly
- âœ… Unit tests pass with 100% coverage
- âœ… NPZ file loads successfully in PyTorch DataLoader

---

### Task 2.3: [Consultant] Create Train/Val/Test Split Strategy
**Owner:** [Consultant]
**Priority:** High
**Estimated Time:** 2 hours

**Objective:**
Design and validate stratified split strategy to ensure balanced representation across ontology sources and chain lengths.

**Split Requirements:**
```python
# File: src/lvm/split_strategy.py

from sklearn.model_selection import StratifiedKFold, train_test_split
import numpy as np

def stratified_split_by_source(
    sequences: List[TrainingSequence],
    train_ratio=0.7,
    val_ratio=0.15,
    test_ratio=0.15,
    random_seed=42
) -> Tuple[List[int], List[int], List[int]]:
    """
    Create stratified train/val/test splits ensuring balanced source distribution.

    Stratification Variables:
    - Primary: Source type (swo, go, dbpedia)
    - Secondary: Chain length bucket (3-5, 6-10, 11-15)

    Validation Checks:
    - Source distribution within Â±3% across splits
    - Chain length distribution within Â±5% across splits
    - No data leakage: Ensure same ontology chain doesn't appear in multiple splits
    """

    # Create stratification labels: "{source}_{length_bucket}"
    strat_labels = []
    for seq in sequences:
        source = seq.metadata['source']
        chain_len = seq.metadata['chain_length']

        if chain_len <= 5:
            bucket = 'short'
        elif chain_len <= 10:
            bucket = 'medium'
        else:
            bucket = 'long'

        strat_labels.append(f"{source}_{bucket}")

    # First split: train vs (val+test)
    train_idx, temp_idx = train_test_split(
        range(len(sequences)),
        test_size=(val_ratio + test_ratio),
        stratify=strat_labels,
        random_state=random_seed
    )

    # Second split: val vs test
    temp_labels = [strat_labels[i] for i in temp_idx]
    val_idx, test_idx = train_test_split(
        temp_idx,
        test_size=test_ratio / (val_ratio + test_ratio),
        stratify=temp_labels,
        random_state=random_seed
    )

    # Validate splits
    validate_splits(sequences, train_idx, val_idx, test_idx)

    return train_idx, val_idx, test_idx


def validate_splits(
    sequences: List[TrainingSequence],
    train_idx: List[int],
    val_idx: List[int],
    test_idx: List[int]
):
    """
    Validate that splits are balanced and have no data leakage.
    """

    def get_distribution(indices, key):
        values = [sequences[i].metadata[key] for i in indices]
        counts = {v: values.count(v) for v in set(values)}
        total = len(values)
        return {k: v/total for k, v in counts.items()}

    # Check source distribution
    train_sources = get_distribution(train_idx, 'source')
    val_sources = get_distribution(val_idx, 'source')
    test_sources = get_distribution(test_idx, 'source')

    print("Source Distribution:")
    print(f"  Train: {train_sources}")
    print(f"  Val:   {val_sources}")
    print(f"  Test:  {test_sources}")

    # Check that distributions are within Â±3%
    for source in train_sources.keys():
        train_pct = train_sources.get(source, 0)
        val_pct = val_sources.get(source, 0)
        test_pct = test_sources.get(source, 0)

        assert abs(train_pct - val_pct) < 0.03, f"{source} imbalance: train={train_pct:.3f}, val={val_pct:.3f}"
        assert abs(train_pct - test_pct) < 0.03, f"{source} imbalance: train={train_pct:.3f}, test={test_pct:.3f}"

    print("âœ… Source distribution balanced within Â±3%")

    # Check for data leakage (same chain in multiple splits)
    train_chains = set(sequences[i].metadata.get('chain_id') for i in train_idx)
    val_chains = set(sequences[i].metadata.get('chain_id') for i in val_idx)
    test_chains = set(sequences[i].metadata.get('chain_id') for i in test_idx)

    overlap = (train_chains & val_chains) | (train_chains & test_chains) | (val_chains & test_chains)
    assert len(overlap) == 0, f"Data leakage: {len(overlap)} chains in multiple splits"

    print("âœ… No data leakage detected")
```

**Validation Metrics:**
- Source distribution: swo/go/dbpedia within Â±3% across splits
- Chain length distribution: short/medium/long within Â±5% across splits
- Size ratios: 70/15/15 (Â±2% acceptable)
- No data leakage: Zero chains appearing in multiple splits

**Deliverables:**
- `src/lvm/split_strategy.py` implementation
- Validation report: Distribution tables and leakage check
- Unit tests: `tests/test_split_strategy.py`
- Documentation: Stratification methodology

**Exit Criteria:**
- âœ… 70/15/15 split ratios achieved (Â±2%)
- âœ… Source distribution balanced within Â±3%
- âœ… Chain length distribution balanced within Â±5%
- âœ… Zero data leakage detected

---

### Task 2.4: [Architect] Pre-computed Vector Validation
**Owner:** [Architect]
**Priority:** Sprint Exit Gate
**Estimated Time:** 1-2 hours

**Objective:**
Verify all 9,477 vectors are accessible, normalized, and ready for training with no runtime embedding overhead.

**Validation Script:**
```python
# File: tests/validate_vectors_ready.py

import psycopg2
import numpy as np

def validate_all_vectors():
    """
    Comprehensive validation of pre-computed vectors in database.

    Checks:
    1. All 9,477 CPE entries have corresponding vectors
    2. All vectors have correct dimensions (768D concept, 16D TMD, 784D fused)
    3. All vectors are normalized (L2 norm â‰ˆ 1.0)
    4. No NaN or Inf values
    5. Vectors are accessible in <1ms per query
    """

    conn = psycopg2.connect("dbname=lnsp")
    cur = conn.cursor()

    # Check 1: Count completeness
    cur.execute("SELECT COUNT(*) FROM cpe_entry;")
    entry_count = cur.fetchone()[0]

    cur.execute("SELECT COUNT(*) FROM cpe_vectors;")
    vector_count = cur.fetchone()[0]

    assert entry_count == vector_count, f"Mismatch: {entry_count} entries, {vector_count} vectors"
    print(f"âœ… Vector completeness: {vector_count}/{entry_count} (100%)")

    # Check 2: Dimension validation
    cur.execute("""
        SELECT
            array_length(concept_vec::float[], 1) as concept_dim,
            array_length(tmd_dense::float[], 1) as tmd_dim,
            array_length(fused_vec::float[], 1) as fused_dim
        FROM cpe_vectors
        LIMIT 1;
    """)
    concept_dim, tmd_dim, fused_dim = cur.fetchone()

    assert concept_dim == 768, f"Concept vec should be 768D, got {concept_dim}D"
    assert tmd_dim == 16, f"TMD vec should be 16D, got {tmd_dim}D"
    assert fused_dim == 784, f"Fused vec should be 784D, got {fused_dim}D"
    print(f"âœ… Vector dimensions: concept={concept_dim}D, tmd={tmd_dim}D, fused={fused_dim}D")

    # Check 3: Normalization
    cur.execute("SELECT concept_vec FROM cpe_vectors LIMIT 100;")
    vectors = [np.array(row[0]) for row in cur.fetchall()]
    norms = [np.linalg.norm(v) for v in vectors]

    mean_norm = np.mean(norms)
    assert 0.95 < mean_norm < 1.05, f"Vectors not normalized: mean norm = {mean_norm:.4f}"
    print(f"âœ… Vector normalization: mean L2 norm = {mean_norm:.4f}")

    # Check 4: No NaN/Inf
    cur.execute("""
        SELECT COUNT(*) FROM cpe_vectors
        WHERE 'NaN' = ANY(concept_vec::text::float[])
           OR 'Infinity' = ANY(concept_vec::text::float[]);
    """)
    nan_count = cur.fetchone()[0]
    assert nan_count == 0, f"Found {nan_count} vectors with NaN/Inf values"
    print(f"âœ… No NaN/Inf values detected")

    # Check 5: Query performance
    import time
    cur.execute("SELECT concept_vec FROM cpe_vectors ORDER BY RANDOM() LIMIT 1000;")

    start = time.time()
    vectors = cur.fetchall()
    elapsed = time.time() - start

    ms_per_query = (elapsed / 1000) * 1000
    assert ms_per_query < 1.0, f"Slow vector access: {ms_per_query:.2f}ms per query"
    print(f"âœ… Vector access performance: {ms_per_query:.3f}ms per query")

    print("\nðŸŽ‰ All vectors validated and ready for training!")
    return True


if __name__ == "__main__":
    validate_all_vectors()
```

**Deliverables:**
- `tests/validate_vectors_ready.py` validation script
- Validation report: Pass/fail for all checks
- Performance benchmarks: Vector access latency
- Sprint 2 completion checklist

**Exit Criteria:**
- âœ… All 9,477 vectors accessible in database
- âœ… All vectors have correct dimensions and normalization
- âœ… No NaN/Inf values detected
- âœ… Vector access latency <1ms per query
- âœ… Ready for Sprint 3 training pipeline