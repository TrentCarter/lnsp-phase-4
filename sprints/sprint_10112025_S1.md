# Sprint S1: Sequential Training Data Pipeline

**Date**: October 11, 2025  
**Goal**: Replace ontological training data with sequential document corpus

---

## Problem Identified

Current LVM training uses **ontological hierarchies** (WordNet/SWO/GO), which are **taxonomic classifications**, not **sequential narratives**.

**Wrong data example** (taxonomy):
```
"dog" ‚Üí "canine" ‚Üí "mammal" ‚Üí "animal"
```

**Correct data example** (sequence):
```
"Photosynthesis begins..." ‚Üí "Light is absorbed..." ‚Üí "Energy is converted..."
```

---

## Sprint Tasks

### Phase 1: Validation & Schema ‚úÖ COMPLETE (Oct 11)
- [x] Created `tools/test_sequential_coherence.py` (CLI tool, not FastAPI)
- [x] Updated CLAUDE.md with ontology warnings
- [x] Archived ontological data to `artifacts/archive/ontological_DEPRECATED_20251011/`
- [x] Created PRD: `docs/PRDs/PRD_Sequential_Training_Data.md`
- [x] Created migration: `migrations/003_add_sequence_metadata.sql`

### Phase 1b: Schema Migration (Oct 11) üîÑ IN PROGRESS
- [ ] Apply migration script to database
- [ ] Verify document_id + sequence_index populated
- [ ] Verify indexes created
- [ ] Fix validation script to use document_id + sequence_index (not `id`)
- [ ] Test on watercycle-mini (establish coherence baseline)

### Phase 2: Pilot Ingestion (Oct 12)
- [ ] Ingest 10 Wikipedia articles (using Option A: Simple episode ‚Üí Semantic chunking)
- [ ] Validate document_id assignment
- [ ] Validate sequence_index ordering
- [ ] Test coherence (target >80%)
- [ ] Fix any issues before full ingestion

### Phase 3: Full Ingestion (Oct 13-15)
- [ ] Ingest 3000 Wikipedia articles (increased from 1000)
- [ ] Validate coherence >80% across all documents
- [ ] Monitor ingestion performance (target: 3 articles/sec = ~17 min total)
- [ ] Fix any issues

### Phase 4: Training Data Generation (Oct 16-17)
- [ ] Create `tools/export_training_sequences.py`
- [ ] Generate 100K+ training sequences
- [ ] Export to NPZ format

### Phase 5: LVM Prep (Oct 18)
- [ ] Update `src/lvm/train_mamba.py`
- [ ] Create training config
- [ ] Pre-training validation

---

## Success Criteria

| Metric | Target | Validation |
|--------|--------|------------|
| Sequential coherence | >80% | Coherence test |
| Mean cosine similarity | >0.6 | Per-sequence |
| Total chunks | 100K+ | PostgreSQL count |
| Data sources | 1000+ articles | Wikipedia |

---

## Technical Architecture

### Option A: Simplified Pipeline (APPROVED)
```
Wikipedia Article (50KB)
    ‚Üì
[Episode Chunker] (Simple mode, 1-10 paragraphs) ‚Üí episode_id assigned
    ‚Üì
Episodes (5x ~10KB spans)
    ‚Üì
[Semantic Chunker :8001] (Breakpoint=75, per episode) ‚Üí document_id + sequence_index assigned
    ‚Üì
Fine Chunks (50x ~1KB concepts)
    ‚Üì
[Ingest :8004] (parent/child populated within episodes)
    ‚Üì
PostgreSQL (cpe_entry + cpe_vectors) + FAISS
```

### Schema (APPROVED)
```sql
ALTER TABLE cpe_entry
ADD COLUMN document_id TEXT NOT NULL,      -- Groups chunks from same source
ADD COLUMN sequence_index INTEGER NOT NULL, -- Order within document (0, 1, 2...)
ADD COLUMN episode_id TEXT,                -- Episode identifier (optional)
ADD COLUMN parent_cpe_id UUID,             -- Previous chunk (low overhead)
ADD COLUMN child_cpe_id UUID,              -- Next chunk (low overhead)
ADD COLUMN last_accessed TIMESTAMP;

-- Critical performance index:
CREATE INDEX idx_document_sequence ON cpe_entry(document_id, sequence_index);
```

### Training Data Extraction (Fast Query)
```sql
-- O(log N) performance with index:
SELECT cv.concept_vec, ce.sequence_index
FROM cpe_entry ce
JOIN cpe_vectors cv ON ce.cpe_id = cv.cpe_id
WHERE ce.document_id = 'wikipedia_12345'
  AND ce.dataset_source LIKE 'wikipedia-%'
ORDER BY ce.sequence_index;
```

---

## Deliverables

**Phase 1 (Complete)** ‚úÖ:
- `tools/test_sequential_coherence.py` (CLI tool)
- Updated CLAUDE.md (ontology warnings)
- Archived ontological data (artifacts/archive/ontological_DEPRECATED_20251011/)
- `docs/PRDs/PRD_Sequential_Training_Data.md` (requirements doc)
- `migrations/003_add_sequence_metadata.sql` (schema migration)

**Phase 1b-5 (Pending)**:
- Fixed validation script (use document_id + sequence_index)
- Migrated database schema
- Pilot test results (10 articles)
- `artifacts/wikipedia_corpus/` (3000 articles)
- `tools/export_training_sequences.py` (training data exporter)
- `artifacts/lvm/wikipedia_training_sequences.npz` (100K+ sequences)

---

## Open Questions (Require Decisions)

### Q1: `document_id` Format ‚ö†Ô∏è
**Options**:
- A) `"wikipedia_12345"` (source + article ID) ‚Üê **RECOMMENDED**
- B) `"https://en.wikipedia.org/wiki/Article"` (full URL)
- C) `"wiki-en-Article_Name"` (source-lang-title)

**Decision**: Pending (recommend Option A)

---

### Q2: `sequence_index` Reset Strategy ‚ö†Ô∏è
**Options**:
- A) Reset to 0 per episode ‚Üê **RECOMMENDED** (cleaner separation)
- B) Continuous across episodes (global sequence)

**Impact**: Training data extraction - do we skip episode boundaries?

**Decision**: Pending (recommend Option A)

---

### Q3: Parent/Child Population Logic ‚ö†Ô∏è
**Options**:
- A) Always populate (within same document)
- B) Only within same episode ‚Üê **RECOMMENDED** (respects coherence)
- C) Manual population later

**Implementation**:
```python
if chunk.sequence_index > 0 and chunk.episode_id == prev.episode_id:
    chunk.parent_cpe_id = prev.cpe_id
    prev.child_cpe_id = chunk.cpe_id  # Update database
```

**Decision**: Pending (recommend Option B)

---

### Q4: Coherence Threshold ‚ö†Ô∏è
**Current**: 0.6 (tentative)

**Approach**:
1. Test on watercycle-mini
2. If mean=0.7 ‚Üí use 0.6 (gives buffer)
3. If mean=0.4 ‚Üí lower to 0.3 or fix chunking

**Decision**: Wait for test results

---

## Quick Commands

```bash
# 1. Apply schema migration
psql lnsp < migrations/003_add_sequence_metadata.sql

# 2. Verify migration
psql lnsp -c "SELECT dataset_source, document_id, sequence_index, episode_id FROM cpe_entry LIMIT 10;"

# 3. Validate coherence (after fixing script)
LNSP_DB_USER=lnsp ./tools/test_sequential_coherence.py \
  --dataset "watercycle-mini|semantic-75" \
  --test-count 50 \
  --walk-count 5

# 4. Ingest Wikipedia (pilot: 10 articles)
./tools/fetch_serialize_episodes.py \
  --config data/wikipedia_pilot.yaml \
  --outdir artifacts/episodes/ \
  --tau-local 0.6 \
  --target-len 512

# 5. Ingest Wikipedia (full: 3000 articles)
./tools/fetch_serialize_episodes.py \
  --config data/wikipedia_full.yaml \
  --outdir artifacts/episodes/ \
  --tau-local 0.6

# 6. Validate ingestion
LNSP_DB_USER=lnsp ./tools/test_sequential_coherence.py \
  --dataset "wikipedia-%" \
  --test-count 100 \
  --order random

# 7. Export training sequences
./tools/export_training_sequences.py \
  --dataset "wikipedia-%" \
  --output artifacts/lvm/wikipedia_training_sequences.npz \
  --min-chain-length 3
```

---

**Sprint Owner**: Trent Carter  
**Review Date**: October 18, 2025
