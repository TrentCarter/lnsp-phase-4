# Sprint LVM Training Preparation - October 7, 2025

## Sprint Context

**Date**: October 7, 2025
**Previous Sprints**:
- Oct 1 S1: Graph + vecRAG Infrastructure (6deg shortcuts, FAISS index)
- Oct 1 S2: OCP Training Data Preparation (sequence extraction design)
- Oct 1 S6: Relation-Conditioned LVM Training (future enhancement)
**Current Focus**: Prepare for **Tokenless Mamba LVM** training after successful 10K ontology ingestion
**Duration**: 1 day sprint
**Status**: Planning ‚Üí Execution

**Key References**:
- `docs/PRDs/PRD_P15_Latent_LVM_Implementation_Plan.md` - Full LVM architecture
- `sprints/sprint_10012025_S2.md` - OCP sequence extraction algorithms
- `sprints/sprint_10012025_S6.md` - Relation-conditioned training (future)

---

## üéØ Sprint Goal

**PRIMARY OBJECTIVE**: Train first Tokenless Mamba LVM using 8K WordNet vectors already ingested in ontological order.

**SUCCESS CRITERIA**:
- ‚úÖ Verify data readiness (NPZ has concept_texts, cpe_ids, vectors)
- ‚úÖ Prepare training sequences from ordered chains (predict vector[i+1] from vector[0:i])
- ‚úÖ Implement Mamba-2 LVM architecture (tokenless, 768D vector-native)
- ‚úÖ Train first model on WordNet data (10 epochs)
- ‚úÖ Evaluate on held-out test set

---

## üìä Current State (as of 2025-10-07)

### Available Data

```
‚úÖ 8,000 concepts @ 784D (768D semantic + 16D TMD)
‚úÖ 99.3% WordNet (pure ontological lexical data)
‚úÖ 2,000 ordered chains (4-12 concepts each)

NPZ File: artifacts/fw10k_vectors.npz
- concept_texts: Array[8000] (concept strings)
- cpe_ids: Array[8000] (UUIDs for correlation)
- vectors: Array[8000, 784] (768D + 16D TMD)

Chains: artifacts/ontology_chains/wordnet_chains.jsonl
- 2,000 chains with hierarchical relationships
- Example: ormer ‚Üí abalone ‚Üí gastropod ‚Üí mollusk ‚Üí invertebrate
```

### Why WordNet is Perfect for LVM Training

1. **Hierarchical structure**: Concepts organized by semantic relationships
2. **Short, clear concepts**: Single words/phrases (no noise)
3. **Ordered chains**: Natural progression (specific ‚Üí general or vice versa)
4. **Pure ontological**: No FactoidWiki noise, all semantic structure

### Critical Understanding

**Training uses JSONL chains directly, NOT Neo4j graph!**
- Chains are already ordered from ontology ingestion
- Vectors are already computed (NPZ file)
- Training = predict vector[i+1] from vector[0:i]
- Neo4j graph is for INFERENCE (GraphRAG), not training data extraction

---

## üìã Sprint Tasks

### Phase 1: Data Verification (Morning)

#### Task 1.1: Verify NPZ Data Structure
**Owner**: Data Validation
**Priority**: CRITICAL ‚ö°
**Estimated Time**: 5 minutes

```python
# Verify NPZ contains all required arrays
import numpy as np

npz = np.load("artifacts/fw10k_vectors.npz")
print("Arrays in NPZ:", list(npz.keys()))

# Check shapes
print(f"concept_texts shape: {npz['concept_texts'].shape}")
print(f"cpe_ids shape: {npz['cpe_ids'].shape}")
print(f"vectors shape: {npz['vectors'].shape}")

# Verify alignment
assert len(npz['concept_texts']) == len(npz['cpe_ids']) == len(npz['vectors'])
print("‚úÖ All arrays aligned!")

# Sample concepts
print("\nSample concepts:")
for i in range(5):
    print(f"  {i}: {npz['concept_texts'][i]} (ID: {npz['cpe_ids'][i]})")
```

**Acceptance Criteria**:
- [ ] NPZ has `concept_texts`, `cpe_ids`, `vectors` arrays
- [ ] All arrays have same length (8,000)
- [ ] Vectors are 784D (768D semantic + 16D TMD)

---

#### Task 1.2: Verify Chain Structure
**Owner**: Data Validation
**Priority**: CRITICAL ‚ö°
**Estimated Time**: 5 minutes

```bash
# Count chains
wc -l artifacts/ontology_chains/wordnet_chains.jsonl

# Sample first 3 chains
head -3 artifacts/ontology_chains/wordnet_chains.jsonl | jq .
```

**Acceptance Criteria**:
- [ ] File exists with 2,000 chains
- [ ] Each chain has `concepts` array (4-12 items)
- [ ] Concepts match entries in NPZ `concept_texts`

---

### Phase 2: Training Data Preparation (Afternoon)

#### Task 2.1: Create Training Data Preparation Script
**Owner**: Training Pipeline
**Priority**: CRITICAL ‚ö°
**Estimated Time**: 1 hour

**File**: `src/lvm/prepare_training_data.py`

**Reference**: `docs/LVM_TRAINING_CRITICAL_FACTS.md` (read this first!)

```python
"""
Prepare training data for LVM from ordered ontology chains.

This script loads:
1. Ordered chains from JSONL (already ingested)
2. Vectors from NPZ (already computed)

Creates training sequences: predict vector[i+1] from vector[0:i]

NO NEO4J! NO GRAPH WALKS! Just use the data we already have!
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Tuple

def load_chains_and_vectors(
    chains_path: str = "artifacts/ontology_chains/wordnet_chains.jsonl",
    npz_path: str = "artifacts/fw10k_vectors.npz"
) -> Tuple[List[dict], np.ndarray, np.ndarray]:
    """
    Load chains and their vectors.

    Returns:
        chains: List of chain dicts
        concept_texts: Array of concept strings (for matching)
        vectors: Array of vectors (768D or 784D)
    """
    # Load chains
    chains = []
    with open(chains_path) as f:
        for line in f:
            chain = json.loads(line)
            chains.append(chain)

    print(f"‚úÖ Loaded {len(chains)} chains from {chains_path}")

    # Load vectors
    npz = np.load(npz_path)
    concept_texts = npz['concept_texts']
    vectors = npz['vectors']

    print(f"‚úÖ Loaded {len(vectors)} vectors @ {vectors.shape[1]}D from {npz_path}")

    return chains, concept_texts, vectors

def create_training_sequences(
    chains: List[dict],
    concept_texts: np.ndarray,
    vectors: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Create training sequences from chains.

    For each chain: [c0, c1, c2, c3]
    Create sequences:
      - context: [c0], target: c1
      - context: [c0, c1], target: c2
      - context: [c0, c1, c2], target: c3

    Returns:
        context_sequences: List of variable-length contexts
        target_vectors: Array of target vectors
    """
    # Build concept ‚Üí index lookup
    concept_to_idx = {text: i for i, text in enumerate(concept_texts)}

    all_contexts = []
    all_targets = []

    for chain in chains:
        concepts = chain['concepts']

        # Match concepts to vector indices
        indices = []
        for concept in concepts:
            if concept in concept_to_idx:
                indices.append(concept_to_idx[concept])
            else:
                print(f"‚ö†Ô∏è  Concept not found in NPZ: {concept}")

        if len(indices) < 2:
            continue  # Need at least 2 concepts for training

        # Create training pairs
        chain_vecs = vectors[indices]  # Shape: (chain_len, 768/784)

        for i in range(1, len(chain_vecs)):
            context = chain_vecs[:i]  # All vectors up to i
            target = chain_vecs[i]    # Next vector

            all_contexts.append(context)
            all_targets.append(target)

    print(f"‚úÖ Created {len(all_contexts)} training sequences")

    return all_contexts, np.array(all_targets)

def pad_and_save(
    contexts: List[np.ndarray],
    targets: np.ndarray,
    output_path: str,
    train_split: float = 0.7,
    val_split: float = 0.15
):
    """
    Pad sequences and split into train/val/test.
    """
    # Pad contexts to max length
    max_len = max(len(ctx) for ctx in contexts)
    print(f"Max context length: {max_len}")

    padded_contexts = []
    masks = []

    for ctx in contexts:
        # Pad
        pad_len = max_len - len(ctx)
        if pad_len > 0:
            padding = np.zeros((pad_len, ctx.shape[1]))
            padded = np.vstack([ctx, padding])
        else:
            padded = ctx

        # Mask (1 = valid, 0 = padding)
        mask = np.array([1] * len(ctx) + [0] * pad_len)

        padded_contexts.append(padded)
        masks.append(mask)

    contexts = np.array(padded_contexts)  # Shape: (N, max_len, 768/784)
    masks = np.array(masks)               # Shape: (N, max_len)

    # Split
    N = len(contexts)
    train_end = int(N * train_split)
    val_end = int(N * (train_split + val_split))

    train_ctx = contexts[:train_end]
    train_tgt = targets[:train_end]
    train_mask = masks[:train_end]

    val_ctx = contexts[train_end:val_end]
    val_tgt = targets[train_end:val_end]
    val_mask = masks[train_end:val_end]

    test_ctx = contexts[val_end:]
    test_tgt = targets[val_end:]
    test_mask = masks[val_end:]

    # Save
    np.savez_compressed(
        output_path,
        train_contexts=train_ctx,
        train_targets=train_tgt,
        train_masks=train_mask,
        val_contexts=val_ctx,
        val_targets=val_tgt,
        val_masks=val_mask,
        test_contexts=test_ctx,
        test_targets=test_tgt,
        test_masks=test_mask
    )

    print(f"‚úÖ Saved training data to {output_path}")
    print(f"   Train: {len(train_ctx)} sequences")
    print(f"   Val: {len(val_ctx)} sequences")
    print(f"   Test: {len(test_ctx)} sequences")
    print(f"   Context shape: {train_ctx.shape}")
    print(f"   Target shape: {train_tgt.shape}")

if __name__ == "__main__":
    # Load data
    chains, concept_texts, vectors = load_chains_and_vectors()

    # Create sequences
    contexts, targets = create_training_sequences(chains, concept_texts, vectors)

    # Pad and split
    pad_and_save(
        contexts,
        targets,
        "artifacts/lvm/wordnet_training_sequences.npz",
        train_split=0.7,
        val_split=0.15
    )

    print("‚úÖ Training data preparation complete!")
```

**Acceptance Criteria**:
- [ ] Loads chains from JSONL (2,000 chains)
- [ ] Loads vectors from NPZ (8,000 vectors)
- [ ] Matches chain concepts to vector indices
- [ ] Creates ~20-30K training sequences
- [ ] Splits into 70/15/15 train/val/test
- [ ] Saves to `artifacts/lvm/wordnet_training_sequences.npz`

---

#### Task 2.2: Implement Mamba-2 LVM Architecture
**Owner**: Model Development
**Priority**: HIGH
**Estimated Time**: 1 hour

**File**: `src/lvm/models.py`

```python
"""
Latent Vector Model (LVM) - Tokenless Mamba-2 Architecture

Processes 768D vectors directly (no text tokens!)
"""

import torch
import torch.nn as nn

class LatentMamba(nn.Module):
    """
    Simplified Mamba model for vector sequence processing.

    Uses standard LSTM for initial version (Mamba-SSM can be added later).
    Focus: Get training working first, then optimize architecture.
    """

    def __init__(
        self,
        d_input: int = 784,      # Input dim (768 semantic + 16 TMD)
        d_hidden: int = 512,     # Hidden dimension
        n_layers: int = 2,       # Number of layers
        dropout: float = 0.1
    ):
        super().__init__()
        self.d_input = d_input
        self.d_hidden = d_hidden

        # Input projection
        self.input_proj = nn.Linear(d_input, d_hidden)

        # LSTM layers (will replace with Mamba later)
        self.lstm = nn.LSTM(
            d_hidden,
            d_hidden,
            n_layers,
            dropout=dropout if n_layers > 1 else 0,
            batch_first=True
        )

        # Output head (project to 784D)
        self.output_head = nn.Linear(d_hidden, d_input)

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):
        """
        Forward pass.

        Args:
            x: Input vectors [batch, seq_len, 784]
            mask: Attention mask [batch, seq_len] (optional)

        Returns:
            output: Predicted next vector [batch, 784]
        """
        # Input projection
        h = self.input_proj(x)  # [batch, seq_len, d_hidden]
        h = self.dropout(h)

        # LSTM forward
        lstm_out, _ = self.lstm(h)  # [batch, seq_len, d_hidden]

        # Get last valid position
        if mask is not None:
            # Get last non-masked position for each sequence
            lengths = mask.sum(dim=1).long() - 1  # [batch]
            last_h = lstm_out[torch.arange(lstm_out.size(0)), lengths]
        else:
            last_h = lstm_out[:, -1, :]  # [batch, d_hidden]

        # Project to output
        output = self.output_head(last_h)  # [batch, 784]

        return output

    def get_num_params(self):
        """Count trainable parameters."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


if __name__ == "__main__":
    # Quick test
    model = LatentMamba(d_input=784, d_hidden=512, n_layers=2)
    x = torch.randn(4, 10, 784)
    mask = torch.ones(4, 10)
    mask[:, 7:] = 0

    output = model(x, mask=mask)
    print(f"‚úÖ Model has {model.get_num_params():,} parameters")
    print(f"‚úÖ Output shape: {output.shape}")
    assert output.shape == (4, 784)
```

**Acceptance Criteria**:
- [ ] Model instantiates without errors
- [ ] Forward pass works (test with dummy data)
- [ ] Attention masking works correctly
- [ ] Parameter count ~2-5M (lightweight LSTM)

**Why LSTM first?**:
- Get training working immediately (don't wait for Mamba-SSM install)
- Validate full pipeline end-to-end
- Can swap to Mamba later (same interface)

---

### Phase 3: Training & Evaluation (Evening)

#### Task 3.1: Implement Training Loop
**Owner**: Training Pipeline
**Priority**: HIGH
**Estimated Time**: 2 hours

**File**: `src/lvm/train_mamba.py`

```python
"""
Train LVM on ordered ontology sequences.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
from pathlib import Path
import time

from src.lvm.models import LatentMamba

def load_training_data(data_path: str):
    """Load prepared training data."""
    data = np.load(data_path)

    return {
        'train': (data['train_contexts'], data['train_targets'], data['train_masks']),
        'val': (data['val_contexts'], data['val_targets'], data['val_masks']),
        'test': (data['test_contexts'], data['test_targets'], data['test_masks'])
    }

def train_epoch(model, dataloader, optimizer, device):
    """Train for one epoch."""
    model.train()
    total_loss = 0

    for batch_ctx, batch_tgt, batch_mask in dataloader:
        batch_ctx = batch_ctx.to(device)
        batch_tgt = batch_tgt.to(device)
        batch_mask = batch_mask.to(device)

        # Forward pass
        pred = model(batch_ctx, mask=batch_mask)

        # MSE loss (predict next vector)
        loss = F.mse_loss(pred, batch_tgt)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)

def evaluate(model, dataloader, device):
    """Evaluate on validation/test set."""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch_ctx, batch_tgt, batch_mask in dataloader:
            batch_ctx = batch_ctx.to(device)
            batch_tgt = batch_tgt.to(device)
            batch_mask = batch_mask.to(device)

            pred = model(batch_ctx, mask=batch_mask)
            loss = F.mse_loss(pred, batch_tgt)
            total_loss += loss.item()

    return total_loss / len(dataloader)

def train_lvm(
    data_path: str = "artifacts/lvm/wordnet_training_sequences.npz",
    output_path: str = "models/lvm_wordnet.pt",
    epochs: int = 10,
    batch_size: int = 64,
    lr: float = 1e-3,
    device: str = "mps"
):
    """Train LVM end-to-end."""
    print("=== Training Tokenless Mamba LVM ===")

    # Load data
    print(f"\n1. Loading data from {data_path}...")
    data = load_training_data(data_path)
    train_ctx, train_tgt, train_mask = data['train']
    val_ctx, val_tgt, val_mask = data['val']

    print(f"   Train: {len(train_ctx)} sequences")
    print(f"   Val: {len(val_ctx)} sequences")
    print(f"   Context shape: {train_ctx.shape}")

    # Create dataloaders
    train_dataset = TensorDataset(
        torch.from_numpy(train_ctx).float(),
        torch.from_numpy(train_tgt).float(),
        torch.from_numpy(train_mask).float()
    )
    val_dataset = TensorDataset(
        torch.from_numpy(val_ctx).float(),
        torch.from_numpy(val_tgt).float(),
        torch.from_numpy(val_mask).float()
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)

    # Create model
    print(f"\n2. Creating model...")
    d_input = train_ctx.shape[2]  # 784D
    model = LatentMamba(d_input=d_input, d_hidden=512, n_layers=2)
    model = model.to(device)
    print(f"   Parameters: {model.get_num_params():,}")

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # Training loop
    print(f"\n3. Training for {epochs} epochs...")
    best_val_loss = float('inf')

    for epoch in range(1, epochs + 1):
        start = time.time()

        train_loss = train_epoch(model, train_loader, optimizer, device)
        val_loss = evaluate(model, val_loader, device)

        elapsed = time.time() - start

        print(f"Epoch {epoch}/{epochs} ({elapsed:.1f}s)")
        print(f"  Train loss: {train_loss:.6f}")
        print(f"  Val loss: {val_loss:.6f}")

        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), output_path)
            print(f"  ‚úÖ Saved best model (val_loss={val_loss:.6f})")

    print(f"\n‚úÖ Training complete! Best val loss: {best_val_loss:.6f}")
    print(f"Model saved to: {output_path}")

if __name__ == "__main__":
    train_lvm(
        data_path="artifacts/lvm/wordnet_training_sequences.npz",
        output_path="models/lvm_wordnet.pt",
        epochs=10,
        batch_size=64,
        device="mps"  # Use "cuda" if available, "cpu" otherwise
    )
```

**Acceptance Criteria**:
- [ ] Loads training data successfully
- [ ] Trains for 10 epochs without errors
- [ ] Val loss decreases over epochs
- [ ] Saves best model checkpoint
- [ ] Training time ~10-20 min (8K concepts, 20-30K sequences)

---

#### Task 3.2: Implement Evaluation Script
**Owner**: Training Pipeline
**Priority**: MEDIUM
**Estimated Time**: 30 minutes

**File**: `src/lvm/eval_mamba.py`

```python
"""
Evaluate trained LVM on test set.
"""

import torch
import numpy as np
from torch.utils.data import TensorDataset, DataLoader

from src.lvm.models import LatentMamba
from src.lvm.train_mamba import load_training_data, evaluate

def test_lvm(
    model_path: str = "models/lvm_wordnet.pt",
    data_path: str = "artifacts/lvm/wordnet_training_sequences.npz",
    device: str = "mps"
):
    """Evaluate LVM on test set."""
    print("=== Evaluating LVM ===")

    # Load data
    data = load_training_data(data_path)
    test_ctx, test_tgt, test_mask = data['test']

    print(f"Test: {len(test_ctx)} sequences")

    # Create dataloader
    test_dataset = TensorDataset(
        torch.from_numpy(test_ctx).float(),
        torch.from_numpy(test_tgt).float(),
        torch.from_numpy(test_mask).float()
    )
    test_loader = DataLoader(test_dataset, batch_size=64)

    # Load model
    d_input = test_ctx.shape[2]
    model = LatentMamba(d_input=d_input, d_hidden=512, n_layers=2)
    model.load_state_dict(torch.load(model_path))
    model = model.to(device)
    model.eval()

    # Evaluate
    test_loss = evaluate(model, test_loader, device)

    print(f"Test MSE loss: {test_loss:.6f}")
    print("‚úÖ Evaluation complete!")

if __name__ == "__main__":
    test_lvm()
```

**Acceptance Criteria**:
- [ ] Loads trained model
- [ ] Evaluates on test set
- [ ] Reports MSE loss
- [ ] Test loss < 0.1 (reasonable prediction accuracy)

---

### Phase 4: Quick Start Script (Optional)

#### Task 4.1: Create End-to-End Training Script
**Owner**: Automation
**Priority**: LOW
**Estimated Time**: 30 minutes

**File**: `scripts/train_lvm_simple.sh`

```bash
#!/bin/bash
# End-to-end LVM training pipeline

set -e

echo "=== LVM Training Pipeline ==="
echo ""

# 1. Verify data
echo "1. Verifying NPZ data..."
./.venv/bin/python -c "
import numpy as np
npz = np.load('artifacts/fw10k_vectors.npz')
assert 'concept_texts' in npz
assert 'cpe_ids' in npz
assert 'vectors' in npz
print('‚úÖ NPZ verified')
"

# 2. Prepare training data
echo ""
echo "2. Preparing training sequences..."
mkdir -p artifacts/lvm
./.venv/bin/python src/lvm/prepare_training_data.py

# 3. Train model
echo ""
echo "3. Training LVM (10 epochs)..."
mkdir -p models
./.venv/bin/python src/lvm/train_mamba.py

# 4. Evaluate
echo ""
echo "4. Evaluating on test set..."
./.venv/bin/python src/lvm/eval_mamba.py

echo ""
echo "‚úÖ LVM training pipeline complete!"
echo "Model saved to: models/lvm_wordnet.pt"
```

**Acceptance Criteria**:
- [ ] Runs all 4 steps automatically
- [ ] Stops on first error
- [ ] Takes ~1-2 hours end-to-end

---

## üìä Success Metrics

### Data Verification
- [ ] NPZ verified: concept_texts, cpe_ids, vectors all present
- [ ] Chain count: 2,000 WordNet chains
- [ ] Concept match: All chain concepts found in NPZ

### Training Pipeline
- [ ] Training sequences: ~20-30K generated
- [ ] Train/val/test split: 70/15/15
- [ ] Model parameters: 2-5M (lightweight LSTM)

### Training Results
- [ ] Training completes: 10 epochs without errors
- [ ] Val loss decreases: Shows learning progress
- [ ] Test loss: < 0.1 (reasonable vector prediction)

### Time Budget
- [ ] Data prep: < 5 minutes
- [ ] Training: < 20 minutes
- [ ] Total: < 2 hours end-to-end

---

## üö´ Out of Scope for This Sprint

- ‚ùå Mamba-SSM architecture (use LSTM first, swap later)
- ‚ùå Neo4j graph usage (inference only, not training!)
- ‚ùå Vec2Text decoder integration ‚Üí Next sprint
- ‚ùå Inference API deployment ‚Üí Next sprint
- ‚ùå Scaling to 50K+ training sequences ‚Üí Next sprint

---

## üìö Key References

1. **docs/LVM_TRAINING_CRITICAL_FACTS.md** - READ THIS FIRST!
2. **LNSP_LONG_TERM_MEMORY.md** - Cardinal rules
3. **docs/LNSP_System_Flows.md** - Training vs inference distinction
4. **CLAUDE.md** - Rule #6: Data must have unique IDs for correlation

---

## üîÑ Risk Management

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Chain concepts not in NPZ | Low | High | Check concept_to_idx lookup, report missing |
| Insufficient training data | Low | Medium | WordNet chains should give 20-30K sequences |
| LSTM underfits | Medium | Low | Start simple, add layers if needed |
| Training time too long | Low | Low | 8K concepts should train in 10-20 min |

---

## ‚úÖ Definition of Done

**Sprint Complete When**:
1. All Phase 1 tasks completed (data verification)
2. Training data prepared (sequences NPZ file created)
3. LVM model implemented (LSTM version)
4. Training completes successfully (10 epochs)
5. Test evaluation shows reasonable loss (< 0.1)
6. All acceptance criteria met (checklists checked)

**Bonus (if time allows)**:
7. End-to-end script created (`scripts/train_lvm_simple.sh`)

---

## üìû Stakeholder Communication

**End of Day Summary Template**:
```
Training Status: [Complete / In Progress / Blocked]
- Data prep: [‚úÖ / ‚è≥ / ‚ùå]
- Model impl: [‚úÖ / ‚è≥ / ‚ùå]
- Training run: [‚úÖ / ‚è≥ / ‚ùå]
- Evaluation: [‚úÖ / ‚è≥ / ‚ùå]

Files Created:
- src/lvm/prepare_training_data.py
- src/lvm/models.py
- src/lvm/train_mamba.py
- src/lvm/eval_mamba.py
- artifacts/lvm/wordnet_training_sequences.npz
- models/lvm_wordnet.pt

Metrics:
- Training sequences: [number]
- Model parameters: [number]
- Best val loss: [value]
- Test loss: [value]
- Training time: [minutes]

Blockers: [none / list]
```

---

## üéØ Quick Start (TL;DR)

**To train your first LVM**:

```bash
# 1. Verify data exists
ls artifacts/fw10k_vectors.npz
ls artifacts/ontology_chains/wordnet_chains.jsonl

# 2. Create src/lvm/ directory and implement scripts from tasks above

# 3. Run end-to-end
bash scripts/train_lvm_simple.sh

# 4. Check results
cat models/lvm_wordnet.pt  # Model saved here
```

**Expected output**: First trained Tokenless LVM predicting concept vectors from ordered sequences!

---

**Sprint Owner**: Trent Carter
**Document Status**: Active Sprint
**Created**: 2025-10-07
**Goal**: Train first Tokenless Mamba LVM on 8K WordNet vectors
**Approach**: Simple! Use ordered JSONL chains + NPZ vectors (NO graph walks!)
**Key Reference**: `docs/LVM_TRAINING_CRITICAL_FACTS.md`
