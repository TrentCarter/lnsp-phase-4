# Sprint 6: Relation-Conditioned LVM Training

**Status:** üìã Planned (Blocked by S2-S5 completion)
**Dependencies:** S2 (OCP sequences), S3 (Mamba LVM), S4 (Training pipeline)
**Estimated Duration:** 3-4 days
**Owner:** LNSP Phase 4 Team

---

## Objective

Enhance the LVM with **relation-aware training** by incorporating explicit predicate embeddings between concepts in ontology chains. This enables the model to learn proposition semantics (Subject-Predicate-Object) rather than just concept sequences.

---

## Background

**Current State (S2-S5):**
- LVM learns from concept chains: `[Tiger, Mammal, Animal]`
- Relations are **implicit** (learned from context)
- Training uses: 768D concept vectors + optional 16D TMD conditioning

**Enhancement (S6):**
- LVM learns from **proposition chains**: `Tiger --[IS_A]--> Mammal --[IS_A]--> Animal`
- Relations are **explicit** (embedded and conditioned)
- Training adds: Relation embeddings per transition

**Why This Matters:**
1. **Semantic Precision:** Model learns "Tiger IS_A Mammal" vs "Tiger PART_OF Mammal"
2. **Better Generalization:** Can predict appropriate relation types for new concepts
3. **Proposition Retrieval:** Enables queries like "What does X have as parts?" (PART_OF relations)
4. **RL-Friendly:** Confidence-weighted relation learning for dynamic ontologies

---

## Technical Specification

### Architecture Changes

#### 1. Training Data Format (Extended)

**Current (S2-S5):**
```python
# NPZ structure from S2
{
    "concepts": np.array([batch, seq_len, 768]),   # Concept vectors
    "tmd": np.array([batch, seq_len, 16]),         # TMD conditioning (optional)
    "labels": np.array([batch, seq_len]),          # Next concept prediction
}
```

**Enhanced (S6):**
```python
# Extended NPZ structure
{
    "concepts": np.array([batch, seq_len, 768]),      # Concept vectors (unchanged)
    "tmd": np.array([batch, seq_len, 16]),            # TMD conditioning (unchanged)
    "relations": np.array([batch, seq_len-1], dtype=int),  # Relation IDs between concepts
    "relation_confidence": np.array([batch, seq_len-1]),   # Confidence per relation (0-1)
    "labels": np.array([batch, seq_len]),             # Next concept prediction (unchanged)
}
```

**Example:**
```
Sequence: Tiger ‚Üí Mammal ‚Üí Animal
concepts: [vec(tiger), vec(mammal), vec(animal)]  # 3 √ó 768D
relations: [IS_A, IS_A]                            # 2 relations (seq_len - 1)
relation_confidence: [1.0, 1.0]                    # Both from ontology (high confidence)
```

---

#### 2. Relation Vocabulary

**File:** `artifacts/lvm/relation_vocab.json`

```json
{
  "IS_A": 0,
  "PART_OF": 1,
  "HAS_PROPERTY": 2,
  "USES": 3,
  "LOCATED_IN": 4,
  "RELATED_TO": 5,
  "<UNK>": 6,
  "<PAD>": 7
}
```

**Extraction Script:**
```python
# src/lvm/build_relation_vocab.py

def build_relation_vocab():
    """Extract unique relation types from ontology_edge table."""
    conn = psycopg2.connect("dbname=lnsp")
    cur = conn.cursor()

    cur.execute("""
        SELECT DISTINCT relation_type, COUNT(*) as freq
        FROM ontology_edge
        GROUP BY relation_type
        ORDER BY freq DESC;
    """)

    vocab = {"<PAD>": 0, "<UNK>": 1}
    for i, (rel_type, _) in enumerate(cur.fetchall(), start=2):
        vocab[rel_type] = i

    with open("artifacts/lvm/relation_vocab.json", "w") as f:
        json.dump(vocab, f, indent=2)

    return vocab
```

---

#### 3. Sequence Extraction (Enhanced)

**File:** `src/lvm/extract_ocp_sequences.py` (modified from S2)

**New Function:**
```python
def extract_sequences_with_relations():
    """
    Extract training sequences WITH relation labels.

    Query:
    - Get concept chains from ontology_closure
    - For each edge in chain, get relation_type from ontology_edge
    - Include confidence scores from ontology_edge
    """

    conn = psycopg2.connect("dbname=lnsp")
    cur = conn.cursor()

    # Get all chains (length 3-15) with relation metadata
    cur.execute("""
        WITH RECURSIVE chain AS (
            -- Base: Start nodes
            SELECT
                e.parent_id as concepts,
                e.child_id as next_concept,
                ARRAY[e.relation_type] as relations,
                ARRAY[e.confidence] as confidences,
                1 as depth
            FROM ontology_edge e

            UNION ALL

            -- Recursive: Extend chain
            SELECT
                c.concepts || e.child_id,
                e.child_id,
                c.relations || e.relation_type,
                c.confidences || e.confidence,
                c.depth + 1
            FROM chain c
            JOIN ontology_edge e ON c.next_concept = e.parent_id
            WHERE c.depth < 15  -- Max chain length
        )
        SELECT concepts, relations, confidences
        FROM chain
        WHERE depth >= 3  -- Min chain length
        ORDER BY depth DESC;
    """)

    sequences = []
    for concepts, relations, confidences in cur.fetchall():
        # Load concept vectors from cpe_vectors
        concept_vecs = load_vectors(concepts)  # [seq_len, 768]
        tmd_vecs = load_tmd(concepts)          # [seq_len, 16]

        # Map relation types to IDs
        relation_ids = [relation_vocab.get(r, relation_vocab["<UNK>"]) for r in relations]

        sequences.append({
            "concepts": concept_vecs,
            "tmd": tmd_vecs,
            "relations": np.array(relation_ids),
            "relation_confidence": np.array(confidences),
        })

    return sequences
```

**Command:**
```bash
# Re-extract training data with relation labels
./.venv/bin/python -m src.lvm.extract_ocp_sequences \
    --output artifacts/lvm/ocp_training_9k_relations.npz \
    --include-relations \
    --min-chain-length 3 \
    --max-chain-length 15
```

---

#### 4. Model Architecture (Enhanced Mamba)

**File:** `src/lvm/mamba_lvm.py` (modified from S3)

**New Components:**
```python
class RelationConditionedMambaLVM(nn.Module):
    def __init__(
        self,
        d_model=768,           # Concept vector dim
        tmd_dim=16,            # TMD conditioning dim (optional)
        relation_vocab_size=8, # Number of relation types
        relation_emb_dim=32,   # Relation embedding size
        n_layers=4,
        **mamba_kwargs
    ):
        super().__init__()

        # Concept projection (unchanged)
        self.concept_proj = nn.Linear(d_model, d_model)

        # TMD conditioning (unchanged)
        if tmd_dim > 0:
            self.tmd_proj = nn.Linear(tmd_dim, d_model)

        # === NEW: Relation embedding layer ===
        self.relation_emb = nn.Embedding(
            num_embeddings=relation_vocab_size,
            embedding_dim=relation_emb_dim,
            padding_idx=relation_vocab["<PAD>"]
        )

        # === NEW: Relation fusion layer ===
        # Combines concept[i] with relation[i-1‚Üíi] for conditioning
        self.relation_fusion = nn.Linear(relation_emb_dim, d_model)

        # Mamba layers (unchanged)
        self.mamba_layers = nn.ModuleList([
            MambaBlock(d_model=d_model, **mamba_kwargs)
            for _ in range(n_layers)
        ])

        # Output projection (unchanged)
        self.output_proj = nn.Linear(d_model, d_model)

    def forward(self, concepts, relations=None, tmd=None, relation_confidence=None):
        """
        Args:
            concepts: [batch, seq_len, 768] ‚Äî Concept vectors
            relations: [batch, seq_len-1] ‚Äî Relation IDs between concepts
            tmd: [batch, seq_len, 16] ‚Äî TMD conditioning (optional)
            relation_confidence: [batch, seq_len-1] ‚Äî Confidence per relation (optional)

        Returns:
            outputs: [batch, seq_len, 768] ‚Äî Predicted next concepts
        """
        batch, seq_len, _ = concepts.shape

        # Project concepts
        x = self.concept_proj(concepts)  # [batch, seq_len, d_model]

        # Add TMD conditioning (if provided)
        if tmd is not None:
            x = x + self.tmd_proj(tmd)

        # === NEW: Add relation conditioning ===
        if relations is not None:
            # Embed relations: [batch, seq_len-1, relation_emb_dim]
            rel_emb = self.relation_emb(relations)

            # Weight by confidence (if provided)
            if relation_confidence is not None:
                rel_emb = rel_emb * relation_confidence.unsqueeze(-1)  # [batch, seq_len-1, emb_dim]

            # Project to d_model: [batch, seq_len-1, d_model]
            rel_fusion = self.relation_fusion(rel_emb)

            # Add relation conditioning to tokens 1...seq_len (skip first token)
            # Token[i] is conditioned by relation[i-1‚Üíi]
            x[:, 1:, :] = x[:, 1:, :] + rel_fusion

        # Mamba processing (unchanged)
        for layer in self.mamba_layers:
            x = layer(x)

        # Output projection
        outputs = self.output_proj(x)

        return outputs
```

**Key Changes:**
1. **Relation embeddings**: Learn 32D vectors for each relation type
2. **Relation fusion**: Add relation conditioning to each token (except first)
3. **Confidence weighting**: Scale relation embeddings by confidence scores
4. **Backward compatible**: If `relations=None`, model behaves like S3 baseline

---

#### 5. Training Loop (Enhanced)

**File:** `src/lvm/train.py` (modified from S4)

**Config Updates:**
```python
# config/lvm_config.yaml

model:
  d_model: 768
  tmd_dim: 16
  n_layers: 4

  # === NEW: Relation-conditioning params ===
  use_relations: true              # Enable relation conditioning
  relation_vocab_size: 8           # From relation_vocab.json
  relation_emb_dim: 32             # Relation embedding size
  relation_confidence_weight: true # Use confidence scores

training:
  # ... (unchanged from S4)

  # === NEW: Relation-specific losses ===
  relation_prediction_loss: false  # (Optional) Predict next relation type
```

**Training Code:**
```python
def train_step(batch, model, optimizer):
    """Enhanced training step with relation conditioning."""

    concepts = batch["concepts"]        # [batch, seq_len, 768]
    tmd = batch.get("tmd")             # [batch, seq_len, 16] (optional)
    relations = batch.get("relations") # [batch, seq_len-1] (NEW)
    rel_conf = batch.get("relation_confidence")  # [batch, seq_len-1] (NEW)

    # Forward pass
    outputs = model(
        concepts=concepts,
        relations=relations,
        tmd=tmd,
        relation_confidence=rel_conf
    )

    # Compute loss (next concept prediction)
    # Shift targets: predict concepts[i+1] from concepts[0...i]
    targets = concepts[:, 1:, :]  # [batch, seq_len-1, 768]
    preds = outputs[:, :-1, :]    # [batch, seq_len-1, 768]

    loss = F.mse_loss(preds, targets)

    # Backprop
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()
```

**Command:**
```bash
# Train with relation conditioning
./.venv/bin/python -m src.lvm.train \
    --config config/lvm_relation_config.yaml \
    --data artifacts/lvm/ocp_training_9k_relations.npz \
    --output models/mamba_lvm_relations.pt \
    --use-relations
```

---

#### 6. Evaluation Metrics (Enhanced)

**File:** `src/lvm/eval.py` (modified from S5)

**New Metrics:**
```python
def evaluate_by_relation_type(model, test_data):
    """
    Evaluate prediction accuracy per relation type.

    Answers: Does the model learn different prediction patterns
    for IS_A vs PART_OF relations?
    """

    results_by_relation = defaultdict(list)

    for batch in test_data:
        outputs = model(batch["concepts"], relations=batch["relations"])
        targets = batch["concepts"][:, 1:, :]

        # Compute MSE per sample
        mse_per_sample = ((outputs[:, :-1, :] - targets) ** 2).mean(dim=-1)  # [batch, seq_len-1]

        # Group by relation type
        for i, relation_id in enumerate(batch["relations"][0]):  # Assume batch_size=1 for simplicity
            relation_name = id_to_relation[relation_id]
            results_by_relation[relation_name].append(mse_per_sample[0, i].item())

    # Report metrics per relation
    for rel_name, losses in results_by_relation.items():
        print(f"{rel_name:15s}: MSE={np.mean(losses):.4f} (n={len(losses)})")
```

**Expected Output:**
```
Relation Type   : MSE (Next Concept Prediction)
IS_A            : MSE=0.0234 (n=3200)  ‚Üê Most common, best learned
PART_OF         : MSE=0.0412 (n=850)
HAS_PROPERTY    : MSE=0.0389 (n=620)
USES            : MSE=0.0567 (n=210)   ‚Üê Rare, harder to learn
LOCATED_IN      : MSE=0.0523 (n=180)
```

---

## Tasks Breakdown

### Task 1: Data Preparation (1 day)
**Owner:** Data Pipeline Team

```bash
# Step 1: Build relation vocabulary
./.venv/bin/python -m src.lvm.build_relation_vocab \
    --output artifacts/lvm/relation_vocab.json

# Step 2: Re-extract sequences with relations
./.venv/bin/python -m src.lvm.extract_ocp_sequences \
    --output artifacts/lvm/ocp_training_9k_relations.npz \
    --include-relations \
    --min-chain-length 3 \
    --max-chain-length 15

# Step 3: Verify data
./.venv/bin/python -m src.lvm.verify_relation_data \
    --input artifacts/lvm/ocp_training_9k_relations.npz
```

**Exit Criteria:**
- ‚úÖ `relation_vocab.json` contains 5-8 relation types
- ‚úÖ `ocp_training_9k_relations.npz` has `relations` and `relation_confidence` arrays
- ‚úÖ Data verification passes (shapes match, no NaNs, confidence ‚àà [0,1])

---

### Task 2: Model Implementation (1 day)
**Owner:** Model Architecture Team

```bash
# Implement RelationConditionedMambaLVM
# File: src/lvm/mamba_lvm.py

# Test forward pass
./.venv/bin/python -c "
from src.lvm.mamba_lvm import RelationConditionedMambaLVM
import torch

model = RelationConditionedMambaLVM(
    d_model=768,
    relation_vocab_size=8,
    relation_emb_dim=32
)

# Test batch
concepts = torch.randn(2, 10, 768)
relations = torch.randint(0, 8, (2, 9))
tmd = torch.randn(2, 10, 16)
rel_conf = torch.rand(2, 9)

outputs = model(concepts, relations, tmd, rel_conf)
print('Output shape:', outputs.shape)  # Should be [2, 10, 768]
"
```

**Exit Criteria:**
- ‚úÖ Forward pass works with relations + confidence
- ‚úÖ Backward pass computes gradients correctly
- ‚úÖ Model is backward-compatible (works with `relations=None`)

---

### Task 3: Training Pipeline (1 day)
**Owner:** Training Team

```bash
# Train baseline (S3-S5) for comparison
./.venv/bin/python -m src.lvm.train \
    --config config/lvm_baseline_config.yaml \
    --data artifacts/lvm/ocp_training_9k.npz \
    --output models/mamba_lvm_baseline.pt

# Train relation-conditioned model
./.venv/bin/python -m src.lvm.train \
    --config config/lvm_relation_config.yaml \
    --data artifacts/lvm/ocp_training_9k_relations.npz \
    --output models/mamba_lvm_relations.pt \
    --use-relations
```

**Exit Criteria:**
- ‚úÖ Both models train to convergence (loss plateaus)
- ‚úÖ Relation-conditioned model achieves **lower test loss** than baseline
- ‚úÖ Training logs saved to `logs/lvm_relations/`

---

### Task 4: Evaluation & Analysis (0.5 day)
**Owner:** Evaluation Team

```bash
# Evaluate both models
./.venv/bin/python -m src.lvm.eval \
    --baseline models/mamba_lvm_baseline.pt \
    --relations models/mamba_lvm_relations.pt \
    --test-data artifacts/lvm/ocp_test_9k_relations.npz \
    --output eval/s6_relation_results.json
```

**Metrics to Compare:**
1. **Overall MSE**: Next concept prediction accuracy
2. **Per-Relation MSE**: Performance breakdown by relation type
3. **Inference Speed**: Tokens/sec (relations add overhead?)
4. **Model Size**: Parameters added by relation embeddings

**Expected Results:**
| Metric | Baseline (S3-S5) | Relations (S6) | Improvement |
|--------|------------------|----------------|-------------|
| Overall MSE | 0.0342 | **0.0298** | **13% better** |
| IS_A MSE | 0.0234 | **0.0189** | 19% better |
| PART_OF MSE | 0.0567 | **0.0412** | 27% better |
| Inference (tok/sec) | 1200 | 1150 | 4% slower (acceptable) |
| Params (M) | 12.3 | 12.4 | +0.1M (256K for relations) |

---

### Task 5: Documentation (0.5 day)
**Owner:** Documentation Team

**Files to Update:**
1. `docs/PRDs/PRD_P15_Latent_LVM_Implementation_Plan.md`
   - Add section: "Relation-Conditioned Training (S6)"
   - Update config examples with `use_relations` flag

2. `sprints/sprint_10012025_S3.md`
   - Add note: "See S6 for relation-conditioning extension"

3. `sprints/sprint_10012025_S6.md`
   - Mark as ‚úÖ COMPLETE

**Exit Criteria:**
- ‚úÖ PRD updated with S6 details
- ‚úÖ S3 sprint doc cross-references S6
- ‚úÖ Example configs checked into `config/lvm_relation_config.yaml`

---

## Success Criteria

### Quantitative
1. ‚úÖ Relation-conditioned model **outperforms baseline** by ‚â•10% on next-concept prediction
2. ‚úÖ Per-relation metrics show **improved learning** for rare relation types (PART_OF, USES)
3. ‚úÖ Inference overhead < 10% (relation embeddings are cheap)
4. ‚úÖ All tests pass: `pytest tests/test_lvm_relations.py`

### Qualitative
1. ‚úÖ Model learns semantically meaningful relation embeddings (t-SNE clustering by type)
2. ‚úÖ Prediction quality **improves for multi-parent DAG nodes** (Tiger ‚Üí Mammal + Carnivore)
3. ‚úÖ Code is backward-compatible (can disable relations via config flag)

---

## Risks & Mitigations

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Relation embeddings don't improve performance | Medium | High | Compare against baseline; if no gain, make optional |
| Rare relations are poorly learned (USES, LOCATED_IN) | High | Low | Expected; report metrics separately; consider class balancing |
| Inference overhead > 10% | Low | Medium | Profile; optimize relation fusion layer |
| Data extraction breaks for complex DAGs | Medium | High | Add integration test for multi-parent chains |

---

## Testing Plan

### Unit Tests
```python
# tests/test_lvm_relations.py

def test_relation_embedding_shape():
    """Verify relation embeddings have correct dimensionality."""
    model = RelationConditionedMambaLVM(relation_vocab_size=8, relation_emb_dim=32)
    assert model.relation_emb.weight.shape == (8, 32)

def test_forward_with_relations():
    """Test forward pass with relation conditioning."""
    model = RelationConditionedMambaLVM()
    concepts = torch.randn(2, 10, 768)
    relations = torch.randint(0, 8, (2, 9))
    outputs = model(concepts, relations=relations)
    assert outputs.shape == (2, 10, 768)

def test_backward_compatible():
    """Model works without relations (backward compatible)."""
    model = RelationConditionedMambaLVM()
    concepts = torch.randn(2, 10, 768)
    outputs = model(concepts, relations=None)  # No relations
    assert outputs.shape == (2, 10, 768)

def test_confidence_weighting():
    """Verify low-confidence relations have less impact."""
    model = RelationConditionedMambaLVM()
    concepts = torch.randn(1, 10, 768)
    relations = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]])  # All IS_A

    # High confidence
    high_conf = torch.ones(1, 9)
    out_high = model(concepts, relations, relation_confidence=high_conf)

    # Low confidence (should have less impact)
    low_conf = torch.ones(1, 9) * 0.1
    out_low = model(concepts, relations, relation_confidence=low_conf)

    # Outputs should differ (low conf closer to baseline)
    assert not torch.allclose(out_high, out_low)
```

### Integration Tests
```python
# tests/test_lvm_integration.py

def test_end_to_end_training():
    """Full training loop with relation conditioning."""
    # Load data
    data = np.load("artifacts/lvm/ocp_training_9k_relations.npz")

    # Train for 10 steps
    model = RelationConditionedMambaLVM()
    optimizer = torch.optim.Adam(model.parameters())

    for _ in range(10):
        loss = train_step(data, model, optimizer)

    assert loss < 1.0  # Should converge somewhat
```

---

## Timeline

| Day | Task | Owner |
|-----|------|-------|
| 1 | Data prep (vocab + sequence extraction) | Data Pipeline |
| 2 | Model implementation + unit tests | Model Architecture |
| 3 | Training pipeline + baseline comparison | Training |
| 3.5 | Evaluation + per-relation metrics | Evaluation |
| 4 | Documentation + PR review | Documentation |

**Total:** 3-4 days

---

## Dependencies

**Blocked by:**
- ‚úÖ S2: OCP sequence extraction (baseline)
- ‚úÖ S3: Mamba LVM architecture
- ‚úÖ S4: Training pipeline
- ‚úÖ S5: Evaluation framework

**Blocks:**
- Future: S7+ (multi-modal extensions, RL integration)

---

## Appendix: Relation Embedding Visualization

**After training, visualize relation embeddings:**

```python
# tools/visualize_relation_embeddings.py

import torch
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Load trained model
model = torch.load("models/mamba_lvm_relations.pt")
relation_emb = model.relation_emb.weight.detach().cpu().numpy()  # [8, 32]

# t-SNE projection to 2D
tsne = TSNE(n_components=2, perplexity=3)
emb_2d = tsne.fit_transform(relation_emb)

# Plot
plt.figure(figsize=(8, 6))
for i, rel_name in enumerate(relation_vocab.keys()):
    plt.scatter(emb_2d[i, 0], emb_2d[i, 1], s=100)
    plt.annotate(rel_name, (emb_2d[i, 0], emb_2d[i, 1]))

plt.title("Relation Embedding Space (t-SNE)")
plt.savefig("eval/relation_embeddings_tsne.png")
```

**Expected:** IS_A, PART_OF cluster separately; semantically similar relations are closer.

---

## References

- **S2 Sprint:** `sprints/sprint_10012025_S2.md` (OCP sequence extraction)
- **S3 Sprint:** `sprints/sprint_10012025_S3.md` (Mamba LVM baseline)
- **PRD Ontology Chains:** `docs/PRDs/PRD_ontology_chains.md` (SPO triple schema - Appendix C)
- **Academic Reference:** [Relation-Aware Graph Attention Networks](https://arxiv.org/abs/1904.05811)

---

**Status:** üìã Planned ‚Äî Awaiting S2-S5 completion
**Next Sprint:** S7 (TBD - Multi-modal or RL integration)
