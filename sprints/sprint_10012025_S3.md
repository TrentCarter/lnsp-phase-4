  Sprint 3: Minimal Mamba LVM Training (Path 1A)

  Approach: Pure autoregressive next-vector prediction on OCP sequences

  Architecture:
  class OCP_Mamba_LVM(nn.Module):
      def __init__(self, d_model=768, n_layers=6, d_state=16):
          super().__init__()

          # No input projection needed - already 768D
          self.layers = nn.ModuleList([
              Mamba2Layer(d_model, d_state, d_conv=4, expand=2)
              for _ in range(n_layers)
          ])

          # Optional TMD conditioning
          self.tmd_adapter = nn.Linear(16, d_model)

          # Output head
          self.output = nn.Linear(d_model, 768)

      def forward(self, seq_vectors, tmd=None):
          # seq_vectors: [batch, seq_len, 768]
          h = seq_vectors

          if tmd is not None:
              tmd_emb = self.tmd_adapter(tmd).unsqueeze(1)
              h = h + tmd_emb  # Broadcast conditioning

          for layer in self.layers:
              h = layer(h)

          # Predict next vector from last position
          return self.output(h[:, -1, :])

  Training script:
  # 2-hour training run on single GPU
  ./.venv/bin/python -m src.lvm.train_ocp_mamba \
      --data artifacts/lvm/ocp_training.npz \
      --model-size small \
      --n-layers 6 \
      --batch-size 64 \
      --max-epochs 10 \
      --early-stopping \
      --output artifacts/lvm/ocp_mamba_v1.pt

  Evaluation metrics (automated):
  # Run on validation set
  metrics = {
      'next_vec_cosine': 0.0,      # Target: >0.60 with improvement
      'faiss_retrieval_p1': 0.0,   # After vecRAG lookup
      'echo_loop_similarity': 0.0, # Full inference
      'train_time_minutes': 0,
      'inference_latency_ms': 0
  }

  Exit criteria:
  - Next-vector cosine >0.60 on validation set
  - Improvement trend visible across epochs (not plateauing)
  - Training completes in <2 hours
  - Model saved and ready for inference testing

# Sprint 3 Update 1:
  Sprint 3: Minimal Mamba LVM Training

  NO CHANGES - architecture stays same, just smaller dataset:

  # Train on 45K sequences (not 800K)
  ./.venv/bin/python -m src.lvm.train_ocp_mamba \
      --data artifacts/lvm/ocp_training_9k.npz \
      --model-size small \
      --n-layers 6 \
      --batch-size 64 \
      --max-epochs 10 \
      --early-stopping \
      --output artifacts/lvm/ocp_mamba_v1_9k.pt

  Expected outcomes with 45K examples:
  - Training time: ~1-1.5 hours (smaller dataset)
  - Next-vector cosine: Target >0.60
  - May see MORE overfitting risk (smaller data)