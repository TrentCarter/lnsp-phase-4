[Architect]
1) Kill the “search probe” error at startup (2-line, safe)
A) Ensure gate_cfg exists before any probe code ever references it
Right after you construct ctx (or in RetrievalContext.__init__), add:
# src/api/retrieve.py (module scope during startup)
import os
from src.utils.gating import CPESHGateConfig

# ...after ctx is created and before any optional probes...
ctx.gate_cfg = CPESHGateConfig(
    q_min=float(os.getenv("LNSP_CPESH_Q_MIN", "0.82")),
    cos_min=float(os.getenv("LNSP_CPESH_COS_MIN", "0.55")),
    nprobe_cpesh=int(os.getenv("LNSP_NPROBE_CPESH", "8")),
    nprobe_fallback=int(os.getenv("LNSP_NPROBE_DEFAULT", "16")),
    lane_overrides={"L1_FACTOID": {"q_min": 0.85}},
)
B) Disable any eager “search probe” on startup (optional, recommended)
If you have something like ctx.search_probe() or a “warm check” that runs at startup, guard it:
try:
    if os.getenv("LNSP_DISABLE_STARTUP_PROBE", "1") != "1":
        ctx.search_probe()  # or whatever your eager check is named
except Exception as e:
    # log-but-don’t-fail startup
    print(f"[startup] probe skipped: {e}")
Then restart:
FAISS_NUM_THREADS=1 OMP_NUM_THREADS=1 VECLIB_MAXIMUM_THREADS=1 make api PORT=8094
PORT=8094 make lnsp-status
Health’s error field should now be empty (or at worst a harmless prior snapshot).
2) Tiny dashboard improvement: warn if 40× safe bound is off
Your Index table shows Vectors=10000 but max_safe=512. By the 40× rule, with 10k vectors we expect max_safe ≈ 250. This likely means your metadata’s max_safe_nlist is derived from ntrain (e.g., 20,480/40 → 512), not from N. That’s fine if intentional—just be explicit.
Add a gentle warning row in tools/lnsprag_status.py:
# after meta is loaded
N = meta.get("count")
safe_meta = meta.get("max_safe_nlist")
safe_expected = (N // 40) if isinstance(N, int) and N else None
warn = ""
if isinstance(safe_meta, int) and isinstance(safe_expected, int) and safe_meta != safe_expected:
    warn = f"max_safe_nlist={safe_meta} (meta) vs {safe_expected} (N//40)"
    # Optionally: print() a visible note.

# augment the Index table output to include a note column when warn exists
(Or just keep as-is if you’re deliberately keying max_safe to ntrain—now you know why it reads 512.)
3) Make sure the right shard is the one the API uses
You’ve got two shards listed:
fw10k_ivf_768      IVF_FLAT 10k
fw10k_ivf_flat_ip  IVF_FLAT 10k
If your API takes an env, pin the intended one explicitly:
export LNSP_FAISS_INDEX=artifacts/fw10k_ivf_flat_ip.index
make api PORT=8094
(Or whichever file you want as the canonical shard.)


[Consultant]

Thanks for sharing the Consultant’s notes. They’re solid. Here’s a straight-shooting gap review plus commit-ready fixes to close each item fast. I’ll keep this tight and actionable.
TL;DR action plan
#	Gap (from Consultant)	Priority	Fix (summary)	What you’ll get
1	CPESH schema too minimal (missing TMD, mission/source, counters, timestamps)	P0	Extend src/schemas.py + legacy migration	Authoritative model in code; JSONL/Parquet rows validate
2	Tiered storage lacks cpesh_index.db and cross-tier sampler	P0	Add SQLite index builder + fast sampler	Warm/cold lookup + nightly refresh + training batches
3	GET /cpesh/segments not implemented	P0	Add FastAPI endpoint + Makefile curl	Operators can enumerate segments via API
4	GraphRAG not exposed in API	P1	Add /graph/search + /graph/hop thin wrappers w/ feature flag	Minimal GraphRAG serving (keeps CLI as fallback)
5	SLO under target (Hit@1~43% / Hit@3~49%)	P0	Quick tuning bundle + grid script	Path to ≥45/55 with reproducible sweeps
Below are the concrete changes.
1) CPESH schema — authoritative in code (P0)
src/schemas.py (add/extend)
from typing import List, Optional
from pydantic import BaseModel, Field

class ChunkPosition(BaseModel):
    doc_id: str
    start: int
    end: int

class RelationTriple(BaseModel):
    subj: str
    pred: str
    obj: str

class CPESH(BaseModel):
    # core
    cpe_id: str
    concept_text: str
    probe_question: str
    expected_answer: str
    soft_negative: Optional[str] = None
    hard_negative: Optional[str] = None

    # mission/source & structure
    mission_text: Optional[str] = None
    dataset_source: Optional[str] = None
    content_type: Optional[str] = None
    chunk_position: Optional[ChunkPosition] = None
    relations_text: Optional[List[RelationTriple]] = None

    # routing
    tmd_bits: Optional[int] = None            # packed Task|Modifier|Domain
    tmd_lane: Optional[str] = None
    lane_index: Optional[int] = None          # 0..32767

    # quality & flags
    quality: Optional[float] = None
    echo_score: Optional[float] = None
    insufficient_evidence: bool = False

    # audit
    created_at: Optional[str] = None          # ISO8601
    last_accessed: Optional[str] = None       # ISO8601
    access_count: int = 0
Legacy migration (lightweight)
Add to src/utils/timestamps.py (or a new src/utils/migrate.py):
def migrate_cpesh_record(rec: dict) -> dict:
    # Fill required keys if missing, without changing semantics.
    rec.setdefault("access_count", 0)
    rec.setdefault("insufficient_evidence", False)
    cpesh = rec.get("cpesh") or rec  # support old nested shape
    cpesh.setdefault("tmd_bits", None)
    cpesh.setdefault("tmd_lane", None)
    cpesh.setdefault("lane_index", None)
    cpesh.setdefault("created_at", get_iso_timestamp())
    cpesh.setdefault("last_accessed", cpesh["created_at"])
    return rec
Use this wherever you load JSONL (Active/legacy) before validation.
2) Tiered storage — cpesh_index.db + cross-tier sampler (P0)
A) Build/refresh a tiny SQLite index
File: scripts/cpesh_index_refresh.py (no extra deps)
#!/usr/bin/env python
import os, json, sqlite3
from pathlib import Path
import pyarrow.parquet as pq

MANIFEST = Path("artifacts/cpesh_manifest.jsonl")
DBPATH   = Path("artifacts/cpesh_index.db")

def ensure_schema(cur):
    cur.execute("""CREATE TABLE IF NOT EXISTS cpesh_index(
        cpe_id TEXT PRIMARY KEY,
        segment_path TEXT NOT NULL,
        row_offset INTEGER NOT NULL,
        lane_index INTEGER,
        created_at TEXT
    )""")

def refresh():
    if not MANIFEST.exists():
        print("[cpesh-index] no manifest; nothing to do"); return
    conn = sqlite3.connect(DBPATH); cur = conn.cursor(); ensure_schema(cur)

    with MANIFEST.open() as mf:
        for line in mf:
            seg = json.loads(line)
            path = seg["path"]
            table = pq.read_table(path, columns=["cpe_id","lane_index","created_at"])
            # Append rows
            cpe = table.column("cpe_id").to_pylist()
            lane = table.column("lane_index").to_pylist() if "lane_index" in table.schema.names else [None]*len(cpe)
            cat  = table.column("created_at").to_pylist() if "created_at" in table.schema.names else [None]*len(cpe)
            cur.executemany(
                "INSERT OR REPLACE INTO cpesh_index(cpe_id,segment_path,row_offset,lane_index,created_at) VALUES (?,?,?,?,?)",
                [(cpe[i], path, i, lane[i], cat[i]) for i in range(len(cpe))]
            )
            conn.commit()
    conn.close()
    print(f"[cpesh-index] refreshed {DBPATH}")

if __name__ == "__main__":
    refresh()
Makefile target:
.PHONY: cpesh-index
cpesh-index:
	@./.venv/bin/python scripts/cpesh_index_refresh.py
Run nightly (or after cpesh-rotate):
make cpesh-rotate && make cpesh-index
B) Cross-tier sampler
File: tools/cpesh_sample.py
#!/usr/bin/env python
import os, json, random
from pathlib import Path
import pyarrow as pa, pyarrow.dataset as ds

ACTIVE = Path("artifacts/cpesh_active.jsonl")
MANIFEST = Path("artifacts/cpesh_manifest.jsonl")

def sample_active(k):
    out = []
    if ACTIVE.exists():
        with ACTIVE.open() as f:
            for i, line in enumerate(f):
                if random.random() < min(1.0, k/100000.0):  # fast reservoir-ish
                    try: out.append(json.loads(line))
                    except: pass
                if len(out) >= k: break
    return out[:k]

def sample_segments(k):
    if not MANIFEST.exists(): return []
    segs = [json.loads(x) for x in MANIFEST.open()]
    if not segs: return []
    take = max(1, k // len(segs))
    out = []
    for seg in segs:
        d = ds.dataset(seg["path"], format="parquet")
        # approximate random sample without full scan
        tbl = d.take(indices=list(range(0, min(10000, d.count()), max(1, d.count()//take))))  # light sampling
        for row in tbl.to_pylist():
            out.append(row)
            if len(out) >= k: return out[:k]
    return out[:k]

if __name__ == "__main__":
    K = int(os.getenv("K", "1000"))
    mix = int(os.getenv("ACTIVE_FRAC", "30"))  # 30% from Active by default
    a = sample_active(int(K*mix/100))
    s = sample_segments(K - len(a))
    print(json.dumps({"count": len(a)+len(s)}))
(We can evolve this to stratify by lane/time/quality, but this gets you moving today.)
3) GET /cpesh/segments (P0)
Patch: src/api/retrieve.py
from fastapi import HTTPException
import os, json, time

@app.get("/cpesh/segments")
def list_cpesh_segments():
    """
    Enumerate Parquet segments from the manifest with sizes and timestamps.
    """
    manifest = "artifacts/cpesh_manifest.jsonl"
    if not os.path.exists(manifest):
        return {"segments": []}
    segs = []
    with open(manifest) as f:
        for line in f:
            try:
                j = json.loads(line)
                p = j.get("path")
                size = os.path.getsize(p) if p and os.path.exists(p) else None
                segs.append({
                    "segment_id": j.get("segment_id"),
                    "path": p,
                    "rows": j.get("rows"),
                    "bytes": size,
                    "created_utc": j.get("created_utc"),
                })
            except Exception:
                continue
    return {"segments": segs, "count": len(segs)}
CLI check:
curl -s http://127.0.0.1:8094/cpesh/segments | jq
4) Minimal GraphRAG serving (P1)
Add feature-flagged endpoints that call your LightRAG adapter. If missing/unavailable, return 501—not a crash.
@app.post("/graph/search")
def graph_search(q: str, top_k: int = 10):
    if os.getenv("LNSP_GRAPHRAG_ENABLED", "0") != "1":
        raise HTTPException(status_code=501, detail="GraphRAG disabled")
    try:
        from src.adapters.lightrag.graphrag_runner import run_search
        return run_search(q, top_k=top_k)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GraphRAG error: {e}")

@app.post("/graph/hop")
def graph_hop(node_id: str, max_hops: int = 2, top_k: int = 25):
    if os.getenv("LNSP_GRAPHRAG_ENABLED", "0") != "1":
        raise HTTPException(status_code=501, detail="GraphRAG disabled")
    try:
        from src.adapters.lightrag.graphrag_runner import run_hop
        return run_hop(node_id, max_hops=max_hops, top_k=top_k)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GraphRAG error: {e}")
This makes the objective testable from the API while keeping your CLI path available.
5) SLO tuning bundle (P0)
You’re at ~43/49. Do three quick, high-leverage changes:
Fallback breadth: raise fallback nprobe → 24 (keep CPESH path at 8).
export LNSP_NPROBE_DEFAULT=24
Trainer points: remove small-corpus cap; train on all vectors when N ≤ 20k (patch provided earlier). This improves centroid quality.
RRF fusion (cheap) across two query vectors when CPESH passes: query_vec and expected_vec → search both with small nprobe and fuse by rank. Minimal change, big lift on Hit@1.
Patch: inside your /search path where CPESH is used:
def fuse_rrf(a, b, k=60):
    # a and b are lists of (doc_id, score) or similar; implement tiny RRF
    rank = {}
    for r, lst in enumerate([a, b]):
        for i, it in enumerate(lst):
            key = it["id"]
            rank[key] = rank.get(key, 0) + 1.0/(k + i + 1)
    # rebuild merged list sorted by fused score
    # fetch metadata as you already do
    return sorted(rank.items(), key=lambda kv: kv[1], reverse=True)

if used_cpesh and cpesh_entry.get("expected_vec") is not None:
    res1 = ctx.index.search(query_vec, nprobe=gate.nprobe_cpesh, top_k=request.top_k)
    res2 = ctx.index.search(cpesh_entry["expected_vec"], nprobe=gate.nprobe_cpesh, top_k=request.top_k)
    results = fuse_rrf(res1, res2)
else:
    results = ctx.index.search(query_vec, nprobe=gate.nprobe_fallback, top_k=request.top_k)
Grid harness (quick)
tools/slo_grid.py to sweep nprobe_default ∈ {16,24,32} and gate thresholds (q_min ∈ {0.82,0.85,0.88}, cos_min ∈ {0.50,0.55,0.60}), logging Hit@1/3. (I can drop the full script if you want it now.)
Housekeeping
Your PRD filename in the Consultant note differs slightly from ours; we standardized on docs/PRDs/lnsprag_data_lake_and_serving_prd.md. No action needed beyond consistency in links.
The /health/faiss “probe” error string you still see is harmless, but apply the earlier fix to remove it entirely (initialize gate_cfg before any probes, or disable probes on startup).
Definition of Done for this batch
src/schemas.py: CPESH matches PRD; legacy JSONL reads without errors.
scripts/cpesh_index_refresh.py + make cpesh-index: creates/updates artifacts/cpesh_index.db.
tools/cpesh_sample.py: prints a JSON summary ({"count": N}) and returns sampled rows when imported.
GET /cpesh/segments: returns segment list with sizes; visible in lnsp-status (next iteration can read it too).
Optional: GraphRAG endpoints respond 501 when disabled, 200 when enabled.
SLO sweep shows an improved configuration (targeting ≥45/55). At minimum, set LNSP_NPROBE_DEFAULT=24 and enable RRF for CPESH-assisted path.

