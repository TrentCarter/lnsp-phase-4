[Architect]

**Note thsi is bore the Manager read your full report from S3. So first go though and make sure we have not already solved these items one at a time.

Where you stand (net of the Consultant gaps)
✅ Addressed
Schema: CPESH widened (routing bits, mission/source, counters, timestamps) with migration helpers.
Tiering: Parquet rotation + manifest working; SQLite index refresher + sampler landed.
Serving: /cpesh/segments live; GraphRAG endpoints behind a feature flag; RRF assist wired for CPESH.
Ops: Correct shard selection (env) + startup probe fixed; dashboard shows real stats.
⚠️ Remaining friction
pytest exits via signal on this macOS arm64 + FAISS combo.
Make tests stable on macOS arm64 (no more signals)
Pick one; both work.
Option A (most robust): run tests in the conda env (Py 3.11)
You already have the strategy in scripts/setup_faiss_env.sh.
# one time
make faiss-setup ARGS="--strategy conda --py 3.11"

# test run (inside that env)
conda activate lnsp-faiss
PYTHONPATH=src pytest -q
Option B (stay in your venv 3.13): pin threads at import time + skip if FAISS missing
Add a tiny tests/conftest.py:
import os
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("FAISS_NUM_THREADS", "1")

import pytest
try:
    import faiss  # noqa
    HAS_FAISS = True
except Exception:
    HAS_FAISS = False

def pytest_configure(config):
    config.addinivalue_line("markers", "faiss: marks tests that require FAISS")

def pytest_collection_modifyitems(config, items):
    if HAS_FAISS:
        return
    skip_faiss = pytest.mark.skip(reason="FAISS not importable on this runtime")
    for item in items:
        if "faiss" in item.keywords:
            item.add_marker(skip_faiss)
Then mark any FAISS-touching tests with @pytest.mark.faiss. Run:
PYTHONPATH=src pytest -q
Wire tiering into nightly ops (one-liner)
Add a chain target so rotation always refreshes the lookup index:
.PHONY: cpesh-rotate-nightly
cpesh-rotate-nightly:
	@./.venv/bin/python scripts/cpesh_rotate.py
	@./.venv/bin/python scripts/cpesh_index_refresh.py
(Use this in cron/launchd, or just make cpesh-rotate-nightly after big ingests.)
Quick API checks (prove the new endpoints)
# segments
curl -s http://127.0.0.1:8094/cpesh/segments | jq '.count, .segments[0]'

# graph (feature-flagged)
export LNSP_GRAPHRAG_ENABLED=1
curl -s -X POST 'http://127.0.0.1:8094/graph/search?q=photosynthesis&top_k=5' | jq '.[0]'
curl -s -X POST 'http://127.0.0.1:8094/graph/hop' -d 'node_id=Q123&max_hops=2&top_k=10' | jq
S3 (today) or S4 (next) – tight plan & DoD
Tests green
DoD: PYTHONPATH=src pytest -q runs with 0 signals; FAISS tests run (or cleanly skip) on arm64.
Nightly tier refresh in workflows
DoD: make cpesh-rotate-nightly emits new segment + updates cpesh_index.db; dashboard segment count increments.
/metrics/slo snapshot
Add endpoint + harness POST.
DoD: curl /metrics/slo returns last sweep (Hit@1/3, P50/P95); dashboard can render it.
Hit SLO ≥ 45/55
Set LNSP_NPROBE_DEFAULT=24, keep CPESH path at 8, train IVF on all points for N ≤ 20k, keep RRF fusion for CPESH.
DoD: eval report shows ≥45/55 on the 100-query slice.
Dashboard “training_pairs”
Count CPESH rows with present concept_text, probe_question, expected_answer.
DoD: status prints a non-“—” count.
Tiny notes on what you asked earlier
Yep: the PRD’s “+ metadata” explicitly includes TMD, timestamps (created/last_accessed), IDs, access_count, mission/source, etc.—that’s the model you just implemented and migrated toward.
Your Segments table showing 1 / 135 rows / Parquet (ZSTD) confirms the lake is now real and queryable; keep rotating and indexing.