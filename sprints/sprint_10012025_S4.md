Sprint 4: Scaling Test (2x Dataset)

  IF Sprint 3 succeeds (>60% + improving):

  1. Expand dataset:
    - Ingest remaining ontology chains (from 9.5K â†’ 20K concepts)
    - Regenerate training sequences (~1.6M examples)
  2. Retrain same architecture:
    - Same hyperparameters as Sprint 3
    - Budget: 6-10 hours GPU
  3. Measure scaling coefficient:
  scaling_coeff = performance_20K / performance_9.5K
  # Target: â‰¥0.85 (acceptable degradation <15%)

  Exit criteria:
  - Scaling coefficient â‰¥0.85
  - Next-vector cosine maintains >0.60
  - Decision: Scale to full 173K or optimize architecture

  ---
  ðŸ”¬ EXPERIMENTAL VARIANTS (If Sprint 3 Succeeds)

  Only AFTER validating Path 1A (pure OCP), test these variants in parallel:

  | Variant | Change                     | Hypothesis                             |
  |---------|----------------------------|----------------------------------------|
  | 1B      | Add TMD conditioning       | Improves lane-specific predictions     |
  | 1C      | Curriculum learning        | Train on short chains first, then long |
  | 1D      | Larger model (12 layers)   | More capacity for complex patterns     |
  | 1E      | Contrastive auxiliary loss | Use CPESH soft/hard negatives          |

  Run all variants in PARALLEL (5x 2-hour jobs):
  # Launch 5 experiments simultaneously
  for variant in 1B 1C 1D 1E; do
      ./.venv/bin/python -m src.lvm.train_ocp_mamba \
          --variant $variant \
          --max-time 2h &
  done
  wait

  # Compare results
  ./.venv/bin/python -m src.lvm.compare_experiments \
      --results artifacts/lvm/experiments/*.json \
      --output docs/experiments/sprint3_results.md

  ---
  ðŸ“Š RESULTS TRACKING: docs/experiments/lvm_training_results.md

  # LVM Training Experiments

  ## Sprint 3: OCP Baseline (Path 1A)

  | Metric | Target | Result | Status |
  |--------|--------|--------|--------|
  | Next-vector cosine | >0.60 | 0.73 | âœ… |
  | Faiss retrieval P@1 | >0.50 | 0.68 | âœ… |
  | Echo loop similarity | >0.70 | 0.81 | âœ… |
  | Training time | <2h | 1.8h | âœ… |
  | Inference latency | <100ms | 45ms | âœ… |

  **Conclusion:** OCP autoregressive training WORKS. Proceed to scaling.

  ## Sprint 4: Scaling to 2x Dataset

  | Metric | Baseline (9.5K) | Scaled (20K) | Coefficient |
  |--------|----------------|--------------|-------------|
  | Next-vector cosine | 0.73 | 0.71 | 0.97 âœ… |
  | Training time | 1.8h | 7.2h | N/A |

  **Conclusion:** Scales well. Proceed to full 173K.

## Sprint 4: Update 1
  Sprint 4: Scale to Full 173K (FUTURE)

  ONLY if Sprint 3 succeeds:
  1. Ingest remaining ontology chains (9.5K â†’ 173K concepts)
  2. Re-extract sequences (~800K examples)
  3. Retrain same architecture (6-10 hour budget)