[Consultant] 
Live sanity checks (non-intrusive)
You can run these in another terminal; they don’t touch the running process.
# 1) Quick progress pulse (process + memory)
ps -o pid,%mem,%cpu,etime,command -p 29958

# 2) Active CPESH rows (gives you a feel for throughput)
wc -l artifacts/cpesh_active.jsonl 2>/dev/null || echo "active jsonl not created yet"

# 3) Postgres parity (entries vs vectors)
psql "$PG_DSN" -c "SELECT
  (SELECT count(*) FROM cpe_entry) AS entry_cnt,
  (SELECT count(*) FROM cpe_vectors) AS vector_cnt;"

# 4) Neo4j counts (concepts/edges)
cypher-shell -u "$NEO4J_USER" -p "$NEO4J_PASSWORD" \
'CALL { MATCH (c:Concept) RETURN count(c) AS n } 
 CALL { MATCH ()-[r:RELATES_TO]->() RETURN count(r) AS m }
 RETURN n AS concepts, m AS edges;'

# 5) NPZ presence (created near the end) + shape if present
python - <<'PY'
import os, numpy as np
p="artifacts/fw10k_vectors_768.npz"
print("[npz exists]", os.path.exists(p))
if os.path.exists(p):
    z=np.load(p, allow_pickle=True)
    print("vectors", z["vectors"].shape, "concept", z["concept_vecs"].shape, "question", z["question_vecs"].shape)
PY
Tip: if RAM climbs uncomfortably, you can pause (Ctrl-Z) then bg to resume, or lower BATCH_SIZE for future runs (e.g., 250).
Post-run checklist (execute once the ingest exits cleanly)
# 1) Validate DB parity and vector dims
psql "$PG_DSN" -c "
  SELECT
    (SELECT count(*) FROM cpe_entry)   AS entry_cnt,
    (SELECT count(*) FROM cpe_vectors) AS vector_cnt;"

# (optional, if pgvector functions are enabled)
psql "$PG_DSN" -c "
  SELECT COUNT(*) FROM cpe_vectors
  WHERE vector_dims(concept_vec)=768 AND vector_dims(question_vec)=768;"

# 2) Build FAISS (dynamic nlist will auto-downshift if needed)
make build-faiss ARGS="--type ivf_flat --metric ip --nlist 512 --nprobe 16"

# 3) Rotate CPESH to Parquet and refresh the lightweight index
make cpesh-rotate-nightly

# 4) Bring API + GraphRAG online and verify
export LNSP_GRAPHRAG_ENABLED=1
PORT=8094 make api &
sleep 2
PORT=8094 make graph-smoke
PORT=8094 make lnsp-status

# 5) Capture an SLO snapshot (synthetic or labeled eval if you have it)
PORT=8094 make slo-grid SLO_NOTES="10k post-ingest"
PORT=8094 make slo-snapshot
PORT=8094 make lnsp-status
Invariants to check (fast eyeball)
Postgres parity: cpe_entry = cpe_vectors.
Vector shapes: fused 784D, concept 768D, question 768D, TMD 16D.
FAISS meta: ntotal ≈ 10000, nlist obeys ≤ vectors/40 guard.
Dashboard: segments ≥1 (Parquet), gating metrics non-empty, SLO row populated.
GraphRAG: /graph/health shows non-zero concepts; /graph/search returns results.

[Consultant Progress 2025-03-07]
- `ps -o pid,%mem,%cpu,etime,command -p 29958` → sandbox blocked `/bin/ps` (Operation not permitted), so live pulse unavailable.
- `wc -l artifacts/cpesh_active.jsonl` → file missing (`No such file or directory`), indicating CPESH active export has not been produced yet.
- `psql "$PG_DSN" ... entry_cnt/vector_cnt` → psql cannot connect (`Operation not permitted` on `/tmp/.s.PGSQL.5432`); Postgres service not reachable and `PG_DSN` unset.
- `cypher-shell ...` → fails immediately (Operation not permitted); Neo4j CLI cannot attach under current sandbox and credentials are unset.
- `.venv/bin/python artifacts/fw10k_vectors_768.npz` check → reports `[npz exists] False`; vectors for the 10k ingest are absent.
- Post-run parity + vector-dims queries retried and hit the same Postgres socket error.
- `make build-faiss ...` → initial call missing `python`; retried with `PATH=.venv/bin:$PATH` but aborted because `faiss_index.py` found no NPZ input.
- `make cpesh-rotate-nightly` (with `.venv` on PATH) succeeded but rotated/indexed 0 records (`cpesh_index.db` empty), consistent with no ingest artifacts.
- API bring-up (`make api`, `graph-smoke`, `lnsp-status`) and SLO snapshot steps were skipped—the prerequisites (running API services, populated datasets) are unavailable in the sandbox, so launching long-lived servers was not attempted.
- Overall status: runbook blocked awaiting Postgres + Neo4j services and vector artifacts; once those are online, rerun steps 1–5 followed by FAISS build and SLO capture.
- Applied the provided storage-matrix patch to `tools/lnsprag_status.py` and ran `PYTHONPATH=src ./.venv/bin/python tools/lnsprag_status.py --matrix`; command succeeded, output shows ✓ for active JSONL/parquet/sqlite/faiss metadata, while Postgres/Neo4j columns remain `—` and NPZ vector store absent—exactly reflecting current disk/env state.



[Consultant]
here’s a clean, additive patch that adds --matrix to tools/lnsprag_status.py. It reads what’s on disk and prints the exact storage matrix with dynamic ✓/— markers. It doesn’t touch your existing status output; when --matrix is passed, it prints the matrix and exits.
Apply this patch and run:
PYTHONPATH=src ./.venv/bin/python tools/lnsprag_status.py --matrix
or via your Make target:
make lnsp-status ARGS="--matrix"
Unified diff (add-only)
diff --git a/tools/lnsprag_status.py b/tools/lnsprag_status.py
index 0000000..0000001 100644
--- a/tools/lnsprag_status.py
+++ b/tools/lnsprag_status.py
@@ -1,10 +1,22 @@
 import argparse
+import os
+from glob import glob
+from pathlib import Path
+from typing import Dict, List, Any
 
 # ... existing imports remain ...
 
+# ------------------------------
+# Storage-matrix helpers (add)
+# ------------------------------
+ART_DIR = Path("artifacts")
+ACTIVE_JSONL = ART_DIR / "cpesh_active.jsonl"
+ACTIVE_LEGACY = ART_DIR / "cpesh_cache.jsonl"      # legacy name if present
+SEGMENT_DIR = ART_DIR / "segments"
+MANIFEST = ART_DIR / "cpesh_manifest.jsonl"
+SQLITE_IDX = ART_DIR / "cpesh_index.db"
+FAISS_META = ART_DIR / "index_meta.json"
+GATING_DECISIONS = ART_DIR / "gating_decisions.jsonl"
+SLO_JSON = ART_DIR / "metrics_slo.json"
 
-def main():
-    ap = argparse.ArgumentParser()
+def _file_exists(p: Path) -> bool:
+    try:
+        return p.exists() and p.is_file() and p.stat().st_size > 0
+    except Exception:
+        return False
+
+def _any(pattern: str) -> bool:
+    try:
+        return any(Path(x).is_file() and Path(x).stat().st_size >= 0 for x in glob(pattern))
+    except Exception:
+        return False
+
+def _presence_from_disk() -> Dict[str, Any]:
+    """Return presence booleans for on-disk artifacts + env-configured services."""
+    present_active = _file_exists(ACTIVE_JSONL) or _file_exists(ACTIVE_LEGACY)
+    present_parquet = SEGMENT_DIR.exists() and any(SEGMENT_DIR.glob("*.parquet"))
+    present_manifest = _file_exists(MANIFEST)
+    present_sqlite = _file_exists(SQLITE_IDX)
+    present_npz = _any(str(ART_DIR / "fw*.npz"))
+    present_faiss_idx = _any(str(ART_DIR / "*.index")) and _file_exists(FAISS_META)
+    present_gating = _file_exists(GATING_DECISIONS)
+    present_slo = _file_exists(SLO_JSON)
+    # Best-effort service presence: mark ✓ if configured (does not dial network)
+    present_pg = bool(os.getenv("PG_DSN"))
+    present_neo4j = bool(os.getenv("NEO4J_URI") or os.getenv("LNSP_GRAPHRAG_ENABLED") == "1")
+    return {
+        "active_jsonl": present_active,
+        "parquet": present_parquet and present_manifest,
+        "sqlite": present_sqlite,
+        "npz": present_npz,
+        "faiss": present_faiss_idx,
+        "gating": present_gating,
+        "slo": present_slo,
+        "pg": present_pg,
+        "neo4j": present_neo4j,
+    }
+
+def _tick(ok: bool) -> str:
+    return "✓" if ok else "—"
+
+def _fmt_table(headers: List[str], rows: List[List[str]]) -> str:
+    widths = [len(h) for h in headers]
+    for r in rows:
+        for i, cell in enumerate(r):
+            widths[i] = max(widths[i], len(str(cell)))
+    def line(sep="+", fill="-"):
+        return sep + sep.join(fill*(w+2) for w in widths) + sep
+    def row(cells: List[str]):
+        return "| " + " | ".join(str(c).ljust(widths[i]) for i, c in enumerate(cells)) + " |"
+    out = [line(), row(headers), line()]
+    out += [row(r) for r in rows]
+    out.append(line())
+    return "\n".join(out)
+
+def _print_matrix(p: Dict[str, Any]) -> None:
+    # Text + Metadata matrix
+    headers1 = [
+        "Data Item",
+        "Active JSONL",
+        "Parquet Segments",
+        "Postgres cpe_entry",
+        "Postgres cpe_vectors",
+        "Neo4j :Concept",
+        "Neo4j :RELATES_TO",
+    ]
+    col_active = _tick(p["active_jsonl"])
+    col_parquet = _tick(p["parquet"])
+    col_pg_entry = _tick(p["pg"])       # presence == configured
+    col_pg_vec = _tick(p["pg"])         # presence == configured
+    col_neo_concept = _tick(p["neo4j"]) # presence == configured
+    col_neo_edge = _tick(p["neo4j"])    # presence == configured
+    rows1 = [
+        ["cpe_id",               col_active, col_parquet, col_pg_entry, col_pg_vec, col_neo_concept, "—"],
+        ["doc_id",               col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, _tick(p["neo4j"])],
+        ["concept_text",         col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, "—"],
+        ["probe_question",       col_active, col_parquet, col_pg_entry, "—",        "—",             "—"],
+        ["expected_answer",      col_active, col_parquet, col_pg_entry, "—",        "—",             "—"],
+        ["soft_negatives[3]",    col_active, col_parquet, col_pg_entry, "—",        "—",             "—"],
+        ["hard_negatives[3]",    col_active, col_parquet, col_pg_entry, "—",        "—",             "—"],
+        ["tmd_bits",             col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, _tick(p["neo4j"])],
+        ["tmd_text (decoded)",   "∆",        "∆",          "∆",         "—",        "∆",             "∆"],
+        ["lane_index",           col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, _tick(p["neo4j"])],
+        ["created_at",           col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, _tick(p["neo4j"])],
+        ["last_accessed",        col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, _tick(p["neo4j"])],
+        ["access_count",         col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, _tick(p["neo4j"])],
+        ["quality / echo_score", col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, _tick(p["neo4j"])],
+        ["insufficient_evidence",col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, _tick(p["neo4j"])],
+        ["dataset_source",       col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, _tick(p["neo4j"])],
+        ["content_type",         col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, "—"],
+        ["chunk_position{...}",  col_active, col_parquet, col_pg_entry, "—",        col_neo_concept, "—"],
+        ["relations_text",       col_active, col_parquet, "—",          "—",        "→ edges",       _tick(p["neo4j"])],
+    ]
+
+    # Vectors + Indexes matrix
+    headers2 = [
+        "Vector / Index",
+        "Active JSONL",
+        "Parquet Segments",
+        "Postgres cpe_vectors",
+        "NPZ Vector Store",
+        "FAISS Index",
+    ]
+    rows2 = [
+        ["concept_vec 768D",  "—", "—", _tick(p["pg"]), _tick(p["npz"]), "used"],
+        ["question_vec 768D", "—", "—", _tick(p["pg"]), _tick(p["npz"]), "used"],
+        ["tmd_dense 16D",     "—", "—", _tick(p["pg"]), _tick(p["npz"]), "fused"],
+        ["fused_vec 784D",    "—", "—", _tick(p["pg"]), _tick(p["npz"]), _tick(p["faiss"])],
+        ["index meta",        "—", "—", "—",           "—",             _tick(p["faiss"])],
+    ]
+
+    # Ops/sidecars
+    headers3 = ["Artifact", "Path", "Present"]
+    rows3 = [
+        ["Active lake (JSONL)",        str(ACTIVE_JSONL if _file_exists(ACTIVE_JSONL) else ACTIVE_LEGACY), _tick(p["active_jsonl"])],
+        ["Warm segments (Parquet)",     str(SEGMENT_DIR/"*.parquet"),                                      _tick(p["parquet"])],
+        ["SQLite locator",              str(SQLITE_IDX),                                                   _tick(p["sqlite"])],
+        ["FAISS meta",                  str(FAISS_META),                                                   _tick(p["faiss"])],
+        ["Gating decisions (JSONL)",    str(GATING_DECISIONS),                                             _tick(p["gating"])],
+        ["SLO snapshot (JSON)",         str(SLO_JSON),                                                     _tick(p["slo"])],
+    ]
+
+    print("\nLNSP RAG — Storage Matrix\n")
+    print(_fmt_table(headers1, rows1))
+    print(_fmt_table(headers2, rows2))
+    print(_fmt_table(headers3, rows3))
+    print("\nLegend: ✓ present / configured   — absent   ∆ decoded at read-time   → materialized into graph edges\n")
+
@@
-    # existing CLI args...
+def main():
+    ap = argparse.ArgumentParser(description="LNSP RAG status")
+    # existing CLI args come here...
+    ap.add_argument("--matrix", action="store_true",
+                    help="Print storage-location matrix (based on on-disk artifacts/env) and exit")
+    args, unknown = ap.parse_known_args()
+
+    # New: --matrix short-circuit
+    if getattr(args, "matrix", False):
+        pres = _presence_from_disk()
+        _print_matrix(pres)
+        return
+
+    # ... existing behavior continues here, using `args` as before ...
 
 if __name__ == "__main__":
     main()
Notes
DB columns show ✓ when configured (env present), since the matrix is requested to be disk-based; we don’t open sockets. If you prefer a live probe, you can swap the present_pg/present_neo4j lines to actually connect or hit /graph/health.
The tables mirror the ones I showed earlier and flip to — automatically when files are absent (e.g., before rotation, before FAISS build).
