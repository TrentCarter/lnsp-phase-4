#!/usr/bin/env python3
"""
FIXED TMD Benchmark (addresses consultant review)

Fixes:
1. âœ… Uses CANONICAL test cases from TMD-Schema.md (verified ground truth)
2. âœ… Tests ALL 3 codes (domain, task, modifier) - not just 2
3. âœ… Proper warm-up: measures ONLY 3rd query latency (not mixed cold/warm)
4. âœ… Logs RAW LLM output before parsing
5. âœ… Larger dataset (20+ samples, not 10)
6. âœ… Uses direct LLM client (not route_concept wrapper)

Usage:
    LNSP_LLM_ENDPOINT=http://localhost:11434 ./tools/benchmark_tmd_fixed.py
"""

import os
import sys
import time
import json
from typing import Dict, List
from dataclasses import dataclass
import statistics

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from src.llm.local_llama_client import call_local_llama_simple
import re


@dataclass
class CanonicalTestCase:
    """Verified test case from TMD-Schema.md"""
    text: str
    domain: int
    task: int
    modifier: int
    source: str  # "schema" or "additional"


# CANONICAL TEST CASES from docs/PRDs/TMD-Schema.md:166-173
CANONICAL_CASES = [
    CanonicalTestCase("software", 2, 5, 9, "schema"),
    CanonicalTestCase("Gene Ontology", 4, 5, 55, "schema"),
    CanonicalTestCase("Python programming language", 2, 14, 9, "schema"),
    CanonicalTestCase("World War II", 7, 6, 45, "schema"),
    CanonicalTestCase("cardiac arrest", 4, 5, 9, "schema"),
]

# Additional test cases (expand to 20+)
ADDITIONAL_CASES = [
    # Medicine domain
    CanonicalTestCase("diabetes mellitus", 4, 1, 29, "additional"),  # Medicine/Definition/Diagnostic
    CanonicalTestCase("myocardial infarction", 4, 5, 9, "additional"),  # Medicine/Entity/Technical

    # Software domain
    CanonicalTestCase("binary search algorithm", 15, 14, 2, "additional"),  # Software/Code Gen/Computational
    CanonicalTestCase("REST API endpoint", 15, 5, 9, "additional"),  # Software/Entity/Technical

    # Science domain
    CanonicalTestCase("photosynthesis", 0, 3, 0, "additional"),  # Science/Causal/Biochemical
    CanonicalTestCase("quantum mechanics", 0, 1, 11, "additional"),  # Science/Definition/Abstract

    # Mathematics
    CanonicalTestCase("Pythagorean theorem", 1, 16, 3, "additional"),  # Math/Proof/Logical
    CanonicalTestCase("prime factorization", 1, 4, 2, "additional"),  # Math/Classification/Computational

    # History
    CanonicalTestCase("French Revolution", 7, 0, 5, "additional"),  # History/Fact/Historical
    CanonicalTestCase("Cold War", 7, 6, 16, "additional"),  # History/Relationship/Temporal

    # Technology
    CanonicalTestCase("machine learning", 2, 1, 2, "additional"),  # Tech/Definition/Computational
    CanonicalTestCase("neural networks", 2, 1, 36, "additional"),  # Tech/Definition/Structural

    # Engineering
    CanonicalTestCase("bridge construction", 3, 19, 36, "additional"),  # Engineering/Spatial/Structural
    CanonicalTestCase("circuit design", 3, 14, 35, "additional"),  # Engineering/Code Gen/Functional

    # Philosophy
    CanonicalTestCase("existentialism", 6, 1, 7, "additional"),  # Philosophy/Definition/Philosophical
    CanonicalTestCase("epistemology", 6, 1, 56, "additional"),  # Philosophy/Definition/Epistemic
]

ALL_TEST_CASES = CANONICAL_CASES + ADDITIONAL_CASES


TMD_PROMPT = """You are a metadata extraction expert. Analyze this concept and assign ONE code from each category.

CONCEPT: "{text}"

=== DOMAINS (choose ONE, 0-15) ===
0=Science, 1=Mathematics, 2=Technology, 3=Engineering, 4=Medicine,
5=Psychology, 6=Philosophy, 7=History, 8=Literature, 9=Art,
10=Economics, 11=Law, 12=Politics, 13=Education, 14=Environment, 15=Software

=== TASKS (choose ONE, 0-31) ===
0=Fact Retrieval, 1=Definition Matching, 2=Analogical Reasoning, 3=Causal Inference,
4=Classification, 5=Entity Recognition, 6=Relationship Extraction, 7=Schema Adherence,
8=Summarization, 9=Paraphrasing, 10=Translation, 11=Sentiment Analysis,
12=Argument Evaluation, 13=Hypothesis Testing, 14=Code Generation, 15=Function Calling,
16=Mathematical Proof, 17=Diagram Interpretation, 18=Temporal Reasoning, 19=Spatial Reasoning,
20=Ethical Evaluation, 21=Policy Recommendation, 22=Roleplay Simulation, 23=Creative Writing,
24=Instruction Following, 25=Error Detection, 26=Output Repair, 27=Question Generation,
28=Conceptual Mapping, 29=Knowledge Distillation, 30=Tool Use, 31=Prompt Completion

=== MODIFIERS (choose ONE, 0-63) ===
0=Biochemical, 1=Evolutionary, 2=Computational, 3=Logical, 4=Ethical,
5=Historical, 6=Legal, 7=Philosophical, 8=Emotional, 9=Technical,
10=Creative, 11=Abstract, 12=Concrete, 13=Visual, 14=Auditory,
15=Spatial, 16=Temporal, 17=Quantitative, 18=Qualitative, 19=Procedural,
20=Declarative, 21=Comparative, 22=Analogical, 23=Causal, 24=Hypothetical,
25=Experimental, 26=Narrative, 27=Descriptive, 28=Prescriptive, 29=Diagnostic,
30=Predictive, 31=Reflective, 32=Strategic, 33=Tactical, 34=Symbolic,
35=Functional, 36=Structural, 37=Semantic, 38=Syntactic, 39=Pragmatic,
40=Normative, 41=Statistical, 42=Probabilistic, 43=Deterministic, 44=Stochastic,
45=Modular, 46=Hierarchical, 47=Distributed, 48=Localized, 49=Global,
50=Contextual, 51=Generalized, 52=Specialized, 53=Interdisciplinary, 54=Multimodal,
55=Ontological, 56=Epistemic, 57=Analog-sensitive, 58=Schema-bound, 59=Role-based,
60=Feedback-driven, 61=Entailment-aware, 62=Alignment-focused, 63=Compression-optimized

CRITICAL INSTRUCTIONS:
1. Analyze the concept semantically
2. Choose the BEST matching codes (not defaults!)
3. Return ONLY THREE NUMBERS separated by commas
4. DO NOT include explanations, domain names, or any other text
5. Example valid output: "15,14,9"
6. Example INVALID output: "Software, 14, Technical" or "Domain: 15..."

OUTPUT ONLY THREE NUMBERS:"""


def extract_tmd_direct(text: str, model: str) -> tuple[Dict, str]:
    """Extract TMD directly via LLM client, return parsed result + raw output."""

    prompt = TMD_PROMPT.format(text=text)

    # Set environment for LLM client
    os.environ["LNSP_LLM_MODEL"] = model

    # Call LLM
    raw_output = call_local_llama_simple(prompt)

    # Parse response
    match = re.search(r'(\d+),\s*(\d+),\s*(\d+)', raw_output)
    if not match:
        return {'domain': 0, 'task': 0, 'modifier': 0, 'parse_error': True}, raw_output

    d, t, m = int(match.group(1)), int(match.group(2)), int(match.group(3))

    # Clamp to valid ranges
    d = min(max(d, 0), 15)
    t = min(max(t, 0), 31)
    m = min(max(m, 0), 63)

    return {'domain': d, 'task': t, 'modifier': m, 'parse_error': False}, raw_output


def benchmark_model(model_name: str, test_cases: List[CanonicalTestCase]):
    """Benchmark a model with proper warm-up and timing."""

    print(f"\n{'='*70}")
    print(f"Testing: {model_name}")
    print(f"{'='*70}")

    # WARM-UP: Run 3 queries, discard timing
    print(f"\nðŸ”¥ Warming up model (3 queries, timing discarded)...")
    warmup_text = "Machine learning is a subset of artificial intelligence"
    for i in range(3):
        _, raw = extract_tmd_direct(warmup_text, model_name)
        print(f"  Warm-up {i+1}/3: raw='{raw[:40]}...'")

    print(f"\nâœ… Warm-up complete. Starting timed benchmark...\n")

    results = []
    latencies = []
    domain_correct = 0
    task_correct = 0
    modifier_correct = 0
    all_correct = 0
    parse_errors = 0

    for i, case in enumerate(test_cases, 1):
        print(f"\nTest {i}/{len(test_cases)}: {case.text}")
        print(f"  Expected: D={case.domain}, T={case.task}, M={case.modifier}")

        # Measure latency
        start = time.time()
        result, raw_output = extract_tmd_direct(case.text, model_name)
        latency_ms = (time.time() - start) * 1000
        latencies.append(latency_ms)

        # Check accuracy
        domain_match = result['domain'] == case.domain
        task_match = result['task'] == case.task
        modifier_match = result['modifier'] == case.modifier

        if domain_match:
            domain_correct += 1
        if task_match:
            task_correct += 1
        if modifier_match:
            modifier_correct += 1
        if domain_match and task_match and modifier_match:
            all_correct += 1
        if result.get('parse_error'):
            parse_errors += 1

        # Print result
        d_sym = "âœ“" if domain_match else "âœ—"
        t_sym = "âœ“" if task_match else "âœ—"
        m_sym = "âœ“" if modifier_match else "âœ—"

        print(f"  Actual:   D={result['domain']} {d_sym}, T={result['task']} {t_sym}, M={result['modifier']} {m_sym}")
        print(f"  Raw output: '{raw_output[:60]}...'")
        print(f"  Latency: {latency_ms:.1f}ms")

        results.append({
            'text': case.text,
            'expected': {'domain': case.domain, 'task': case.task, 'modifier': case.modifier},
            'actual': result,
            'raw_output': raw_output,
            'latency_ms': latency_ms
        })

    return {
        'model': model_name,
        'total_tests': len(test_cases),
        'domain_accuracy': domain_correct / len(test_cases),
        'task_accuracy': task_correct / len(test_cases),
        'modifier_accuracy': modifier_correct / len(test_cases),
        'combined_accuracy': all_correct / len(test_cases),
        'parse_errors': parse_errors,
        'avg_latency_ms': statistics.mean(latencies),
        'min_latency_ms': min(latencies),
        'max_latency_ms': max(latencies),
        'results': results
    }


def print_comparison(results: List[Dict]):
    """Print comparison table."""

    print("\n" + "="*80)
    print("FIXED TMD BENCHMARK RESULTS")
    print("="*80)

    print("\nðŸ“Š ACCURACY (ALL 3 CODES TESTED):")
    print("-" * 80)
    print(f"{'Model':<20} {'Domain':>10} {'Task':>10} {'Modifier':>10} {'All 3':>10} {'Errors':>10}")
    print("-" * 80)

    for r in results:
        print(f"{r['model']:<20} "
              f"{r['domain_accuracy']*100:>8.1f}%  "
              f"{r['task_accuracy']*100:>8.1f}%  "
              f"{r['modifier_accuracy']*100:>8.1f}%  "
              f"{r['combined_accuracy']*100:>8.1f}%  "
              f"{r['parse_errors']:>10}")

    print("\nâš¡ LATENCY (3RD QUERY AFTER WARM-UP):")
    print("-" * 80)
    print(f"{'Model':<20} {'Avg':>12} {'Min':>12} {'Max':>12}")
    print("-" * 80)

    for r in results:
        print(f"{r['model']:<20} "
              f"{r['avg_latency_ms']:>10.1f}ms  "
              f"{r['min_latency_ms']:>10.1f}ms  "
              f"{r['max_latency_ms']:>10.1f}ms")

    if len(results) == 2:
        speedup = results[0]['avg_latency_ms'] / results[1]['avg_latency_ms']
        print(f"\nðŸš€ Speedup: {speedup:.1f}x")

    print("-" * 80)


def main():
    print("ðŸ”¬ FIXED TMD BENCHMARK")
    print("="*80)
    print(f"Test cases: {len(ALL_TEST_CASES)} (5 canonical + {len(ADDITIONAL_CASES)} additional)")
    print(f"Fixes:")
    print("  âœ… All 3 codes validated (domain, task, modifier)")
    print("  âœ… Proper warm-up (3rd query timing only)")
    print("  âœ… Raw LLM output logged")
    print("  âœ… Canonical ground truth from TMD-Schema.md")

    models = ["llama3.1:8b", "tinyllama:1.1b"]

    all_results = []
    for model in models:
        result = benchmark_model(model, ALL_TEST_CASES)
        all_results.append(result)

    print_comparison(all_results)

    # Save results
    output_path = "artifacts/tmd_benchmark_fixed.json"
    with open(output_path, 'w') as f:
        json.dump(all_results, f, indent=2)
    print(f"\nðŸ’¾ Results saved to: {output_path}")


if __name__ == "__main__":
    main()
