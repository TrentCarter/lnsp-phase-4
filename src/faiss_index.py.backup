from __future__ import annotations
    import sys
    import os
    import time
    import json
    import math
    import numpy as np
    import argparse
    from dataclasses import dataclass

    # Add the parent directory to sys.path for direct execution
    if __name__ == "__main__":
        current_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(current_dir)
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)

    try:
        import faiss
    except Exception:  # allow import before faiss is installed
        faiss = None  # type: ignore

    from utils.norms import l2_normalize


    @dataclass
    class IndexMetadata:
        """Metadata container for index information."""
        vectors: int
        trained: bool
        nlist: int
        nprobe: int
        metric: str
        index_type: str
        code_size: int
        build_time: float
        requested_nlist: int | None = None
        max_safe_nlist: int | None = None


    def calculate_nlist(n_vectors: int, requested_nlist: int | None = None) -> int:
        """Calculate safe nlist based on vector count and training requirements.

        Args:
            n_vectors: Number of vectors in the dataset
            requested_nlist: User-requested nlist (optional)

        Returns:
            Safe nlist value that satisfies 40× training rule
        """
        max_safe_nlist = max(1, n_vectors // 40)  # 40× training rule

        if requested_nlist:
            if requested_nlist > max_safe_nlist:
                print(f"[faiss_index] Warning: reducing nlist from {requested_nlist} -> {max_safe_nlist} to satisfy 40× rule")
            return min(requested_nlist, max_safe_nlist)

        # auto-select by scale
        if n_vectors < 8000:
            return max_safe_nlist
        elif n_vectors < 20000:
            return min(200, max_safe_nlist)
        elif n_vectors < 40000:
            return min(512, max_safe_nlist)
        else:
            return min(int(math.sqrt(n_vectors)), max_safe_nlist)


def build_ivf_flat_cosine(vectors: np.ndarray, nlist: int = 256, train_frac: float = 0.05):
    """Build an IVF-Flat index with cosine similarity (IP over L2-normalized).
    Returns (index, trained_bool). If faiss unavailable, returns (None, False).
    """
    if faiss is None:
        return None, False

    assert vectors.ndim == 2, "vectors must be (N, D)"
    vecs = l2_normalize(vectors.astype(np.float32))
    N, D = vecs.shape
    # Build IVF-Flat via index_factory with inner-product metric
    spec = f"IVF{max(8, int(nlist))},Flat"
    index = faiss.index_factory(D, spec, faiss.METRIC_INNER_PRODUCT)

    # training sample
    k = max(1, int(N * train_frac))
    ids = np.random.default_rng(0).choice(N, size=min(k, N), replace=False)
    
    # Training validation: ensure adequate training data
    if N < nlist * 40:
        print(f"[ERROR] Insufficient training vectors: {N} < 40×nlist ({nlist*40})")
        print(f"[HINT] Reduce nlist to ≤ {N//40} or increase training data")
        print(f"[HINT] Current nlist={nlist} requires ≥ {nlist*40} training vectors")
        return None, False
        
    index.train(vecs[ids])
    index.add(vecs)
    # Default nprobe can be overridden via env
    try:
        default_nprobe = int(os.getenv("FAISS_NPROBE", "16"))
    except Exception:
        default_nprobe = 16
    index.nprobe = max(1, default_nprobe)

    # Startup guard: warn if vectors < nlist*4
    if N < nlist * 4:
        recommended_nprobe = max(8, nlist // 8)
        print(f"[WARN] vectors ({N}) < nlist*4 ({nlist*4}). Setting nprobe={recommended_nprobe}")
        index.nprobe = recommended_nprobe
    return index, True


def build_ivf_pq_cosine(vectors: np.ndarray, nlist: int = 256, m: int = 8, nbits: int = 8, train_frac: float = 0.05):
    """Build an IVF-PQ index with cosine similarity.
    Returns (index, trained_bool). If faiss unavailable, returns (None, False).
    """
    if faiss is None:
        return None, False

    assert vectors.ndim == 2, "vectors must be (N, D)"
    vecs = l2_normalize(vectors.astype(np.float32))
    N, D = vecs.shape

    # Training rule of thumb: warn if N < 40*nlist
    if N < nlist * 40:
        print(f"[WARN] Training vectors ({N}) < 40×nlist ({nlist*40}). Consider reducing nlist.")
        print(f"[HINT] Recommended: nlist ≤ {N//40} (or increase training data)")

    # Build IVF-PQ via index_factory with inner-product metric
    spec = f"IVF{max(8, int(nlist))},PQ{m}x{nbits}"
    index = faiss.index_factory(D, spec, faiss.METRIC_INNER_PRODUCT)

    # training sample
    k = max(1, int(N * train_frac))
    ids = np.random.default_rng(0).choice(N, size=min(k, N), replace=False)
    index.train(vecs[ids])
    index.add(vecs)

    # Default nprobe can be overridden via env
    try:
        default_nprobe = int(os.getenv("FAISS_NPROBE", "16"))
    except Exception:
        default_nprobe = 16
    index.nprobe = max(1, default_nprobe)
    return index, True


def search(index, queries: np.ndarray, topk: int = 10):
    if faiss is None or index is None:
        return None
    q = l2_normalize(queries.astype(np.float32))
    scores, ids = index.search(q, topk)
    return scores, ids


def _detect_npz() -> str | None:
    candidates = [
        os.getenv("FAISS_NPZ_PATH"),
        "artifacts/fw10k_vectors.npz",
        "artifacts/fw1k_vectors.npz",
    ]
    for p in candidates:
        if p and os.path.isfile(p):
            return p
    return None


def _save_index_meta(index_path: str, meta: dict):
    """Save index metadata to artifacts/index_meta.json"""
    meta_path = "artifacts/index_meta.json"
    os.makedirs("artifacts", exist_ok=True)

    # Load existing meta if present
    existing_meta = {}
    if os.path.exists(meta_path):
        try:
            with open(meta_path, 'r') as f:
                existing_meta = json.load(f)
        except Exception:
            pass

    # Update with new index info
    existing_meta[index_path] = meta

    with open(meta_path, 'w') as f:
        json.dump(existing_meta, f, indent=2)
    print(f"[faiss_index] Updated metadata in {meta_path}")


def main() -> int:
    parser = argparse.ArgumentParser(description="Build and save FAISS index from NPZ vectors")
    parser.add_argument("--npz", type=str, default=None, help="Path to NPZ file containing 'vectors'")
    parser.add_argument("--metric", type=str, default="ip", choices=["ip", "l2"], help="Distance metric (ip=inner product, l2=euclidean)")
    parser.add_argument("--nlist", type=int, default=512, help="Number of lists (nlist) for IVF")
    parser.add_argument("--nprobe", type=int, default=8, help="Number of probes for search")
    parser.add_argument("--type", type=str, default="ivf_flat", choices=["ivf_flat", "ivf_pq"], help="Index type")
    parser.add_argument("--pq-m", type=int, default=8, help="PQ sub-dimensions (for IVF_PQ)")
    parser.add_argument("--pq-nbits", type=int, default=8, help="PQ bits per sub-dimension (for IVF_PQ)")
    parser.add_argument("--out", type=str, default=None, help="Output index path (.index)")
    args = parser.parse_args()

    npz_path = args.npz or _detect_npz()
    if not npz_path:
        print("[faiss_index] No NPZ file found. Use --npz to specify path.")
        return 2
    if not os.path.isfile(npz_path):
        print(f"[faiss_index] NPZ file not found: {npz_path}")
        return 2

    try:
        npz = np.load(npz_path)
        if "vectors" not in npz.files:
            print(f"[faiss_index] NPZ missing 'vectors' key: {npz_path}")
            return 2
        vectors = npz["vectors"].astype(np.float32)
        if vectors.ndim != 2 or vectors.shape[0] == 0:
            print(f"[faiss_index] No vectors to index in {npz_path}")
            return 1
    except Exception as exc:
        print(f"[faiss_index] Error loading NPZ: {exc}")
        return 2

    if faiss is None:
        print("[faiss_index] FAISS not available; please install faiss-cpu.")
        return 2

    N, D = vectors.shape
    print(f"[faiss_index] Building {args.type} index with {N} vectors of dim {D}")
    print(f"[faiss_index] Metric: {args.metric}, nlist: {args.nlist}, nprobe: {args.nprobe}")

    start_time = time.time()
    if args.type == "ivf_pq":
        index, trained = build_ivf_pq_cosine(
            vectors, nlist=args.nlist, m=args.pq_m, nbits=args.pq_nbits
        )
    else:
        index, trained = build_ivf_flat_cosine(vectors, nlist=args.nlist)

    if not trained or index is None:
        print("[faiss_index] Failed to build index")
        return 2

    build_time = time.time() - start_time
    index.nprobe = args.nprobe

    # Calculate index size
    import tempfile
    with tempfile.NamedTemporaryFile(suffix='.index', delete=False) as tmp:
        faiss.write_index(index, tmp.name)
        index_size = os.path.getsize(tmp.name)
        os.unlink(tmp.name)

    out_path = args.out or (f"artifacts/fw10k_{args.type}_{args.metric}.index")
    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
    faiss.write_index(index, out_path)

    # Save metadata
    meta = {
        "vectors": int(N),
        "trained": bool(trained),
        "nlist": int(args.nlist),
        "nprobe": int(args.nprobe),
        "metric": str(args.metric),
        "index_type": str(args.type),
        "code_size": index_size,
        "build_time": round(build_time, 2),
        "pq_m": getattr(args, 'pq_m', None),
        "pq_nbits": getattr(args, 'pq_nbits', None)
    }
    _save_index_meta(out_path, meta)

    print(f"[faiss_index] Wrote index to {out_path}")
    print(f"[faiss_index] Index size: {index_size} bytes, build time: {build_time:.2f}s")
    print(f"[faiss_index] Metadata saved to artifacts/index_meta.json")
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
