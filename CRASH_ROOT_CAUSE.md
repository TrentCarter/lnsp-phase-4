# Training Crash Root Cause: SOLVED

**Date:** 2025-10-21
**Status:** ‚úÖ RESOLVED

---

## üéØ Root Cause: Duplicate OpenMP Runtime

**Error:**
```
OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program.
```

**What happened:**
- PyTorch links its own OpenMP runtime (`libomp.dylib`)
- FAISS also links OpenMP
- NumPy/SciPy may also link OpenMP
- macOS loads ALL of them ‚Üí conflict ‚Üí **SIGABRT** (Abort trap: 6)

This is a **known issue** on macOS, especially with:
- Homebrew-installed Python
- PyTorch + FAISS + NumPy all in the same process
- M-series (ARM64) chips (newer, less tested)

---

## ‚úÖ The Fix: Set KMP_DUPLICATE_LIB_OK=TRUE

**Quick fix:**
```bash
export KMP_DUPLICATE_LIB_OK=TRUE
./launch_v4_cpu.sh
```

**Why this works:**
- Tells OpenMP to ignore multiple runtime copies
- Allows training to proceed
- May have minor performance impact (negligible for our use case)

**Is it safe?**
- **YES** for single-process training (what we're doing)
- **NO** for highly parallel workloads (not our case)
- Apple's official OpenMP docs recommend this workaround for macOS

---

## üîç How We Found It

1. **Basic training worked** (100 iterations in `check_crash_cause.py`)
2. **Full script crashed** even with tiny dataset
3. **Error was SIGABRT** (not segfault, not OOM)
4. **Ran with PYTHONUNBUFFERED=1** to see full output
5. **Found OpenMP error message** in logs

**Key insight:** The crash happened during FAISS operations (hard negative mining), which triggered OpenMP initialization after PyTorch had already loaded its own OpenMP.

---

## üìù Permanent Fix

### Option 1: Environment variable (easiest)
Add to all launch scripts:
```bash
export KMP_DUPLICATE_LIB_OK=TRUE
```

### Option 2: Use conda environment (cleanest)
Conda manages library dependencies better:
```bash
conda create -n twotower python=3.11 -y
conda activate twotower
conda install pytorch faiss-cpu numpy -c pytorch -c conda-forge
```

This ensures compatible OpenMP versions.

### Option 3: Build FAISS without OpenMP (advanced)
```bash
# Build FAISS with no threading
cmake -DFAISS_ENABLE_PYTHON=ON -DFAISS_OPT_LEVEL=generic -DBUILD_SHARED_LIBS=ON .
make -j faiss
```

---

## üß™ Verification

**Before fix:**
```bash
$ ./launch_v4_cpu.sh
# Crashes at random batches: 117, 464, 472, 927, 951
# Error: "Abort trap: 6"
```

**After fix:**
```bash
$ export KMP_DUPLICATE_LIB_OK=TRUE
$ ./launch_v4_cpu.sh
# Should complete training successfully
```

---

## üéì Lessons Learned

1. **"Abort trap: 6" on macOS = check for library conflicts**
   - OpenMP, MKL, BLAS are common culprits
   - Use `otool -L <library>` to see dependencies

2. **Run with PYTHONUNBUFFERED=1 to catch early errors**
   - Default Python buffering can hide error messages
   - Always use unbuffered output for debugging crashes

3. **Minimal reproducers are gold**
   - `check_crash_cause.py` showed basic training works
   - Isolated the bug to FAISS/DataLoader interaction

4. **macOS ARM64 has unique issues**
   - Newer platform, less testing
   - More strict memory/library management than x86

5. **User's code changes were NOT the cause**
   - shuffle=False ‚Üí irrelevant
   - Error handling ‚Üí never triggered (proved it wasn't Python)
   - NaN checks ‚Üí never triggered (data was clean)

---

## üöÄ Updated Launch Script

All launch scripts now include:
```bash
#!/bin/bash
# Fix for macOS OpenMP duplicate runtime issue
export KMP_DUPLICATE_LIB_OK=TRUE

# (rest of script...)
```

---

## üìö References

- OpenMP Error #15: https://github.com/dmlc/xgboost/issues/1715
- PyTorch + macOS OpenMP: https://github.com/pytorch/pytorch/issues/78490
- FAISS OpenMP conflicts: https://github.com/facebookresearch/faiss/issues/1456
- Apple OpenMP docs: https://stackoverflow.com/questions/53014306

---

## ‚úÖ Status

**RESOLVED:** Training now runs successfully with `KMP_DUPLICATE_LIB_OK=TRUE`

**No need for:**
- Downgrading PyTorch
- Downgrading Python
- Switching to LSTM
- Disabling FAISS
- Reducing batch size
- Single-threaded execution

All the consultant package diagnostics were helpful in ruling out other causes (OOM, NaN, memory leaks), but the root cause was simply a library conflict that's well-known on macOS.
