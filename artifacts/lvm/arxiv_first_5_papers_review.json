[
  {
    "paper_id": "2510.27688v1",
    "total_chunks": 358,
    "index_range": {
      "start": 0,
      "end": 357
    },
    "chunks": [
      {
        "chunk_index": 0,
        "relative_chunk_number": 1,
        "text": "Preprint CONTINUOUS AUTOREGRESSIVE LANGUAGE MODELS Chenze Shao1, Darren Li1,2, Fandong Meng1\u2217, Jie Zhou1 1WeChat AI, Tencent Inc 2Qiuzhen College, Tsinghua University ABSTRACT The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process.",
        "text_length": 303,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.00955979898571968,
          0.006246630102396011,
          -0.012744622305035591,
          0.0033791903406381607,
          -0.04504238814115524,
          0.019135337322950363,
          0.018675003200769424,
          0.015026208944618702,
          -0.02380538545548916,
          0.00364682893268764
        ]
      },
      {
        "chunk_index": 1,
        "relative_chunk_number": 2,
        "text": "We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autore- gressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction.",
        "text_length": 312,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006402336526662111,
          0.017903052270412445,
          -0.03831738606095314,
          0.0018948258366435766,
          0.00537129957228899,
          -0.01547605823725462,
          0.019680410623550415,
          -0.003163429908454418,
          -0.013123326934874058,
          -0.020183218643069267
        ]
      },
      {
        "chunk_index": 2,
        "relative_chunk_number": 3,
        "text": "CALM uses a high-fidelity au- toencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9% accuracy. This allows us to model language as a sequence of continuous vectors instead of dis- crete tokens, which reduces the number of generative steps by a factor of K.",
        "text_length": 344,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.015687687322497368,
          0.014432074502110481,
          -0.05110127478837967,
          0.023221513256430626,
          -0.04897551238536835,
          0.028437025845050812,
          0.021054740995168686,
          -0.02355608157813549,
          -0.02640620432794094,
          0.0063614314422011375
        ]
      },
      {
        "chunk_index": 3,
        "relative_chunk_number": 4,
        "text": "The paradigm shift necessitates a new modeling toolkit; therefore, we develop a com- prehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain.",
        "text_length": 215,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0031109775882214308,
          0.013992034830152988,
          0.0034478059969842434,
          0.038138072937726974,
          0.011733273044228554,
          -0.04614199325442314,
          0.019339699298143387,
          -0.00670252088457346,
          -0.025480743497610092,
          -0.005146066192537546
        ]
      },
      {
        "chunk_index": 4,
        "relative_chunk_number": 5,
        "text": "Experiments show that CALM significantly improves the performance-compute trade-off, achieving the perfor- mance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models.",
        "text_length": 328,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010308596305549145,
          0.01787787862122059,
          -0.054141078144311905,
          0.008684657514095306,
          -0.03790641203522682,
          -0.0026148289907723665,
          0.01793377846479416,
          -0.026532620191574097,
          -0.01718844287097454,
          -0.04713165760040283
        ]
      },
      {
        "chunk_index": 5,
        "relative_chunk_number": 6,
        "text": "Code: https://github.com/shaochenze/calm Project: https://shaochenze.github.io/blog/2025/CALM 1 INTRODUCTION Large Language Models (LLMs) have revolutionized the field of artificial intelligence, demonstrat- ing unprecedented capabilities in understanding, generating, and reasoning with human language (Achiam et al., 2023; Google, 2025; DeepSeek-AI, 2025).",
        "text_length": 358,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04162343963980675,
          -0.01090223528444767,
          -0.021371014416217804,
          0.0035198722034692764,
          -0.011921600438654423,
          0.00030893285293132067,
          0.015187867917120457,
          0.023713620379567146,
          -0.019722873345017433,
          -0.04260971024632454
        ]
      },
      {
        "chunk_index": 6,
        "relative_chunk_number": 7,
        "text": "However, this remarkable success is shadowed by a critical challenge: their immense computational demands. The training and in- ference of state-of-the-art LLMs demand massive computational resources, leading to prohibitive expenses and significant environmental concerns (Strubell et al., 2019; Bender et al., 2021).",
        "text_length": 317,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03493165224790573,
          0.010200711898505688,
          -0.01632244884967804,
          -0.0005620777956210077,
          -0.008069142699241638,
          0.01608904078602791,
          0.019796764478087425,
          0.05067269876599312,
          -0.028181051835417747,
          -0.057059019804000854
        ]
      },
      {
        "chunk_index": 7,
        "relative_chunk_number": 8,
        "text": "At the heart of this inefficiency lies the foundational paradigm of these models: an autoregressive genera- tion process that operates on a sequence of discrete tokens.",
        "text_length": 168,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.042031485587358475,
          0.03187863901257515,
          0.010267561301589012,
          0.026133261620998383,
          -0.04457658901810646,
          -0.051845576614141464,
          0.022625911980867386,
          -0.03551173955202103,
          -0.02959533780813217,
          0.058686744421720505
        ]
      },
      {
        "chunk_index": 8,
        "relative_chunk_number": 9,
        "text": "Because the computational cost scales with the length of the sequence, generating long-form text or processing extensive contexts remains a fundamental bottleneck, limiting the scalability and accessibility of these powerful models. The now-ubiquitous use of discrete tokens in LLMs is the result of a pivotal evolution from ear- lier modeling paradigms.",
        "text_length": 354,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.023423710837960243,
          0.0005758597981184721,
          -0.029380828142166138,
          -0.0030769880395382643,
          0.014562567695975304,
          -0.02431073598563671,
          0.021405158564448357,
          0.011293760500848293,
          -0.012594783678650856,
          -0.03615875542163849
        ]
      },
      {
        "chunk_index": 9,
        "relative_chunk_number": 10,
        "text": "Initially, models that operated at the character level struggled with the computational burden of extremely long sequences (Sutskever et al., 2011; Kim et al., 2016). The subsequent shift to modern subword tokenization (Sennrich et al., 2016) was driven by a crucial in- sight: increasing the information density of each text unit reduces sequence length and dramatically boosts model efficiency.",
        "text_length": 396,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.007881578989326954,
          0.011186293326318264,
          -0.03719822317361832,
          0.024083398282527924,
          0.048904385417699814,
          0.014378007501363754,
          0.020244402810931206,
          -0.020017489790916443,
          -0.02370487153530121,
          0.01664412021636963
        ]
      },
      {
        "chunk_index": 10,
        "relative_chunk_number": 11,
        "text": "This historical success suggests a clear path for unlocking the next order of magnitude in efficiency: continue to increase the semantic bandwidth of each predictive unit. We argue, however, that this path has reached a fundamental limit, constrained by the very nature of discrete representation.",
        "text_length": 297,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00804976187646389,
          0.03372422978281975,
          -0.04214506223797798,
          0.03949269652366638,
          -0.0698297768831253,
          -0.04379845783114433,
          0.018176084384322166,
          -0.017577318474650383,
          -0.03232991322875023,
          0.006457436829805374
        ]
      },
      {
        "chunk_index": 11,
        "relative_chunk_number": 12,
        "text": "With typical vocabularies in modern LLMs ranging from approximately 32,000 to 256,000 entries, each token carries a surprisingly small amount of information\u2014merely 15 to 18 bits (e.g., log2(32768) = 15).",
        "text_length": 203,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.054830215871334076,
          0.018256627023220062,
          -0.07292918860912323,
          -0.0008617221028544009,
          -0.007475285325199366,
          0.03875267878174782,
          0.019480742514133453,
          0.015534812584519386,
          -0.022993341088294983,
          -0.023809755221009254
        ]
      },
      {
        "chunk_index": 12,
        "relative_chunk_number": 13,
        "text": "To increase this capacity\u2014for instance, to represent a whole phrase\u2014the vocabulary size would need to grow exponentially, making the final softmax computa- tion over this vocabulary an untenable bottleneck. This reveals a critical limitation: the information \u2217Corresponding author.",
        "text_length": 281,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02267174981534481,
          0.008350490592420101,
          -0.024099666625261307,
          0.04080966114997864,
          -0.03592396154999733,
          -0.03434854745864868,
          0.01760704256594181,
          0.015858162194490433,
          -0.023431992158293724,
          -0.03012818843126297
        ]
      },
      {
        "chunk_index": 13,
        "relative_chunk_number": 14,
        "text": "1 arXiv:2510.27688v1 [cs.CL] 31 Oct 2025 Preprint Conventional LM: Next-Token Prediction The cat sat on the mat Sequence Length = T CALM: Next-Vector Prediction Vector 1 Vector 2 Sequence Length = T/K Autoencoder (K=3 tokens to 1 vector) Figure 1: Comparison between conventional token-by-token generation and our proposed vector-by- vector framework (CALM).",
        "text_length": 358,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.017166493460536003,
          0.023828232660889626,
          -0.04868331179022789,
          0.010660799220204353,
          0.01639772579073906,
          0.07582688331604004,
          0.023181527853012085,
          0.010678750462830067,
          -0.02182222716510296,
          -0.05216362699866295
        ]
      },
      {
        "chunk_index": 14,
        "relative_chunk_number": 15,
        "text": "By compressing K tokens into a single vector, we reduce the sequence length K-fold, fundamentally improving computational efficiency. density of discrete tokens is not scalable. Consequently, a profound mismatch has emerged: while model capacity has scaled to unprecedented levels, the task itself\u2014predicting low-information dis- crete units one at a time\u2014has not evolved.",
        "text_length": 372,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.005179575644433498,
          0.014026429504156113,
          -0.018212325870990753,
          0.044593121856451035,
          -0.02640449069440365,
          -0.03227464109659195,
          0.01950644701719284,
          -0.056780215352773666,
          -0.03088044561445713,
          0.01258080918341875
        ]
      },
      {
        "chunk_index": 15,
        "relative_chunk_number": 16,
        "text": "We are now deploying models of immense representa- tional power on a task that fundamentally limits their throughput, forcing them to laboriously predict simple, low-information tokens one by one. In this work, we confront this limitation directly by introducing a paradigm shift from discrete to- kens to a continuous-domain representation.",
        "text_length": 341,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02413303405046463,
          0.033824142068624496,
          -0.024090031161904335,
          0.030867399647831917,
          -0.023053539916872978,
          -0.06601797044277191,
          0.02085983008146286,
          -0.01767691969871521,
          -0.030629470944404602,
          0.01641751080751419
        ]
      },
      {
        "chunk_index": 16,
        "relative_chunk_number": 17,
        "text": "Central to our approach is an autoencoder trained to compress a chunk of K tokens into a single, dense continuous vector and, crucially, reconstruct the original tokens from this vector with high fidelity.",
        "text_length": 205,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.015316350385546684,
          0.01588704064488411,
          -0.03434429317712784,
          0.0421629399061203,
          -0.09063902497291565,
          -0.008586753159761429,
          0.021958043798804283,
          -0.03106733411550522,
          -0.04365084320306778,
          0.02877155691385269
        ]
      },
      {
        "chunk_index": 17,
        "relative_chunk_number": 18,
        "text": "Unlike the discrete paradigm, where increasing information density requires an exponential growth in vocabulary size, our continuous representa- tion offers a scalable path forward: the vector\u2019s information capacity can be gracefully expanded by simply increasing its dimensionality to accommodate a larger K. This design directly reduces the number of autoregressive steps by a factor of K.",
        "text_length": 391,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01595388725399971,
          0.025615641847252846,
          -0.03520921617746353,
          0.030112173408269882,
          -0.025445913895964622,
          -0.05148061737418175,
          0.01812620647251606,
          -0.015275504440069199,
          -0.022375913336873055,
          -0.021962741389870644
        ]
      },
      {
        "chunk_index": 18,
        "relative_chunk_number": 19,
        "text": "Ultimately, it allows us to reframe language modeling from a task of next-token prediction on discrete token sequences to next-vector prediction on continuous vector sequences, as conceptually illustrated in Figure 1.",
        "text_length": 217,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.022798599675297737,
          0.040216416120529175,
          -0.03337687999010086,
          0.042312026023864746,
          -0.00434905057772994,
          -0.003999414853751659,
          0.02249675802886486,
          -0.036693423986434937,
          -0.041862357407808304,
          0.022883709520101547
        ]
      },
      {
        "chunk_index": 19,
        "relative_chunk_number": 20,
        "text": "However, shifting to the continuous domain introduces a significant challenge: without a finite vo- cabulary, a model cannot compute an explicit probability distribution over all possible outcomes using a standard softmax layer. To address this, we develop a comprehensive, likelihood-free frame- work for our Continuous Autoregressive Language Models (CALM).",
        "text_length": 359,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.005404832307249308,
          0.02814008854329586,
          -0.02826845645904541,
          0.028001602739095688,
          -0.049199968576431274,
          -0.036487266421318054,
          0.02243865095078945,
          0.02840050682425499,
          -0.01634678803384304,
          -0.011130326427519321
        ]
      },
      {
        "chunk_index": 20,
        "relative_chunk_number": 21,
        "text": "Our primary contributions, which structure the remainder of this paper, are as follows: \u2022 A Powerful and Lightweight Autoencoder (Section 2): We first introduce an efficient autoencoder architecture designed to produce robust vector representations.",
        "text_length": 249,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010437674820423126,
          0.04261656850576401,
          -0.020687205716967583,
          0.028311805799603462,
          -0.12506558001041412,
          -0.0254626777023077,
          0.016874367371201515,
          -0.006284903269261122,
          -0.042174000293016434,
          -0.04159175604581833
        ]
      },
      {
        "chunk_index": 21,
        "relative_chunk_number": 22,
        "text": "We demonstrate that this model can be both compact and powerful, ensuring high-fidelity reconstruction of the original tokens, which is a prerequisite for the downstream language modeling task.",
        "text_length": 193,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0018653121078386903,
          0.04229012504220009,
          -0.014624368399381638,
          0.03532438725233078,
          -0.013592228293418884,
          -0.021057989448308945,
          0.02019502781331539,
          -0.019288554787635803,
          -0.03717022389173508,
          0.04188654571771622
        ]
      },
      {
        "chunk_index": 22,
        "relative_chunk_number": 23,
        "text": "\u2022 Likelihood-Free Language Modeling (Section 3): To perform generative modeling in the continuous vector space, we employ a lightweight generative head that conditions on the last hidden state to generate the output vector.",
        "text_length": 223,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.002120847813785076,
          0.015843870118260384,
          0.02005220390856266,
          0.056447748094797134,
          -0.015458602458238602,
          -0.025499192997813225,
          0.019643085077404976,
          -0.04348008334636688,
          -0.03672840818762779,
          0.03838268667459488
        ]
      },
      {
        "chunk_index": 23,
        "relative_chunk_number": 24,
        "text": "While the generative head can be any continuous generative model, options like Diffusion (Ho et al., 2020; Li et al., 2024) or Flow Match- ing (Lipman et al., 2023) rely on an iterative sampling process, re-introducing a significant inference bottleneck.",
        "text_length": 254,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006886992137879133,
          -0.009392079897224903,
          -0.0039481353014707565,
          0.03957570344209671,
          -0.027625061571598053,
          -0.02826976589858532,
          0.018244797363877296,
          -0.0248127281665802,
          -0.027771884575486183,
          -0.03082095831632614
        ]
      },
      {
        "chunk_index": 24,
        "relative_chunk_number": 25,
        "text": "Our framework therefore specifically adopts the Energy Transformer (Shao et al., 2025b), a recent architecture designed for efficient, single-step generation of continuous vectors, while empirically demonstrating superior generation quality. \u2022 Likelihood-Free LM Evaluation (Section 4): The absence of explicit likelihoods makes traditional metrics like Perplexity inapplicable.",
        "text_length": 378,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0011185490293428302,
          0.031554918736219406,
          -0.01126279029995203,
          0.02578381821513176,
          -0.03181840851902962,
          -0.05298878997564316,
          0.01961919479072094,
          -0.011043637059628963,
          -0.029059721156954765,
          -0.03615402802824974
        ]
      },
      {
        "chunk_index": 25,
        "relative_chunk_number": 26,
        "text": "We address this by proposing BrierLM, a novel metric for language modeling based on the Brier score (Brier, 1950). We show that BrierLM is strictly proper, theoretically ensuring a fair comparison of model capabilities. Crucially, BrierLM can be estimated unbiasedly by only drawing samples from the model, making it perfectly suited for CALM where likelihoods are intractable.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009082743898034096,
          0.014691034331917763,
          -0.043462734669446945,
          0.0191180557012558,
          -0.031926319003105164,
          0.013843018561601639,
          0.01989881694316864,
          -0.0008402590174227953,
          -0.013763443566858768,
          -0.015955956652760506
        ]
      },
      {
        "chunk_index": 26,
        "relative_chunk_number": 27,
        "text": "2 Preprint \u2022 Likelihood-Free Temperature Sampling (Section 5): Controlled generation via temper- ature sampling is an indispensable feature of modern LLMs, yet it relies on the explicit manipulation of a probability distribution.",
        "text_length": 229,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004092790186405182,
          0.016420723870396614,
          -0.009295688942074776,
          0.02938983030617237,
          0.051429830491542816,
          0.01007006037980318,
          0.01956804096698761,
          -0.010792267508804798,
          -0.033895161002874374,
          0.014248632825911045
        ]
      },
      {
        "chunk_index": 27,
        "relative_chunk_number": 28,
        "text": "We introduce a principled, likelihood-free sam- pling algorithm that can, in theory, draw samples from the exact temperature distribution, and we accompany it with a highly efficient batch approximation. We empirically validate our CALM framework on standard language modeling benchmarks, which demonstrates a superior performance-compute trade-off.",
        "text_length": 349,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.015675334259867668,
          0.023779338225722313,
          -0.02682688646018505,
          0.022439828142523766,
          -0.007148508448153734,
          -0.0013774876715615392,
          0.019508618861436844,
          0.004882865119725466,
          -0.02216634340584278,
          -0.00740470876917243
        ]
      },
      {
        "chunk_index": 28,
        "relative_chunk_number": 29,
        "text": "For instance, a CALM grouping K=4 to- kens delivers performance comparable to strong discrete baselines, but at a significantly lower com- putational cost. This findings highlight a new design axis for language models: rather than solely scaling parameters and data for performance, one can now scale the information capacity of each step as a powerful new lever for computational efficiency.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.001466033747419715,
          0.014654717408120632,
          -0.060944970697164536,
          0.021965354681015015,
          -0.01308092288672924,
          -0.024264950305223465,
          0.020631518214941025,
          -0.04889683797955513,
          -0.01877020299434662,
          0.020424900576472282
        ]
      },
      {
        "chunk_index": 29,
        "relative_chunk_number": 30,
        "text": "2 AUTOENCODER 2.1 HIGH-FIDELITY RECONSTRUCTION The foundational component of our CALM framework is an autoencoder tasked with learning a bijective mapping between a chunk of K discrete tokens and a continuous vector. Formally, we seek an encoder fenc : VK \u2192Rl and a decoder gdec : Rl \u2192VK, where V is the vocabulary, such that for a given token sequence x1:K = (x1, . . .",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.017181841656565666,
          0.007953434251248837,
          -0.024308590218424797,
          0.0184651967138052,
          -0.04573019966483116,
          -0.02183060348033905,
          0.02041633240878582,
          -0.007809553295373917,
          -0.025929290801286697,
          0.022265082225203514
        ]
      },
      {
        "chunk_index": 30,
        "relative_chunk_number": 31,
        "text": ", xK), the reconstruction gdec(fenc(x1:K)) closely approximates x1:K. For simplicity and computational efficiency, we design our autoencoder to be context-free, meaning it processes each token chunk independently of its surrounding sequence.",
        "text_length": 241,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01227137167006731,
          0.01789906620979309,
          -0.04637261480093002,
          0.005260842386633158,
          -0.049615539610385895,
          -0.06180635467171669,
          0.022998083382844925,
          -0.017613768577575684,
          -0.04093632102012634,
          0.043586403131484985
        ]
      },
      {
        "chunk_index": 31,
        "relative_chunk_number": 32,
        "text": "A context-aware autoencoder that also conditions on previous vector representations is a natural and promising next step, which we leave for future exploration. The encoder begins by mapping the input sequence x1:K to K embeddings. Each embedding is independently processed by a position-wise feed-forward network (FFN).",
        "text_length": 320,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.001966756070032716,
          0.03346582129597664,
          -0.00896406453102827,
          0.04375358298420906,
          -0.03812867030501366,
          -0.011250223964452744,
          0.02089463174343109,
          -0.01399889774620533,
          -0.0357799269258976,
          0.05496668070554733
        ]
      },
      {
        "chunk_index": 32,
        "relative_chunk_number": 33,
        "text": "The resulting K hidden states are then flattened and compressed by a linear layer: RKd \u2192Rd. This unified representation is passed through a second FFN and a linear projection to produce the l-dimensional latent vector z. The decoder architecture mirrors the encoder.",
        "text_length": 266,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.017097920179367065,
          0.04079572856426239,
          -0.008917546831071377,
          0.03791720047593117,
          0.002722901990637183,
          -0.016720417886972427,
          0.018833892419934273,
          -0.016471222043037415,
          -0.02940405160188675,
          0.0021381834521889687
        ]
      },
      {
        "chunk_index": 33,
        "relative_chunk_number": 34,
        "text": "It first transforms z using a linear layer and an FFN to obtain a d-dimensional hidden state, which is then expanded by another linear layer to dimension Kd and reshaped into a sequence of K hidden states. Each of these states is passed through a second FFN, followed by a projection to vocabulary logits using the tied input embedding matrix.",
        "text_length": 343,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03204839304089546,
          0.012774580158293247,
          -0.007520079147070646,
          0.037993013858795166,
          0.01661967672407627,
          -0.009126132354140282,
          0.020835157483816147,
          -0.011728379875421524,
          -0.03280728682875633,
          -0.022180093452334404
        ]
      },
      {
        "chunk_index": 34,
        "relative_chunk_number": 35,
        "text": "Finally, the tokens are reconstructed by applying an argmax operation over these logits. The autoencoder is trained to minimize the reconstruction error by optimizing the standard cross- entropy loss across all K token positions: Lae(x1:K) = \u2212 K X i=1 log pdec(xi|z = fenc(x1:K)). (1) We empirically validate this architecture and find it to be both highly effective and efficient.",
        "text_length": 381,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03344563767313957,
          0.04190300777554512,
          -0.04299342259764671,
          -0.01042777020484209,
          -0.07296456396579742,
          0.04448365420103073,
          0.021443037316203117,
          -0.01471131294965744,
          -0.0293990857899189,
          0.021572716534137726
        ]
      },
      {
        "chunk_index": 35,
        "relative_chunk_number": 36,
        "text": "For instance, when grouping K = 4 tokens, a latent vector of just l = 10 dimensions is sufficient to achieve high-fidelity reconstruction, with a token-level accuracy of over 99.9%. Moreover, the au- toencoder is exceptionally lightweight; with a shallow architecture and a modest hidden dimension of d = 512, its computational overhead is nearly negligible compared to that of language model.",
        "text_length": 393,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00877522211521864,
          0.02116173505783081,
          -0.04905601218342781,
          0.005565328989177942,
          -0.003170412965118885,
          0.020837418735027313,
          0.020988089963793755,
          -0.016959814354777336,
          -0.026472801342606544,
          0.014217141084372997
        ]
      },
      {
        "chunk_index": 36,
        "relative_chunk_number": 37,
        "text": "2.2 ROBUST VECTOR REPRESENTATION While the autoencoder described above achieves near-perfect reconstruction, we found that it is practically impossible to effectively train a continuous language model based on the vector space it produces. The root cause of this challenge is that an autoencoder optimized solely for reconstruction learns an exceptionally brittle representation.",
        "text_length": 379,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.02486605942249298,
          0.03990257531404495,
          -0.03964295610785484,
          0.021145835518836975,
          -0.09095904976129532,
          -0.027820151299238205,
          0.022357089444994926,
          0.040139321237802505,
          -0.037161972373723984,
          0.03927361220121384
        ]
      },
      {
        "chunk_index": 37,
        "relative_chunk_number": 38,
        "text": "Lacking any incentive to form a smooth latent man- ifold, the encoder learns to pack information with maximum efficiency, creating a highly irregular mapping. In such a space, a minor perturbation to a latent vector z\u2014such as the small, inevitable errors made by a generative model can cause the decoder to reconstruct a completely unrelated token sequence.",
        "text_length": 357,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.002009246964007616,
          0.03351650759577751,
          -0.00033718408667482436,
          0.027507362887263298,
          0.0042680478654801846,
          -0.02579142153263092,
          0.01806717924773693,
          -0.023309141397476196,
          -0.026042966172099113,
          -0.01318300049751997
        ]
      },
      {
        "chunk_index": 38,
        "relative_chunk_number": 39,
        "text": "Therefore, for our CALM framework to be viable, the autoencoder must satisfy another critical objective: its vector representation should be robust. 3 Preprint Variational Regularization.",
        "text_length": 187,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0047162799164652824,
          0.04237410053610802,
          -0.02983447164297104,
          0.02872377075254917,
          -0.03349226340651512,
          0.025952428579330444,
          0.02363552525639534,
          0.01353655755519867,
          -0.035043373703956604,
          -0.08809326589107513
        ]
      },
      {
        "chunk_index": 39,
        "relative_chunk_number": 40,
        "text": "To build a robust latent space, our primary strategy is to smooth the la- tent manifold by moving from a deterministic autoencoder to a variational one (Kingma & Welling, 2014), aligning our approach with prominent generative models (Rombach et al., 2022; Liu et al., 2023) that operate within a smooth and structured latent space.",
        "text_length": 331,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.011491675861179829,
          0.015992069616913795,
          -0.01302005909383297,
          0.017550308257341385,
          -0.0198273416608572,
          -0.023725369945168495,
          0.018117431551218033,
          -0.011728334240615368,
          -0.031208215281367302,
          -0.005051799118518829
        ]
      },
      {
        "chunk_index": 40,
        "relative_chunk_number": 41,
        "text": "Instead of mapping an input chunk directly to a vector z, the encoder now outputs the parameters of a diagonal Gaussian distribution, \u00b5 and \u03c3, from which the latent vector is sampled: z \u223cN(\u00b5, \u03c32I). This change is accompanied by a new objective term, a KL divergence loss that penalizes the deviation of the encoded distribu- tion from a standard normal prior, N(0, I).",
        "text_length": 368,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.026986222714185715,
          0.029263969510793686,
          -0.013060519471764565,
          0.04581647366285324,
          -0.030356986448168755,
          0.002455133246257901,
          0.017362669110298157,
          -0.0389702133834362,
          -0.03549003601074219,
          -0.03946049138903618
        ]
      },
      {
        "chunk_index": 41,
        "relative_chunk_number": 42,
        "text": "The total loss function is thus a weighted sum of the reconstruction and regularization terms: Ltotal = Lae + \u03b2 \u00b7 LKL, (2) where \u03b2 is a hyperparameter balancing the two objectives (we set \u03b2 = 0.001), and LKL is the KL divergence, defined as: LKL(pE(z|x1:K)\u2225N(0, I)) = \u22121 2 l X i=1 (1 + log \u03c32 i \u2212\u03c32 i \u2212\u00b52 i ).",
        "text_length": 309,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.005196206737309694,
          0.013811367563903332,
          -0.007579218130558729,
          -0.014993102289736271,
          -0.02055951952934265,
          -0.012510321103036404,
          0.01651832088828087,
          -0.0031401750165969133,
          -0.015203871764242649,
          -0.009073902852833271
        ]
      },
      {
        "chunk_index": 42,
        "relative_chunk_number": 43,
        "text": "(3) This variational objective discourages the encoder from relying on arbitrarily precise or large- magnitude values in z, thereby promoting a smoother and more regularized latent manifold that is more amenable to generative modeling. Preventing Posterior Collapse. A significant challenge in training VAEs is posterior collapse.",
        "text_length": 330,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.018432004377245903,
          0.035754453390836716,
          -0.007268989458680153,
          0.0454634390771389,
          -0.04463774338364601,
          -0.010271981358528137,
          0.020917821675539017,
          0.016277363523840904,
          -0.020192639902234077,
          -0.08359786868095398
        ]
      },
      {
        "chunk_index": 43,
        "relative_chunk_number": 44,
        "text": "This issue manifested in our model as a tendency for some latent dimensions to fully collapse to the standard normal prior. While collapsing a dimension drives its KL divergence to zero, it renders that dimension uninformative for reconstruction.",
        "text_length": 246,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02163447067141533,
          0.03344855457544327,
          -0.003085583681240678,
          0.015464049763977528,
          -0.02773808129131794,
          -0.016771329566836357,
          0.02260512113571167,
          -0.025496365502476692,
          -0.029419131577014923,
          -0.007527639623731375
        ]
      },
      {
        "chunk_index": 44,
        "relative_chunk_number": 45,
        "text": "More critically, these pure noise dimensions introduce a chaotic signal that interferes with the training of the downstream language model, destabilizing the learning process. To mitigate this, we adopt the KL clipping strategy from Kingma et al.",
        "text_length": 246,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.013235569931566715,
          0.03986135497689247,
          -0.003994258586317301,
          0.04401516914367676,
          -0.016437042504549026,
          0.01608126051723957,
          0.025202462449669838,
          -0.009322558529675007,
          -0.03604495897889137,
          0.009407468140125275
        ]
      },
      {
        "chunk_index": 45,
        "relative_chunk_number": 46,
        "text": "(2016), which modifies the objective by clipping each dimension\u2019s KL loss at a constant floor: Lclip KL = l X i=1 max(\u03bbKL, LKL,i), (4) where LKL,i is the KL divergence for the i-th dimension and \u03bbKL is the threshold (we use \u03bbKL = 0.5).",
        "text_length": 235,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.022478248924016953,
          0.018141796812415123,
          -0.04763086140155792,
          0.012591153383255005,
          -0.02712591364979744,
          0.02625899389386177,
          0.02019515633583069,
          -0.01013245154172182,
          -0.02100115455687046,
          0.03611480072140694
        ]
      },
      {
        "chunk_index": 46,
        "relative_chunk_number": 47,
        "text": "This technique ensures that every dimension is encouraged to actively participate in reconstruction, thus preventing collapse and fostering a dense, structured representation. Dropout for Enhanced Robustness. Beyond structuring the latent space with variational methods, we further enhance its robustness by injecting noise during training using two complementary forms of dropout.",
        "text_length": 381,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.005358718801289797,
          0.022714294493198395,
          -0.03242265805602074,
          0.01772158592939377,
          -0.07176575064659119,
          -0.02945469506084919,
          0.020826857537031174,
          0.012772868387401104,
          -0.03744357451796532,
          -0.008288382552564144
        ]
      },
      {
        "chunk_index": 47,
        "relative_chunk_number": 48,
        "text": "First, we apply dropout with a rate of p = 0.15 to the latent vector z before it is passed to the decoder. This forces the autoencoder to learn a redundant representation, making it robust to minor prediction errors from the downstream generative model. Second, we apply dropout to input tokens by randomly masking a fraction (p = 0.15) of tokens.",
        "text_length": 347,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004868705291301012,
          0.021984202787280083,
          -0.03655378147959709,
          0.024106640368700027,
          -0.08727189898490906,
          -0.00791777204722166,
          0.02023598551750183,
          -0.03471909835934639,
          -0.034989263862371445,
          0.022749891504645348
        ]
      },
      {
        "chunk_index": 48,
        "relative_chunk_number": 49,
        "text": "Analogous to the Continuous Bag-of- Words (CBOW) method (Mikolov et al., 2013), this compels the autoencoder to infer masked tokens from their context, thereby enriching the latent vector with the chunk\u2019s semantic context rather than just performing a simple token-index compression.",
        "text_length": 283,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006283511407673359,
          0.013189328834414482,
          -0.05064569413661957,
          0.04066675901412964,
          -0.05748802796006203,
          -0.02483697421848774,
          0.01860831119120121,
          -0.03114282712340355,
          -0.03074568510055542,
          0.061828359961509705
        ]
      },
      {
        "chunk_index": 49,
        "relative_chunk_number": 50,
        "text": "Crucially, these dropout techniques are employed exclusively during the autoencoder\u2019s training phase to build a robust latent representation; they are disabled during the subsequent training and inference of the continuous language model. The synthesis of these techniques produces a powerful and robust autoencoder.",
        "text_length": 316,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008934160694479942,
          0.027821043506264687,
          -0.02887812815606594,
          0.0011263665510341525,
          -0.14659909904003143,
          0.009671174921095371,
          0.020889580249786377,
          -0.011261852458119392,
          -0.04376482218503952,
          0.012960783205926418
        ]
      },
      {
        "chunk_index": 50,
        "relative_chunk_number": 51,
        "text": "For a chunk of K = 4 tokens, we now employ a latent vector of l = 128 dimensions, providing the necessary capacity to encode information redundantly. The encoder learns a posterior distribution where the standard deviations, \u03c3i, converge to approximately 0.3. This means that sampling the latent vector z effectively perturbs the predicted mean \u00b5 with a substantial Gaussian noise \u03c3 \u22480.3I.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.017955519258975983,
          0.018578553572297096,
          -0.03671757131814957,
          0.00039858653326518834,
          0.027884673327207565,
          0.017578138038516045,
          0.021100183948874474,
          -0.056204017251729965,
          -0.02351376973092556,
          -0.040945377200841904
        ]
      },
      {
        "chunk_index": 51,
        "relative_chunk_number": 52,
        "text": "Despite this significant latent perturbation, the decoder still maintains a token-level accuracy exceeding 99.9%. This vector representation, which combines high fidelity with high robustness, lays a solid founda- tion for the subsequent learning of Continuous Autoregressive Language Models (CALM).",
        "text_length": 299,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.020100630819797516,
          0.023122891783714294,
          -0.04883356764912605,
          0.0016178087098523974,
          -0.028108805418014526,
          0.02288626693189144,
          0.017038527876138687,
          0.003415606915950775,
          -0.022841429337859154,
          -0.015081934630870819
        ]
      },
      {
        "chunk_index": 52,
        "relative_chunk_number": 53,
        "text": "4 Preprint 3 LIKELIHOOD-FREE LANGUAGE MODELING 3.1 NEXT-VECTOR PREDICTION The autoencoder developed in Section 2 establishes a robust and high-fidelity mapping between a chunk of K discrete tokens and a single continuous vector, which allow us to reframe language modeling from a task of next-token prediction on discrete token sequences to next-vector prediction on continuous vector sequences.",
        "text_length": 395,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.029174821451306343,
          0.020272035151720047,
          -0.013517999090254307,
          0.03198634460568428,
          -0.028807567432522774,
          0.014985951595008373,
          0.021470356732606888,
          -0.007303350605070591,
          -0.031762152910232544,
          0.027962839230895042
        ]
      },
      {
        "chunk_index": 53,
        "relative_chunk_number": 54,
        "text": "Specifically, a sequence of T tokens, X = (x1, . . . , xT ), is first grouped into L = T/K non-overlapping chunks. The encoder, fenc, then transforms the original sequence into a new, more compact sequence of continuous vectors: Z = (z1, z2, . . . , zL), where zi = fenc(x(i\u22121)K+1, . . . , xiK).",
        "text_length": 295,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012674691155552864,
          0.020218105986714363,
          -0.043769583106040955,
          0.004612844903022051,
          0.005900310352444649,
          0.021352970972657204,
          0.014495011419057846,
          -0.028970252722501755,
          -0.029177356511354446,
          0.007266221102327108
        ]
      },
      {
        "chunk_index": 54,
        "relative_chunk_number": 55,
        "text": "(5) Consequently, the autoregressive objective evolves to predicting the next vector in the sequence: p(Z) = L Y i=1 p(zi|z<i). (6) While this autoregressive structure is preserved, the underlying mechanism for predicting the next element must be redesigned.",
        "text_length": 258,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.020397670567035675,
          0.05508115142583847,
          -0.036429308354854584,
          0.009692303836345673,
          -0.04084072634577751,
          0.003855329006910324,
          0.01743656024336815,
          -0.018842313438653946,
          -0.025690516456961632,
          0.0003962927439715713
        ]
      },
      {
        "chunk_index": 55,
        "relative_chunk_number": 56,
        "text": "Unlike standard language models, which rely on a softmax layer to compute a probability distribution over a finite vocabulary, our model must predict a vector within the infinite space Rl. The softmax function is not applicable over this uncountable set, rendering the explicit probability density p(zi|z<i) intractable.",
        "text_length": 320,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.025954123586416245,
          0.03411431238055229,
          -0.008935483172535896,
          0.0278738122433424,
          -0.06494248658418655,
          -0.03813505545258522,
          0.022567912936210632,
          -0.036584753543138504,
          -0.025779806077480316,
          -0.03524935990571976
        ]
      },
      {
        "chunk_index": 56,
        "relative_chunk_number": 57,
        "text": "This introduces two critical challenges: \u2022 Training: The likelihood p(zi|z<i) becomes intractable, precluding the use of maximum likelihood estimation (i.e., minimizing cross-entropy loss) for training. \u2022 Evaluation: Standard evaluation metrics like Perplexity, which are derived directly from the model\u2019s likelihood, can no longer be computed to measure model performance.",
        "text_length": 373,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.053138621151447296,
          0.03213826194405556,
          -0.030967624858021736,
          0.004854679107666016,
          -0.05837451294064522,
          -0.020402779802680016,
          0.020054910331964493,
          -0.04351936653256416,
          -0.019769275560975075,
          -0.04473746195435524
        ]
      },
      {
        "chunk_index": 57,
        "relative_chunk_number": 58,
        "text": "We address both of these challenges in turn. For the training problem, we introduce our approach to likelihood-free language modeling in the remainder of this section. For the evaluation problem, we propose a likelihood-free evaluation methodology in Section 4.",
        "text_length": 261,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.021298609673976898,
          0.030102191492915154,
          0.0018143506022170186,
          0.03429128974676132,
          -0.0735534280538559,
          -0.017701655626296997,
          0.017079690471291542,
          0.004212878178805113,
          -0.04000866040587425,
          -0.0007997609791345894
        ]
      },
      {
        "chunk_index": 58,
        "relative_chunk_number": 59,
        "text": "3.2 GENERATIVE HEAD Generative modeling of continuous data (Kingma & Welling, 2014; Goodfellow et al., 2014; Ho et al., 2020) is a well-established field, foundational to domains such as image and audio synthesis where data is inherently continuous.",
        "text_length": 249,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.026570705696940422,
          0.008145456202328205,
          -0.01462672371417284,
          0.04322338104248047,
          0.0325634628534317,
          -0.03406243771314621,
          0.019871601834893227,
          -0.01721673645079136,
          -0.02902201935648918,
          0.0060086939483881
        ]
      },
      {
        "chunk_index": 59,
        "relative_chunk_number": 60,
        "text": "A promising recent paradigm (Tschannen et al., 2023; Li et al., 2024; Shao et al., 2025b) combines these approaches with autoregressive models: a Transformer backbone predicts a conditioning hidden state, which is used by a subsequent generative model to produce the continuous output for each step.",
        "text_length": 299,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02374986559152603,
          0.028117215260863304,
          0.007915911264717579,
          0.0295744389295578,
          0.00443918677046895,
          -0.03400300815701485,
          0.017256351187825203,
          -0.06245611235499382,
          -0.02717299573123455,
          0.018986694514751434
        ]
      },
      {
        "chunk_index": 60,
        "relative_chunk_number": 61,
        "text": "Our Continuous Autoregressive Language Models (CALM) adapts this paradigm, but with a critical focus on computational efficiency that constrains the design of this generative component. We therefore conceptualize this component as a lightweight generative head.",
        "text_length": 261,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.015553342178463936,
          0.014356444589793682,
          -0.03322117030620575,
          0.015937568619847298,
          0.004583896137773991,
          -0.025076638907194138,
          0.017466342076659203,
          -0.033822618424892426,
          -0.02064693532884121,
          -0.0442114882171154
        ]
      },
      {
        "chunk_index": 61,
        "relative_chunk_number": 62,
        "text": "Formally, the generative head is a stochastic function that takes the Transformer\u2019s hidden state, hi\u22121 \u2208Rd, and draws a sample zi \u2208Rl from the conditional distribution: hi\u22121 = Transformer(z1:i\u22121), zi \u223cp(\u00b7|hi\u22121).",
        "text_length": 211,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.029382478445768356,
          0.007270241156220436,
          0.010705731809139252,
          0.052987221628427505,
          0.05802734196186066,
          -0.03769370913505554,
          0.015522699803113937,
          -0.059648215770721436,
          -0.024811647832393646,
          -0.013771220110356808
        ]
      },
      {
        "chunk_index": 62,
        "relative_chunk_number": 63,
        "text": "(7) While the generative head can be any continuous generative model, prominent options like Dif- fusion (Ho et al., 2020; Li et al., 2024; Fan et al., 2025) or Flow Matching (Lipman et al., 2023; Ren et al., 2025a;b) are misaligned with our goal of efficiency.",
        "text_length": 261,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.013250106014311314,
          0.001821449724957347,
          -0.011056225746870041,
          0.034426990896463394,
          0.012234047055244446,
          -0.0386127345263958,
          0.014648132026195526,
          -0.00035592628410086036,
          -0.03130725771188736,
          -0.05767417326569557
        ]
      },
      {
        "chunk_index": 63,
        "relative_chunk_number": 64,
        "text": "These models rely on an iterative sampling process\u2014requiring dozens or even hundreds of network evaluations to produce a single vector\u2014which directly counteracts the speedup gained from reducing the number of autoregres- sive steps. The CALM architecture therefore demands a generative head capable of high-quality, single-step generation, a challenge we address next with an energy-based objective.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010157284326851368,
          0.028589075431227684,
          -0.008831588551402092,
          0.026483584195375443,
          -0.046796005219221115,
          -0.012226010672748089,
          0.01824394054710865,
          -0.013525516726076603,
          -0.026628728955984116,
          -0.009984277188777924
        ]
      },
      {
        "chunk_index": 64,
        "relative_chunk_number": 65,
        "text": "3.3 ENERGY TRANSFORMER 3.3.1 STRICTLY PROPER SCORING RULES To meet the demand for a generative head capable of high-quality, single-step generation, we draw inspiration from Shao et al. (2024; 2025b), which frames the generative task as the optimization of 5 Preprint strictly proper scoring rules (Gneiting & Raftery, 2007).",
        "text_length": 325,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007119705434888601,
          -0.0013304812600836158,
          -0.0029972027987241745,
          0.04927070811390877,
          0.012546383775770664,
          0.021268527954816818,
          0.0218645092099905,
          -0.04471934214234352,
          -0.04022514447569847,
          -0.06711115688085556
        ]
      },
      {
        "chunk_index": 65,
        "relative_chunk_number": 66,
        "text": "Formally, a scoring rule S(P, y) assigns a numerical score to a predictive distribution P upon observing an outcome y, where higher scores are better. The quality of a forecast P against the true data-generating distribution Q is measured by its expected score, defined as S(P, Q) = Ey\u223cQ[S(P, y)].",
        "text_length": 297,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.05514972284436226,
          0.03009968064725399,
          -0.002847811905667186,
          0.029150592163205147,
          0.02605382911860943,
          -0.006380768027156591,
          0.021899783983826637,
          -0.053405020385980606,
          -0.025171171873807907,
          -0.057428572326898575
        ]
      },
      {
        "chunk_index": 66,
        "relative_chunk_number": 67,
        "text": "A scoring rule is considered proper if the expected score is maximized when the predictive distribution P matches the data distribution Q: S(P, Q) \u2264S(Q, Q) for all distributions P. (8) This property ensures that the scoring rule does not incentivize the model to predict a biased or distorted distribution.",
        "text_length": 306,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04866255447268486,
          0.021039500832557678,
          -0.022888800129294395,
          0.0445856973528862,
          0.07363198697566986,
          -0.007417129818350077,
          0.02013496868312359,
          -0.021369848400354385,
          -0.02619795873761177,
          -0.08767545968294144
        ]
      },
      {
        "chunk_index": 67,
        "relative_chunk_number": 68,
        "text": "Furthermore, a scoring rule is strictly proper if equality holds only when P = Q, meaning that the optimal score can only be achieved by reporting the true distribution.",
        "text_length": 169,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.06554557383060455,
          0.03807358071208,
          -0.025401605293154716,
          0.011901581659913063,
          0.043113935738801956,
          0.01800345815718174,
          0.02061545103788376,
          -0.031337738037109375,
          -0.04917757213115692,
          -0.09668329358100891
        ]
      },
      {
        "chunk_index": 68,
        "relative_chunk_number": 69,
        "text": "The use of a strictly proper scoring rule as a training objective is therefore a powerful and princi- pled approach for training our generative head, as maximizing the expected score is equivalent to driving the model\u2019s predictive distribution to match the true distribution.",
        "text_length": 275,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.017152342945337296,
          0.03164951503276825,
          -0.002210591221228242,
          0.02222617343068123,
          0.02060314454138279,
          -0.0016870179679244757,
          0.018519949167966843,
          -0.05387621372938156,
          -0.030644629150629044,
          -0.08527781069278717
        ]
      },
      {
        "chunk_index": 69,
        "relative_chunk_number": 70,
        "text": "This principle offers a di- rect generalization of maximum likelihood estimation, where the negative log-likelihood is a special case corresponding to the logarithmic score (Good, 1952). While the likelihood is intractable in the continuous domain, the theory of scoring rules provides a rich family of alternatives.",
        "text_length": 316,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009211122989654541,
          0.03233205899596214,
          -0.016849655658006668,
          -0.009033906273543835,
          0.003856535768136382,
          -0.0032085797283798456,
          0.02099374495446682,
          -0.05905013158917427,
          -0.03436632081866264,
          -0.047150108963251114
        ]
      },
      {
        "chunk_index": 70,
        "relative_chunk_number": 71,
        "text": "3.3.2 ENERGY LOSS We build our training objective using the Energy Score (Sz\u00b4ekely, 2003), a strictly proper scoring rule that has proven effective across a range of generative applications (Gritsenko et al., 2020; Vahidi et al., 2024; Pacchiardi et al., 2024; Shao et al., 2025b; Ma et al., 2025).",
        "text_length": 298,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04301002621650696,
          0.0031971591524779797,
          -0.0012897507986053824,
          0.010959993116557598,
          0.012382336892187595,
          0.005316272843629122,
          0.015665415674448013,
          -0.04046085476875305,
          -0.03268637880682945,
          -0.07147897034883499
        ]
      },
      {
        "chunk_index": 71,
        "relative_chunk_number": 72,
        "text": "The energy score is entirely likelihood-free; rather than evaluating probability densities, it measures the alignment between the prediction and the observation via sample distances. For a predictive distribution P and a ground truth observation y, the energy score is defined as: S(P, y) = Ex\u2032,x\u2032\u2032\u223cP [\u2225x\u2032 \u2212x\u2032\u2032\u2225\u03b1] \u22122 Ex\u223cP [\u2225x \u2212y\u2225\u03b1], (9) where x, x\u2032 and x\u2032\u2032 are independent samples drawn from P.",
        "text_length": 394,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0031518558971583843,
          0.009752359241247177,
          0.003099697409197688,
          0.019932199269533157,
          -0.011672863736748695,
          -0.05444924905896187,
          0.016297342255711555,
          -0.06893018633127213,
          -0.021772295236587524,
          -0.06704595685005188
        ]
      },
      {
        "chunk_index": 72,
        "relative_chunk_number": 73,
        "text": "The score is strictly proper for any \u03b1 \u2208(0, 2). Typically, \u03b1 is set to 1. The first term encourages diversity, penalizing the model for producing collapsed or overly confident predictions where all samples are identical. The second term encourages fidelity, driving the model\u2019s predictions to be close to the ground truth observation.",
        "text_length": 334,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.022146321833133698,
          0.06505406647920609,
          -0.03301147744059563,
          0.02860553003847599,
          0.040333133190870285,
          -0.04562593996524811,
          0.021702734753489494,
          -0.04014366865158081,
          -0.03301535174250603,
          -0.032829370349645615
        ]
      },
      {
        "chunk_index": 73,
        "relative_chunk_number": 74,
        "text": "While the expectations in Equation 9 make the energy score intractable to compute exactly, we can construct an unbiased Monte Carlo estimator to serve as a practical loss function, which we term the energy loss. To do this, we draw N candidate samples, {\u02dczi,1, . . . , \u02dczi,N}, from the generative head at each step i.",
        "text_length": 317,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04682435840368271,
          0.0250848475843668,
          -0.016422610729932785,
          0.0024740437511354685,
          -0.030551200732588768,
          -0.05513391271233559,
          0.018049638718366623,
          -0.017087388783693314,
          -0.02829682268202305,
          -0.09000618755817413
        ]
      },
      {
        "chunk_index": 74,
        "relative_chunk_number": 75,
        "text": "Furthermore, we leverage a unique property of our setup: our autoencoder does not map a token chunk to a fixed point, but rather to a conditional Gaussian posterior zi \u223cq(\u00b7|x(i\u22121)K+1:iK). Relying on a single sample zi as ground-truth can introduce high variance into the energy loss. To mitigate this and stabilize training, we draw M target samples, {zi,1, . . . , zi,M}, from this posterior.",
        "text_length": 393,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.011822185479104519,
          0.018967945128679276,
          -0.016892267391085625,
          0.01953245885670185,
          0.0048685576766729355,
          0.006827992852777243,
          0.018273117020726204,
          -0.019543049857020378,
          -0.03320470079779625,
          -0.032574936747550964
        ]
      },
      {
        "chunk_index": 75,
        "relative_chunk_number": 76,
        "text": "Combining these sample sets, the final energy loss is formulated as: Lenergy = L X i=1 ( 2 NM N X n=1 M X m=1 \u2225zi,m \u2212\u02dczi,n\u2225\u2212 1 N(N \u22121) X n\u0338=k \u2225\u02dczi,n \u2212\u02dczi,k\u2225). (10) In practice, we set N =8 and M =100. The number of model samples N directly scales the training cost, as each sample requires an evaluation of the generative head; we therefore use a small N to maintain high training efficiency.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.07863948494195938,
          -0.00661407271400094,
          -0.010188507847487926,
          0.016430392861366272,
          -0.0036732300650328398,
          -0.012509115971624851,
          0.0158825796097517,
          -0.03887515515089035,
          -0.024082688614726067,
          -0.024522246792912483
        ]
      },
      {
        "chunk_index": 76,
        "relative_chunk_number": 77,
        "text": "The overhead of drawing target vectors from a known Gaussian posterior is almost negligible, which allows us to use a large M to reduce the variance of loss.",
        "text_length": 157,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.001239632023498416,
          0.012091283686459064,
          -0.048741668462753296,
          0.014005003497004509,
          0.047734592109918594,
          -0.022330617532134056,
          0.01897839456796646,
          -0.020985107868909836,
          -0.023755624890327454,
          -0.039327409118413925
        ]
      },
      {
        "chunk_index": 77,
        "relative_chunk_number": 78,
        "text": "A key advantage of this likelihood-free training objective is its flexibility: it only requires the ability to draw samples from the generative head, placing minimal constraints on its internal architecture and allowing for the simple and efficient designs we explore next. 3.3.3 MODEL ARCHITECTURE We now detail our model architecture.",
        "text_length": 336,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.03366837650537491,
          0.008968613110482693,
          -0.01744147017598152,
          0.037513379007577896,
          -0.06748133152723312,
          -0.026766376569867134,
          0.01743033155798912,
          0.010353132151067257,
          -0.025136081501841545,
          0.006599253509193659
        ]
      },
      {
        "chunk_index": 78,
        "relative_chunk_number": 79,
        "text": "We use a standard Transformer backbone, with modifications focused on the output-side generative head and the input-side adaptation. Energy-Based Generative Head.",
        "text_length": 162,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03012232854962349,
          -0.0193757563829422,
          -0.02118353731930256,
          0.024744804948568344,
          0.007757539860904217,
          -0.02949441224336624,
          0.020098861306905746,
          -0.01847738027572632,
          -0.0380140095949173,
          -0.04878293350338936
        ]
      },
      {
        "chunk_index": 79,
        "relative_chunk_number": 80,
        "text": "The inputs to the generative head are twofold: the hidden state hi\u22121 from the Transformer backbone, which provides the conditional context, and a random noise 6 Preprint Input Tokens Token Em- beddings Input Com- pression MLP Transformer Backbone Energy-Based Generative Head h Random Noise \u03b50 Linear \u03b5L AE Decoder z Output Tokens \u03b5l h Linear Linear SwiGLU + \u03b5l+1 \u00d7L Figure 2: The Architecture of the Continuous Autoregressive Language Model (CALM).",
        "text_length": 449,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.009417138062417507,
          0.006166927982121706,
          -0.004175439942628145,
          0.06532861292362213,
          0.007703390438109636,
          -0.014595803804695606,
          0.02059873379766941,
          -0.04251730069518089,
          -0.026842286810278893,
          0.015919247642159462
        ]
      },
      {
        "chunk_index": 80,
        "relative_chunk_number": 81,
        "text": "Left: The main autoregressive loop where discrete tokens are compressed to condition a Transformer, whose output hidden state h guides an energy-based head to predict a continuous vector z. The AE decoder then maps z back to discrete tokens for the next step. Right: A detailed view of the generative head, showing how it refines a noise vector \u03b50 through a series of residual MLP blocks.",
        "text_length": 388,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -9.55912983044982e-05,
          0.010049533098936081,
          -0.0028154016472399235,
          0.04114445671439171,
          -0.030340535566210747,
          -0.041466839611530304,
          0.016075946390628815,
          -0.03640258312225342,
          -0.030286438763141632,
          -0.0003466295020189136
        ]
      },
      {
        "chunk_index": 81,
        "relative_chunk_number": 82,
        "text": "vector \u03b5 \u2208Rdnoise, which provides the necessary stochasticity for sampling. Each dimension of \u03b5 is drawn independently from a uniform distribution U[\u22120.5, 0.5]. Both the hidden state hi\u22121 and the noise vector \u03b5 are projected by independent linear layers to match the head\u2019s internal dimension, which we set to match the Transformer\u2019s hidden dimension d.",
        "text_length": 353,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02030986361205578,
          0.01426082756370306,
          0.015447360463440418,
          0.060424238443374634,
          0.014603588730096817,
          -0.011888572946190834,
          0.021307727321982384,
          -0.030132459476590157,
          -0.03382013738155365,
          -0.03373420238494873
        ]
      },
      {
        "chunk_index": 82,
        "relative_chunk_number": 83,
        "text": "The core of the generative head is a stack of L residual MLP blocks that progressively refine the initial noise representation \u03b50 = \u03b5 into the final output vector. As illustrated in Figure 2, each MLP block first fuses the current representation \u03b5l with the hidden state via two linear layers. This is followed by a SwiGLU layer (Shazeer, 2020b) with an intermediate dimension of d.",
        "text_length": 382,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0073600285686552525,
          0.008137737400829792,
          -0.008008148521184921,
          0.025435663759708405,
          -0.008434892632067204,
          -0.05349532887339592,
          0.013919939287006855,
          -0.030488746240735054,
          -0.026451312005519867,
          0.013624737970530987
        ]
      },
      {
        "chunk_index": 83,
        "relative_chunk_number": 84,
        "text": "A residual connection then adds the block\u2019s input to its output. This process concludes with a final linear layer that projects the representation to the target dimension l, producing the output vector zi. A single MLP block contains approximately 6d2 parameters.",
        "text_length": 263,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014740655198693275,
          0.03727796673774719,
          -0.028437869623303413,
          0.02117711678147316,
          -0.003392257960513234,
          -0.03175245597958565,
          0.017948083579540253,
          0.016624977812170982,
          -0.034728020429611206,
          -0.059764571487903595
        ]
      },
      {
        "chunk_index": 84,
        "relative_chunk_number": 85,
        "text": "We set the number of blocks to a quarter of the number of Transformer layers; the entire generative head therefore accounts for only about 10% of the total model parameters, making its computational overhead minimal. Discrete Token Input.",
        "text_length": 238,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.005324166733771563,
          0.008430670015513897,
          -0.011599977500736713,
          0.05755062401294708,
          0.004117182921618223,
          -0.027971521019935608,
          0.021111153066158295,
          -0.08044777065515518,
          -0.043046049773693085,
          -0.03804640844464302
        ]
      },
      {
        "chunk_index": 85,
        "relative_chunk_number": 86,
        "text": "An intuitive approach for the model\u2019s input would be to embed the pre- dicted latent vectors zi\u22121 from the previous step into the Transformer\u2019s hidden dimension d using a linear projection.",
        "text_length": 189,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.026382874697446823,
          0.03394753113389015,
          -0.011017829179763794,
          0.028631962835788727,
          0.04300052300095558,
          -0.01694386638700962,
          0.02135721780359745,
          -0.011413227766752243,
          -0.02384633757174015,
          -0.07911575585603714
        ]
      },
      {
        "chunk_index": 86,
        "relative_chunk_number": 87,
        "text": "However, we empirically found that using these latent vectors as input for the Transformer leads to a noticeable degradation in performance, as the model struggles to unpack the semantic information from such a compact input representation. To circumvent this, we ground the model\u2019s autoregressive process in the discrete token space.",
        "text_length": 334,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01247758511453867,
          0.0051962025463581085,
          -0.017585596069693565,
          0.022101474925875664,
          -0.04400661960244179,
          -0.028073392808437347,
          0.01943974383175373,
          -0.025212226435542107,
          -0.029494689777493477,
          0.014951611869037151
        ]
      },
      {
        "chunk_index": 87,
        "relative_chunk_number": 88,
        "text": "Dur- ing training, the input for each step is formed by the K tokens from the previous step. To maintain efficiency, we use a lightweight input compression module\u2014a two-layer MLP\u2014to map the K em- beddings into a single input representation. The inference process unfolds as follows: 1.",
        "text_length": 285,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.006579853594303131,
          0.021923702210187912,
          -0.038328416645526886,
          0.02373744733631611,
          -0.04949942231178284,
          0.017756052315235138,
          0.020399069413542747,
          -0.0285851638764143,
          -0.02883702516555786,
          0.02105342037975788
        ]
      },
      {
        "chunk_index": 88,
        "relative_chunk_number": 89,
        "text": "Input Processing: At step i, the previously generated chunk of K tokens are embedded and compressed into a single input representation and fed into the Transformer. 2. Continuous Prediction: The Transformer outputs the hidden state hi\u22121, which our energy- based generative head then uses to predict the next continuous vector, zi. 7 Preprint 3.",
        "text_length": 344,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.030831119045615196,
          -0.0025608742143958807,
          -0.007021087687462568,
          0.06283167749643326,
          -0.026365850120782852,
          -0.030144892632961273,
          0.01972387358546257,
          -0.03401891142129898,
          -0.037284404039382935,
          -0.008408105000853539
        ]
      },
      {
        "chunk_index": 89,
        "relative_chunk_number": 90,
        "text": "Discrete Feedback Loop: The predicted vector zi is immediately passed through the frozen decoder of our pre-trained autoencoder, gdec, to reconstruct the next K discrete tokens. The complete architecture of CALM is illustrated in Figure 2.",
        "text_length": 239,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010647204704582691,
          0.008271558210253716,
          -0.024030601605772972,
          0.04230135679244995,
          0.004765067249536514,
          -0.018242258578538895,
          0.02556503564119339,
          -0.027616694569587708,
          -0.04299497976899147,
          -0.046892642974853516
        ]
      },
      {
        "chunk_index": 90,
        "relative_chunk_number": 91,
        "text": "4 LIKELIHOOD-FREE LM EVALUATION 4.1 PRINCIPLES OF LM EVALUATION The CALM framework operates as an implicit generative model, whose predictive probability dis- tribution is defined by its sampling process. Consequently, standard LM evaluation metrics like Perplexity, which are defined in terms of explicit likelihoods, can no longer be employed to measure model performance.",
        "text_length": 374,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.02457626536488533,
          0.013818971812725067,
          -0.0035775271244347095,
          0.024050939828157425,
          0.02553127519786358,
          -0.01871659979224205,
          0.02567390352487564,
          -0.027456915006041527,
          -0.017660312354564667,
          -0.015907423570752144
        ]
      },
      {
        "chunk_index": 91,
        "relative_chunk_number": 92,
        "text": "Furthermore, the energy loss used for training is itself unsuitable for evaluation, as its magnitude is subjective to the specific latent space shaped by the autoencoder. This necessi- tates the development of a model-agnostic evaluation metric, one that can faithfully assess language modeling capabilities in a principled, yet entirely likelihood-free, manner.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.033978912979364395,
          0.00903113093227148,
          0.008870689198374748,
          0.019293680787086487,
          -0.045655231922864914,
          -0.05120578780770302,
          0.016420936211943626,
          -0.00857809092849493,
          -0.021411912515759468,
          -0.0028452444821596146
        ]
      },
      {
        "chunk_index": 92,
        "relative_chunk_number": 93,
        "text": "The goal of a evaluation metric is to quantify the divergence between the model\u2019s predictive distri- bution, P, and the true data distribution, Q. This principle is formalized by the property that the metric is uniquely optimized when the model accurately recovers the data distribution (P = Q).",
        "text_length": 295,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012443659827113152,
          0.01724855788052082,
          -0.01920790784060955,
          0.01708708517253399,
          0.0026131290942430496,
          -0.03744538500905037,
          0.021923135966062546,
          -0.03376736864447594,
          -0.02741354890167713,
          -0.10539327561855316
        ]
      },
      {
        "chunk_index": 93,
        "relative_chunk_number": 94,
        "text": "This ensures the evaluation is fair and cannot be hacked by a model that systematically distorts its predictions. For instance, the conventional metric of Perplexity serves as a prime example of this principle.",
        "text_length": 210,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014015085995197296,
          0.026454107835888863,
          -0.008065670728683472,
          0.027742592617869377,
          -0.055485501885414124,
          -0.030493339523673058,
          0.019909577444195747,
          -0.048300839960575104,
          -0.03297349065542221,
          -0.024088719859719276
        ]
      },
      {
        "chunk_index": 94,
        "relative_chunk_number": 95,
        "text": "It is grounded in the expected negative log-likelihood, which can be decomposed into the sum of the KL divergence and data entropy: Ey\u223cQ[\u2212log P(y)] = Ey\u223cQ \u0014 log Q(y) P(y) \u0015 + Ey\u223cQ[\u2212log Q(y)] = DKL(Q\u2225P) | {z } Minimized at P =Q + H(Q) | {z } Constant .",
        "text_length": 251,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009824215434491634,
          0.02890283428132534,
          -0.02712051197886467,
          0.014626458287239075,
          -0.045864153653383255,
          0.011749579571187496,
          0.01683720573782921,
          0.006207469385117292,
          -0.020927222445607185,
          -0.03147415071725845
        ]
      },
      {
        "chunk_index": 95,
        "relative_chunk_number": 96,
        "text": "(11) This property establishes Perplexity as a theoretically sound measure of a model\u2019s capability to capture the true distribution, which is uniquely minimized when P = Q. In contrast, a naive metric like the raw likelihood of the observed outcome, P(y), fails this principle.",
        "text_length": 277,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02404855564236641,
          0.022733289748430252,
          -0.01569792628288269,
          0.014919053763151169,
          -0.009741578251123428,
          -0.04232574254274368,
          0.020327473059296608,
          -0.024456793442368507,
          -0.025519249960780144,
          -0.05747850611805916
        ]
      },
      {
        "chunk_index": 96,
        "relative_chunk_number": 97,
        "text": "The expected score under this metric, Ey\u223cQ[P(y)], is maximized by a deterministic prediction that assigns a probability of 1 to the single most frequent outcome, i.e., P(arg maxy Q(y)) = 1. Such a metric would therefore incorrectly favor an overconfident model that fails to capture the underlying data uncertainty.",
        "text_length": 315,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0013102117227390409,
          0.025817880406975746,
          -0.029440583661198616,
          0.011220330372452736,
          -0.01610654592514038,
          -0.07174244523048401,
          0.017826782539486885,
          -0.041100338101387024,
          -0.019029345363378525,
          -0.07805698364973068
        ]
      },
      {
        "chunk_index": 97,
        "relative_chunk_number": 98,
        "text": "This highlights a critical distinction: a principled metric must balance rewarding accuracy with correctly representing the predictive uncertainty. The naive likelihood P(y) only addresses the former, making it an inadequate measure of a model\u2019s predictive quality.",
        "text_length": 265,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.011335857212543488,
          0.04238523915410042,
          -0.018420012667775154,
          0.02517024241387844,
          -0.028804641216993332,
          -0.008080939762294292,
          0.02098623290657997,
          -0.006795026361942291,
          -0.02515513449907303,
          0.0025869361124932766
        ]
      },
      {
        "chunk_index": 98,
        "relative_chunk_number": 99,
        "text": "4.2 BRIERLM: BRIER FOR LANGUAGE MODELING For a principled and likelihood-free evaluation, we turn to the Brier score (Brier, 1950), a classic strictly proper scoring rule now widely used to assess the calibration of modern neural networks (Lakshminarayanan et al., 2017; Ovadia et al., 2019; Gruber & Buettner, 2022).",
        "text_length": 317,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.043531399220228195,
          -0.007999427616596222,
          -0.046746544539928436,
          0.01535166148096323,
          -0.006402552127838135,
          0.012797756120562553,
          0.017724130302667618,
          -0.04967807978391647,
          -0.020225388929247856,
          0.0007128280703909695
        ]
      },
      {
        "chunk_index": 99,
        "relative_chunk_number": 100,
        "text": "For a predictive distribution P and a ground-truth outcome y, the Brier score is defined as: Brier(P, y) = 2P(y) \u2212 X x P(x)2. (12) Unlike the raw likelihood P(y), which solely measures accuracy, the Brier score incorporates an ad- ditional term, P x P(x)2, to quantify predictive uncertainty. This structure balances two competing objectives, which ultimately rewards a well-calibrated prediction.",
        "text_length": 397,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04167996719479561,
          0.020653152838349342,
          -0.03533491492271423,
          0.034572869539260864,
          -0.02273353561758995,
          -0.023112773895263672,
          0.022856030613183975,
          -0.06440448760986328,
          -0.02072870172560215,
          -0.007254403550177813
        ]
      },
      {
        "chunk_index": 100,
        "relative_chunk_number": 101,
        "text": "This property is revealed by the following decomposition of the expected Brier score: Ey\u223cQ[Brier(P, y)] = \u2212 X x (P(x) \u2212Q(x))2 | {z } Squared Error (minimized at P =Q) + X x Q(x)2 | {z } Data Variance (constant) . (13) While the Brier score is theoretically sound, its direct computation remains intractable for CALM, as it requires knowledge of the full predictive distribution P.",
        "text_length": 380,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.009522434324026108,
          0.01138044148683548,
          -0.0491468608379364,
          0.027499880641698837,
          0.008583961054682732,
          -0.006762571167200804,
          0.01914057694375515,
          -0.03216489031910896,
          -0.023936433717608452,
          -0.058652251958847046
        ]
      },
      {
        "chunk_index": 101,
        "relative_chunk_number": 102,
        "text": "We find, however, that an unbiased 8 Preprint Monte Carlo estimator for the Brier score can be constructed in an entirely likelihood-free manner, using only samples drawn from the model. Specifically, the uncertainty term, P x P(x)2, can be interpreted as the collision probability of two independent samples.",
        "text_length": 309,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.014885633252561092,
          0.009668989107012749,
          -0.03880750760436058,
          0.02421525865793228,
          -0.014940209686756134,
          -0.011657395400106907,
          0.02433871105313301,
          -0.0460294708609581,
          -0.028874246403574944,
          -0.020222017541527748
        ]
      },
      {
        "chunk_index": 102,
        "relative_chunk_number": 103,
        "text": "Therefore, its unbiased estimator is simply the indicator function I{x1 = x2}, where x1, x2 \u223cP. Similarly, the accuracy term P(y) can be estimated by I{x = y} using a single sample x \u223cP. Combining these, we construct a practical, unbiased estimator for the Brier score using two samples drawn from the model: Brier(P, y) \u2248I{x1 = y} + I{x2 = y} \u2212I{x1 = x2}, x1, x2 \u223cP.",
        "text_length": 367,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012594308704137802,
          0.01506813894957304,
          -0.048313822597265244,
          0.02611970715224743,
          -0.07862769067287445,
          -0.014791355468332767,
          0.01751605048775673,
          -0.021202994510531425,
          -0.02088833786547184,
          0.004345652647316456
        ]
      },
      {
        "chunk_index": 103,
        "relative_chunk_number": 104,
        "text": "(14) This estimator enables a likelihood-free evaluation of CALM\u2019s predictive capabilities. A straight- forward approach is to assess next-token prediction performance in a teacher-forcing setting. This would involve generating two latent vectors at each step, decoding them using the frozen autoen- coder\u2019s decoder, and computing the Brier score using only the first token of each resulting chunk.",
        "text_length": 398,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.002830532379448414,
          0.03522302210330963,
          -0.042558759450912476,
          0.018274590373039246,
          -0.03566712141036987,
          0.007125611416995525,
          0.02209661714732647,
          0.001893834676593542,
          -0.02978329546749592,
          -0.020339448004961014
        ]
      },
      {
        "chunk_index": 104,
        "relative_chunk_number": 105,
        "text": "However, such an evaluation is insufficient as it ignores the generation quality of the remaining K \u22121 tokens. To address this limitation, we further introduce Brier-n, a metric that computes the Brier score over entire n-grams. In this formulation, the indicator functions of the estimator treat the n-gram as a single, atomic outcome.",
        "text_length": 336,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.011667303740978241,
          -0.0016737151890993118,
          -0.03604340925812721,
          0.03170314058661461,
          -0.04144996404647827,
          -0.005574820563197136,
          0.020593587309122086,
          -0.06261726468801498,
          -0.03547845035791397,
          0.023564286530017853
        ]
      },
      {
        "chunk_index": 105,
        "relative_chunk_number": 106,
        "text": "Finally, following the convention of established n-gram-based metrics like BLEU (Papineni et al., 2002), we define our composite metric, BrierLM (Brier for Lan- guage Modeling), as the geometric mean of Brier-n scores for n = 1 to 4, which we then scale by 100 to place it on a more interpretable 0-100 range: BrierLM = 100 \u00b7 4 Y n=1 Brier-n !0.25 .",
        "text_length": 349,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.017184168100357056,
          0.009707759134471416,
          -0.042156852781772614,
          0.023149903863668442,
          -0.028427937999367714,
          -0.011755426414310932,
          0.020595135167241096,
          -0.05748311057686806,
          -0.014082691632211208,
          -0.009373747743666172
        ]
      },
      {
        "chunk_index": 106,
        "relative_chunk_number": 107,
        "text": "(15) The utility of BrierLM extends beyond CALM, serving as a universal evaluation protocol that is also applicable to conventional autoregressive models. For such models, the BrierLM estimator can be applied by simply drawing samples from the final softmax distribution, enabling direct and fair comparisons with our likelihood-free framework.",
        "text_length": 344,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.023923151195049286,
          0.024750176817178726,
          -0.039802778512239456,
          0.02450007200241089,
          -0.03580677509307861,
          0.006179069634526968,
          0.023595482110977173,
          -0.0038962513208389282,
          -0.02092163823544979,
          -0.020081695169210434
        ]
      },
      {
        "chunk_index": 107,
        "relative_chunk_number": 108,
        "text": "To validate this, we evaluated both cross-entropy and BrierLM throughout the training of our baseline autoregressive models (detailed in Section 7.1). Figure 3 visualizes the joint distribution of the two metrics.",
        "text_length": 213,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01896691881120205,
          0.022734567523002625,
          -0.04682309925556183,
          0.025231121107935905,
          -0.05713547393679619,
          -0.016836078837513924,
          0.02373735047876835,
          0.011973611079156399,
          -0.0400138720870018,
          -0.014642915688455105
        ]
      },
      {
        "chunk_index": 108,
        "relative_chunk_number": 109,
        "text": "Intriguingly, we find that BrierLM is highly consistent with cross-entropy loss, exhibiting a nearly linear relationship with a Pearson correlation coefficient of -0.966 and a Spearman\u2019s rank correlation of -0.991. This strong monotonic alignment confirms that BrierLM is a reliable measure of language modeling capability, establishing it as a trustworthy likelihood-free alternative to Perplexity.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0038109298329800367,
          0.02993743121623993,
          -0.05246885120868683,
          0.024133535102009773,
          -0.03781726956367493,
          0.0003331359475851059,
          0.020652003586292267,
          0.014376766048371792,
          -0.012331678532063961,
          0.004757866263389587
        ]
      },
      {
        "chunk_index": 109,
        "relative_chunk_number": 110,
        "text": "2.8 3.0 3.2 3.4 3.6 3.8 Cross-Entropy Loss 4 5 6 7 8 9 BrierLM Pearson Correlation: -0.966 Spearman Correlation: -0.991 Transformer-S Transformer-M Transformer-L Figure 3: Joint distribution of the cross-entropy loss and the BrierLM score across different models and training checkpoints.",
        "text_length": 288,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010164973326027393,
          0.017061157152056694,
          -0.058923523873090744,
          0.048506926745176315,
          0.01873205229640007,
          -0.011574683710932732,
          0.024539534002542496,
          0.014413080178201199,
          -0.027702253311872482,
          -0.05618324875831604
        ]
      },
      {
        "chunk_index": 110,
        "relative_chunk_number": 111,
        "text": "Furthermore, BrierLM offers a particularly significant advantage for the growing class of implicit generative models, such as diffusion-based language models (Austin et al., 2021; Han et al., 2023; Lou et al., 2024; Arriola et al., 2025).",
        "text_length": 238,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.011447282508015633,
          0.01608296111226082,
          -0.041236940771341324,
          0.04650583863258362,
          -0.029727276414632797,
          -0.014405759051442146,
          0.017618857324123383,
          0.028173603117465973,
          -0.017959680408239365,
          0.015481843613088131
        ]
      },
      {
        "chunk_index": 111,
        "relative_chunk_number": 112,
        "text": "These models have historically been challenging to eval- uate, often relying on the complex and sometimes loose estimation of variational lower bounds (ELBOs) to approximate Perplexity. BrierLM circumvents this entire challenge, offering a direct, unbiased method to faithfully assess their language modeling capabilities and enabling fair compar- isons across different model families.",
        "text_length": 386,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.035912565886974335,
          0.030498331412672997,
          -0.043184518814086914,
          0.03268175199627876,
          -0.07342339307069778,
          -0.00046434783143922687,
          0.018868956714868546,
          0.03487459570169449,
          -0.024088343605399132,
          0.022865351289510727
        ]
      },
      {
        "chunk_index": 112,
        "relative_chunk_number": 113,
        "text": "9 Preprint Algorithm 1 Likelihood-free Temperature Sampling Input: A base sampler S for an implicit discrete distribution P(x); A target temperature T \u2208(0, 1) Output: Sample x accepted with probability PT (x) \u221dP(x)1/T 1: procedure SAMPLEATTEMPERATURE(S, T) 2: n \u2190\u230a1/T\u230b. \u25b7Integer part of 1/T 3: \u03b1 \u21901/T \u2212n. \u25b7Fractional part, 0 \u2264\u03b1 < 1 4: Stage 1: Integer Part (n) 5: Draw n i.i.d. samples x1, . . .",
        "text_length": 395,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008560815826058388,
          0.01574033312499523,
          -0.0178365595638752,
          0.04370267689228058,
          0.013228795491158962,
          0.008422684855759144,
          0.0138919148594141,
          -0.016995519399642944,
          -0.029321325942873955,
          -0.024040745571255684
        ]
      },
      {
        "chunk_index": 113,
        "relative_chunk_number": 114,
        "text": ", xn \u223cS 6: if x1 = \u00b7 \u00b7 \u00b7 = xn then 7: x\u2217\u2190x1 \u25b7Find candidate x\u2217 8: else 9: restart from stage 1 \u25b7Rejection 10: if \u03b1 = 0 then 11: return x\u2217 \u25b7Accept x\u2217as Stage 2 is not needed 12: Stage 2: Fractional Part (\u03b1) 13: i \u21901 14: loop 15: Draw x \u223cS 16: if x = x\u2217then 17: return x\u2217 \u25b7Accept candidate 18: else 19: Draw u \u223cU(0, 1) \u25b7Uniform distribution 20: if u < \u03b1/i then 21: restart from stage 1 \u25b7Rejection 22: else 23: i \u2190i + 1 \u25b7Continue to next iteration 5 LIKELIHOOD-FREE TEMPERATURE SAMPLING 5.1 EXACT TEMPERATURE SAMPLING VIA REJECTION Controlled generation via temperature sampling is an indispensable feature of modern LLMs.",
        "text_length": 619,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.030996518209576607,
          0.015052590519189835,
          -0.021205434575676918,
          0.016206486150622368,
          -0.029205145314335823,
          0.020768744871020317,
          0.014184865169227123,
          0.027808982878923416,
          -0.019276872277259827,
          -0.005684696603566408
        ]
      },
      {
        "chunk_index": 114,
        "relative_chunk_number": 115,
        "text": "Con- ventionally, this technique is implemented by rescaling pre-softmax logits, a mechanism that re- quires explicit access to the model\u2019s probability distribution. However, this approach is incompatible with our CALM framework, whose generative head is likelihood-free and provides only a sampler. This presents a critical challenge: performing temperature sampling with only a black-box sam- pler.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03014967404305935,
          0.030349424108862877,
          0.02807529643177986,
          0.020333144813776016,
          -0.0020138875115662813,
          0.01998901553452015,
          0.018504273146390915,
          -0.04008694738149643,
          -0.0317995660007,
          -0.028391309082508087
        ]
      },
      {
        "chunk_index": 115,
        "relative_chunk_number": 116,
        "text": "In this section, we address this challenge by developing an exact algorithm, grounded in the principles of rejection sampling, that provably achieves this goal. The intuition for our algorithm stems from the relationship between repeated sampling and prob- ability exponentiation. In the context of CALM, a sample x corresponds to a complete chunk of K tokens produced at each step.",
        "text_length": 382,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01502178329974413,
          0.020659495145082474,
          -0.025040078908205032,
          0.04442045837640762,
          -0.06599673628807068,
          0.025942929089069366,
          0.017482323572039604,
          -0.03734057396650314,
          -0.030803626403212547,
          0.01967041753232479
        ]
      },
      {
        "chunk_index": 116,
        "relative_chunk_number": 117,
        "text": "Consider the simple case where the temperature T = 1/n for an integer n, which makes the target distribution PT (x) \u221dP(x)n. The probability of drawing the exact same sample x in n independent trials from the sampler is also P(x)n. This motivates an elegant rejection sampling scheme: we draw n samples and accept them if and only if all n samples are identical.",
        "text_length": 361,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0036792848259210587,
          0.02348273992538452,
          -0.039248231798410416,
          0.05088561028242111,
          -0.019251050427556038,
          0.0014040193054825068,
          0.020469706505537033,
          -0.030703207477927208,
          -0.035143595188856125,
          -0.03441379591822624
        ]
      },
      {
        "chunk_index": 117,
        "relative_chunk_number": 118,
        "text": "Otherwise, we reject the entire set and restart the process. The distribution of accepted samples is thus provably proportional to P(x)n, providing a foundation for our general algorithm. To generalize this approach for any arbitrary temperature T \u2208(0, 1), we decompose the exponent 1/T into its integer part, n = \u230a1/T\u230b, and fractional part, \u03b1 = 1/T \u2212n.",
        "text_length": 353,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007604373153299093,
          0.035671330988407135,
          -0.02530256286263466,
          0.04126344248652458,
          -0.023806866258382797,
          -0.011769993230700493,
          0.015931634232401848,
          -0.043657585978507996,
          -0.04554678872227669,
          -0.026592856273055077
        ]
      },
      {
        "chunk_index": 118,
        "relative_chunk_number": 119,
        "text": "This decomposition structures our algorithm as a two-stage rejection sampling process. The first stage handles the integer component n using the repetition-based scheme described above, producing a candidate sample x only if n independent draws are identical. The second stage, which handles the fractional exponent \u03b1, requires a more subtle approach.",
        "text_length": 351,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.001126909046433866,
          0.03164578229188919,
          0.0016062881331890821,
          0.03938664123415947,
          -0.05728529766201973,
          -0.03763437271118164,
          0.023324213922023773,
          -0.009924054145812988,
          -0.04140247777104378,
          0.04810426011681557
        ]
      },
      {
        "chunk_index": 119,
        "relative_chunk_number": 120,
        "text": "Here, we draw upon the theory of Bernoulli Factory (Keane & O\u2019Brien, 1994; Mendo, 2019) to construct an iterative procedure that simulates a biased coin flip with a success probability of P(x)\u03b1. A sample is accepted only if it passes both stages; failure at any point triggers a restart of the entire process. The complete procedure is formally detailed in Algorithm 1.",
        "text_length": 369,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.008355372585356236,
          0.012840749695897102,
          -0.016074245795607567,
          0.021390998736023903,
          -0.08847624063491821,
          -0.017868226394057274,
          0.01598571240901947,
          -0.00040908949449658394,
          -0.028623774647712708,
          0.031177207827568054
        ]
      },
      {
        "chunk_index": 120,
        "relative_chunk_number": 121,
        "text": "The following theorem guarantees its correctness. 10 Preprint Theorem 1. For an implicit discrete distribution P(x) with sampler S and a temperature T \u2208(0, 1), Algorithm 1 generates samples distributed as: PT (x) = P(x)1/T ZT , ZT = X x P(x)1/T . The proof is provided in Appendix A.1.",
        "text_length": 285,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004790381994098425,
          0.006037107203155756,
          -0.04278020188212395,
          0.04603107273578644,
          0.012371990829706192,
          0.0187327042222023,
          0.018691951408982277,
          -0.05851343646645546,
          -0.03693920001387596,
          -0.08633284270763397
        ]
      },
      {
        "chunk_index": 121,
        "relative_chunk_number": 122,
        "text": "5.2 EXPECTED SAMPLING COST While Algorithm 1 provides an exact solution for likelihood-free temperature sampling, its practical viability hinges on its computational efficiency. A central concern is the expected number of samples it requires, as each sampler call involves a forward pass through the generative head and autoencoder.",
        "text_length": 332,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.05116928368806839,
          0.035733435302972794,
          -0.005248105153441429,
          0.04264664649963379,
          -0.009912722744047642,
          0.004948335699737072,
          0.018830478191375732,
          -0.02384912595152855,
          -0.046388186514377594,
          -0.04643448442220688
        ]
      },
      {
        "chunk_index": 122,
        "relative_chunk_number": 123,
        "text": "Although these forward passes can be executed in parallel during inference, a prohibitively large number of samples would still create a significant computational bottleneck. The following theorem provides a closed-form expression for this expected number of sampler calls, with Corollary 2.1 offering a more interpretable upper bound. The proof is provided in Appendix 2. Theorem 2.",
        "text_length": 383,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01629745401442051,
          0.02749546244740486,
          -0.01608934812247753,
          0.06757152825593948,
          -0.014688890427350998,
          0.011830190196633339,
          0.01965435966849327,
          0.0018733590841293335,
          -0.04046047851443291,
          -0.05867847427725792
        ]
      },
      {
        "chunk_index": 123,
        "relative_chunk_number": 124,
        "text": "The expected number of calls to the base sampler S, denoted E[Ntotal], required to generate one sample using Algorithm 1 is: E[Ntotal] = n + I(\u03b1 > 0) P x P(x)1/T \u22121 ZT where ZT = P x P(x)1/T , n = \u230a1/T\u230b, \u03b1 = 1/T \u2212n, and I(\u00b7) is the indicator function. Corollary 2.1. Let |X| be the size of sample space.",
        "text_length": 303,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04823276400566101,
          0.013679048046469688,
          -0.021743889898061752,
          0.029423709958791733,
          -0.036199528723955154,
          0.022204607725143433,
          0.01405932568013668,
          -0.042529135942459106,
          -0.023891674354672432,
          -0.02630767412483692
        ]
      },
      {
        "chunk_index": 124,
        "relative_chunk_number": 125,
        "text": "The expected number of sampler calls E[Ntotal] at temperature T \u2208(0, 1) is bounded by: E[Ntotal] \u2264 \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 + n ZT , if 0 < T \u22640.5 1 + |X|2\u22121/T ZT , if 0.5 < T < 1 where n = \u230a1/T\u230band ZT = P x P(x)1/T . These results highlight that the algorithm\u2019s practicality is highly sensitive to the temperature T.",
        "text_length": 304,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.032254695892333984,
          0.02821093052625656,
          -0.014648595824837685,
          0.035666365176439285,
          0.02749972604215145,
          0.02160211093723774,
          0.012945298105478287,
          -0.054987888783216476,
          -0.02717931941151619,
          -0.05539611354470253
        ]
      },
      {
        "chunk_index": 125,
        "relative_chunk_number": 126,
        "text": "A potential limitation first emerges for T \u21921, as the cost can scale up to the size of sample space |X| = |V|K. It is therefore advisable to avoid using temperatures in this high-temperature regime to prevent a potential computational bottleneck. Conversely, at low temperatures, the integer part n = \u230a1/T\u230bbecomes large.",
        "text_length": 320,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.028028123080730438,
          0.04296846687793732,
          -0.01203212235122919,
          0.010233611799776554,
          0.004340606275945902,
          -0.007601952645927668,
          0.019785039126873016,
          -0.07807108759880066,
          -0.02733459882438183,
          0.0070125628262758255
        ]
      },
      {
        "chunk_index": 126,
        "relative_chunk_number": 127,
        "text": "The algorithm\u2019s success requires drawing n identical samples, an event with a vanishingly small probability for a large n that leads to an extremely high rejection rate. A more sample-efficient approximate algorithm is therefore needed to enhance its practical utility.",
        "text_length": 269,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03499686345458031,
          0.0417904257774353,
          -0.005641533527523279,
          0.043306827545166016,
          -0.08439197391271591,
          -0.03357212245464325,
          0.019762765616178513,
          -0.048436570912599564,
          -0.043426886200904846,
          -0.016381794586777687
        ]
      },
      {
        "chunk_index": 127,
        "relative_chunk_number": 128,
        "text": "5.3 BATCH APPROXIMATION The practical limitations of the exact algorithm become most pronounced in the low-temperature regime, where the requirement of drawing n = \u230a1/T\u230bidentical samples leads to an extremely high rejection rate that results in poor sample utilization. To address this, we propose an efficient approximate algorithm tailored for low temperatures of the form T = 1/n.",
        "text_length": 383,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014968451112508774,
          0.025284143164753914,
          -0.04368605464696884,
          0.01610696315765381,
          0.010797359049320221,
          0.011690798215568066,
          0.020692981779575348,
          -0.03039342351257801,
          -0.040769755840301514,
          -0.011478464119136333
        ]
      },
      {
        "chunk_index": 128,
        "relative_chunk_number": 129,
        "text": "The key insight is to shift from a single, high-risk trial to a combinatorial search within a large batch of N samples (N \u226bn). This shift allows a single batch to constitute \u0000N n \u0001 distinct candidates, which dramatically improves sample utilization and increases the probability of finding a successful match in a single round.",
        "text_length": 327,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03360513970255852,
          0.013454105705022812,
          -0.019168995320796967,
          0.04900175705552101,
          -0.0022512769792228937,
          0.02163446880877018,
          0.020921288058161736,
          -0.03221011534333229,
          -0.029562631621956825,
          -0.04377530887722969
        ]
      },
      {
        "chunk_index": 129,
        "relative_chunk_number": 130,
        "text": "For example, to sample at T = 0.5(n = 2), we might draw a batch of N = 10 samples, such as {A, C, A, D, B, E, A, F, B, G}. Here, sample A appears three times, and sample B appears twice. The algorithm then counts the number of successful n-tuple candidates within this batch. For sample A, there are \u00003 2 \u0001 = 3 successful candidates. For sample B, there is only \u00002 2 \u0001 = 1 successful candidate.",
        "text_length": 394,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008252670057117939,
          0.02170064114034176,
          -0.038210056722164154,
          0.033779487013816833,
          -0.04957186058163643,
          0.015004049986600876,
          0.016474710777401924,
          -0.04048642888665199,
          -0.031231563538312912,
          -0.0074276975356042385
        ]
      },
      {
        "chunk_index": 130,
        "relative_chunk_number": 131,
        "text": "Finally, the output is sampled from the set of valid candidates {A, B} according to their weighted probabilities, where P(A) = 3/4 and P(B) = 1/4. In the rare case that no sample 11 Preprint Algorithm 2 Approximate Temperature Sampling Input: A base sampler S; Target temperature T = 1/n; Batch size N \u226bn. Output: A sample x approximating the distribution PT (x) \u221dP(x)n.",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.013793149031698704,
          0.025224274024367332,
          -0.019057050347328186,
          0.028516408056020737,
          -0.0070004151202738285,
          0.061657823622226715,
          0.014857740141451359,
          -0.053621936589479446,
          -0.03240564092993736,
          -0.036280978471040726
        ]
      },
      {
        "chunk_index": 131,
        "relative_chunk_number": 132,
        "text": "1: procedure APPROXIMATETEMPSAMPLE(S, n, N) 2: Draw a batch of N samples B = {x1, . . . , xN} from sampler S. 3: Compute counts cx for each unique sample x \u2208B. 4: for m \u2190n down to 1 do \u25b7Start with the target n and fallback if needed 5: Initialize candidate set Xcand \u2190\u2205. 6: Initialize weights list W \u2190\u2205. 7: for each unique sample x with count cx \u2265m do 8: Add x to Xcand.",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03675449639558792,
          0.00929691269993782,
          -0.021578935906291008,
          0.04389755055308342,
          -0.012305956333875656,
          -0.01836618408560753,
          0.016494762152433395,
          0.004226816352456808,
          -0.025672242045402527,
          -0.035085856914520264
        ]
      },
      {
        "chunk_index": 132,
        "relative_chunk_number": 133,
        "text": "9: Add weight wx = \u0000cx m \u0001 to W. \u25b7Weight is the number of combinations 10: if Xcand is not empty then 11: break \u25b7Found a valid candidate set, exit fallback loop 12: Sample xout from Xcand with probabilities proportional to weights in W. 13: return xout appears at least n times, the candidate set would be empty.",
        "text_length": 312,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02234163135290146,
          0.030514370650053024,
          -0.02072283998131752,
          0.024532662704586983,
          -0.0028224887792021036,
          -0.0214019026607275,
          0.01863740012049675,
          -0.021880727261304855,
          -0.030533507466316223,
          -0.03227534890174866
        ]
      },
      {
        "chunk_index": 133,
        "relative_chunk_number": 134,
        "text": "To ensure the algorithm always produces an output, we introduce a fallback mechanism that iteratively reduce the matching requirement from n to n \u22121, n \u22122, . . . , until a non-empty candidate set is found. The detailed process is illustrated in Algorithm 2. For any finite batch size N, the algorithm is biased.",
        "text_length": 311,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.023350117728114128,
          0.05045124143362045,
          -0.011165213771164417,
          0.013522207736968994,
          -0.0019328695489093661,
          0.04239765182137489,
          0.01808447204530239,
          -0.006137272343039513,
          -0.0264423917979002,
          0.019831815734505653
        ]
      },
      {
        "chunk_index": 134,
        "relative_chunk_number": 135,
        "text": "This bias arises because the output probability is determined by the ratio of weights calculated within a single stochastic batch, and the expectation of a ratio is generally not equal to the ratio of expectations. However, its key strength is that it is asymptotically unbiased: as the batch size N approaches infinity, the output distribution converges to the true target distribution.",
        "text_length": 387,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00766743253916502,
          0.04898218438029289,
          -0.009505807422101498,
          0.04505316540598869,
          -0.030971363186836243,
          0.0019560307264328003,
          0.022544655948877335,
          0.0021938742138445377,
          -0.025811685249209404,
          -0.06381388008594513
        ]
      },
      {
        "chunk_index": 135,
        "relative_chunk_number": 136,
        "text": "We formalize this crucial property in the following theorem. Theorem 3. Let Palg(x; N) be the probability of sampling x using Algorithm 2 with a batch size of N, and let PT (x) = P(x)n/ZT be the true target distribution at temperature T = 1/n, where ZT = P x P(x)n. The algorithm is asymptotically unbiased: lim N\u2192\u221ePalg(x; N) = PT (x). The proof is provided in Appendix A.3.",
        "text_length": 374,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.005611554719507694,
          0.018364200368523598,
          -0.031986456364393234,
          0.026416348293423653,
          -0.0069345165975391865,
          0.0076546682976186275,
          0.015176605433225632,
          -0.031198758631944656,
          -0.030079538002610207,
          -0.03194364905357361
        ]
      },
      {
        "chunk_index": 136,
        "relative_chunk_number": 137,
        "text": "This property of consistency establishes the algorithm as a principled approximation, where the batch size N serves as a practical lever for the trade- off between efficiency and accuracy. Because the algorithm relies solely on a black-box sampling interface, its utility extends naturally beyond the CALM framework to the entire class of implicit language models.",
        "text_length": 364,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.012688163667917252,
          0.02810005284845829,
          -0.024060513824224472,
          0.024942437186837196,
          -0.040606290102005005,
          0.028775280341506004,
          0.02077820524573326,
          -0.0038945116102695465,
          -0.02684764564037323,
          -0.004542638547718525
        ]
      },
      {
        "chunk_index": 137,
        "relative_chunk_number": 138,
        "text": "This positions it as a universal toolkit for controlled decoding in discrete spaces. 6 RELATED WORK 6.1 AUTOENCODER Latent Generative Modeling. A prominent paradigm in generative modeling involves a two-stage process: first learning a compressed latent representation of the data, and then training a generative model within that latent space.",
        "text_length": 343,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.024000586941838264,
          0.037536364048719406,
          0.00445141363888979,
          0.014925478026270866,
          -0.057057373225688934,
          -0.03474825620651245,
          0.022860221564769745,
          -0.02909652702510357,
          -0.03557449206709862,
          0.028299162164330482
        ]
      },
      {
        "chunk_index": 138,
        "relative_chunk_number": 139,
        "text": "This approach often begins with a Variational Autoencoder (Kingma & Welling, 2014), which learns a mapping from a high-dimensional data space into a compact, con- tinuous latent space. This principle enables modern architectures, such as latent diffusion models (Rombach et al., 2022; Liu et al., 2023), to efficiently generate high-dimensional data from a con- tinuous latent representation.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.041254494339227676,
          0.022509928792715073,
          -0.019082583487033844,
          -0.0007807064685039222,
          0.00387784605845809,
          -0.04292440041899681,
          0.021980738267302513,
          -0.026019642129540443,
          -0.02827409841120243,
          -0.03217363357543945
        ]
      },
      {
        "chunk_index": 139,
        "relative_chunk_number": 140,
        "text": "An alternative path, the Vector Quantized VAE (VQ-VAE, van den Oord et al., 2017), learns a discrete latent space by mapping inputs to a finite, learned codebook.",
        "text_length": 162,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012283640913665295,
          0.01931215263903141,
          -0.003923839423805475,
          0.013340516947209835,
          -0.009572683833539486,
          -0.017116984352469444,
          0.023165814578533173,
          -0.006636370439082384,
          -0.03575111925601959,
          0.044366564601659775
        ]
      },
      {
        "chunk_index": 140,
        "relative_chunk_number": 141,
        "text": "This approach has been foundational to the autoregressive generation of continuous data like images (Razavi et al., 2019; Esser et al., 2021; Ramesh et al., 2021; Sun et al., 2024a) and audio (Dhariwal et al., 2020; Zeghidour et al., 2021; D\u00b4efossez et al., 2023). Our approach introduces a distinct way by performing a discrete-to-continuous mapping.",
        "text_length": 351,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.023033883422613144,
          0.008170892484486103,
          -0.026649851351976395,
          0.03674880042672157,
          -0.02407337725162506,
          -0.03419364616274834,
          0.010461552068591118,
          -0.0020507487934082747,
          -0.03352772071957588,
          0.009553882293403149
        ]
      },
      {
        "chunk_index": 141,
        "relative_chunk_number": 142,
        "text": "Driven by the pursuit of efficiency, it significantly reduces the number of autoregressive steps required for language generation. 12 Preprint Text Compression. Compressing long text into compact vector representations is a foundational concept in sequence modeling.",
        "text_length": 266,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.017488013952970505,
          0.036662764847278595,
          -0.006279791705310345,
          0.02572771906852722,
          -0.027905495837330818,
          0.008203793317079544,
          0.02274162881076336,
          -0.005370874889194965,
          -0.03644123673439026,
          0.001795445685274899
        ]
      },
      {
        "chunk_index": 142,
        "relative_chunk_number": 143,
        "text": "For instance, Recurrent Neural Networks can be viewed as implic- itly compressing the entire history of a sequence into a single hidden state vector (Elman, 1990; Hochreiter & Schmidhuber, 1997). In the era of LLMs, this concept has been revitalized, with a fo- cus on prompt compression to improve inference efficiency. For example, Mu et al.",
        "text_length": 343,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.06414473801851273,
          0.02144761011004448,
          -0.04312806949019432,
          0.02481324039399624,
          0.002836789470165968,
          0.037172574549913406,
          0.017996298149228096,
          0.005962937138974667,
          -0.03234373405575752,
          0.02295352891087532
        ]
      },
      {
        "chunk_index": 143,
        "relative_chunk_number": 144,
        "text": "(2023) designed a modified attention mechanism to distill prompt information into a few memory tokens. Chevalier et al. (2023); Ge et al. (2024); Gao et al. (2024) further introduced explicit reconstruction objectives to promote high fidelity compression. Recently, Li et al. (2025); Kuratov et al.",
        "text_length": 298,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.018757661804556847,
          -0.007326638326048851,
          -0.017988093197345734,
          0.014391912147402763,
          0.011908216401934624,
          -0.007752572651952505,
          0.018256740644574165,
          0.004374299664050341,
          -0.03399675339460373,
          0.06311611831188202
        ]
      },
      {
        "chunk_index": 144,
        "relative_chunk_number": 145,
        "text": "(2025); Mezentsev & Oseledets (2025) pushed the limits of compression to a ratio up to 1568x, underscoring the in- herent sparsity of discrete text representations. More recently, DeepSeek-OCR (Wei et al., 2025) demonstrated compressing text into continuous image tokens, showing promise for applications like long-context compression.",
        "text_length": 335,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010803403332829475,
          -0.013280726037919521,
          -0.03582677245140076,
          0.03268988057971001,
          -0.0055853985249996185,
          -0.03529196232557297,
          0.0206811111420393,
          -0.021620100364089012,
          -0.03269396349787712,
          -0.012169579043984413
        ]
      },
      {
        "chunk_index": 145,
        "relative_chunk_number": 146,
        "text": "The primary focus of these methods on prompt compression places a greater emphasis on reconstruction fidelity than on the robustness of the resulting representation. Our work, by contrast, prioritizes the creation of a robust and smooth latent manifold, which is a critical prerequisite for stable downstream generative modeling.",
        "text_length": 329,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00381145509891212,
          0.026345038786530495,
          -0.012224617414176464,
          0.03247087076306343,
          -0.003093238454312086,
          -0.06296783685684204,
          0.01943456195294857,
          0.016688929870724678,
          -0.024448253214359283,
          0.015362318605184555
        ]
      },
      {
        "chunk_index": 146,
        "relative_chunk_number": 147,
        "text": "6.2 LIKELIHOOD-FREE LANGUAGE MODELING Continuous Autoregressive Generation.",
        "text_length": 75,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04393874108791351,
          0.014578282833099365,
          -0.029825573787093163,
          0.015443113632500172,
          0.00776513060554862,
          0.009613130241632462,
          0.021314730867743492,
          -0.03315595164895058,
          -0.029410023242235184,
          0.01133767981082201
        ]
      },
      {
        "chunk_index": 147,
        "relative_chunk_number": 148,
        "text": "Autoregressive generation over continuous vectors is an emerging research frontier, with notable successes in domains such as image (Tschannen et al., 2023; Li et al., 2024; Shao et al., 2025b; Fan et al., 2025; Team, 2025), video (Chen et al., 2024; Deng et al., 2025), and audio synthesis (Turetzky et al., 2024; Sun et al., 2024b; Ma et al., 2025).",
        "text_length": 351,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.019163895398378372,
          0.028814399614930153,
          -0.015413572080433369,
          0.03126866742968559,
          -0.0066607980988919735,
          -0.03830916807055473,
          0.006945851258933544,
          -0.03504746034741402,
          -0.02144879475235939,
          0.05695420876145363
        ]
      },
      {
        "chunk_index": 148,
        "relative_chunk_number": 149,
        "text": "GIVT (Tschannen et al., 2023) pioneered this direction by fitting the distribution of the target vector with a Gaussian Mixture Model. However, the expressive power of GIVT is confined to the pre-defined family of Gaussian mixtures, a constraint that limits its ability to capture complex distributions. Li et al.",
        "text_length": 313,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03625236079096794,
          -0.0005767453112639487,
          -0.02675805427134037,
          0.04487181454896927,
          0.023596011102199554,
          -0.02014707587659359,
          0.02042158879339695,
          -0.033134181052446365,
          -0.03379816934466362,
          0.019046520814299583
        ]
      },
      {
        "chunk_index": 149,
        "relative_chunk_number": 150,
        "text": "(2024) overcomes this limitation by employing a lightweight diffusion head to model the vector distribution. While being more expressive, this method comes at the cost of inference effi- ciency due to its iterative sampling process. More recently, Shao et al. (2025b) introduced a general framework based on strictly proper scoring rules.",
        "text_length": 338,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03590802475810051,
          0.006837517488747835,
          -0.014335241168737411,
          0.035681869834661484,
          -0.002283289097249508,
          -0.00736044580116868,
          0.020937006920576096,
          -0.008563967421650887,
          -0.029646731913089752,
          -0.03255186602473259
        ]
      },
      {
        "chunk_index": 150,
        "relative_chunk_number": 151,
        "text": "The Energy Transformer was presented as a con- crete and powerful instance of this framework, capable of high-quality, single-step generation.",
        "text_length": 142,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.023942137137055397,
          0.026196714490652084,
          0.003808271372690797,
          0.043698787689208984,
          0.012625723145902157,
          -0.041352223604917526,
          0.018397092819213867,
          0.0017199430149048567,
          -0.040134768933057785,
          -0.03840131312608719
        ]
      },
      {
        "chunk_index": 151,
        "relative_chunk_number": 152,
        "text": "Our work adopts the core Energy Transformer framework but introduce several key improvements to the generative head architecture, the energy loss, and the model\u2019s input structure, to further enhance its performance and stability for the specific challenges of language modeling. Parallel Token Prediction.",
        "text_length": 305,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00955465342849493,
          0.0050748856738209724,
          0.007630620617419481,
          0.0564308688044548,
          0.02459743805229664,
          -0.07939252257347107,
          0.020630693063139915,
          -0.03872955963015556,
          -0.03437279164791107,
          -0.067779541015625
        ]
      },
      {
        "chunk_index": 152,
        "relative_chunk_number": 153,
        "text": "The goal of predicting multiple tokens in parallel to overcome the sequential bottleneck of autoregressive models is a long-standing pursuit in sequence modeling.",
        "text_length": 162,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0035693610552698374,
          0.04116799309849739,
          -0.010232052765786648,
          0.02872195653617382,
          -0.03138423711061478,
          -0.029645675793290138,
          0.022212142124772072,
          -0.04095297306776047,
          -0.02760244905948639,
          0.005964315962046385
        ]
      },
      {
        "chunk_index": 153,
        "relative_chunk_number": 154,
        "text": "Early efforts in this area were pioneered by non-autoregressive machine translation (Gu et al., 2018; Gu & Kong, 2021; Shao et al., 2021; Shao & Feng, 2022; Huang et al., 2022; Gui et al., 2023), which aims to generate an entire target sentence in a single step.",
        "text_length": 262,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0029290581587702036,
          0.028800956904888153,
          -0.013957483693957329,
          0.024393882602453232,
          -0.0421154759824276,
          -0.015630684792995453,
          0.01632029563188553,
          -0.012137848883867264,
          -0.04080676659941673,
          0.012895883992314339
        ]
      },
      {
        "chunk_index": 154,
        "relative_chunk_number": 155,
        "text": "While effective for highly constrained conditional tasks like translation, these methods often struggle with the inherent multi-modality of open-ended language generation.",
        "text_length": 171,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008906961418688297,
          0.049079086631536484,
          -0.031553003937006,
          0.04544084519147873,
          -0.04915148392319679,
          0.028516393154859543,
          0.022397367283701897,
          0.002892567776143551,
          -0.041955266147851944,
          0.04723670333623886
        ]
      },
      {
        "chunk_index": 155,
        "relative_chunk_number": 156,
        "text": "A different line of work uses multi-token prediction to enrich training signals (Gloeckle et al., 2024; Shao et al., 2025a) or provide candidates for speculative decoding (Stern et al., 2018; Leviathan et al., 2023), while the underlying generation remains single- token autoregressive.",
        "text_length": 286,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.033904775977134705,
          0.014644501730799675,
          -0.012158100493252277,
          0.0054604108445346355,
          -0.027935655787587166,
          -0.002801169641315937,
          0.018127771094441414,
          -0.008837133646011353,
          -0.02701692283153534,
          0.0037594116292893887
        ]
      },
      {
        "chunk_index": 156,
        "relative_chunk_number": 157,
        "text": "A more direct approach involves hierarchical modeling, where a global model predicts large semantic chunks, which are then decoded by a local model (Lee et al., 2022; YU et al., 2023; Ho et al., 2024; team et al., 2024; Pagnoni et al., 2025; Neitemeier et al., 2025).",
        "text_length": 267,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00860774889588356,
          0.02754075638949871,
          -0.014076340943574905,
          0.03653496131300926,
          -0.03261516988277435,
          0.017796458676457405,
          0.012225457467138767,
          0.02444447949528694,
          -0.022995546460151672,
          -0.016380714252591133
        ]
      },
      {
        "chunk_index": 157,
        "relative_chunk_number": 158,
        "text": "For instance, MegaByte (YU et al., 2023) uses a global Transformer to predict blocks of tokens, but still relies on a local autoregressive model to generate tokens sequentially within each block. Conceptually closer to our work, Large Concept Models (team et al., 2024) also adopt a hierarchical structure, where their global model autoregressively predicts continuous sentence embeddings.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.016978371888399124,
          0.018082786351442337,
          -0.024628443643450737,
          0.029715200886130333,
          -0.012665540911257267,
          -0.035144202411174774,
          0.020889153704047203,
          -0.0279201902449131,
          -0.03215441480278969,
          0.02736290916800499
        ]
      },
      {
        "chunk_index": 158,
        "relative_chunk_number": 159,
        "text": "However, this approach faces several challenges that our CALM framework is designed to address: its SONAR autoencoder (Duquenne et al., 2023) is computationally heavy and fragile, and its reliance on a diffusion-based generative process introduces a iterative inference bottleneck.",
        "text_length": 281,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.002193544525653124,
          0.0157049261033535,
          -0.023994097486138344,
          0.0057404134422540665,
          -0.021870708093047142,
          -0.03016042895615101,
          0.019494744017720222,
          0.016572099179029465,
          -0.02627512812614441,
          -0.03549189493060112
        ]
      },
      {
        "chunk_index": 159,
        "relative_chunk_number": 160,
        "text": "Finally, another paradigm for parallel generation is diffusion models for text, which iteratively refine a sequence of tokens from noise, either at the full sentence (Austin et al., 2021; Li et al., 2022; Lou et al., 2024) or block level (Han et al., 2023; Arriola et al., 2025).",
        "text_length": 279,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.028679508715867996,
          0.026020057499408722,
          -0.012071802280843258,
          0.03145243600010872,
          -0.01744196005165577,
          -0.04794308543205261,
          0.01456649973988533,
          -0.002864559879526496,
          -0.0254916250705719,
          -0.03555307537317276
        ]
      },
      {
        "chunk_index": 160,
        "relative_chunk_number": 161,
        "text": "These models, which currently operate in the challenging discrete token space, could potentially benefit from the robust continuous space our autoencoder provides. 13 Preprint 6.3 LIKELIHOOD-FREE LM EVALUATION LM metrics.",
        "text_length": 221,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04460549354553223,
          0.0009004959720186889,
          -0.01838918961584568,
          0.01972903124988079,
          -0.027999408543109894,
          0.04519304633140564,
          0.024576174095273018,
          0.020656555891036987,
          -0.035025112330913544,
          -0.020487170666456223
        ]
      },
      {
        "chunk_index": 161,
        "relative_chunk_number": 162,
        "text": "The evaluation of language models is split into two distinct paradigms, which reflects the separation between assessing the quality of generated output and the fidelity of the learned distribution. On one hand, likelihood-based metrics, such as Perplexity, offer a principled way to evaluate the learned distribution, but they are limited to models where likelihoods are tractable.",
        "text_length": 381,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.019582245498895645,
          0.01795627921819687,
          -0.0037267967127263546,
          0.013437082059681416,
          -0.022498726844787598,
          -0.01582728512585163,
          0.018941882997751236,
          -0.01235604751855135,
          -0.018883798271417618,
          0.04952308163046837
        ]
      },
      {
        "chunk_index": 162,
        "relative_chunk_number": 163,
        "text": "On the other hand, a diverse family of sample-based metrics focuses on the generated output. Classic methods like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) assess the quality of generated text by comparing it to reference outputs.",
        "text_length": 239,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0018677124753594398,
          -0.0018953124526888132,
          -0.021134426817297935,
          0.04099782183766365,
          0.011081228964030743,
          0.02873070538043976,
          0.020450105890631676,
          -0.016444506123661995,
          -0.04029297083616257,
          0.01928478479385376
        ]
      },
      {
        "chunk_index": 163,
        "relative_chunk_number": 164,
        "text": "More recent approaches such as MAUVE (Pillutla et al., 2021) or LLM-as-a-judge (Zheng et al., 2023) allows for reference-free evaluation, but they rely on heuristics or black-box models and lack the formal guarantees of scoring rules.",
        "text_length": 234,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012304653413593769,
          0.02041143737733364,
          -0.012889954261481762,
          -0.012640168890357018,
          0.055241648107767105,
          0.024394359439611435,
          0.02374895289540291,
          -0.012954933568835258,
          -0.02710837870836258,
          -0.030507422983646393
        ]
      },
      {
        "chunk_index": 164,
        "relative_chunk_number": 165,
        "text": "Our proposed metric, BrierLM, is designed to bridge this gap by combining the advantages of both paradigms: it operates exclusively on model samples, yet as a strictly proper scoring rule, it offers a faithful assessment of the model\u2019s predictive quality, akin to perplexity. Brier Score.",
        "text_length": 288,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.016361212357878685,
          0.01997111178934574,
          -0.04059717431664467,
          0.020068557932972908,
          -0.0015101295430213213,
          0.01629466935992241,
          0.02123216725885868,
          -0.027222076430916786,
          -0.028138745576143265,
          -0.05950240045785904
        ]
      },
      {
        "chunk_index": 165,
        "relative_chunk_number": 166,
        "text": "The Brier Score was originally proposed by Brier (1950) for the evaluation of prob- abilistic weather forecasts. It is a classic example of a strictly proper scoring rules, theoretically guaranteeing that a model is incentivized to report its true belief to achieve the optimal score (Gneit- ing & Raftery, 2007).",
        "text_length": 313,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.043006766587495804,
          0.030694372951984406,
          -0.03061697632074356,
          0.026638908311724663,
          0.0263446643948555,
          0.03338164463639259,
          0.021581249311566353,
          -0.06385030597448349,
          -0.034209243953228,
          -0.01616695523262024
        ]
      },
      {
        "chunk_index": 166,
        "relative_chunk_number": 167,
        "text": "Consequently, it has been widely adopted in classification tasks, primarily for evaluating the quality of probabilistic forecasts (Sanders, 1963; fer, 2009; Hui & Belkin, 2021) and assessing model calibration (Lakshminarayanan et al., 2017; Ovadia et al., 2019; Gruber & Buettner, 2022).",
        "text_length": 287,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0030695823952555656,
          0.016548719257116318,
          -0.00724630244076252,
          0.053988758474588394,
          -0.037979040294885635,
          0.009566346183419228,
          0.015053261071443558,
          -0.024285491555929184,
          -0.033837057650089264,
          0.04489096626639366
        ]
      },
      {
        "chunk_index": 167,
        "relative_chunk_number": 168,
        "text": "The innovation of our work is twofold: first, we introduce a method to unbiasedly estimate the Brier score in a likelihood-free manner; second, we generalize its application from a metric for simple classification tasks to one capable of assessing language modeling capabilities. 6.4 LIKELIHOOD-FREE TEMPERATURE SAMPLING Bernoulli Factory.",
        "text_length": 339,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.002042243257164955,
          -0.0003612148866523057,
          -0.003636034205555916,
          0.03408542275428772,
          -0.031076550483703613,
          0.004735750611871481,
          0.02374282479286194,
          0.008560600690543652,
          -0.03288543224334717,
          0.020787877961993217
        ]
      },
      {
        "chunk_index": 168,
        "relative_chunk_number": 169,
        "text": "The temperature sampling problem is conceptually related to the classic problem of the Bernoulli Factory (Keane & O\u2019Brien, 1994; Occil, 2020), which addresses the challenge of simulating a new coin with a success probability of f(p) given only a coin with an unknown success probability p.",
        "text_length": 289,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004705195315182209,
          0.0056315818801522255,
          -0.025851735845208168,
          0.04015437513589859,
          -0.059685833752155304,
          0.0076722148805856705,
          0.01982385665178299,
          -0.023117011412978172,
          -0.04287463799118996,
          -0.03192273899912834
        ]
      },
      {
        "chunk_index": 169,
        "relative_chunk_number": 170,
        "text": "This mirrors our challenge of achieving a target probability proportional to P(x)1/T using only a base sampler for the implicit distribution P(x). A key distinction is that the Bernoulli Factory problem assumes a binary outcome, whereas we operate over a large discrete sample space. Our two-stage algorithm elegantly bridges this gap.",
        "text_length": 335,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0217676293104887,
          0.019908620044589043,
          -0.01626809500157833,
          0.030529776588082314,
          -0.016332373023033142,
          -0.035345520824193954,
          0.01956164836883545,
          0.003672003047540784,
          -0.036225102841854095,
          -0.06167984753847122
        ]
      },
      {
        "chunk_index": 170,
        "relative_chunk_number": 171,
        "text": "The first stage isolates a single candidate x\u2217 and reduces the problem to a binary one, and the second stage directly applies an existing Bernoulli Factory algorithm (Mendo, 2019) to construct an event with probability P(x\u2217)\u03b1. Controlled Generation.",
        "text_length": 249,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.021865371614694595,
          0.03860294073820114,
          -0.003623794298619032,
          0.026600798591971397,
          -0.06234489381313324,
          -0.025027338415384293,
          0.01917378231883049,
          0.0007467485847882926,
          -0.035421378910541534,
          0.029827870428562164
        ]
      },
      {
        "chunk_index": 171,
        "relative_chunk_number": 172,
        "text": "While many generative models lack the explicit probabilistic controls for temperature sampling, they have developed alternative strategies to navigate the trade-off between sample quality and diversity.",
        "text_length": 202,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0022053648717701435,
          0.013112092390656471,
          0.00021144229685887694,
          0.058362171053886414,
          0.00879843533039093,
          -0.0014517897507175803,
          0.019621863961219788,
          -0.02342762053012848,
          -0.03949299082159996,
          0.003538002260029316
        ]
      },
      {
        "chunk_index": 172,
        "relative_chunk_number": 173,
        "text": "For instance, VAEs and normalizing flows (Kingma & Welling, 2014; Rezende & Mohamed, 2015) often achieve this by adjusting the variance of their prior latent distri- bution (Kingma & Dhariwal, 2018). In Generative Adversarial Networks (Goodfellow et al., 2014), the truncation trick restricts sampling to a high-density region of the latent space (Brock et al., 2019).",
        "text_length": 368,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0010124781401827931,
          0.000279079657047987,
          -0.015513810329139233,
          0.0428747683763504,
          -0.026314133778214455,
          -0.06179467961192131,
          0.01998295821249485,
          -0.014590276405215263,
          -0.030172070488333702,
          0.047262851148843765
        ]
      },
      {
        "chunk_index": 173,
        "relative_chunk_number": 174,
        "text": "Similarly, diffusion models can control stochasticity by altering the noise variance during the reverse sampling process (Song et al., 2021). These techniques, however, are fundamentally heuristic, as it is generally intractable to characterize the shape of the modified output distribution, and they all require white-box access to model internals like the latent space.",
        "text_length": 371,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009174305945634842,
          0.010807756334543228,
          -0.03143468126654625,
          0.037673767656087875,
          0.01074029691517353,
          -0.03558263182640076,
          0.019984973594546318,
          -0.027665939182043076,
          -0.025619158521294594,
          -0.01719329133629799
        ]
      },
      {
        "chunk_index": 174,
        "relative_chunk_number": 175,
        "text": "Our work, in contrast, proposes a universal, black-box algorithm for temperature sampling from implicit models over discrete spaces, offering a provably exact method for this broad class of models. 7 EXPERIMENTS 7.1 SETTINGS Datasets. We train our models on the Pile uncopyrighted dataset (Gao et al., 2020).",
        "text_length": 308,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.013507294468581676,
          0.03351697698235512,
          -0.019527560099959373,
          0.0481480248272419,
          0.015556049533188343,
          -0.037583064287900925,
          0.015952330082654953,
          -0.029897363856434822,
          -0.03257414698600769,
          -0.013704968616366386
        ]
      },
      {
        "chunk_index": 175,
        "relative_chunk_number": 176,
        "text": "The raw text is processed with the Llama 3 tokenizer (Grattafiori et al., 2024), resulting in a training set of \u223c230B tokens. We evaluate model performance on the WikiText-103 benchmark (Merity et al., 2017). Model. Our models are built upon a standard Transformer backbone.",
        "text_length": 274,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010169423185288906,
          0.010516654700040817,
          -0.01744784787297249,
          0.01502852700650692,
          -0.028934268280863762,
          0.007882057689130306,
          0.023546693846583366,
          -0.012006510980427265,
          -0.03763893246650696,
          -0.004423672799021006
        ]
      },
      {
        "chunk_index": 176,
        "relative_chunk_number": 177,
        "text": "We adopt most of the ar- chitecture designs from the LLaMA family (Touvron et al., 2023), including RMSNorm (Zhang & 14 Preprint Table 1: Performance and computational cost comparison between Transformer baselines and CALM (K=4). CALM\u2019s reported parameter counts and FLOPs include all overhead from the au- toencoder (75M parameters, training cost, and encoding/decoding FLOPs).",
        "text_length": 378,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00031980095081962645,
          0.02541254460811615,
          -0.017149098217487335,
          0.035369813442230225,
          -0.012967728078365326,
          -0.0038917616475373507,
          0.024931391701102257,
          -0.0014283190248534083,
          -0.0359453521668911,
          -0.044439107179641724
        ]
      },
      {
        "chunk_index": 177,
        "relative_chunk_number": 178,
        "text": "Attention FLOPs are calculated assuming a context length of 2048.",
        "text_length": 65,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.07844653725624084,
          0.008534717373549938,
          -0.029179980978369713,
          0.03816240653395653,
          0.06470804661512375,
          -0.009535955265164375,
          0.020592231303453445,
          -0.0029783977661281824,
          -0.024355696514248848,
          0.0377490296959877
        ]
      },
      {
        "chunk_index": 178,
        "relative_chunk_number": 179,
        "text": "Model #Params Train FLOPs Infer FLOPs BrierLM (total, 1e20) (per token, 1e8) Transformer-S 281M 6.6 4.4 6.05 Transformer-M 465M 11.9 7.9 7.07 Transformer-L 849M 22.5 15.0 8.98 CALM-M (K=4) 371M 3.7 2.9 5.72 CALM-L (K=4) 735M 7.7 4.6 6.58 CALM-XL (K=4) 1.82B 19.5 9.4 8.53 Sennrich, 2019), SwiGLU activation (Shazeer, 2020a), and rotary positional embeddings (Su et al., 2021).",
        "text_length": 376,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02713770605623722,
          -0.004755019210278988,
          -0.028821296989917755,
          0.04685046896338463,
          -0.002370510483160615,
          -0.016966210678219795,
          0.01611277088522911,
          0.017218010500073433,
          -0.014081630855798721,
          -0.011647951789200306
        ]
      },
      {
        "chunk_index": 179,
        "relative_chunk_number": 180,
        "text": "We experiment with four scales: S (12 layers, hidden size=768, intermediate size=2048), M (16 layers, hidden size=1024, intermediate size=2752), L (16 layers, hidden size=1536, intermedi- ate size=4096), and XL (16 layers, hidden size=2560, intermediate size=6880). Training Details. The training process for our CALM framework is two-staged.",
        "text_length": 342,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.025545265525579453,
          -0.009773836471140385,
          -0.024791967123746872,
          0.013254499062895775,
          0.03602542728185654,
          -0.0005173730314709246,
          0.01587248034775257,
          -0.022950759157538414,
          -0.016594290733337402,
          -0.05066617578268051
        ]
      },
      {
        "chunk_index": 180,
        "relative_chunk_number": 181,
        "text": "We first train a suite of autoencoders on a 15B token subset of the Pile to map token chunks of size K \u2208{1, 2, 4, 8} into continuous vectors. These autoencoders use a hidden size of 512, a latent dimension of 32K, have approximately 75M parameters, and are trained for 30k steps with a batch size of 512k tokens.",
        "text_length": 312,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00012414131197147071,
          0.011034786701202393,
          -0.03447601944208145,
          0.02017018385231495,
          -0.0009933288674801588,
          0.03823934495449066,
          0.021800782531499863,
          -0.027485070750117302,
          -0.03510192036628723,
          -0.014452110044658184
        ]
      },
      {
        "chunk_index": 181,
        "relative_chunk_number": 182,
        "text": "Following this, the CALM models are trained on the remaining data for 250k steps with a batch size of 2 million tokens. The context length is set to 2048 steps; for CALM, this corresponds to 2048K tokens. All models are optimized using the AdamW optimizer (Loshchilov & Hutter, 2019) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 1e \u22128.",
        "text_length": 320,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.026892436668276787,
          0.026228100061416626,
          -0.029132843017578125,
          0.020680325105786324,
          0.026274140924215317,
          0.040820006281137466,
          0.016717780381441116,
          -0.021926991641521454,
          -0.020484372973442078,
          -0.03664529696106911
        ]
      },
      {
        "chunk_index": 182,
        "relative_chunk_number": 183,
        "text": "We use a learning rate of 3 \u00d7 10\u22124 with a constant schedule and a warmup of 2000 steps, a weight decay of 0.1, and gradient clipping of 1.0. 7.2 MAIN RESULTS We present the primary results of our comparison between the standard Transformer baselines and our CALM framework (with a fixed chunk size of K=4) in Table 1.",
        "text_length": 317,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006584471557289362,
          0.00898680742830038,
          -0.035657744854688644,
          0.023962901905179024,
          0.032275985926389694,
          -0.025635061785578728,
          0.0181376151740551,
          -0.04454431310296059,
          -0.02643117494881153,
          -0.054085198789834976
        ]
      },
      {
        "chunk_index": 183,
        "relative_chunk_number": 184,
        "text": "The results demonstrate that CALM establishes a new, more efficient performance-compute frontier for language modeling. By increasing the semantic bandwidth of each autoregressive step, CALM is allowed to be substantially larger in parameter count while demanding fewer FLOPs for both training and inference.",
        "text_length": 308,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01621958799660206,
          0.0294154305011034,
          -0.04906165972352028,
          0.023035719990730286,
          -0.03381457179784775,
          0.009372793138027191,
          0.021133918315172195,
          -0.020479414612054825,
          -0.02080906182527542,
          0.0010143150575459003
        ]
      },
      {
        "chunk_index": 184,
        "relative_chunk_number": 185,
        "text": "For in- stance, our 371M parameter CALM-M model achieves a BrierLM score comparable to the 281M Transformer-S baseline, yet requires 44% fewer training FLOPs and 34% fewer inference FLOPs. Furthermore, the results confirm that CALM benefits from scaling just as effectively as traditional Transformers, allowing performance to be consistently improved by increasing model size.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.022897759452462196,
          0.012927875854074955,
          -0.05192763730883598,
          0.020228339359164238,
          0.013558220118284225,
          -0.009723171591758728,
          0.0197514109313488,
          -0.02854606881737709,
          -0.020238475874066353,
          -0.0725899189710617
        ]
      },
      {
        "chunk_index": 185,
        "relative_chunk_number": 186,
        "text": "In addition to scaling model size, our framework introduces the semantic bandwidth K as a new lever for navigating the performance-compute landscape. Figure 4 illustrates this by plotting the performance of CALM-L with varying K against the standard Transformer scaling curve.",
        "text_length": 276,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.032664697617292404,
          0.007793058175593615,
          -0.022717703133821487,
          0.02572270669043064,
          -0.003978319466114044,
          -0.023471621796488762,
          0.02318277582526207,
          -0.049283094704151154,
          -0.018913552165031433,
          0.0017712999833747745
        ]
      },
      {
        "chunk_index": 186,
        "relative_chunk_number": 187,
        "text": "Notably, CALM-L with K = 1 operates at a significant disadvantage, demanding more FLOPs for lower performance compared to its discrete counterpart. The gap arises because the model undertakes the more challenging continuous prediction task, which in turn highlights the significant room for future architectural and algorithmic optimizations. The advantages of CALM become apparent as we increase K.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.026568014174699783,
          0.028934868052601814,
          -0.07372137159109116,
          0.032573066651821136,
          -0.030808813869953156,
          0.020299026742577553,
          0.019334204494953156,
          -0.03157804161310196,
          -0.01978568360209465,
          -0.0031300524715334177
        ]
      },
      {
        "chunk_index": 187,
        "relative_chunk_number": 188,
        "text": "Moving from K = 1 to K = 2 nearly halves the cost with only a marginal drop in performance, and at K = 4, the CALM model surpasses the baseline performance-compute frontier. This finding validates our central hypothesis: scaling the semantic bandwidth of each generative step provides a new and highly effective axis for optimizing the performance-compute trade-off in language models.",
        "text_length": 385,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.009488281793892384,
          0.03983621299266815,
          -0.05106868967413902,
          0.01768249087035656,
          -0.019374459981918335,
          0.009614644572138786,
          0.022812392562627792,
          -0.06830985099077225,
          -0.020749956369400024,
          -0.0026237971615046263
        ]
      },
      {
        "chunk_index": 188,
        "relative_chunk_number": 189,
        "text": "Further increasing the chunk size to K = 8 leads to a larger performance drop, which is likely a model capacity limitation. We hypothesize that larger models may be required to leverage the benefits of higher semantic bandwidths.",
        "text_length": 229,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.018557382747530937,
          0.028065497055649757,
          -0.0369313545525074,
          0.036727700382471085,
          -4.8156762204598635e-05,
          -0.0003191180294379592,
          0.021962497383356094,
          -0.024672856554389,
          -0.023396100848913193,
          0.0067412699572741985
        ]
      },
      {
        "chunk_index": 189,
        "relative_chunk_number": 190,
        "text": "15 Preprint 0.5 1.0 1.5 2.0 2.5 3.0 Training FLOPs 1e21 6 7 8 9 BrierLM K=1 K=2 K=4 K=8 Transformer (Varying Size) CALM-L (Varying K) Figure 4: The effect of chunk size K on the performance-compute trade-off. 0 50 100 150 200 250 Training Steps (k) 0 2 4 6 8 10 BrierLM Transformer-L Transformer-M CALM-XL Figure 5: Training progress of CALM and tra- ditional Transformer models.",
        "text_length": 379,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006222775671631098,
          0.015456055290997028,
          -0.04212797433137894,
          0.04955810308456421,
          -0.0011998012196272612,
          0.009346880950033665,
          0.023231666535139084,
          -0.004197983071208,
          -0.019052639603614807,
          -0.004518185276538134
        ]
      },
      {
        "chunk_index": 190,
        "relative_chunk_number": 191,
        "text": "To further investigate the learning dynamics of our framework, we plot the training curves of CALM- XL against the Transformer baselines in Figure 5. The baseline Transformer models exhibit rapid initial gains before their performance gradually begins to saturate.",
        "text_length": 264,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.007322633173316717,
          0.015899328514933586,
          -0.03001146949827671,
          0.04429629072546959,
          0.02899128943681717,
          -0.057135727256536484,
          0.021305354312062263,
          -0.004685328807681799,
          -0.025741465389728546,
          -0.06281932443380356
        ]
      },
      {
        "chunk_index": 191,
        "relative_chunk_number": 192,
        "text": "In contrast, CALM-XL displays a more patient but ultimately steeper learning curve, initially trailing Transformer-M but progres- sively closing the performance gap with the Transformer-L model. We attribute this phenomenon to the different nature of the predictive task.",
        "text_length": 271,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.006861126981675625,
          0.0035428053233772516,
          -0.07816077023744583,
          0.023728149011731148,
          -0.002491101622581482,
          -0.019691964611411095,
          0.020619820803403854,
          -0.0052165016531944275,
          -0.020269466564059258,
          -0.06781625747680664
        ]
      },
      {
        "chunk_index": 192,
        "relative_chunk_number": 193,
        "text": "While the baseline models learn the relatively simple task of predicting a single, low-information discrete token, our CALM model must learn to model the complex, high-dimensional distribution of continuous vectors, which explains the slower initial progress.",
        "text_length": 259,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.019219981506466866,
          0.008577276021242142,
          -0.04111843928694725,
          0.011573569849133492,
          -0.007959302514791489,
          -0.028821924701333046,
          0.018966609612107277,
          -0.01406059879809618,
          -0.026635177433490753,
          -0.05242500454187393
        ]
      },
      {
        "chunk_index": 193,
        "relative_chunk_number": 194,
        "text": "However, once this ability is established, the model can unlock the potential of its large parameter count, entering a phase of more significant and sustained improvements. 7.3 EFFECT OF AUTOENCODER In this section, we study the effect of the autoencoder\u2019s design choices on the final performance of the CALM framework.",
        "text_length": 319,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.028002556413412094,
          0.036993157118558884,
          -0.03043508715927601,
          0.037681397050619125,
          -0.08277571201324463,
          0.004926809575408697,
          0.0194462388753891,
          -0.05005844309926033,
          -0.03832899406552315,
          -0.003573987167328596
        ]
      },
      {
        "chunk_index": 194,
        "relative_chunk_number": 195,
        "text": "The autoencoder is a critical component, as it defines the latent space in which the continuous language model operates. To isolate its effects, we hold the downstream language model fixed across all experiments in this section: an Energy Transformer with a hidden size of 768, 12 hidden layers, 16 attention heads, an FFN intermediate size of 2048, and a generative head with 3 MLP blocks.",
        "text_length": 390,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03472570329904556,
          0.015469268895685673,
          -0.027811838313937187,
          0.024807797744870186,
          -0.06942345201969147,
          -0.03416501358151436,
          0.022627050057053566,
          -0.016561299562454224,
          -0.036036137491464615,
          -0.01978749781847
        ]
      },
      {
        "chunk_index": 195,
        "relative_chunk_number": 196,
        "text": "Each model configuration is trained for 50,000 steps. Unless otherwise specified, the autoencoder uses the default parameters as described in Section 7.1. We begin with a comprehensive ablation study to validate the contribution of each proposed technique, followed by a detailed analysis of the effect of several key hyperparameters.",
        "text_length": 334,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.019480060786008835,
          0.04888797551393509,
          -0.00015570549294352531,
          0.03234834969043732,
          -0.09135669469833374,
          0.013282795436680317,
          0.019606176763772964,
          -0.05606047436594963,
          -0.0377502366900444,
          -0.05396226793527603
        ]
      },
      {
        "chunk_index": 196,
        "relative_chunk_number": 197,
        "text": "We first validate the design choices for the autoencoder, with results detailed in Table 2. While a standard, reconstruction-only autoencoder provides a reasonable baseline, naively incorporating a variational objective leads to a significant drop in performance.",
        "text_length": 263,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.05104653537273407,
          0.04426730051636696,
          -0.03912215307354927,
          0.018634961917996407,
          -0.06669734418392181,
          -0.001574771711602807,
          0.01750071346759796,
          0.013524915091693401,
          -0.03378067910671234,
          -0.008414443582296371
        ]
      },
      {
        "chunk_index": 197,
        "relative_chunk_number": 198,
        "text": "This degradation is traced to a severe instance of posterior collapse, where we found that 71 of the 128 latent dimensions had collapsed to the standard normal prior. The introduction of the KL clipping strategy proves to be the crucial remedy, which effectively prevents dimensional collapse and leads to a notable perfor- mance improvement.",
        "text_length": 342,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03036656230688095,
          0.0308070108294487,
          -0.01735619455575943,
          -0.0025053212884813547,
          0.030315987765789032,
          -0.014104390516877174,
          0.01934823952615261,
          -0.042043693363666534,
          -0.027434885501861572,
          -0.019650563597679138
        ]
      },
      {
        "chunk_index": 198,
        "relative_chunk_number": 199,
        "text": "Furthermore, applying dropout regularization to both the input tokens and the latent vector yields considerable, orthogonal performance benefits, confirming that each technique contributes uniquely to shaping a high-fidelity and robust latent space. KL weight.",
        "text_length": 260,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0179846603423357,
          0.024855097755789757,
          -0.021698953583836555,
          0.03466847166419029,
          -0.02217126078903675,
          0.03863096982240677,
          0.0212834682315588,
          -0.019847534596920013,
          -0.029312435537576675,
          0.019729768857359886
        ]
      },
      {
        "chunk_index": 199,
        "relative_chunk_number": 200,
        "text": "We next examine the model\u2019s sensitivity to the KL divergence weight, \u03b2, which gov- erns the trade-off between reconstruction fidelity and latent space regularization. We varied \u03b2 across several orders of magnitude and present the results in Figure 6.",
        "text_length": 250,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01847236230969429,
          0.029635384678840637,
          -0.019858738407492638,
          0.028370682150125504,
          0.0063584232702851295,
          -0.002576414728537202,
          0.0227128304541111,
          -0.04073834419250488,
          -0.033766355365514755,
          0.0018973747501149774
        ]
      },
      {
        "chunk_index": 200,
        "relative_chunk_number": 201,
        "text": "Starting from a baseline with no KL regularization (\u03b2 = 0), we observe that introducing a small amount of variational regularization significantly improves the final BrierLM score, which confirms our hypothesis: a moderate regu- larization effectively smooths the latent manifold, making it more learnable for the Energy Trans- former, while leaving reconstruction accuracy almost unaffected.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.027532203122973442,
          0.03171289712190628,
          -0.03094528429210186,
          0.03479236364364624,
          0.01029893197119236,
          -0.015299533493816853,
          0.024318348616361618,
          -0.027919718995690346,
          -0.022190378978848457,
          -0.01816744916141033
        ]
      },
      {
        "chunk_index": 201,
        "relative_chunk_number": 202,
        "text": "However, this trend reverses as the regularization becomes overly aggressive. At \u03b2 = 0.1, the BrierLM score drops sharply, a de- cline directly linked to the autoencoder\u2019s compromised reconstruction fidelity, which falls to \u223c99%. Based on these findings, we selected \u03b2 = 0.001 to train our autoencoder. 16 Preprint Table 2: Ablation study of the autoencoder\u2019s regularization techniques.",
        "text_length": 386,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01686222478747368,
          0.045282986015081406,
          -0.056105755269527435,
          0.04145609587430954,
          -0.03969329223036766,
          -0.015318518504500389,
          0.02179674059152603,
          -0.05278721824288368,
          -0.03421613201498985,
          -0.0021170289255678654
        ]
      },
      {
        "chunk_index": 202,
        "relative_chunk_number": 203,
        "text": "Performance is measured by BrierLM on the downstream language modeling task.",
        "text_length": 76,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.020864691585302353,
          0.010671396739780903,
          -0.03290558233857155,
          0.039999037981033325,
          -0.030975446105003357,
          0.010670806281268597,
          0.02302892692387104,
          -0.006944673135876656,
          -0.023852011188864708,
          0.016292428597807884
        ]
      },
      {
        "chunk_index": 203,
        "relative_chunk_number": 204,
        "text": "LKL Lclip KL DropToken DropLatent BrierLM 3.99 \u2713 3.48 \u2713 4.13 \u2713 \u2713 4.55 \u2713 \u2713 4.46 \u2713 \u2713 \u2713 4.70 0 0.0001 0.001 0.01 0.1 KL Divergence Weight ( ) 4.1 4.2 4.3 4.4 4.5 4.6 4.7 BrierLM 99.0 99.2 99.4 99.6 99.8 100.0 Accuracy (%) Figure 6: Effect of the KL divergence weight on the autoencoder\u2019s reconstruction accuracy and the downstream BrierLM score.",
        "text_length": 342,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.016261041164398193,
          0.0021580448374152184,
          -0.04024956002831459,
          0.03606293722987175,
          -0.07379645854234695,
          0.02287473902106285,
          0.022205278277397156,
          0.01481237169355154,
          -0.016268834471702576,
          0.020672570914030075
        ]
      },
      {
        "chunk_index": 204,
        "relative_chunk_number": 205,
        "text": "32 64 128 256 Latent Demension 3.4 3.6 3.8 4.0 4.2 4.4 4.6 BrierLM 99.90 99.92 99.94 99.96 Accuracy (%) Figure 7: Effect of the latent demension on the autoencoder\u2019s reconstruction accuracy and the downstream BrierLM score. Latent Demension. We next examine the influence of the latent dimension, l, which functions as the information bottleneck of the autoencoder.",
        "text_length": 365,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03310175612568855,
          -0.0022565426770597696,
          -0.04112982377409935,
          0.007766674738377333,
          -0.025505105033516884,
          -0.01302506122738123,
          0.023610876873135567,
          -0.01719108782708645,
          -0.03160473704338074,
          -0.03054586425423622
        ]
      },
      {
        "chunk_index": 205,
        "relative_chunk_number": 206,
        "text": "We evaluated latent dimensions of 32, 64, 128, and 256, and respectively scaled the dropout rate to 0.05, 0.1, 0.15, and 0.2. The results are illustrated in Figure 7. As seen, the reconstruction accuracy remains consistently high across all configurations, but the downstream performance varies, peaking at l = 128. This suggests a trade-off in selecting the optimal dimension.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01873081550002098,
          0.015342993661761284,
          -0.05176985636353493,
          -0.009767387062311172,
          -0.0020382069051265717,
          -0.00545030552893877,
          0.015986602753400803,
          0.0027279856149107218,
          -0.025533583015203476,
          -0.03016909398138523
        ]
      },
      {
        "chunk_index": 206,
        "relative_chunk_number": 207,
        "text": "A latent space that is too small, such as with l = 32, forces the autoencoder to learn an overly compact and brittle representation. Conversely, a large latent dimension may lead the autoencoder to encode noisy or irrelevant features from the input tokens.",
        "text_length": 256,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01997845061123371,
          0.009629440493881702,
          -0.030829202383756638,
          -0.0019862728659063578,
          -0.00862703938037157,
          0.0022320544812828302,
          0.021634390577673912,
          -0.005681328941136599,
          -0.02576805278658867,
          -0.043237704783678055
        ]
      },
      {
        "chunk_index": 207,
        "relative_chunk_number": 208,
        "text": "This forces the Energy Transformer to expend its finite capacity modeling this noise, making it more challenging to discern the underlying data manifold. A dimension of l = 128 thus appears to strike an optimal balance, providing sufficient capacity for a robust representation while maintaining a structured and learnable latent space for the downstream generative model. Scale.",
        "text_length": 379,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.025129742920398712,
          0.0172782801091671,
          -0.023689651861786842,
          0.011266971006989479,
          -0.012622549198567867,
          -0.027203194797039032,
          0.01987621933221817,
          -0.02409038320183754,
          -0.025979189202189445,
          -0.06007026880979538
        ]
      },
      {
        "chunk_index": 208,
        "relative_chunk_number": 209,
        "text": "Finally, we examine the impact of scaling the autoencoder. We explored several axes of scaling: doubling the number of layers in both the encoder and decoder to 4, doubling the hidden dimension to 1024, and expanding the training dataset to 100B tokens. Interestingly, none of these scaling efforts resulted in a significant improvement in the final BrierLM score.",
        "text_length": 364,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.005166934337466955,
          0.02827260084450245,
          -0.0334426648914814,
          0.02153189666569233,
          -0.06800875067710876,
          -0.012752334587275982,
          0.020741939544677734,
          -0.01500830426812172,
          -0.02787010371685028,
          -0.025321755558252335
        ]
      },
      {
        "chunk_index": 209,
        "relative_chunk_number": 210,
        "text": "This finding suggests that the autoencoder\u2019s task is inherently simple and does not benefit from the aggressive scaling. A lightweight architecture, trained on a relatively modest amount of data, is sufficient to learn the high-fidelity and robust representation required for our framework.",
        "text_length": 290,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.021053092554211617,
          0.05437217652797699,
          -0.008215534500777721,
          0.0078804399818182,
          -0.05676018074154854,
          -0.04039672017097473,
          0.016785260289907455,
          -0.010102714411914349,
          -0.04098279029130936,
          -0.024467406794428825
        ]
      },
      {
        "chunk_index": 210,
        "relative_chunk_number": 211,
        "text": "This is a desirable property, as it allows the autoencoder to be a computationally negligible component of the overall system. 7.4 EFFECT OF MODEL ARCHITECTURE In this section, we conduct ablation studies on the CALM model architecture to investigate the im- pact of different design choices on model performance.",
        "text_length": 313,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.023069847375154495,
          0.02415563352406025,
          -0.026615405455231667,
          0.04963231086730957,
          -0.07988180220127106,
          -0.024520516395568848,
          0.019367370754480362,
          -0.059853628277778625,
          -0.031079698354005814,
          -0.00909810047596693
        ]
      },
      {
        "chunk_index": 211,
        "relative_chunk_number": 212,
        "text": "Unless otherwise specified, all experiments are conducted using a base configuration with a hidden size of 768, 12 hidden layers, 16 attention heads, and an FFN intermediate size of 2048. The generative head consists of 3 MLP blocks, and all models are trained for 50,000 steps. Diffusion and Flow Matching.",
        "text_length": 307,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008693587966263294,
          0.007464119233191013,
          -0.018643245100975037,
          0.03949448838829994,
          0.014391517266631126,
          -0.031538937240839005,
          0.018848232924938202,
          -0.051020119339227676,
          -0.03119000792503357,
          -0.055153738707304
        ]
      },
      {
        "chunk_index": 212,
        "relative_chunk_number": 213,
        "text": "Since the generative head can be any continuous generative model, we also evaluate two prominent choices as alternatives: diffusion (Ho et al., 2020) and flow matching 17 Preprint 0 10 20 30 40 50 Training Steps (k) 0 1 2 3 4 5 BrierLM Energy Flow Matching Diffusion Figure 8: BrierLM scores during training for different generative heads.",
        "text_length": 339,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02170773223042488,
          -0.003928112331777811,
          -0.0324372872710228,
          0.04131307452917099,
          -0.023649483919143677,
          -0.03738802298903465,
          0.021324126049876213,
          -0.010558471083641052,
          -0.03483756631612778,
          -0.04426630586385727
        ]
      },
      {
        "chunk_index": 213,
        "relative_chunk_number": 214,
        "text": "100 101 102 Iterations 0 1 2 3 4 BrierLM Energy Flow_midpoint Flow_eluer Diffusion Figure 9: Effect of sampling steps on the gen- eration quality of diffusion and flow matching. (Lipman et al., 2023). For these experiments, we adopt an architecture consistent with the diffusion head used in Li et al. (2024).",
        "text_length": 309,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.020326688885688782,
          0.020098863169550896,
          -0.0072545683942735195,
          0.059728655964136124,
          -0.019048670306801796,
          0.00801415927708149,
          0.021399563178420067,
          -0.029138704761862755,
          -0.031791165471076965,
          -0.048271071165800095
        ]
      },
      {
        "chunk_index": 214,
        "relative_chunk_number": 215,
        "text": "To ensure a fair comparison, we replicate the input hidden state N = 8 times for both models, mirroring the multi-sample approach used for our energy loss and promoting stable learning. During inference, we use 100 iterative steps by default. For the Flow Matching model, we use a midpoint sampler. Figure 8 compares the performance of the diffusion, flow matching, and energy-based generative heads.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.026319945231080055,
          0.0014715399593114853,
          -0.03384937345981598,
          0.01883131079375744,
          -0.005527026951313019,
          0.007543270941823721,
          0.02250821702182293,
          -0.0298079214990139,
          -0.029628653079271317,
          -0.046831563115119934
        ]
      },
      {
        "chunk_index": 215,
        "relative_chunk_number": 216,
        "text": "The results show that both flow matching and our energy-based head outperform the diffusion model, exhibiting a noticeable performance gap. Between the two, flow matching exhibits faster initial convergence, whereas our energy-based head reaches a higher performance ceiling. Figure 9 further compares the models\u2019 performance across different numbers of inference iterations.",
        "text_length": 375,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00719505175948143,
          0.007757479324936867,
          -0.04144531115889549,
          0.025130288675427437,
          -0.003094383282586932,
          -0.04696739464998245,
          0.019873004406690598,
          -0.0362369641661644,
          -0.020661376416683197,
          -0.04896584898233414
        ]
      },
      {
        "chunk_index": 216,
        "relative_chunk_number": 217,
        "text": "For the flow matching model, we tested both the Euler and midpoint samplers. As shown, the diffusion model requires a large number of iterations to generate valid results. In contrast, the flow matching model is significantly more efficient; the midpoint sampler, in particular, achieves decent quality in just 2 steps and reaches its near-optimal performance within 4 steps.",
        "text_length": 375,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.02374875918030739,
          0.030258020386099815,
          -0.04706388711929321,
          0.014701747335493565,
          -0.004880909807980061,
          -0.002355144824832678,
          0.022089844569563866,
          -0.021128768101334572,
          -0.03410586342215538,
          -0.06024531275033951
        ]
      },
      {
        "chunk_index": 217,
        "relative_chunk_number": 218,
        "text": "Our energy-based generative head achieves the best of both worlds: it delivers superior performance while completely eliminating the need for iterative decoding, making it a compelling choice for the CALM framework. Energy Loss. We now analyze the impact of the energy loss formulation on model performance.",
        "text_length": 307,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04787624999880791,
          0.003547911997884512,
          -0.015415079891681671,
          0.021956373006105423,
          0.04809008538722992,
          -0.0319269597530365,
          0.017522074282169342,
          -0.008536244742572308,
          -0.025334812700748444,
          -0.08316166698932648
        ]
      },
      {
        "chunk_index": 218,
        "relative_chunk_number": 219,
        "text": "Our energy loss (Equation 10) involves two sampling hyperparameters: the number of model-generated samples, N, and the number of target samples, M. Larger values for N and M provide a better esti- mation of the true energy score, but also increase the computational cost. Our default configuration is N = 8 and M = 100.",
        "text_length": 319,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.018777726218104362,
          0.00737643102183938,
          -0.02784581296145916,
          0.02772555500268936,
          -0.006567157339304686,
          -0.027114437893033028,
          0.019911035895347595,
          -0.06859796494245529,
          -0.026347864419221878,
          -0.10537046194076538
        ]
      },
      {
        "chunk_index": 219,
        "relative_chunk_number": 220,
        "text": "Table 3 shows the results of varying N and M, which reveal a clear trade- off between performance and computational cost. As expected, increasing the number of samples consistently improves the BrierLM score, but this comes at a nearly linear increase in training cost.",
        "text_length": 269,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.025511864572763443,
          0.027924684807658195,
          -0.04445413500070572,
          0.03497528284788132,
          -0.03542628139257431,
          -0.03030838817358017,
          0.019858282059431076,
          -0.013402213342487812,
          -0.03007267229259014,
          -0.029539138078689575
        ]
      },
      {
        "chunk_index": 220,
        "relative_chunk_number": 221,
        "text": "Our default setting of N = 8 and M = 100 is thus justified as a balanced configuration, leveraging a moderately sized N for a robust gradient signal and a large M to stabilize training. Table 3: Effect of model samples N and target samples M on model performance and training cost.",
        "text_length": 281,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.030388178303837776,
          0.025230295956134796,
          -0.06218668073415756,
          0.01829553209245205,
          -0.0077240620739758015,
          -0.019800331443548203,
          0.020378828048706055,
          -0.021626746281981468,
          -0.01565273478627205,
          -0.045101623982191086
        ]
      },
      {
        "chunk_index": 221,
        "relative_chunk_number": 222,
        "text": "Varying N (fixed M = 100) Varying M (fixed N = 8) N = 2 N = 4 N = 8 N = 12 M = 1 M = 16 M = 100 M = 200 BrierLM 4.37 4.53 4.70 4.72 4.50 4.56 4.70 4.67 Relative Cost 0.82\u00d7 0.91\u00d7 1.0\u00d7 1.13\u00d7 0.92\u00d7 0.94\u00d7 1.0\u00d7 1.07\u00d7 We also investigate the effect of the exponent \u03b1 in the energy score (Equation 9), which is guaranteed to be strictly proper for any \u03b1 \u2208(0, 2).",
        "text_length": 355,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.015418138355016708,
          0.02661425806581974,
          -0.02427232451736927,
          0.024411039426922798,
          0.021397508680820465,
          -0.01630471646785736,
          0.01209250371903181,
          0.01915026269853115,
          -0.018977832049131393,
          -0.010545605793595314
        ]
      },
      {
        "chunk_index": 222,
        "relative_chunk_number": 223,
        "text": "As shown in Table 4, our empirical results align with this theoretical property. We observe that training fails for \u03b1 < 1 (e.g., \u03b1 = 0.75), a phenomenon previously analyzed by Shao et al. (2025b) and attributed to gradient explosion issues. For values of \u03b1 within the range of [1, 2), the model achieves decent performance, with the best empirical results obtained at our default setting of \u03b1 = 1.",
        "text_length": 397,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.013460022397339344,
          0.06391899287700653,
          -0.04254218190908432,
          0.0215293038636446,
          -0.08572409301996231,
          -0.03751971572637558,
          0.01572551392018795,
          -0.06823400408029556,
          -0.02930636890232563,
          -0.05623328313231468
        ]
      },
      {
        "chunk_index": 223,
        "relative_chunk_number": 224,
        "text": "The model\u2019s BrierLM score drops to 0 at \u03b1 = 2. This is expected, as the energy score is only proper but not strictly proper when \u03b1 = 2. Consequently, the energy loss can no longer guide the model to uniquely match the true data distribution, leading to a collapse in modeling capability. 18 Preprint Table 4: Effect of the exponent \u03b1 in the energy score.",
        "text_length": 354,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.017278684303164482,
          0.032619908452034,
          -0.03181163966655731,
          0.03847461938858032,
          -0.021899225190281868,
          -0.03778380528092384,
          0.02343066781759262,
          -0.03460186347365379,
          -0.025911753997206688,
          -0.08589453250169754
        ]
      },
      {
        "chunk_index": 224,
        "relative_chunk_number": 225,
        "text": "\u03b1 0.75 1 1.25 1.5 1.75 2 BrierLM Fail 4.70 4.42 4.46 4.30 0 Model Input. A critical design choice in our CALM framework is the input representation fed into the Transformer backbone at each autoregressive step.",
        "text_length": 210,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.014840977266430855,
          0.009723842144012451,
          -0.040312182158231735,
          0.032437361776828766,
          0.004146631807088852,
          -0.01972663216292858,
          0.023733321577310562,
          -0.0020961579866707325,
          -0.028682399541139603,
          -0.020268753170967102
        ]
      },
      {
        "chunk_index": 225,
        "relative_chunk_number": 226,
        "text": "We evaluate three distinct input schemes: (1) Discrete input, which first decodes the previously generated vector zi\u22121 into K discrete tokens, then passes them through an embedding layer and an input compression MLP to form the next step input; (2) Continuous input, a more direct alternative where the vector zi\u22121 is directly projected to the Transformer\u2019s hidden dimension via a single linear layer; (3) Combined input, which fuses the representations from the discrete and continuous methods through element-wise addition.",
        "text_length": 525,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0021259041968733072,
          -0.00106358143966645,
          -0.036164529621601105,
          0.06129543110728264,
          0.0034431391395628452,
          -0.05827115848660469,
          0.017756082117557526,
          -0.040139928460121155,
          -0.02879815734922886,
          -0.014349238015711308
        ]
      },
      {
        "chunk_index": 226,
        "relative_chunk_number": 227,
        "text": "Table 5: Effect of model input on language modeling performance. Performance is evaluated using Brier-n scores and the composite BrierLM. Higher scores are better. Model Input Brier-1 Brier-2 Brier-3 Brier-4 BrierLM Discrete 21.81 6.88 2.59 1.25 4.70 Continuous 17.43 5.04 1.74 0.73 3.25 Both 21.17 6.49 2.44 1.12 4.40 As summarized in Table 5, the results clearly favor the discrete input strategy.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00651425588876009,
          0.010700283572077751,
          -0.04492466151714325,
          0.053531475365161896,
          -0.017129648476839066,
          -0.01912464201450348,
          0.020415935665369034,
          -0.05033626779913902,
          -0.025871319696307182,
          0.005327256862074137
        ]
      },
      {
        "chunk_index": 227,
        "relative_chunk_number": 228,
        "text": "The combined input offers no advantage and slightly degrades performance, while the purely continuous input leads to a substantial performance drop.",
        "text_length": 148,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02099999412894249,
          0.03280503302812576,
          -0.03997353836894035,
          0.041843559592962265,
          -0.006126957945525646,
          -0.060113079845905304,
          0.015957798808813095,
          -0.02758459746837616,
          -0.034868787974119186,
          0.012441432103514671
        ]
      },
      {
        "chunk_index": 228,
        "relative_chunk_number": 229,
        "text": "This confirms our hypothesis: although the continuous vector theoretically contains all the information of its corresponding discrete tokens, its highly compact and brittle nature makes it challenging for the model to unpack the underlying semantic information.",
        "text_length": 261,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02489393576979637,
          0.03595792502164841,
          -0.02688901498913765,
          0.052086398005485535,
          0.013311793096363544,
          0.015996837988495827,
          0.01748962141573429,
          -0.03389543667435646,
          -0.03220417723059654,
          0.004727451596409082
        ]
      },
      {
        "chunk_index": 229,
        "relative_chunk_number": 230,
        "text": "Grounding the autoregressive process in the discrete token space provides a more structured and stable input signal, which is therefore critical for achieving optimal performance.",
        "text_length": 179,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.019173994660377502,
          0.004754242952913046,
          -0.02559550106525421,
          0.019591541960835457,
          -0.04963305592536926,
          -0.040860552340745926,
          0.01984472945332527,
          -0.01453588530421257,
          -0.026585042476654053,
          0.059988945722579956
        ]
      },
      {
        "chunk_index": 230,
        "relative_chunk_number": 231,
        "text": "7.5 TEMPERATURE SAMPLING In this section, we conduct a fine-grained analysis to characterize the practical behavior of our approximate temperature sampling algorithm (Algorithm 2), i.e., how the algorithm navigates the trade-off between predictive accuracy and generative diversity.",
        "text_length": 282,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.028853941708803177,
          0.04167754575610161,
          -0.012043865397572517,
          0.04755774512887001,
          0.002140551805496216,
          0.01853257790207863,
          0.0196552574634552,
          -0.045948516577482224,
          -0.04955890774726868,
          0.013627130538225174
        ]
      },
      {
        "chunk_index": 231,
        "relative_chunk_number": 232,
        "text": "To quantify them, we decompose the Brier score estimator, Brier(P, y) \u2248I{x1 = y} + I{x2 = y} \u2212I{x1 = x2}, into two metrics: \u2022 Accuracy E[I{x = y}]: This metric measures the probability that a single sample drawn from the model matches the ground truth. It directly reflects the model\u2019s accuracy.",
        "text_length": 295,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003630288876593113,
          -8.758165495237336e-05,
          -0.033920492976903915,
          0.014589441940188408,
          -0.014292845502495766,
          -0.006872659083455801,
          0.02288985252380371,
          -0.0432869978249073,
          -0.02346709556877613,
          -0.005520915612578392
        ]
      },
      {
        "chunk_index": 232,
        "relative_chunk_number": 233,
        "text": "\u2022 Collision Rate E[I{x1 = x2}]: This metric measures the collision probability that two independent samples drawn from the model are identical. It serves as an inverse proxy for diversity, where a higher collision rate indicates that the output distribution is less diverse.",
        "text_length": 274,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008004247210919857,
          0.016705170273780823,
          -0.04658002406358719,
          0.012641025707125664,
          -0.014576202258467674,
          -0.03536592051386833,
          0.02021957002580166,
          -0.059542976319789886,
          -0.025990568101406097,
          -0.026722092181444168
        ]
      },
      {
        "chunk_index": 233,
        "relative_chunk_number": 234,
        "text": "Consistent with our primary BrierLM metric, we report both accuracy and collision rate as the ge- ometric mean of scores over n-grams from n=1 to 4. We first investigate how the two key hyperpa- rameters of our algorithm\u2014temperature T and batch size N\u2014influence these metrics.",
        "text_length": 276,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.026638759300112724,
          0.02050226926803589,
          -0.05594424158334732,
          0.03483579307794571,
          -0.030074629932641983,
          0.014451241120696068,
          0.021309591829776764,
          -0.017949730157852173,
          -0.0262902844697237,
          -0.012145635671913624
        ]
      },
      {
        "chunk_index": 234,
        "relative_chunk_number": 235,
        "text": "Specifically, we conduct two sets of experiments: for a fixed temperature T = 1/3, we vary the batch size N \u2208{1, 10, 20, 50, 100, 200, 500, 1000}; for a fixed batch size N = 500, we vary the the tempera- ture T \u2208{1/2, 1/3, 1/4, 1/5, 1/6}.",
        "text_length": 238,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.026680869981646538,
          0.032514411956071854,
          -0.04055128991603851,
          0.01830170676112175,
          0.04388264939188957,
          0.05908353626728058,
          0.019462795928120613,
          -0.045316267758607864,
          -0.02911442331969738,
          -0.0676719918847084
        ]
      },
      {
        "chunk_index": 235,
        "relative_chunk_number": 236,
        "text": "As shown in Figure 10, both increasing the batch size N and decreasing the temperature T sharpen the output distribution, achieving higher accuracy at the cost of reduced diversity (i.e., a higher col- lision rate). A key observation, however, is the dominant role of the batch size N, which covers a substantially greater range of this trade-off than the temperature T.",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01491096056997776,
          0.035661570727825165,
          -0.055214978754520416,
          0.03118606097996235,
          -0.0012529937084764242,
          0.030862830579280853,
          0.021275237202644348,
          -0.017672274261713028,
          -0.021223636344075203,
          -0.08336406201124191
        ]
      },
      {
        "chunk_index": 236,
        "relative_chunk_number": 237,
        "text": "Intuitively, a larger batch pro- vides a clearer statistical picture of the true distribution, making it easier to indentify high-probability candidates and confidently output them.",
        "text_length": 181,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0034368878696113825,
          0.03694184496998787,
          -0.015634017065167427,
          0.06429855525493622,
          -0.040185682475566864,
          0.013583746738731861,
          0.017050432041287422,
          0.010422540828585625,
          -0.03523936867713928,
          -0.04589598998427391
        ]
      },
      {
        "chunk_index": 237,
        "relative_chunk_number": 238,
        "text": "In contrast, the effectiveness of temperature T is capped 19 Preprint 10 20 30 40 Collision Rate (%) 8 9 10 11 12 13 14 Accuracy (%) T=1/3, varying N N=500, varying T Figure 10: The accuracy-diversity trade-off in CALM as a function of batch size N and tem- perature T.",
        "text_length": 269,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02494368515908718,
          0.001973310951143503,
          -0.06060872599482536,
          0.01926407590508461,
          0.024661894887685776,
          0.07108014822006226,
          0.020779570564627647,
          -0.044298458844423294,
          -0.02642648294568062,
          -0.05285763740539551
        ]
      },
      {
        "chunk_index": 238,
        "relative_chunk_number": 239,
        "text": "10 20 30 40 Collision Rate (%) 8 9 10 11 12 13 14 Accuracy (%) CALM (T=1/3, varying N) Transformer (varying T) Figure 11: Comparison of the temperature sampling performance between CALM and the baseline Transformer. by the information available within the finite batch.",
        "text_length": 269,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03158210963010788,
          -0.006597976665943861,
          -0.04124301299452782,
          0.05216073989868164,
          0.010237242095172405,
          0.012285267934203148,
          0.02073683962225914,
          -0.020843805745244026,
          -0.03615644574165344,
          -0.07062128186225891
        ]
      },
      {
        "chunk_index": 239,
        "relative_chunk_number": 240,
        "text": "Thus, while temperature serves its conventional purpose, these empirical results suggest that the batch size N is a more effective tool for navigating the accuracy-diversity frontier in our likelihood-free framework. Finally, we compare the behavior of our sampling algorithm against that of a traditional Trans- former.",
        "text_length": 320,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.017977258190512657,
          0.03505900874733925,
          -0.024945402517914772,
          0.034274496138095856,
          -0.01641640067100525,
          0.014992880634963512,
          0.020545070990920067,
          -0.02362222597002983,
          -0.03229334577918053,
          -0.044869426637887955
        ]
      },
      {
        "chunk_index": 240,
        "relative_chunk_number": 241,
        "text": "We ensure a fair comparison by selecting model checkpoints with nearly identical BrierLM scores. For CALM, we fix the temperature at T = 1/3 while varying the batch size N \u2208 {1, 10, 20, 50, 100, 200, 500, 1000}; for the Transformer baseline, we adjust its softmax tempera- ture across T \u2208{1, 0.9, . . . , 0.4}.",
        "text_length": 310,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0035326520446687937,
          0.00994054414331913,
          -0.057066816836595535,
          0.022184137254953384,
          0.03483704477548599,
          0.034726060926914215,
          0.021406106650829315,
          -0.006005061790347099,
          -0.02071741223335266,
          -0.03147468343377113
        ]
      },
      {
        "chunk_index": 241,
        "relative_chunk_number": 242,
        "text": "The results, plotted in Figure 11, are compelling: the accuracy-diversity trajectory traced by tuning N in CALM is nearly identical to the one produced by tuning T in the traditional Transformer. This alignment shows that we can accurately replicate the generative behavior of traditional models across a wide spectrum of temperatures.",
        "text_length": 335,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0556783489882946,
          0.009816394187510014,
          -0.029107369482517242,
          0.0254081841558218,
          -0.004486646968871355,
          -0.029826443642377853,
          0.019556960090994835,
          0.019245820119976997,
          -0.03037301078438759,
          -0.07627250999212265
        ]
      },
      {
        "chunk_index": 242,
        "relative_chunk_number": 243,
        "text": "For instance, matching T = 0.6 requires a batch size of approximately N = 100, while simulating a lower temperature of T = 0.5 necessitates a larger batch of N = 200. This suggests a clear and predictable trade-off: the ability to simulate lower- temperature, higher-fidelity generation comes at the cost of an increased number of samples.",
        "text_length": 339,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.027926543727517128,
          0.037485238164663315,
          -0.05312652140855789,
          0.031958721578121185,
          0.029087528586387634,
          0.044191379100084305,
          0.021201493218541145,
          -0.017555207014083862,
          -0.02549816109240055,
          -0.04553204029798508
        ]
      },
      {
        "chunk_index": 243,
        "relative_chunk_number": 244,
        "text": "8 CONCLUSION AND FUTURE WORK In this work, we challenge the inefficient, token-by-token paradigm of LLMs by introducing Contin- uous Autoregressive Language Models (CALM), a framework that shifts generation from discrete tokens to a continuous vector space where a single vector represents K tokens.",
        "text_length": 299,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.011568421497941017,
          0.019338609650731087,
          -0.04077037796378136,
          0.019297251477837563,
          0.02548694983124733,
          0.016900938004255295,
          0.022542493417859077,
          -0.0024496358819305897,
          -0.01678279973566532,
          -0.0025791514199227095
        ]
      },
      {
        "chunk_index": 244,
        "relative_chunk_number": 245,
        "text": "To support this ap- proach, we develop a comprehensive likelihood-free toolkit: a robust and high-fidelity autoencoder, the energy loss for generative modeling, the BrierLM metric for LM evaluation, and a new suite of algorithms for temperature sampling.",
        "text_length": 254,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03369348868727684,
          -0.0003722209658008069,
          -0.008671602234244347,
          0.01539307739585638,
          -0.0517277754843235,
          -0.029201189056038857,
          0.02274523675441742,
          -0.0017650974914431572,
          -0.032485365867614746,
          -0.0105553874745965
        ]
      },
      {
        "chunk_index": 245,
        "relative_chunk_number": 246,
        "text": "Empirical results show that CALM achieves a superior performance-compute trade-off, highlighting a new scaling axis for language modeling: scaling the semantic bandwidth of each generative step to push the performance-compute frontier of LLMs.",
        "text_length": 243,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0020151392091065645,
          0.02822498418390751,
          -0.04165901988744736,
          0.02383560501039028,
          -0.03446009010076523,
          -0.015004687011241913,
          0.023831883445382118,
          -0.015259605832397938,
          -0.022303881123661995,
          -0.020489217713475227
        ]
      },
      {
        "chunk_index": 246,
        "relative_chunk_number": 247,
        "text": "Despite these promising results, CALM still has significant room for future architectural and algo- rithmic optimizations, as indicated by the performance gap between CALM at K=1 and a standard Transformer baseline. We identify several key areas with great potential for future research: \u2022 Autoencoder.",
        "text_length": 302,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.002223543357104063,
          0.012902355752885342,
          -0.043013185262680054,
          0.03219624608755112,
          -0.05092070624232292,
          -0.005492836236953735,
          0.020579101517796516,
          0.009399880655109882,
          -0.02896755188703537,
          -0.048418134450912476
        ]
      },
      {
        "chunk_index": 247,
        "relative_chunk_number": 248,
        "text": "The autoencoder is the cornerstone of the CALM framework, which directly governs the quality of the latent space. One key limitation of the current autoencoder is its primary focus on reconstruction, with less emphasis on semantic structure.",
        "text_length": 241,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0002029526513069868,
          0.01374511793255806,
          -0.023155653849244118,
          0.006102887913584709,
          -0.052315980195999146,
          0.0009311007452197373,
          0.023094339296221733,
          -0.0044159372337162495,
          -0.03708958625793457,
          0.008629814721643925
        ]
      },
      {
        "chunk_index": 248,
        "relative_chunk_number": 249,
        "text": "A promis- ing direction is to design an autoencoder that learns a semantically grounded latent space, where proximity in the latent space corresponds to semantic similarity. We note that build- ing such semantically rich latent spaces has been a recent trend in the vision domain (Zheng et al., 2025). This could provide a powerful inductive bias for the downstream generative model.",
        "text_length": 383,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01658705063164234,
          0.02771211974322796,
          -0.003869792679324746,
          0.018951164558529854,
          -0.03454448655247688,
          -0.02005859836935997,
          0.022265158593654633,
          0.02515682950615883,
          -0.02143930085003376,
          0.08989495038986206
        ]
      },
      {
        "chunk_index": 249,
        "relative_chunk_number": 250,
        "text": "Another direction is the development of more powerful architectures. Designs that 20 Preprint are context-aware or autoregressive, for example, could offer superior robustness and more reliable reconstruction. \u2022 Model. The core generative model also presents significant avenues for exploration.",
        "text_length": 295,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.008664045482873917,
          0.023667819797992706,
          -0.02275780960917473,
          0.03530177101492882,
          -0.004844118840992451,
          -0.040689848363399506,
          0.020175378769636154,
          -0.026100507006049156,
          -0.032801080495119095,
          0.015488843433558941
        ]
      },
      {
        "chunk_index": 250,
        "relative_chunk_number": 251,
        "text": "For instance, in terms of architecture, our current design employs a Transformer backbone fol- lowed by a lightweight generative head; while efficient, an alternative is to explore a more integrated, end-to-end generative Transformer, which may yield stronger generative model- ing capabilities.",
        "text_length": 295,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.019440704956650734,
          0.021557290107011795,
          -0.03133268654346466,
          0.03452819585800171,
          0.010651715099811554,
          -0.06219866871833801,
          0.019133727997541428,
          -0.006389905232936144,
          -0.0356137678027153,
          -0.026575546711683273
        ]
      },
      {
        "chunk_index": 251,
        "relative_chunk_number": 252,
        "text": "In terms of the training objective, while the energy loss provides a robust foundation, investigating other strictly proper scoring rules or generative models is worth- while, as they may offer different optimization dynamics and improved sample quality. \u2022 Sampling.",
        "text_length": 266,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.031482014805078506,
          0.020793385803699493,
          0.024695364758372307,
          0.02952941320836544,
          0.007524901535362005,
          -0.017933815717697144,
          0.018317727372050285,
          0.015004524029791355,
          -0.02856111153960228,
          -0.04129498451948166
        ]
      },
      {
        "chunk_index": 252,
        "relative_chunk_number": 253,
        "text": "While our work introduces a provably exact algorithm for likelihood-free tem- perature sampling, its reliance on rejection sampling can introduce significant inference overhead. A promising direction for future work is to explore more lightweight, heuristic methods for navigating the diversity-fidelity trade-off at inference time.",
        "text_length": 332,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0197196863591671,
          0.019416937604546547,
          -0.006211209576576948,
          0.03202180936932564,
          -0.026618752628564835,
          -0.008846924640238285,
          0.020635422319173813,
          -0.04804282635450363,
          -0.035599250346422195,
          0.06954988092184067
        ]
      },
      {
        "chunk_index": 253,
        "relative_chunk_number": 254,
        "text": "This could include techniques such as manipulating the scale of the input noise to the generative head, or fine- tuning the model with a modified loss function to steer its generative behavior. \u2022 Scaling. A critical next step is to investigate the scaling properties of CALM. A hypothesis to validate is that larger models possess the requisite capacity to support higher semantic bandwidths.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.019272204488515854,
          0.00104333960916847,
          -0.009494761005043983,
          0.014619345776736736,
          0.0017206660704687238,
          -0.0415157675743103,
          0.022291509434580803,
          -0.02964549884200096,
          -0.026311807334423065,
          -0.024981055408716202
        ]
      },
      {
        "chunk_index": 254,
        "relative_chunk_number": 255,
        "text": "A further pursuit is to establish a new family of scaling laws. While traditional laws (Kaplan et al., 2020) model performance as a function of model and data size, our framework introduces the semantic bandwidth K as a third variable. Formulating a unified scaling law would enable the principled selection of an optimal K for any compute budget. \u2022 Algorithmic Toolkit.",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.005620779935270548,
          0.0488266795873642,
          0.0041208392940461636,
          0.020930476486682892,
          0.023463081568479538,
          0.006323788780719042,
          0.02116563729941845,
          -0.08155619353055954,
          -0.034380655735731125,
          -0.004817469511181116
        ]
      },
      {
        "chunk_index": 255,
        "relative_chunk_number": 256,
        "text": "The paradigm shift from discrete tokens to a continuous domain ne- cessitates a re-evaluation of the standard LLM algorithmic toolkit. For instance, policy op- timization methods in reinforcement learning typically update the model by increasing the log-probability of rewarded samples, a quantity that CALM cannot directly compute.",
        "text_length": 332,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00151313585229218,
          0.03167195990681648,
          -0.03321751579642296,
          0.022251803427934647,
          0.013507801108062267,
          -0.02093755453824997,
          0.024555005133152008,
          0.011423498392105103,
          -0.027117135003209114,
          0.0037723304703831673
        ]
      },
      {
        "chunk_index": 256,
        "relative_chunk_number": 257,
        "text": "Sim- ilarly, knowledge distillation often requires minimizing the KL divergence between teacher and student distributions, which is intractable without access to the full probability mass function. How to reformulate these techniques to operate in a sample-based regime is an important question for future research. REFERENCES An experimental comparison of performance measures for classification.",
        "text_length": 397,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008537943474948406,
          0.019265523180365562,
          -0.02088051475584507,
          0.052629027515649796,
          -0.05239902064204216,
          -0.017570579424500465,
          0.023562680929899216,
          0.018955782055854797,
          -0.028090983629226685,
          -0.02582445926964283
        ]
      },
      {
        "chunk_index": 257,
        "relative_chunk_number": 258,
        "text": "Pattern recognition letters, 30(1):27\u201338, 2009. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale- man, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.",
        "text_length": 277,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0028413536492735147,
          -0.0011966206366196275,
          -0.01913391426205635,
          0.023887252435088158,
          0.013151193037629128,
          0.04281337186694145,
          0.01173193845897913,
          -0.011009640991687775,
          -0.029595330357551575,
          0.01631985977292061
        ]
      },
      {
        "chunk_index": 258,
        "relative_chunk_number": 259,
        "text": "Marianne Arriola, Subham Sekhar Sahoo, Aaron Gokaslan, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Justin T Chiu, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Repre- sentations, 2025. URL https://openreview.net/forum?id=tyEyYT267x. Jacob Austin, Daniel D.",
        "text_length": 366,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.034914035350084305,
          0.002935731550678611,
          -0.010473916307091713,
          0.03128741309046745,
          -0.013661659322679043,
          -0.066779725253582,
          0.015168627724051476,
          -0.016975609585642815,
          -0.03452825918793678,
          0.025079263374209404
        ]
      },
      {
        "chunk_index": 259,
        "relative_chunk_number": 260,
        "text": "Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In M. Ranzato, A. Beygelz- imer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural In- formation Processing Systems, volume 34, pp. 17981\u201317993. Curran Associates, Inc., 2021.",
        "text_length": 325,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.03636898100376129,
          0.0007975163171067834,
          -0.005025874357670546,
          0.037040501832962036,
          0.004407615400850773,
          -0.10242640972137451,
          0.014441370032727718,
          -0.03324321657419205,
          -0.026175126433372498,
          0.019224202260375023
        ]
      },
      {
        "chunk_index": 260,
        "relative_chunk_number": 261,
        "text": "URL https://proceedings.neurips.cc/paper_files/paper/2021/ file/958c530554f78bcd8e97125b70e6973d-Paper.pdf. Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, pp. 610\u2013623, New York, NY, USA, 2021.",
        "text_length": 398,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0002669550012797117,
          0.00871026236563921,
          0.006198507733643055,
          0.04022675380110741,
          0.028960028663277626,
          -0.023831946775317192,
          0.016740620136260986,
          0.037225931882858276,
          -0.02795340120792389,
          -0.07077565044164658
        ]
      },
      {
        "chunk_index": 261,
        "relative_chunk_number": 262,
        "text": "Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/ 3442188.3445922. URL https://doi.org/10.1145/3442188.3445922. Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1\u20133, 1950. 21 Preprint Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.013285575434565544,
          0.0069967214949429035,
          0.005993035156279802,
          0.03653770312666893,
          -0.029158111661672592,
          -0.024773165583610535,
          0.017934391275048256,
          -0.01274875272065401,
          -0.02095974050462246,
          0.02480430155992508
        ]
      },
      {
        "chunk_index": 262,
        "relative_chunk_number": 263,
        "text": "In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1xsqj09Fm. Boyuan Chen, Diego Mart\u00b4\u0131 Mons\u00b4o, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitz- mann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In The Thirty- eighth Annual Conference on Neural Information Processing Systems, 2024.",
        "text_length": 368,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004871135577559471,
          -0.00228832196444273,
          -0.006751539185643196,
          0.00449425308033824,
          -0.06492504477500916,
          -0.033631205558776855,
          0.019837895408272743,
          0.010457383468747139,
          -0.026490159332752228,
          -0.015049322508275509
        ]
      },
      {
        "chunk_index": 263,
        "relative_chunk_number": 264,
        "text": "URL https: //openreview.net/forum?id=yDo1ynArjj. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 3829\u20133846, Singapore, December 2023. Association for Computational Linguistics.",
        "text_length": 387,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.015690825879573822,
          0.01960490643978119,
          -0.004208962898701429,
          0.041443582624197006,
          -0.011413659900426865,
          -0.03313438221812248,
          0.017690500244498253,
          -0.02827686071395874,
          -0.031178558245301247,
          0.012316319160163403
        ]
      },
      {
        "chunk_index": 264,
        "relative_chunk_number": 265,
        "text": "doi: 10.18653/v1/2023. emnlp-main.232. URL https://aclanthology.org/2023.emnlp-main.232/. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Alexandre D\u00b4efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. Transactions on Machine Learning Research, 2023.",
        "text_length": 385,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.020991895347833633,
          -0.0010007015662267804,
          -0.01418636180460453,
          0.03858552128076553,
          0.007600986398756504,
          -0.04159615933895111,
          0.014936421997845173,
          0.0005397115019150078,
          -0.021601533517241478,
          0.0489647276699543
        ]
      },
      {
        "chunk_index": 265,
        "relative_chunk_number": 266,
        "text": "ISSN 2835-8856. URL https://openreview.net/forum?id=ivCd8z8zR2. Featured Certification, Repro- ducibility Certification. Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. In The Thirteenth International Conference on Learning Representations, 2025.",
        "text_length": 380,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0013454221189022064,
          0.016850586980581284,
          -0.011367819271981716,
          -0.005205139052122831,
          -0.04173366725444794,
          -0.005133307073265314,
          0.018578235059976578,
          0.06235741823911667,
          -0.027306457981467247,
          0.00513215409591794
        ]
      },
      {
        "chunk_index": 266,
        "relative_chunk_number": 267,
        "text": "URL https: //openreview.net/forum?id=JE9tCwe3lp. Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music, 2020. URL https://arxiv.org/abs/2005. 00341. Paul-Ambroise Duquenne, Holger Schwenk, and Beno\u02c6\u0131t Sagot. Sonar: Sentence-level multimodal and language-agnostic representations, 2023. URL https://arxiv.org/abs/2308.",
        "text_length": 398,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.007441956549882889,
          -0.00010735240357462317,
          0.027299610897898674,
          0.022895460948348045,
          0.018817905336618423,
          -0.04110001027584076,
          0.008305300958454609,
          -0.01049709040671587,
          -0.028363404795527458,
          0.04274015128612518
        ]
      },
      {
        "chunk_index": 267,
        "relative_chunk_number": 268,
        "text": "11466. Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179\u2013211, 1990. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition (CVPR), pp. 12873\u201312883, June 2021.",
        "text_length": 317,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.040400419384241104,
          -0.017401868477463722,
          -0.01750311255455017,
          0.03088962845504284,
          0.01796039380133152,
          -0.026998024433851242,
          0.017819475382566452,
          -0.01849948614835739,
          -0.023972537368535995,
          0.04270542040467262
        ]
      },
      {
        "chunk_index": 268,
        "relative_chunk_number": 269,
        "text": "Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=jQP5o1VAVc. Jun Gao, Ziqiang Cao, and Wenjie Li.",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03890170902013779,
          0.016925644129514694,
          0.006944165099412203,
          0.004949762485921383,
          0.009063084609806538,
          -0.07352767884731293,
          0.018201228231191635,
          0.010571443475782871,
          -0.02612871676683426,
          0.023627199232578278
        ]
      },
      {
        "chunk_index": 269,
        "relative_chunk_number": 270,
        "text": "Selfcp: Compressing over-limit prompt via the frozen large language model itself. Inf. Process. Manage., 61(6), November 2024. ISSN 0306-4573. doi: 10.1016/j.ipm.2024.103873. URL https://doi.org/10.1016/j.ipm.2024.103873. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.",
        "text_length": 365,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012881218455731869,
          -0.0018190452829003334,
          0.0015299030346795917,
          0.054017744958400726,
          0.01784464716911316,
          0.013571745716035366,
          0.01061607152223587,
          -0.01540584210306406,
          -0.031124211847782135,
          0.05422564968466759
        ]
      },
      {
        "chunk_index": 270,
        "relative_chunk_number": 271,
        "text": "The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model. In The Twelfth International Confer- ence on Learning Representations, 2024. URL https://openreview.net/forum?id= uREj4ZuGJE.",
        "text_length": 369,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.03647534176707268,
          0.014470255002379417,
          -0.013929922133684158,
          0.03608621284365654,
          0.022545328363776207,
          -0.02333661913871765,
          0.0190593171864748,
          0.005957969930022955,
          -0.019833095371723175,
          0.04474695026874542
        ]
      },
      {
        "chunk_index": 271,
        "relative_chunk_number": 272,
        "text": "Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi`ere, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. In ICML, 2024. URL https: //openreview.net/forum?id=pEWAcejiU2. 22 Preprint Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.",
        "text_length": 334,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00221355096437037,
          0.020703664049506187,
          -0.011569168418645859,
          0.021791428327560425,
          -0.04024045169353485,
          0.025398528203368187,
          0.016483107581734657,
          0.016734058037400246,
          -0.024449389427900314,
          -0.02834399603307247
        ]
      },
      {
        "chunk_index": 272,
        "relative_chunk_number": 273,
        "text": "Journal of the American statistical Association, 102(477):359\u2013378, 2007. Irving John Good. Rational decisions. Journal of the Royal Statistical Society: Series B (Method- ological), 14(1):107\u2013114, 1952. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.05965681001543999,
          0.010938921943306923,
          -0.013999288901686668,
          -0.006275082007050514,
          0.04116657376289368,
          0.029510073363780975,
          0.014840805903077126,
          -0.00587731646373868,
          -0.022930847480893135,
          -0.028719130903482437
        ]
      },
      {
        "chunk_index": 273,
        "relative_chunk_number": 274,
        "text": "Advances in neural information processing systems, 27, 2014. Gemini Team Google. Gemini: A family of highly capable multimodal models, 2025. URL https: //arxiv.org/abs/2312.11805. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models.",
        "text_length": 373,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.030703026801347733,
          -0.024102242663502693,
          -0.01720350794494152,
          0.03382166475057602,
          -0.012819498777389526,
          0.011879049241542816,
          0.01296053733676672,
          -0.004169289488345385,
          -0.03043949045240879,
          0.00291574583388865
        ]
      },
      {
        "chunk_index": 274,
        "relative_chunk_number": 275,
        "text": "arXiv preprint arXiv:2407.21783, 2024. Alexey Gritsenko, Tim Salimans, Rianne van den Berg, Jasper Snoek, and Nal Kalch- brenner. A spectral energy distance for parallel speech synthesis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural In- formation Processing Systems, volume 33, pp. 13062\u201313072. Curran Associates, Inc., 2020.",
        "text_length": 373,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009963378310203552,
          -0.02577139437198639,
          -0.009115134365856647,
          0.03966624289751053,
          -0.006279221270233393,
          -0.05276860296726227,
          0.012354101054370403,
          -0.03686121106147766,
          -0.027979455888271332,
          0.0011249410454183817
        ]
      },
      {
        "chunk_index": 275,
        "relative_chunk_number": 276,
        "text": "URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/9873eaad153c6c960616c89e54fe155a-Paper.pdf. Sebastian Gruber and Florian Buettner. Better uncertainty calibration via proper scores for classification and beyond. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Process- ing Systems, volume 35, pp. 8618\u20138632.",
        "text_length": 382,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.025857241824269295,
          -0.0035344448406249285,
          -0.01409370731562376,
          0.045257922261953354,
          -0.028033407405018806,
          0.011128921061754227,
          0.01229220349341631,
          -0.03110806830227375,
          -0.02242433652281761,
          -0.020328588783740997
        ]
      },
      {
        "chunk_index": 276,
        "relative_chunk_number": 277,
        "text": "Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 3915a87ddac8e8c2f23dbabbcee6eec9-Paper-Conference.pdf. Jiatao Gu and Xiang Kong. Fully non-autoregressive neural machine translation: Tricks of the trade. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp.",
        "text_length": 396,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004540698137134314,
          0.0013443500502035022,
          -0.0016846222570165992,
          0.015350393019616604,
          -0.0370233990252018,
          -0.03697139769792557,
          0.014156771823763847,
          -0.012790504842996597,
          -0.03112148866057396,
          0.012975059449672699
        ]
      },
      {
        "chunk_index": 277,
        "relative_chunk_number": 278,
        "text": "120\u2013133, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.11. URL https://aclanthology.org/2021.findings-acl.11/. Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. Non- autoregressive neural machine translation.",
        "text_length": 290,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003434316720813513,
          0.001753606484271586,
          0.003956704866141081,
          0.01906096562743187,
          -0.02961718663573265,
          -0.009196672588586807,
          0.014315121807157993,
          -0.04137120768427849,
          -0.029832016676664352,
          0.046440623700618744
        ]
      },
      {
        "chunk_index": 278,
        "relative_chunk_number": 279,
        "text": "In 6th International Conference on Learning Rep- resentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL https://openreview.net/forum?id=B1l8BtlCb. Shangtong Gui, Chenze Shao, Zhengrui Ma, Xishan Zhang, Yunji Chen, and Yang Feng. Non- autoregressive machine translation with probabilistic context-free grammar.",
        "text_length": 366,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.026063675060868263,
          0.02442145347595215,
          -0.00044044197420589626,
          0.023244773969054222,
          -0.03498338907957077,
          -0.03883456438779831,
          0.020212894305586815,
          0.009127136319875717,
          -0.022356169298291206,
          0.002029055031016469
        ]
      },
      {
        "chunk_index": 279,
        "relative_chunk_number": 280,
        "text": "In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=LloZFVwWvj. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. SSD-LM: Semi-autoregressive simplex-based diffusion language model for text generation and modular control.",
        "text_length": 283,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01569388620555401,
          -0.021753128618001938,
          -0.01948470063507557,
          0.012575353495776653,
          0.0002627357607707381,
          -0.005189208779484034,
          0.01790173538029194,
          0.01671869307756424,
          -0.01917019672691822,
          -0.006256627384573221
        ]
      },
      {
        "chunk_index": 280,
        "relative_chunk_number": 281,
        "text": "In Anna Rogers, Jordan Boyd- Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11575\u201311596, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.647. URL https://aclanthology.org/2023.acl-long.647/. Jonathan Ho, Ajay Jain, and Pieter Abbeel.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02238442562520504,
          0.02889210358262062,
          -0.0012541660107672215,
          0.03628912568092346,
          -0.027909062802791595,
          0.015991806983947754,
          0.014753651805222034,
          -0.008561329916119576,
          -0.02102191373705864,
          -0.008377205580472946
        ]
      },
      {
        "chunk_index": 281,
        "relative_chunk_number": 282,
        "text": "Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu- ral Information Processing Systems, volume 33, pp. 6840\u20136851. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.",
        "text_length": 333,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009083211421966553,
          0.0025003200862556696,
          -0.007589234970510006,
          0.029202841222286224,
          -0.031184935942292213,
          -0.08343841135501862,
          0.015105066820979118,
          -0.05364363268017769,
          -0.017173022031784058,
          0.016972681507468224
        ]
      },
      {
        "chunk_index": 282,
        "relative_chunk_number": 283,
        "text": "Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, and Se-Young Yun. Block transformer: Global-to-local language modeling for fast inference, 2024. URL https://arxiv.org/abs/2406.02657. 23 Preprint Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735\u20131780, 1997. doi: 10.1162/neco.1997.9.8.1735.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.024986589327454567,
          0.012823309749364853,
          -0.02001604251563549,
          0.03597002476453781,
          -0.024831784889101982,
          0.018904250115156174,
          0.01488918624818325,
          -0.0044593652710318565,
          -0.02274950034916401,
          0.05889897793531418
        ]
      },
      {
        "chunk_index": 283,
        "relative_chunk_number": 284,
        "text": "Fei Huang, Hao Zhou, Yang Liu, Hang Li, and Minlie Huang. Directed acyclic transformer for non-autoregressive machine translation. In Proceedings of the 39th International Conference on Machine Learning, ICML 2022, 2022. Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross- entropy in classification tasks.",
        "text_length": 352,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01912813074886799,
          0.04759274050593376,
          -0.020138418301939964,
          0.024563467130064964,
          -0.045183487236499786,
          -0.04590720310807228,
          0.019230153411626816,
          0.023410648107528687,
          -0.030970245599746704,
          -0.014394300058484077
        ]
      },
      {
        "chunk_index": 284,
        "relative_chunk_number": 285,
        "text": "In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=hsFN92eQEla. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. M. S. Keane and George L. O\u2019Brien. A bernoulli factory.",
        "text_length": 395,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.007247581612318754,
          0.01572594791650772,
          0.013912258669734001,
          0.012603717856109142,
          0.03630658984184265,
          0.012064517475664616,
          0.015486277639865875,
          -0.019390804693102837,
          -0.029415322467684746,
          0.022746406495571136
        ]
      },
      {
        "chunk_index": 285,
        "relative_chunk_number": 286,
        "text": "ACM Trans. Model. Comput. Simul., 4(2):213\u2013219, April 1994. ISSN 1049-3301. doi: 10.1145/175007.175019. URL https: //doi.org/10.1145/175007.175019. Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural language models. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI\u201916, pp. 2741\u20132749. AAAI Press, 2016. Diederik P.",
        "text_length": 380,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.05662056803703308,
          -0.012543994002044201,
          -0.012775284238159657,
          -0.007121232338249683,
          -0.04435516148805618,
          -0.024637658149003983,
          0.014682559296488762,
          -0.049119021743535995,
          -0.02093452215194702,
          0.06191418692469597
        ]
      },
      {
        "chunk_index": 286,
        "relative_chunk_number": 287,
        "text": "Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 con- volutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R.",
        "text_length": 374,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012308625504374504,
          0.01812409982085228,
          -0.01042970735579729,
          0.039423514157533646,
          -0.03155359625816345,
          0.012980921193957329,
          0.01997758075594902,
          0.013233808800578117,
          -0.028993062674999237,
          -0.018383366987109184
        ]
      },
      {
        "chunk_index": 287,
        "relative_chunk_number": 288,
        "text": "Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/ paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf. Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In D. Lee, M. Sugiyama, U.",
        "text_length": 397,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01944779045879841,
          -0.019189730286598206,
          -0.01758030615746975,
          0.027608783915638924,
          -0.0035789748653769493,
          -0.006552429869771004,
          0.014322299510240555,
          -0.018979862332344055,
          -0.017781976610422134,
          0.009320768527686596
        ]
      },
      {
        "chunk_index": 288,
        "relative_chunk_number": 289,
        "text": "Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural In- formation Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_files/paper/2016/file/ ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf. Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev.",
        "text_length": 312,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.009982658550143242,
          0.0008149035274982452,
          -0.0018168640090152621,
          0.01769893243908882,
          -0.0429302379488945,
          -0.04756370931863785,
          0.011693272739648819,
          -0.015354028902947903,
          -0.026233257725834846,
          0.04418303072452545
        ]
      },
      {
        "chunk_index": 289,
        "relative_chunk_number": 290,
        "text": "Cramming 1568 tokens into a single vector and back again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19323\u201319339, Vienna, Austria, July 2025. Association for Computational Linguistics.",
        "text_length": 395,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.03169233351945877,
          0.02680250257253647,
          0.003169364994391799,
          0.03016815148293972,
          -0.008003992959856987,
          0.00622776011005044,
          0.01506742462515831,
          -0.008102827705442905,
          -0.02994629181921482,
          0.008518503978848457
        ]
      },
      {
        "chunk_index": 290,
        "relative_chunk_number": 291,
        "text": "ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.948. URL https://aclanthology. org/2025.acl-long.948/. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han.",
        "text_length": 379,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009186559356749058,
          -0.011803767643868923,
          -0.014024538919329643,
          0.011757798492908478,
          -0.025511177256703377,
          -0.005473373923450708,
          0.015176299959421158,
          0.024976126849651337,
          -0.01616424135863781,
          -0.013884035870432854
        ]
      },
      {
        "chunk_index": 291,
        "relative_chunk_number": 292,
        "text": "Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11523\u201311532, 2022. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via spec- ulative decoding.",
        "text_length": 279,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00895085372030735,
          0.024083063006401062,
          0.005866499617695808,
          0.03302237391471863,
          -0.03924945741891861,
          -0.039350949227809906,
          0.01794261299073696,
          -0.014470134861767292,
          -0.02540741115808487,
          0.02763594128191471
        ]
      },
      {
        "chunk_index": 292,
        "relative_chunk_number": 293,
        "text": "In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engel- hardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Con- ference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 19274\u201319286. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/ v202/leviathan23a.html.",
        "text_length": 341,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.03281433880329132,
          -0.0005930611514486372,
          -0.020286574959754944,
          0.03023305907845497,
          -0.011087259277701378,
          0.0035600601695477962,
          0.020900344476103783,
          0.0020804358646273613,
          -0.030440092086791992,
          0.00806803535670042
        ]
      },
      {
        "chunk_index": 293,
        "relative_chunk_number": 294,
        "text": "24 Preprint Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image gener- ation without vector quantization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=VNBIF0gmkb. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.015692075714468956,
          0.0039208754897117615,
          -0.0029216185212135315,
          0.003076984314247966,
          -0.015634117648005486,
          -0.04135594516992569,
          0.019354963675141335,
          0.01561677735298872,
          -0.025037061423063278,
          0.016142986714839935
        ]
      },
      {
        "chunk_index": 294,
        "relative_chunk_number": 295,
        "text": "Diffusion- LM improves controllable text generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=3s9IrEsjLyk. Zongqian Li, Yixuan Su, and Nigel Collier. 500xCompressor: Generalized prompt compression for large language models.",
        "text_length": 351,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003446572693064809,
          -0.016514284536242485,
          -0.02539442852139473,
          0.024385493248701096,
          -0.03956851735711098,
          -0.020642852410674095,
          0.014900303445756435,
          0.02375009097158909,
          -0.02655365690588951,
          -0.02224564552307129
        ]
      },
      {
        "chunk_index": 295,
        "relative_chunk_number": 296,
        "text": "In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Moham- mad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pp. 25081\u201325091, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025. acl-long.1219.",
        "text_length": 353,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02999877743422985,
          0.022667791694402695,
          0.020717624574899673,
          0.028715115040540695,
          0.004678186494857073,
          0.006615134887397289,
          0.018390869721770287,
          -0.014244884252548218,
          -0.02636992558836937,
          0.029890310019254684
        ]
      },
      {
        "chunk_index": 296,
        "relative_chunk_number": 297,
        "text": "URL https://aclanthology.org/2025.acl-long.1219/. Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguis- tics. URL https://aclanthology.org/W04-1013/. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0052087451331317425,
          0.00011036316573154181,
          -0.025078648701310158,
          0.042306508868932724,
          -0.0011584028834477067,
          -0.01685291901230812,
          0.015715498477220535,
          -0.013013394549489021,
          -0.029124300926923752,
          0.03914133831858635
        ]
      },
      {
        "chunk_index": 297,
        "relative_chunk_number": 298,
        "text": "Flow matching for generative modeling. In The Eleventh International Conference on Learning Repre- sentations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models.",
        "text_length": 332,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.016408978030085564,
          0.013783645816147327,
          -0.013383795507252216,
          0.010905172675848007,
          0.024774814024567604,
          -0.01858980394899845,
          0.016764571890234947,
          0.02027195692062378,
          -0.028264477849006653,
          -0.0020833818707615137
        ]
      },
      {
        "chunk_index": 298,
        "relative_chunk_number": 299,
        "text": "In An- dreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 21450\u201321474. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/liu23f.html. Ilya Loshchilov and Frank Hutter.",
        "text_length": 366,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03209517151117325,
          -0.016574718058109283,
          -0.0062797097489237785,
          0.011330446228384972,
          -0.023958614096045494,
          0.0008639990701340139,
          0.017611928284168243,
          0.013201667927205563,
          -0.024455029517412186,
          0.007300390396267176
        ]
      },
      {
        "chunk_index": 299,
        "relative_chunk_number": 300,
        "text": "Decoupled weight decay regularization. In International Confer- ence on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Proceedings of the 41st International Conference on Machine Learning, ICML\u201924. JMLR.org, 2024.",
        "text_length": 373,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.019929656758904457,
          -0.008277474902570248,
          -0.014798642136156559,
          0.037541694939136505,
          -0.023738985881209373,
          -0.09393014013767242,
          0.016980919986963272,
          -0.026421692222356796,
          -0.020410113036632538,
          -0.014657784253358841
        ]
      },
      {
        "chunk_index": 300,
        "relative_chunk_number": 301,
        "text": "Zhengrui Ma, Yang Feng, Chenze Shao, Fandong Meng, Jie Zhou, and Min Zhang. Efficient speech language modeling via energy distance in continuous latent space, 2025. URL https: //arxiv.org/abs/2505.13181. Luis Mendo. An asymptotically optimal bernoulli factory for certain functions that can be ex- pressed as power series. Stochastic Processes and their Applications, 129(11):4366\u20134384, 2019.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0059983450919389725,
          0.0052422755397856236,
          -0.004385786596685648,
          0.003365506883710623,
          -0.05583072453737259,
          -0.033952392637729645,
          0.019954726099967957,
          -0.029673362150788307,
          -0.027831975370645523,
          -0.0018497350392863154
        ]
      },
      {
        "chunk_index": 301,
        "relative_chunk_number": 302,
        "text": "ISSN 0304-4149. doi: https://doi.org/10.1016/j.spa.2018.11.017. URL https://www. sciencedirect.com/science/article/pii/S0304414918306768. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mix- ture models. In International Conference on Learning Representations, 2017. URL https: //openreview.net/forum?id=Byj72udxe. Gleb Mezentsev and Ivan Oseledets.",
        "text_length": 385,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00090168381575495,
          -0.004260092042386532,
          0.004342659376561642,
          0.005083912517875433,
          0.04027882590889931,
          0.007887805812060833,
          0.009835178032517433,
          -0.0323195606470108,
          -0.027167974039912224,
          -0.020543081685900688
        ]
      },
      {
        "chunk_index": 302,
        "relative_chunk_number": 303,
        "text": "Exploring the latent capacity of llms for one-step text genera- tion, 2025. URL https://arxiv.org/abs/2505.21189. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen- tations in vector space, 2013. URL https://arxiv.org/abs/1301.3781. Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens.",
        "text_length": 367,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04111800715327263,
          -0.0010784517508000135,
          -0.0199594646692276,
          0.039183709770441055,
          0.024781452491879463,
          -0.013972989283502102,
          0.017580529674887657,
          0.012556416913866997,
          -0.026572052389383316,
          -0.0016246228478848934
        ]
      },
      {
        "chunk_index": 303,
        "relative_chunk_number": 304,
        "text": "In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=2DtxPCL3T5. 25 Preprint Pit Neitemeier, Bj\u00a8orn Deiseroth, Constantin Eichenberg, and Lukas Balles. Hierarchical autore- gressive transformers: Combining byte- and word-level processing for robust, adaptable language models.",
        "text_length": 337,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02898428775370121,
          -0.021484477445483208,
          -0.004465426784008741,
          0.012813184410333633,
          -0.029961587861180305,
          -0.021246127784252167,
          0.02032431773841381,
          -0.004199237562716007,
          -0.028262721374630928,
          0.03379630669951439
        ]
      },
      {
        "chunk_index": 304,
        "relative_chunk_number": 305,
        "text": "In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=tU074jg2vS. Peter Occil. Bernoulli factory algorithms, 2020. URL https://peteroupc.github.io/ bernoulli.html. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.",
        "text_length": 363,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02360501140356064,
          0.008348292671144009,
          -0.0066453441977500916,
          -0.0047247279435396194,
          -0.07174103707075119,
          0.006387931294739246,
          0.010613770224153996,
          0.002162716118618846,
          -0.02623155154287815,
          0.033544715493917465
        ]
      },
      {
        "chunk_index": 305,
        "relative_chunk_number": 306,
        "text": "Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and R. Garnett (eds.), Ad- vances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/8558cb408c1d76621371888657d2eb1d-Paper.pdf.",
        "text_length": 391,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008120204322040081,
          0.025608452036976814,
          0.015296807512640953,
          0.03261655941605568,
          0.01467160414904356,
          -0.017769256606698036,
          0.01636607199907303,
          -0.021443026140332222,
          -0.01947367750108242,
          0.008206410333514214
        ]
      },
      {
        "chunk_index": 306,
        "relative_chunk_number": 307,
        "text": "Lorenzo Pacchiardi, Rilwan A. Adewoyin, Peter Dueben, and Ritabrata Dutta. Probabilistic fore- casting with generative networks via scoring rule minimization. Journal of Machine Learning Research, 25(45):1\u201364, 2024. URL http://jmlr.org/papers/v25/23-0038.html.",
        "text_length": 260,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01637355610728264,
          0.017784593626856804,
          0.006709083914756775,
          0.03536069393157959,
          0.042059801518917084,
          -0.010805153287947178,
          0.014795743860304356,
          -0.040561601519584656,
          -0.024833638221025467,
          -0.012647056952118874
        ]
      },
      {
        "chunk_index": 307,
        "relative_chunk_number": 308,
        "text": "Artidoro Pagnoni, Ramakanth Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Mar- garet Li, Chunting Zhou, Lili Yu, Jason E Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, and Srini Iyer. Byte latent transformer: Patches scale better than to- kens.",
        "text_length": 273,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03828341141343117,
          0.013771905563771725,
          -0.014323377050459385,
          0.004229445941746235,
          0.023810764774680138,
          -0.036096394062042236,
          0.016009891405701637,
          -0.005200579762458801,
          -0.02579142339527607,
          -0.0067142522893846035
        ]
      },
      {
        "chunk_index": 308,
        "relative_chunk_number": 309,
        "text": "In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9238\u20139258, Vienna, Austria, July 2025. Association for Com- putational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.453. URL https://aclanthology.org/2025.acl-long.453/.",
        "text_length": 396,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.018939469009637833,
          0.013325395062565804,
          0.015212862752377987,
          0.03870250657200813,
          0.003732329700142145,
          -0.0038786237128078938,
          0.015738096088171005,
          -0.0017911380855366588,
          -0.022633519023656845,
          0.04243422672152519
        ]
      },
      {
        "chunk_index": 309,
        "relative_chunk_number": 310,
        "text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 2002. doi: 10.3115/1073083.1073135. URL https:// www.aclweb.org/anthology/P02-1040.",
        "text_length": 305,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.0005562103469856083,
          0.007740209810435772,
          -0.0022755300160497427,
          0.014699856750667095,
          -0.06439270824193954,
          -0.026310140267014503,
          0.014511301182210445,
          -0.04167946055531502,
          -0.029059965163469315,
          0.04562770575284958
        ]
      },
      {
        "chunk_index": 310,
        "relative_chunk_number": 311,
        "text": "Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using divergence frontiers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview. net/forum?id=Tqx7nJp7PR.",
        "text_length": 380,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.022354990243911743,
          -0.01633635349571705,
          -0.01055125892162323,
          0.009357533417642117,
          -0.008732917718589306,
          -0.015588242560625076,
          0.012184994295239449,
          -0.03707395866513252,
          -0.02038262039422989,
          0.0342043898999691
        ]
      },
      {
        "chunk_index": 311,
        "relative_chunk_number": 312,
        "text": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8821\u20138831. PMLR, 18\u201324 Jul 2021.",
        "text_length": 352,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.019280143082141876,
          0.016859419643878937,
          0.010639932937920094,
          0.008666166104376316,
          -0.0334547758102417,
          -0.0034390089567750692,
          0.01685304008424282,
          -0.003721361979842186,
          -0.028931202366948128,
          -0.018977323547005653
        ]
      },
      {
        "chunk_index": 312,
        "relative_chunk_number": 313,
        "text": "URL https://proceedings.mlr.press/v139/ramesh21a.html. Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.",
        "text_length": 347,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008523184806108475,
          -0.007850589230656624,
          0.0006485255435109138,
          0.017817268148064613,
          -0.05054621025919914,
          -0.06410887837409973,
          0.014759265817701817,
          0.0073732249438762665,
          -0.023905834183096886,
          0.02978224866092205
        ]
      },
      {
        "chunk_index": 313,
        "relative_chunk_number": 314,
        "text": "URL https://proceedings.neurips.cc/paper_files/ paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf. Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. FlowAR: Scale-wise autoregressive image generation meets flow matching. In Forty-second International Conference on Machine Learning, 2025a. URL https://openreview.net/forum?id= JfLgvNe1tj.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04074157029390335,
          0.026586830615997314,
          0.011179783381521702,
          0.027451474219560623,
          -0.01599441096186638,
          -0.034618575125932693,
          0.011803473345935345,
          0.03247944265604019,
          -0.028338255360722542,
          0.022884812206029892
        ]
      },
      {
        "chunk_index": 314,
        "relative_chunk_number": 315,
        "text": "Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next- token: Next-x prediction for autoregressive visual generation, 2025b. URL https://arxiv. org/abs/2502.20388. 26 Preprint Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows.",
        "text_length": 294,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03975734859704971,
          0.041942398995161057,
          -0.018216440454125404,
          0.01431176159530878,
          -0.049546483904123306,
          -0.042152054607868195,
          0.01839948445558548,
          -0.014099989086389542,
          -0.023668190464377403,
          0.0038502526003867388
        ]
      },
      {
        "chunk_index": 315,
        "relative_chunk_number": 316,
        "text": "In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learn- ing, volume 37 of Proceedings of Machine Learning Research, pp. 1530\u20131538, Lille, France, 07\u201309 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/rezende15. html. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer.",
        "text_length": 355,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03919167444109917,
          0.005588292144238949,
          -0.011110838502645493,
          0.0005629371735267341,
          -0.052951738238334656,
          -0.01815001480281353,
          0.016749080270528793,
          0.013270236551761627,
          -0.028439601883292198,
          0.019953040406107903
        ]
      },
      {
        "chunk_index": 316,
        "relative_chunk_number": 317,
        "text": "High- resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR), pp. 10684\u201310695, June 2022. Frederick Sanders. On subjective probability forecasting. Journal of Applied Meteorology and Climatology, 2(2):191 \u2013 201, 1963. doi: 10.1175/1520-0450(1963)002\u27e80191:OSPF\u27e9 2.0.CO;2.",
        "text_length": 366,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.015424220822751522,
          0.00864658784121275,
          0.008745718747377396,
          0.04188063368201256,
          -0.04643299803137779,
          -0.062056612223386765,
          0.017109887674450874,
          -0.0033668088726699352,
          -0.017418580129742622,
          0.0178497526794672
        ]
      },
      {
        "chunk_index": 317,
        "relative_chunk_number": 318,
        "text": "URL https://journals.ametsoc.org/view/journals/apme/2/2/ 1520-0450_1963_002_0191_ospf_2_0_co_2.xml. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.",
        "text_length": 367,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04920606315135956,
          0.008373089134693146,
          0.007716590538620949,
          0.04373578354716301,
          -0.01624196767807007,
          -0.025262029841542244,
          0.013321603648364544,
          -0.002072841627523303,
          -0.031159987673163414,
          0.04636732116341591
        ]
      },
      {
        "chunk_index": 318,
        "relative_chunk_number": 319,
        "text": "1715\u20131725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162/. Chenze Shao and Yang Feng. Non-monotonic latent alignments for CTC-based non-autoregressive machine translation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.",
        "text_length": 396,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.022972168400883675,
          0.031223973259329796,
          -0.017473388463258743,
          0.030661441385746002,
          0.006194992456585169,
          -0.039807435125112534,
          0.016812128946185112,
          -0.048526257276535034,
          -0.03168848529458046,
          -0.009993970394134521
        ]
      },
      {
        "chunk_index": 319,
        "relative_chunk_number": 320,
        "text": "URL https:// openreview.net/forum?id=Qvh0SAPrYzH. Chenze Shao, Yang Feng, Jinchao Zhang, Fandong Meng, and Jie Zhou. Sequence-level training for non-autoregressive neural machine translation. Computational Linguistics, 47(4):891\u2013925, December 2021. doi: 10.1162/coli a 00421. URL https://aclanthology.org/2021. cl-4.29/. Chenze Shao, Fandong Meng, Yijin Liu, and Jie Zhou.",
        "text_length": 372,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0015183902578428388,
          0.026684323325753212,
          0.0027972133830189705,
          0.010783364996314049,
          -0.025815226137638092,
          -0.027135265991091728,
          0.015191526152193546,
          -0.02037089690566063,
          -0.022236786782741547,
          0.034012164920568466
        ]
      },
      {
        "chunk_index": 320,
        "relative_chunk_number": 321,
        "text": "Language generation with strictly proper scoring rules. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=LALSZ88Xpx. Chenze Shao, Fandong Meng, and Jie Zhou. Beyond next token prediction: Patch-level training for large language models. In The Thirteenth International Conference on Learning Representations, 2025a.",
        "text_length": 369,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.003415221581235528,
          0.020833173766732216,
          0.002567380666732788,
          -0.0030642251949757338,
          -0.004493097309023142,
          -0.010707050561904907,
          0.02121673710644245,
          -0.007740598171949387,
          -0.03605204075574875,
          -0.016472185030579567
        ]
      },
      {
        "chunk_index": 321,
        "relative_chunk_number": 322,
        "text": "URL https://openreview.net/forum?id=dDpB23VbVa. Chenze Shao, Fandong Meng, and Jie Zhou. Continuous visual autoregressive generation via score maximization. In Forty-second International Conference on Machine Learning, 2025b. URL https://openreview.net/forum?id=avGZE46gL6. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020a. Noam Shazeer.",
        "text_length": 375,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.05769303813576698,
          -0.013860158622264862,
          0.001632227678783238,
          0.021035272628068924,
          -0.008336948230862617,
          -0.08462905883789062,
          0.01731942407786846,
          -0.040193114429712296,
          -0.02699398808181286,
          0.02337264083325863
        ]
      },
      {
        "chunk_index": 322,
        "relative_chunk_number": 323,
        "text": "Glu variants improve transformer, 2020b. URL https://arxiv.org/abs/ 2002.05202. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna- tional Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=St1giarCHLP. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autore- gressive models.",
        "text_length": 391,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.052393488585948944,
          0.004483426921069622,
          -0.01043597050011158,
          0.036214977502822876,
          -0.020753609016537666,
          -0.11543653905391693,
          0.018935514613986015,
          -0.015624199993908405,
          -0.019053930416703224,
          0.01644609309732914
        ]
      },
      {
        "chunk_index": 323,
        "relative_chunk_number": 324,
        "text": "Advances in Neural Information Processing Systems, 31, 2018. Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Anna Korhonen, David Traum, and Llu\u00b4\u0131s M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3645\u20133650, Florence, Italy, July 2019.",
        "text_length": 361,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.028496453538537025,
          -0.020934198051691055,
          -0.02640693634748459,
          0.020305907353758812,
          -0.026803860440850258,
          -0.02551901526749134,
          0.013052034191787243,
          0.015533757396042347,
          -0.030992666259407997,
          -0.03995971009135246
        ]
      },
      {
        "chunk_index": 324,
        "relative_chunk_number": 325,
        "text": "Association for Computational Linguistics. doi: 10.18653/v1/P19-1355. URL https://aclanthology.org/P19-1355/. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: En- hanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. 27 Preprint Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.",
        "text_length": 390,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0006946605863049626,
          -0.0018177465535700321,
          -0.008738520555198193,
          0.025445228442549706,
          -0.011605174280703068,
          -0.015794919803738594,
          0.015421587973833084,
          0.00502625061199069,
          -0.0220474973320961,
          -0.048284806311130524
        ]
      },
      {
        "chunk_index": 325,
        "relative_chunk_number": 326,
        "text": "Autoregressive model beats diffusion: Llama for scalable image generation, 2024a. URL https://arxiv.org/abs/2406.06525. Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal latent language modeling with next-token diffusion, 2024b. URL https://arxiv.org/abs/2412.08635. Ilya Sutskever, James Martens, and Geoffrey Hinton.",
        "text_length": 383,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01741846837103367,
          -0.0032129257451742887,
          -0.005844962317496538,
          -0.0038717531133443117,
          -0.002173752523958683,
          -0.047635700553655624,
          0.016096225008368492,
          0.004172660876065493,
          -0.020237751305103302,
          -7.106125121936202e-05
        ]
      },
      {
        "chunk_index": 326,
        "relative_chunk_number": 327,
        "text": "Generating text with recurrent neural networks. In Lise Getoor and Tobias Scheffer (eds.), Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML \u201911, pp. 1017\u20131024, New York, NY, USA, June 2011. ACM. ISBN 978-1-4503-0619-5. G\u00b4abor J Sz\u00b4ekely. E-statistics: The energy of statistical samples.",
        "text_length": 322,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03682875260710716,
          -0.005744869355112314,
          -0.004431891720741987,
          0.024004455655813217,
          0.005426243878901005,
          0.038463469594717026,
          0.01755101978778839,
          0.02759379893541336,
          -0.022855132818222046,
          -0.024068135768175125
        ]
      },
      {
        "chunk_index": 327,
        "relative_chunk_number": 328,
        "text": "Bowling Green State University, Department of Mathematics and Statistics Technical Report, 3(05):1\u201318, 2003. LCM team, Lo\u00a8\u0131c Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Be- len Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R.",
        "text_length": 271,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03658132255077362,
          -0.003978094086050987,
          -0.015661034733057022,
          -0.002836076309904456,
          0.004199456423521042,
          -0.0009236101759597659,
          0.014097345061600208,
          -0.05117655545473099,
          -0.03618597984313965,
          -0.08793233335018158
        ]
      },
      {
        "chunk_index": 328,
        "relative_chunk_number": 329,
        "text": "Costa-juss`a, David Dale, Hady Elsahar, Kevin Heffernan, Jo\u02dcao Maria Janeiro, Tuan Tran, Christophe Rop- ers, Eduardo S\u00b4anchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, and Holger Schwenk. Large concept models: Language modeling in a sentence representation space, 2024. URL https://arxiv.org/abs/2412.08821. NextStep Team.",
        "text_length": 337,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010813446715474129,
          0.028598060831427574,
          -0.0018481340957805514,
          0.011332561261951923,
          0.01092380192130804,
          -0.009570930153131485,
          0.01551193930208683,
          -0.00111953797750175,
          -0.02346806973218918,
          0.043881606310606
        ]
      },
      {
        "chunk_index": 329,
        "relative_chunk_number": 330,
        "text": "Nextstep-1: Toward autoregressive image generation with continuous tokens at scale, 2025. URL https://arxiv.org/abs/2508.10711. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008434445597231388,
          0.0055102757178246975,
          -0.019031327217817307,
          0.005736584775149822,
          -0.0031634406186640263,
          -0.01916971616446972,
          0.014023113995790482,
          0.01212653610855341,
          -0.024695860221982002,
          0.04533018171787262
        ]
      },
      {
        "chunk_index": 330,
        "relative_chunk_number": 331,
        "text": "Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinite-vocabulary transformers. arXiv:2312.02116, 2023. Arnon Turetzky, Nimrod Shabtay, Slava Shechtman, Hagai Aronowitz, David Haws, Ron Hoory, and Avihu Dekel. Continuous speech synthesis using per-token latent diffusion, 2024. URL https://arxiv.org/abs/2410.16048.",
        "text_length": 340,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0038760541938245296,
          -0.011786078102886677,
          -0.009396553039550781,
          0.027764465659856796,
          -0.01880897954106331,
          -0.07422549277544022,
          0.016530899330973625,
          -0.01470915600657463,
          -0.025569923222064972,
          0.031143995001912117
        ]
      },
      {
        "chunk_index": 331,
        "relative_chunk_number": 332,
        "text": "Amirhossein Vahidi, Simon Schosser, Lisa Wimmer, Yawei Li, Bernd Bischl, Eyke H\u00a8ullermeier, and Mina Rezaei. Probabilistic self-supervised representation learning via scoring rules minimization. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=skcTCdJz0f. Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu.",
        "text_length": 376,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0009080026065930724,
          0.015575993806123734,
          0.006115439813584089,
          0.03498667851090431,
          -0.008001300506293774,
          -0.03704814985394478,
          0.010627643205225468,
          -0.01701274886727333,
          -0.020191356539726257,
          0.03832763060927391
        ]
      },
      {
        "chunk_index": 332,
        "relative_chunk_number": 333,
        "text": "Neural discrete representation learn- ing. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Cur- ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf. Haoran Wei, Yaofeng Sun, and Yukun Li.",
        "text_length": 387,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00016890012193471193,
          0.005847271531820297,
          -0.02557668648660183,
          0.009060470387339592,
          -0.040106505155563354,
          -0.06260509043931961,
          0.011381035670638084,
          0.0013818249572068453,
          -0.02714920975267887,
          0.040884535759687424
        ]
      },
      {
        "chunk_index": 333,
        "relative_chunk_number": 334,
        "text": "Deepseek-ocr: Contexts optical compression, 2025. URL https://arxiv.org/abs/2510.18234. LILI YU, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. MEGABYTE: Predicting million-byte sequences with multiscale transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=JTmO2V9Xpz.",
        "text_length": 378,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00379395205527544,
          -0.007162394002079964,
          0.003444166388362646,
          0.04013407230377197,
          0.002799451118335128,
          -0.026786932721734047,
          0.01558766234666109,
          -0.010914345271885395,
          -0.022927792742848396,
          -0.008497469127178192
        ]
      },
      {
        "chunk_index": 334,
        "relative_chunk_number": 335,
        "text": "Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound- stream: An end-to-end neural audio codec, 2021. URL https://arxiv.org/abs/2107. 03312. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Infor- mation Processing Systems, 32, 2019. 28 Preprint Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.037999898195266724,
          -0.011768270283937454,
          -0.017926543951034546,
          0.021212831139564514,
          -0.06432169675827026,
          0.004158128518611193,
          0.012197047472000122,
          -0.017032792791724205,
          -0.025763679295778275,
          0.022442510351538658
        ]
      },
      {
        "chunk_index": 335,
        "relative_chunk_number": 336,
        "text": "Diffusion transformers with repre- sentation autoencoders, 2025. URL https://arxiv.org/abs/2510.11690. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/ abs/2306.05685.",
        "text_length": 383,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004133944399654865,
          0.023428872227668762,
          0.005870122462511063,
          0.027914172038435936,
          -0.02579606883227825,
          -0.057214003056287766,
          0.017582133412361145,
          0.004385433159768581,
          -0.023188915103673935,
          -0.004627360962331295
        ]
      },
      {
        "chunk_index": 336,
        "relative_chunk_number": 337,
        "text": "29 Preprint A PROOF A.1 PROOF OF THEOREM 1 Theorem 1. For an implicit discrete distribution P(x) with sampler S and a temperature T \u2208(0, 1), Algorithm 1 generates samples distributed as: PT (x) = P(x)1/T ZT , ZT = X x P(x)1/T . Proof. Algorithm 1 implements a rejection sampling scheme where sample x is accepted with prob- ability Paccept(x).",
        "text_length": 343,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.010775985196232796,
          0.002514785388484597,
          -0.03497631102800369,
          0.04821467772126198,
          -0.02609577216207981,
          0.019279533997178078,
          0.01899738237261772,
          -0.05655026435852051,
          -0.035233546048402786,
          -0.05388355627655983
        ]
      },
      {
        "chunk_index": 337,
        "relative_chunk_number": 338,
        "text": "The proof proceeds by showing that the acceptance probability Paccept(x) = P(x)1/T , so the rejection sampling procedure yields the desired normalized sample distribution: PT (x) = Paccept(x) P x Paccept(x) = P(x)1/T P x P(x)1/T . (16) The inverse temperature is decomposed as 1/T = n + \u03b1, where n = \u230a1/T\u230bis the integer part and \u03b1 \u2208[0, 1) is the fractional part.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014880388043820858,
          0.020358409732580185,
          -0.02503334730863571,
          0.04227999597787857,
          -0.042542699724435806,
          -0.021082252264022827,
          0.01673879288136959,
          -0.06413101404905319,
          -0.026652289554476738,
          0.02049618400633335
        ]
      },
      {
        "chunk_index": 338,
        "relative_chunk_number": 339,
        "text": "The acceptance probability is the product of the success probabilities of the two corresponding stages. Stage 1 (Integer Part): For the algorithm to proceed to stage 2 with a candidate sample x, it must first draw x for n consecutive times in stage 1. As each draw is independent with probability P(x), the probability of passing stage 1 with candidate x is P(x)n.",
        "text_length": 364,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.05822610855102539,
          0.04048243165016174,
          0.00416867109015584,
          0.03791658580303192,
          -0.12788444757461548,
          -0.06741397827863693,
          0.022275546565651894,
          -0.051459647715091705,
          -0.03740563243627548,
          0.06122487410902977
        ]
      },
      {
        "chunk_index": 339,
        "relative_chunk_number": 340,
        "text": "Stage 2 (Fractional Part): Let p = P(x). The probability of acceptance in stage 2 is the cumulative probability of being accepted at any given iteration i \u22651: Pstage2 = P(accept at i = 1) + P(pass i = 1, accept at i = 2) + . . . = p + (1 \u2212p) \u0010 1 \u2212\u03b1 1 \u0011 p + (1 \u2212p)2 \u0010 1 \u2212\u03b1 1 \u0011 \u0010 1 \u2212\u03b1 2 \u0011 p + . . . = p \u221e X k=0 (1 \u2212p)k k Y j=1 \u0012 1 \u2212\u03b1 j \u0013 = p \u221e X k=0 (p \u22121)k \u0012\u03b1 \u22121 k \u0013 = p \u00b7 (p \u22121 + 1)\u03b1\u22121 = p\u03b1.",
        "text_length": 391,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01236540637910366,
          0.02501058392226696,
          -0.0013824673369526863,
          0.029628796502947807,
          -0.07114272564649582,
          -0.06879153847694397,
          0.012051410973072052,
          -0.07396242767572403,
          -0.01620487868785858,
          0.03521404415369034
        ]
      },
      {
        "chunk_index": 340,
        "relative_chunk_number": 341,
        "text": "(17) The second to last step is due to the generalized binomial theorem. Total Probability: The total probability of accepting a sample x in a single trial is the product of the probabilities from the two stages: Paccept(x) = P(x)n \u00b7 P(x)\u03b1 = P(x)1/T , (18) which completes the proof. 30 Preprint A.2 PROOF OF THEOREM 2 Theorem 2.",
        "text_length": 329,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02240050584077835,
          0.02411319501698017,
          -0.01889524981379509,
          0.008449839428067207,
          -0.06216013431549072,
          0.012171204201877117,
          0.016881247982382774,
          -0.06490685045719147,
          -0.030355386435985565,
          -0.01982295699417591
        ]
      },
      {
        "chunk_index": 341,
        "relative_chunk_number": 342,
        "text": "The expected number of calls to the base sampler S, denoted E[Ntotal], required to generate one sample using Algorithm 1 is: E[Ntotal] = n + I(\u03b1 > 0) P x P(x)1/T \u22121 ZT where ZT = P x P(x)1/T , n = \u230a1/T\u230b, \u03b1 = 1/T \u2212n, and I(\u00b7) is the indicator function. Proof.",
        "text_length": 258,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.048316918313503265,
          0.015220167115330696,
          -0.025283265858888626,
          0.02466433122754097,
          -0.03942106291651726,
          0.02212296426296234,
          0.01507426518946886,
          -0.04723704606294632,
          -0.02674022503197193,
          -0.03204493969678879
        ]
      },
      {
        "chunk_index": 342,
        "relative_chunk_number": 343,
        "text": "The algorithm conducts series of independent trials, each with identical probability of suc- cess, and continues until one trial is successful, thereby following a geometric distribution. There- fore, the expected number of samples is the ratio of the expected number of samples per trial, E[Ntrial], to the probability of a trial\u2019s success, P(success): E[Ntotal] = E[Ntrial] P(success).",
        "text_length": 387,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.004944385029375553,
          0.001531578483991325,
          -0.014493265189230442,
          0.0399877205491066,
          -0.04455040767788887,
          0.02269807457923889,
          0.02007026970386505,
          -0.06706765294075012,
          -0.03227302059531212,
          -0.021623224020004272
        ]
      },
      {
        "chunk_index": 343,
        "relative_chunk_number": 344,
        "text": "(19) Denominator (Success Probability): A trial is successful if any sample x is accepted. The total success probability is the sum of acceptance probabilities over all possible outcomes: P(success) = X x Paccept(x) = X x P(x)1/T = ZT . (20) Numerator (Expected Calls per Trial): Let Ntrial be the number of sampler calls in a single trial.",
        "text_length": 340,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.014563232660293579,
          0.0013564374530687928,
          -0.0221408661454916,
          0.023213258013129234,
          -0.06287548691034317,
          0.028378451243042946,
          0.01870696060359478,
          -0.05926423892378807,
          -0.03230152279138565,
          -0.031620196998119354
        ]
      },
      {
        "chunk_index": 344,
        "relative_chunk_number": 345,
        "text": "A trial always involves N1 calls for stage 1 and may involve N2 calls for stage 2. The number of calls in Stage 1 is fixed at N1 = n. Thus, E[N1] = n. If \u03b1 = 0, stage 2 is never performed, so E[N2] = 0. If \u03b1 > 0, stage 2 is only executed if stage 1 succeeds with some candidate x. Let Ex be this event, so P(Ex) = P(x)n, and we have: E[N2] = X x P(Ex) \u00b7 E[N2|Ex].",
        "text_length": 363,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.03199821338057518,
          0.03612837195396423,
          -0.030065247789025307,
          0.013840347528457642,
          -0.03320683166384697,
          0.028016000986099243,
          0.019976947456598282,
          -0.004362443927675486,
          -0.026994269341230392,
          -0.0391872376203537
        ]
      },
      {
        "chunk_index": 345,
        "relative_chunk_number": 346,
        "text": "(21) The conditional expectation E[N2|Ex] is the expected number of draws in stage 2 given candidate x. Using the formula E[X] = P\u221e k=1 P(X \u2265k), where X is the number of draws in stage 2: E[N2|Ex] = \u221e X k=1 P(stage 2 requires at least k draws) = \u221e X k=0 (1 \u2212P(x))k k Y j=1 \u0012 1 \u2212\u03b1 j \u0013 . (22) This is the same sum we evaluated in Equation 17, which equals P(x)\u03b1\u22121.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03937806934118271,
          0.03184381127357483,
          -0.01809966191649437,
          0.03216392919421196,
          -0.015877794474363327,
          0.007135632447898388,
          0.014706599526107311,
          -0.05891488492488861,
          -0.017523761838674545,
          -0.0001358772860839963
        ]
      },
      {
        "chunk_index": 346,
        "relative_chunk_number": 347,
        "text": "Therefore, if \u03b1 > 0: E[N2] = X x P(x)n \u00b7 P(x)\u03b1\u22121 = X x P(x)n+\u03b1\u22121 = X x P(x)1/T \u22121. (23) Combining the two cases, the total expected number of calls per trial is: E[Ntrial] = E[N1] + E[N2] = n + I(\u03b1 > 0) X x P(x)1/T \u22121. (24) Combining the numerator and denominator gives the final result: E[Ntotal] = n + I(\u03b1 > 0) P x P(x)1/T \u22121 ZT . (25) 31 Preprint Corollary 2.1.",
        "text_length": 364,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03324427828192711,
          0.021680716425180435,
          -0.029407653957605362,
          0.024103950709104538,
          -0.029488716274499893,
          0.009101768024265766,
          0.012113228440284729,
          -0.03872796148061752,
          -0.019749658182263374,
          -0.05649247393012047
        ]
      },
      {
        "chunk_index": 347,
        "relative_chunk_number": 348,
        "text": "Let |X| be the size of sample space. The expected number of sampler calls E[Ntotal] at temperature T \u2208(0, 1) is bounded by: E[Ntotal] \u2264 \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 + n ZT , if 0 < T \u22640.5 1 + |X|2\u22121/T ZT , if 0.5 < T < 1 where n = \u230a1/T\u230band ZT = P x P(x)1/T . Proof. The proof is divided into two cases based on the temperature range.",
        "text_length": 316,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.040793292224407196,
          0.015800120308995247,
          -0.014595641754567623,
          0.03614044189453125,
          0.02868434600532055,
          0.01790054887533188,
          0.012892141938209534,
          -0.040921662002801895,
          -0.029538467526435852,
          -0.05903034284710884
        ]
      },
      {
        "chunk_index": 348,
        "relative_chunk_number": 349,
        "text": "We start from the general formula for the expected cost from Theorem 2: E[Ntotal] = n + I(\u03b1 > 0) P x P(x)1/T \u22121 ZT . (26) Case 1: Low-Temperature Regime (0 < T \u22640.5) In this range, the exponent 1/T \u22121 \u22651. Since P(x) \u2208[0, 1], for any exponent \u03b2 \u22651, we have P(x)\u03b2 \u2264P(x). Thus, by summing over the entire sample space X: X x\u2208X P(x)1/T \u22121 \u2264 X x\u2208X P(x) = 1.",
        "text_length": 352,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.037483587861061096,
          0.03862106055021286,
          -0.02054952085018158,
          0.02279229275882244,
          0.023170899599790573,
          -0.005927074234932661,
          0.011325700208544731,
          -0.06319057941436768,
          -0.021340766921639442,
          -0.03345322981476784
        ]
      },
      {
        "chunk_index": 349,
        "relative_chunk_number": 350,
        "text": "(27) The numerator of the cost formula is therefore bounded by n + 1: n + I(\u03b1 > 0) X x\u2208X P(x)1/T \u22121 \u2264n + I(\u03b1 > 0) \u00b7 1 \u2264n + 1. (28) This establishes the bound for the low-temperature regime. Case 2: High-Temperature Regime (0.5 < T < 1) In this range,the exponent \u03b2 = 1/T \u22121 is in the interval (0, 1). For such an exponent, the function f(p) = p\u03b2 is concave.",
        "text_length": 357,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.023570405319333076,
          0.036198314279317856,
          -0.016319504007697105,
          0.01706288382411003,
          0.01376271154731512,
          -0.03502272814512253,
          0.014861009083688259,
          -0.04774852469563484,
          -0.02840796299278736,
          -0.011872909963130951
        ]
      },
      {
        "chunk_index": 350,
        "relative_chunk_number": 351,
        "text": "By Jensen\u2019s inequality, the sum P x\u2208X P(x)\u03b2 is maximized when P(x) is a uniform distribution over the sample space, i.e., P(x) = 1/K for all x \u2208X. The bound is: X x\u2208X P(x)1/T \u22121 \u2264 X x\u2208X \u0012 1 K \u00131/T \u22121 = K \u00b7 \u0012 1 K \u00131/T \u22121 = K2\u22121/T . (29) Substituting this into the cost formula gives the bound for the high-temperature regime. This com- pletes the proof. 32 Preprint A.3 PROOF OF THEOREM 3 Theorem 3.",
        "text_length": 398,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0006104944623075426,
          0.015493549406528473,
          -0.017248693853616714,
          0.02466224506497383,
          -0.022432174533605576,
          -0.0397561639547348,
          0.01424100250005722,
          -0.07268001139163971,
          -0.023281894624233246,
          -0.024876857176423073
        ]
      },
      {
        "chunk_index": 351,
        "relative_chunk_number": 352,
        "text": "Let Palg(x; N) be the probability of sampling x using Algorithm 2 with a batch size of N, and let PT (x) = P(x)n/ZT be the true target distribution at temperature T = 1/n, where ZT = P x P(x)n. The algorithm is asymptotically unbiased: lim N\u2192\u221ePalg(x; N) = PT (x). Proof. Let B = {x1, . . . , xN} be a batch of N samples drawn i.i.d. from the base distribution P(x).",
        "text_length": 365,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.007809511851519346,
          0.01571998931467533,
          -0.03399588540196419,
          0.03052010014653206,
          -0.021912112832069397,
          -0.0024557767901569605,
          0.016676509752869606,
          -0.013442973606288433,
          -0.025759724900126457,
          -0.011803153902292252
        ]
      },
      {
        "chunk_index": 352,
        "relative_chunk_number": 353,
        "text": "For any sample x \u2208X, let Cx be the random variable for the count of x in B. The weight assigned to x is Wx = \u0000Cx n \u0001 . Let XN be the random variable for the probability of sampling x from B: XN(x) = Wx P z\u2208X Wz = \u0000Cx n \u0001 P z\u2208X \u0000Cz n \u0001. (30) The overall probability we seek to analyze is the expectation of this random variable: Palg(x; N) = E[XN]. Our goal is to show that limN\u2192\u221eE[XN] = PT (x).",
        "text_length": 394,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.027321545407176018,
          6.116493023000658e-05,
          -0.02570248953998089,
          0.041177839040756226,
          -0.01639190874993801,
          -0.04387340694665909,
          0.014928536489605904,
          -0.04955099895596504,
          -0.022024082019925117,
          -0.016651282086968422
        ]
      },
      {
        "chunk_index": 353,
        "relative_chunk_number": 354,
        "text": "The proof proceeds in two main steps: first, we show that the random variable XN converges in probability to PT (x); second, we use the Bounded Convergence Theorem to show that this implies the convergence of its expectation. 1. Convergence in Probability.",
        "text_length": 256,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02386345900595188,
          0.019955504685640335,
          -0.004172367509454489,
          0.03343991935253143,
          -0.05486347898840904,
          -0.014348463155329227,
          0.02004588395357132,
          -0.028741061687469482,
          -0.034532591700553894,
          -0.049404602497816086
        ]
      },
      {
        "chunk_index": 354,
        "relative_chunk_number": 355,
        "text": "By the Weak Law of Large Numbers, the proportion of occurrences of any sample x converges in probability to its true probability P(x): Cx N p\u2212\u2192P(x) as N \u2192\u221e. (31) The weight Wx can be written as a polynomial in Cx, Wx = 1 n!Cx(Cx \u22121) . . . (Cx \u2212n + 1). We normalize this by N n: Wx N n = 1 n! \u0012Cx N \u0013 \u0012Cx \u22121 N \u0013 . . . \u0012Cx \u2212n + 1 N \u0013 .",
        "text_length": 333,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.0031698530074208975,
          0.01706080697476864,
          -0.01087430864572525,
          0.04186887666583061,
          -0.03456931188702583,
          -0.033368129283189774,
          0.015461128205060959,
          -0.05747421830892563,
          -0.01912246271967888,
          -0.008998480625450611
        ]
      },
      {
        "chunk_index": 355,
        "relative_chunk_number": 356,
        "text": "(32) Since Cx N p\u2212\u2192P(x) and k N \u21920 for any constant k, each term in the product converges to P(x). By the Continuous Mapping Theorem, the entire expression converges in probability: Wx N n p\u2212\u21921 n!P(x)n. (33) Now, we analyze the random variable XN by dividing its numerator and denominator by N n: XN = Wx/N n P z\u2208X (Wz/N n).",
        "text_length": 324,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0072629209607839584,
          0.02229248732328415,
          -0.028256863355636597,
          0.06741480529308319,
          -0.026425989344716072,
          -0.04091012105345726,
          0.016519075259566307,
          -0.08237025141716003,
          -0.037255801260471344,
          0.001893645734526217
        ]
      },
      {
        "chunk_index": 356,
        "relative_chunk_number": 357,
        "text": "(34) Applying the Continuous Mapping Theorem again for the ratio, we show that the random variable XN converges in probability to PT (x): XN p\u2212\u2192 1 n!P(x)n P z\u2208X 1 n!P(z)n = P(x)n P z\u2208X P(z)n = PT (x). (35) 2. Convergence of Expectation. We have established that the random variable XN converges in probability to PT (x). Besides, we have that XN is inherently bounded: 0 \u2264XN = Wx P z\u2208X Wz \u22641.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010103379376232624,
          0.02324855886399746,
          -0.022533655166625977,
          0.04873696714639664,
          -0.0011910723987966776,
          -0.035045694559812546,
          0.016234949231147766,
          -0.07354632019996643,
          -0.0345488004386425,
          -0.012978784739971161
        ]
      },
      {
        "chunk_index": 357,
        "relative_chunk_number": 358,
        "text": "(36) We can now invoke the Bounded Convergence Theorem, which states that if a sequence of random variables XN converges in probability to X, and |XN| \u2264M for all N for some constant M, then limN\u2192\u221eE[XN] = E[limN\u2192\u221eXN]. Applying this theorem to our case: lim N\u2192\u221ePalg(x; N) = lim N\u2192\u221eE[XN] = E h lim N\u2192\u221eXN i = PT (x). (37) This completes the proof that the algorithm is asymptotically unbiased. 33",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03678286820650101,
          0.02661372534930706,
          -0.023676520213484764,
          0.02079562470316887,
          -0.026086769998073578,
          -0.028952307999134064,
          0.017311763018369675,
          -0.019265655428171158,
          -0.0337354876101017,
          -0.00044153083581477404
        ]
      }
    ]
  },
  {
    "paper_id": "2510.27680v1",
    "total_chunks": 102,
    "index_range": {
      "start": 358,
      "end": 459
    },
    "chunks": [
      {
        "chunk_index": 358,
        "relative_chunk_number": 1,
        "text": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting Danyal Maqbool1, Changhee Lee1, Zachary Huemann1, Samuel D. Church1, Matthew E. Larson1, Scott B. Perlman1, Tomas A. Romero1, Joshua D. Warner1, Meghan Lubner1, Xin Tie1, Jameson Merkow2, Junjie Hu1, Steve Y. Cho1, Tyler J.",
        "text_length": 329,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03498665988445282,
          0.030027050524950027,
          0.011752687394618988,
          0.026671139523386955,
          -0.028933433815836906,
          0.018576273694634438,
          0.012380030937492847,
          0.045835431665182114,
          -0.025671374052762985,
          0.04495013877749443
        ]
      },
      {
        "chunk_index": 359,
        "relative_chunk_number": 2,
        "text": "Bradshaw1 1University of Wisconsin\u2013Madison, Madison, WI, USA, 2Microsoft, Redmond, WA, USA Abstract Recent advances in vision\u2013language models (VLMs) have enabled impressive multimodal reasoning, yet most medical applications remain limited to 2D imaging.",
        "text_length": 254,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.023994339630007744,
          -0.0059658074751496315,
          -0.014505785889923573,
          0.015869447961449623,
          -0.01942441612482071,
          0.0032646125182509422,
          0.018995514139533043,
          0.08542602509260178,
          -0.02167467214167118,
          0.050713956356048584
        ]
      },
      {
        "chunk_index": 360,
        "relative_chunk_number": 3,
        "text": "In this work, we extend VLMs to 3D positron emission tomography/computed tomography (PET/CT)\u2014a domain characterized by large volumetric data, small and dispersed lesions, and lengthy radiology reports.",
        "text_length": 201,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02780756913125515,
          -0.004508962854743004,
          -0.003480686340481043,
          0.03047669678926468,
          -0.012304541654884815,
          0.026319678872823715,
          0.024788489565253258,
          0.013103168457746506,
          -0.021701836958527565,
          -0.009090906009078026
        ]
      },
      {
        "chunk_index": 361,
        "relative_chunk_number": 4,
        "text": "We introduce a large-scale dataset com- prising over 11,000 lesion-level descriptions paired with 3D segmentations from more than 5,000 PET/CT exams, ex- tracted via a hybrid rule-based and large language model (LLM) pipeline.",
        "text_length": 226,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0028699273243546486,
          0.006233558524399996,
          -0.036547139286994934,
          0.014505366794764996,
          0.03106009215116501,
          0.03757258877158165,
          0.024853413924574852,
          0.03385257348418236,
          -0.03290851041674614,
          0.01602128893136978
        ]
      },
      {
        "chunk_index": 362,
        "relative_chunk_number": 5,
        "text": "Building upon this dataset, we propose PETAR-4B, a 3D mask-aware vision\u2013language model that in- tegrates PET, CT, and lesion contours for spatially grounded report generation. PETAR bridges global contextual reason- ing with fine-grained lesion awareness, producing clinically coherent and localized findings.",
        "text_length": 309,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.04860960692167282,
          0.020763035863637924,
          -0.021596115082502365,
          0.023899633437395096,
          -0.026240820065140724,
          0.006475003901869059,
          0.026247326284646988,
          0.003784329164773226,
          -0.024962522089481354,
          0.017973385751247406
        ]
      },
      {
        "chunk_index": 363,
        "relative_chunk_number": 6,
        "text": "Comprehensive automated and human evaluations demonstrate that PETAR substan- tially improves PET/CT report generation quality, advancing 3D medical vision\u2013language understanding. 1. Introduction Vision\u2013language models (VLMs) have recently achieved remarkable progress in aligning visual understanding with natural language generation, enabling powerful cross-modal reasoning across diverse domains.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.005227184854447842,
          0.01175870094448328,
          -0.027623184025287628,
          0.0220807995647192,
          -0.009287486784160137,
          0.007910349406301975,
          0.023800907656550407,
          0.05297839641571045,
          -0.02130676619708538,
          0.0263522882014513
        ]
      },
      {
        "chunk_index": 364,
        "relative_chunk_number": 7,
        "text": "In the medical imaging field, 2D VLMs have shown promise in radiology report generation, potentially leading to accelerated clinical work- flows and reduced radiologist burnout [9].",
        "text_length": 181,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.042198631912469864,
          0.03565553203225136,
          -0.03033251315355301,
          0.03399477154016495,
          -0.02595655433833599,
          -0.00718392338603735,
          0.022628813982009888,
          0.08216170966625214,
          -0.036521315574645996,
          -0.023390529677271843
        ]
      },
      {
        "chunk_index": 365,
        "relative_chunk_number": 8,
        "text": "However, current medical VLM research has predominantly focused on 2D imaging modalities\u2014such as chest X-rays or individual com- puted tomography (CT) slices\u2014while 3D modalities, such as CT, magnetic resonance (MRI), and positron emission tomography (PET) have proven more challenging.",
        "text_length": 285,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02384191006422043,
          -0.009046937339007854,
          -0.036113470792770386,
          0.025365324690937996,
          0.0006490141968242824,
          0.005018830765038729,
          0.022665735334157944,
          0.027220575138926506,
          -0.02379368618130684,
          0.025670843198895454
        ]
      },
      {
        "chunk_index": 366,
        "relative_chunk_number": 9,
        "text": "This chal- lenge arises not only from the substantially larger and more complex input data, but also from the substantially greater length and complexity of the radiologist\u2019s reports, which can cover multiple anatomical regions and include numerous findings. Of the 3D imaging modalities, PET is severely underrep- resented in medical VLM research.",
        "text_length": 348,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00010736256081145257,
          0.009084273129701614,
          -0.020722651854157448,
          0.031145060434937477,
          -0.02200336940586567,
          0.03193654865026474,
          0.024848923087120056,
          0.03410174697637558,
          -0.02774360217154026,
          -0.03372587263584137
        ]
      },
      {
        "chunk_index": 367,
        "relative_chunk_number": 10,
        "text": "PET imaging plays a critical role in oncology, including for diagnosis, staging, and treatment response assessment, and it is experiencing rapid growth in terms of imaging volumes.",
        "text_length": 180,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.030098043382167816,
          0.008499798364937305,
          -0.019369546324014664,
          0.06651246547698975,
          0.012181863188743591,
          0.020119329914450645,
          0.02001293934881687,
          0.01632438600063324,
          -0.03677189722657204,
          -0.007641840260475874
        ]
      },
      {
        "chunk_index": 368,
        "relative_chunk_number": 11,
        "text": "PET presents several unique challenges to traditional multimodal learn- ing, including lower number of annual scans compared to other modalities, limited availability of public multimodal datasets, whole-body imaging volumes that demand large- scale 3D processing, and small but numerous lesion sites dispersed across scans.",
        "text_length": 324,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.0029472215101122856,
          0.010049660690128803,
          -0.04665568843483925,
          0.03835919499397278,
          0.02807670086622238,
          -0.00015318150690291077,
          0.022516094148159027,
          0.059252191334962845,
          -0.03281262144446373,
          0.0423978827893734
        ]
      },
      {
        "chunk_index": 369,
        "relative_chunk_number": 12,
        "text": "Moreover, PET reports are arguably the lengthiest reports in radiology, and can be 3 times longer than CT reports. These factors make PET interpretation par- ticularly difficult for both conventional models and modern large-scale vision\u2013language systems.",
        "text_length": 254,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00862584076821804,
          0.05374247208237648,
          -0.04088495671749115,
          0.04959667846560478,
          -0.0010121235391125083,
          0.04958811774849892,
          0.023941034451127052,
          0.026056788861751556,
          -0.04507609084248543,
          -0.05662650614976883
        ]
      },
      {
        "chunk_index": 370,
        "relative_chunk_number": 13,
        "text": "Recent works have demonstrated strong performance in PET lesion segmentation (e.g., AutoPET) and there is signif- icant progress in large language models (LLMs) for struc- tured text generation. However, a critical gap remains in connecting these segmentations with grounded, localized lesion descriptions.",
        "text_length": 306,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.029267143458127975,
          0.0011085133301094174,
          -0.03331690654158592,
          0.024976659566164017,
          0.01980215311050415,
          0.05075874924659729,
          0.02257230505347252,
          0.05310666188597679,
          -0.03261818364262581,
          0.05957149714231491
        ]
      },
      {
        "chunk_index": 371,
        "relative_chunk_number": 14,
        "text": "To address this, our work establishes a direct connection between the spatially localized lesion information and the corresponding textual findings. We rely on segmentations to produce these findings, which can be obtained from the aforementioned models or from point-and- click frameworks such as PET-Edge on MIM.",
        "text_length": 314,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03269023448228836,
          -0.0014644298935309052,
          -0.0253743939101696,
          0.039053499698638916,
          0.035921115428209305,
          0.06032722443342209,
          0.021704236045479774,
          0.0301797017455101,
          -0.03843286633491516,
          0.005633510183542967
        ]
      },
      {
        "chunk_index": 372,
        "relative_chunk_number": 15,
        "text": "Our contribu- tions can be summarised as follows: \u2022 We introduce a large-scale PET/CT dataset comprising 11,356 lesion descriptions paired with 3D segmentations from over 5,000 exams. This dataset, the first of its kind, uniquely links PET/CT lesions with descriptive referring expressions extracted from clinical reports using a hybrid pipeline of rule-based filters and LLMs.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.011541277170181274,
          0.010482150129973888,
          -0.028285251930356026,
          0.04853764921426773,
          0.012727933935821056,
          0.03963107243180275,
          0.02120760828256607,
          0.019832592457532883,
          -0.03744133934378624,
          0.02914389595389366
        ]
      },
      {
        "chunk_index": 373,
        "relative_chunk_number": 16,
        "text": "\u2022 We develop a novel 3D mask-aware VLM, PETAR-4B, capable of jointly processing PET, CT, and lesion masks to produce localized, clinically meaningful findings. By 1 arXiv:2510.27680v1 [cs.CV] 31 Oct 2025 Figure 1. An example of mask guided response generation. We want the model to leverage the contour of the volume to generate grounded responses.",
        "text_length": 348,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02008860558271408,
          -0.006867310497909784,
          -0.022108448669314384,
          0.026232102885842323,
          -0.03927348926663399,
          0.022326942533254623,
          0.022072920575737953,
          0.00570256682112813,
          -0.032029975205659866,
          -0.010234449990093708
        ]
      },
      {
        "chunk_index": 374,
        "relative_chunk_number": 17,
        "text": "integrating 3D contour information into the visual encoder, PETAR effectively bridges global contextual reasoning with fine-grained lesion awareness.",
        "text_length": 149,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.06026193127036095,
          0.016316773369908333,
          -0.03215145692229271,
          0.03535523638129234,
          -0.03902508318424225,
          0.007235284894704819,
          0.024972639977931976,
          -0.003485932946205139,
          -0.035210080444812775,
          0.000562227563932538
        ]
      },
      {
        "chunk_index": 375,
        "relative_chunk_number": 18,
        "text": "\u2022 We conduct a comprehensive evaluation of 2D and 3D vision\u2013language models for PET findings generation, us- ing both automated and human assessments to measure the quality of the generated texts and benchmark common metrics to identify the most reliable ones.",
        "text_length": 260,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.019692229107022285,
          0.04101845622062683,
          -0.0013188261073082685,
          0.026479698717594147,
          0.006251244805753231,
          -0.0007895397138781846,
          0.023879399523139,
          0.020569121465086937,
          -0.03029331937432289,
          0.036787036806344986
        ]
      },
      {
        "chunk_index": 376,
        "relative_chunk_number": 19,
        "text": "Through these contributions, PETAR advances the fron- tier of medical vision\u2013language modeling by explicitly in- corporating 3D PET understanding and mask-guided reason- ing. This research was conducted under a protocol approved by our Institutional Review Board (IRB), which granted a waiver of informed consent. All data was fully anonymized prior to use. 2. Related Work 2.1.",
        "text_length": 378,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01666446030139923,
          0.0386461541056633,
          -0.014807982370257378,
          0.029910758137702942,
          -0.052523475140333176,
          -0.00974856223911047,
          0.02087971940636635,
          0.0038163186982274055,
          -0.02588960900902748,
          0.022255828604102135
        ]
      },
      {
        "chunk_index": 377,
        "relative_chunk_number": 20,
        "text": "Medical Vision Language Models (VLMs) Medical VLMs aim to bridge visual and textual modali- ties by encoding multimodal inputs\u2014such as medical im- ages and textual prompts\u2014into a shared embedding space, and leveraging large language models to generate clinically meaningful outputs. Recent studies have explored diverse strategies within this paradigm.",
        "text_length": 352,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02662869356572628,
          0.008294579572975636,
          -0.02202780917286873,
          0.02715330384671688,
          -0.009289255365729332,
          0.009574152529239655,
          0.024388128891587257,
          0.044707171618938446,
          -0.026451261714100838,
          0.009518234059214592
        ]
      },
      {
        "chunk_index": 378,
        "relative_chunk_number": 21,
        "text": "MAIRA [2, 11] generates detailed radiology reports from chest X-rays through aligned vision\u2013language encoders, while LLaVA-Med [12] adapts general-domain VLMs to biomedical applications via cur- riculum learning on figure\u2013caption pairs. However, these methods are limited to the 2D plane, discarding critical spa- tial information inherent in 3D medical volumes.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.021305333822965622,
          0.022797489538788795,
          -0.03175942972302437,
          0.0023475082125514746,
          0.022705484181642532,
          0.0019592302851378918,
          0.026363680139183998,
          0.06605411320924759,
          -0.028334032744169235,
          0.01323738880455494
        ]
      },
      {
        "chunk_index": 379,
        "relative_chunk_number": 22,
        "text": "To extend multimodal reasoning into volumetric contexts, M3D [1] leverages a large-scale multimodal dataset for 3D segmentation and report generation. Merlin [3] aligns 3D CT data with both structured and unstructured clinical in- formation within a unified embedding space, and CT2Rep [7] introduces a causal 3D feature extractor for automated chest CT report generation.",
        "text_length": 372,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003155865240842104,
          0.018307121470570564,
          -0.02197434939444065,
          0.03956879302859306,
          0.017500797286629677,
          -0.00877580139786005,
          0.022194866091012955,
          0.01762709952890873,
          -0.027839388698339462,
          -0.007357968483120203
        ]
      },
      {
        "chunk_index": 380,
        "relative_chunk_number": 23,
        "text": "RadFM [16], trained on both 2D and 3D data, seeks to build a generalist model capable of handling diverse radiologic tasks across modalities. While these works mark substantial progress toward comprehensive 3D understanding, they remain mask-agnostic, limiting their ability to localize and describe specific lesions or regions with fine-grained precision.",
        "text_length": 356,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.013220509514212608,
          0.0708157867193222,
          -0.02917841076850891,
          -0.005849737208336592,
          -0.023081574589014053,
          -0.04178624972701073,
          0.019210493192076683,
          0.05762965604662895,
          -0.029338276013731956,
          0.0003910931700374931
        ]
      },
      {
        "chunk_index": 381,
        "relative_chunk_number": 24,
        "text": "In contrast, our work intro- duces explicit mask-aware conditioning, enabling localized PET/CT report generation that bridges global volumetric reasoning with lesion-level granularity. 2.2. Fine-Grained Captioning Significant advances have also been made toward fine- grained visual grounding in 2D images.",
        "text_length": 306,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.052958566695451736,
          -0.005504304077476263,
          -0.012831084430217743,
          0.05313354730606079,
          -0.0266908947378397,
          -0.06642097234725952,
          0.023659709841012955,
          0.002096360782161355,
          -0.03573332354426384,
          0.08259942382574081
        ]
      },
      {
        "chunk_index": 382,
        "relative_chunk_number": 25,
        "text": "Early methods incorporate explicit spatial cues\u2014such as bounding box co- ordinates\u2014into textual prompts to direct visual attention [5, 14, 18]. More recent approaches have introduced flexible region-level interactions, allowing segmentation masks or point-based cues to guide the model\u2019s focus and improve spatial grounding [4, 8, 15, 17].",
        "text_length": 339,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.027114294469356537,
          0.0024423024151474237,
          -0.02474113367497921,
          0.022627944126725197,
          0.056978024542331696,
          0.0130663076415658,
          0.02015860751271248,
          0.02311471477150917,
          -0.03320375457406044,
          0.06021925061941147
        ]
      },
      {
        "chunk_index": 383,
        "relative_chunk_number": 26,
        "text": "In the medical domain, Reg2RG [6] extends this idea by introducing a mask input channel to generate more de- tailed, region-aware findings from CT scans. However, its scope remains limited to chest CT imaging, without extend- ing to other body regions or additional imaging modalities.",
        "text_length": 285,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.026495231315493584,
          0.03005729429423809,
          -0.01598704792559147,
          0.036413125693798065,
          -0.0038400301709771156,
          -0.00491746049374342,
          0.015737026929855347,
          0.004742458928376436,
          -0.03544507920742035,
          0.049542903900146484
        ]
      },
      {
        "chunk_index": 384,
        "relative_chunk_number": 27,
        "text": "Our work builds upon this direction by introducing a 3D, mask-aware architecture capable of integrating metabolic and anatomical cues, thereby enabling localized and clin- ically grounded captioning across full-body PET/CT vol- umes. Table 1. Comparison of selected models on medical imaging modal- ities.",
        "text_length": 305,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.018695184960961342,
          0.003422580426558852,
          -0.04482607915997505,
          0.03504578769207001,
          -0.006299194879829884,
          -0.03702762350440025,
          0.019709596410393715,
          -0.0138936135917902,
          -0.022322801873087883,
          0.05197426676750183
        ]
      },
      {
        "chunk_index": 385,
        "relative_chunk_number": 28,
        "text": "Model 3D Mask-aware CT PET ViP-LLaVA \u00d7 \u2713 \u00d7 \u00d7 GLAMM \u00d7 \u2713 \u00d7 \u2713 M3D \u2713 \u00d7 \u2713 \u00d7 RadFM \u2713 \u00d7 \u2713 \u00d7 CT2Rep \u2713 \u00d7 \u2713 \u00d7 Med3DVLM \u2713 \u00d7 \u2713 \u00d7 M3D-RAD \u2713 \u00d7 \u2713 \u00d7 Reg2RG \u2713 \u2713 \u2713 \u00d7 PETAR (ours) \u2713 \u2713 \u2713 \u2713 3. Dataset Construction 3.1. Preliminaries on PET/CT Reports PET reports describe how radiotracers are distributed in the body, usually alongside CT or MRI imaging for anatomical reference.",
        "text_length": 358,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014041917398571968,
          -0.0026798921171575785,
          -0.0009621926583349705,
          0.026291726157069206,
          0.0020368716213852167,
          -0.009933354333043098,
          0.013634842820465565,
          0.0024495325051248074,
          -0.015124278143048286,
          -0.042866773903369904
        ]
      },
      {
        "chunk_index": 386,
        "relative_chunk_number": 29,
        "text": "PET scans often cover all major body regions (head-to-thighs or head-to-feet) and are widely used in on- cology. Within each body region, reports list the lesions or regions with abnormal radiotracer uptake and include mea- surements such as standardized uptake value (SUVmax), slice 2 numbers, and qualitative descriptions.",
        "text_length": 324,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0026409770362079144,
          0.019740451127290726,
          -0.01411797571927309,
          0.03180238604545593,
          0.016539698466658592,
          0.021485336124897003,
          0.026042774319648743,
          0.0344117134809494,
          -0.03760695084929466,
          -0.018730750307440758
        ]
      },
      {
        "chunk_index": 387,
        "relative_chunk_number": 30,
        "text": "Because lesions are usually tracked across multiple scans, these measurements and descriptions help future readers identify the lesions of concern. PET reports are written in free text and can vary in length. Moreover, reporting styles differ substantially be- tween physicians and institutions.",
        "text_length": 295,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02064942941069603,
          0.03851235285401344,
          0.008713637478649616,
          0.06051435321569443,
          0.03237049654126167,
          0.0396893285214901,
          0.024429883807897568,
          0.0579826757311821,
          -0.030810784548521042,
          0.000908596150111407
        ]
      },
      {
        "chunk_index": 388,
        "relative_chunk_number": 31,
        "text": "Two physicians describing the same lesion may use substantially different terminology (e.g., \u2019hypermetabolic,\u2019 \u2019hot,\u2019 \u2019high uptake,\u2019 \u2019tracer-avid\u2019) and levels of detail. Such non-uniformity creates a major challenge for automated PET reporting, a challenge further exacerbated by the scarcity of large-scale, annotated datasets, that are visually grounded.",
        "text_length": 356,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.018262330442667007,
          0.008268575184047222,
          0.0016540888464078307,
          0.024276267737150192,
          -0.019814880564808846,
          0.051739729940891266,
          0.019921744242310524,
          0.02118779718875885,
          -0.027028175070881844,
          -0.04256097227334976
        ]
      },
      {
        "chunk_index": 389,
        "relative_chunk_number": 32,
        "text": "To address this, we develop a pipeline that extracts struc- tured lesion-level information from unstructured PET/CT reports. 3.2. Dataset Construction Pipeline We build on the work developed by Huemann et al. [10].",
        "text_length": 214,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.003303684527054429,
          -0.0003522768383845687,
          -0.013863393105566502,
          0.017876705154776573,
          0.018014071509242058,
          0.05779647454619408,
          0.02201460860669613,
          -0.021006990224123,
          -0.039557330310344696,
          0.006364195607602596
        ]
      },
      {
        "chunk_index": 390,
        "relative_chunk_number": 33,
        "text": "Starting from nearly one million sentences in a dataset of clinical PET reports, we use this multi-stage pipeline to identify sentences mentioning both a slice number and an SUVmax value\u2014key indicators that radiologists use when describing lesions.",
        "text_length": 248,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.00937249232083559,
          0.02902768924832344,
          -0.021603329107165337,
          0.04445795714855194,
          -0.010355059057474136,
          0.04185520485043526,
          0.024139609187841415,
          -0.0037018966395407915,
          -0.03376980125904083,
          -0.010430678725242615
        ]
      },
      {
        "chunk_index": 391,
        "relative_chunk_number": 34,
        "text": "Our approach combines rule-based pattern matching, RadGraph (for anatomical term extrac- tion), and large language model (LLM) ensembles (Mistral- 7B-Instruct, Mixtral-8\u00d77B-Instruct, and Dolphin-Instruct). These models filter out irrelevant sentences, disambiguate references to prior studies, and accurately extract the SUVmax and corresponding slice number.",
        "text_length": 359,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01676315814256668,
          0.00987464003264904,
          -0.02111045829951763,
          0.010492618195712566,
          0.011987848207354546,
          0.01761971414089203,
          0.020963426679372787,
          0.03922685608267784,
          -0.03157656639814377,
          -0.032036907970905304
        ]
      },
      {
        "chunk_index": 392,
        "relative_chunk_number": 35,
        "text": "To generate lesion masks, we employ an iterative thresholding algorithm on the PET volume. The process begins by applying a dynamic thresh- old based on the reported SUVmax, producing an initial set of candidate regions. Connected components are then identi- fied, and the component whose SUVmax matches the reported SUVmax (within \u00b10.1) and intersects the reported axial slice is selected.",
        "text_length": 390,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01742619089782238,
          0.02138633094727993,
          -0.008546100929379463,
          0.02623463235795498,
          -0.04801860824227333,
          0.012455135583877563,
          0.021857818588614464,
          0.0026681276503950357,
          -0.031514398753643036,
          0.04693853110074997
        ]
      },
      {
        "chunk_index": 393,
        "relative_chunk_number": 36,
        "text": "This region is refined through adaptive thresh- olding, progressively removing background uptake while preserving the lesion core until the contour stabilizes. The resulting dataset contains 11,356 lesion descriptions across 5,126 unique exams, covering multiple radiotrac- ers including 18F-fluorodeoxyglucose (FDG), 68Ga-(DOTA- (Tyr3)-octreotate) (DOTATATE), 18F-fluciclovine, and 18F- DCFPyL.",
        "text_length": 395,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.034789253026247025,
          0.014412473887205124,
          -0.030146492645144463,
          0.01384245790541172,
          0.023481160402297974,
          -0.0009141411283053458,
          0.020576808601617813,
          0.049156494438648224,
          -0.03565443679690361,
          0.05023610219359398
        ]
      },
      {
        "chunk_index": 394,
        "relative_chunk_number": 37,
        "text": "All PET, CT, and segmentation volumes were resampled to 3 mm isotropic resolution, and standardized to a fixed dimension of 192 \u00d7 192 \u00d7 352 voxels. To enhance textual consistency, we further processed each lesion description using an LLM (Qwen3-30B-A3B) that formats free-text sentences into a structured schema: region, organ, anatomic subsite, report.",
        "text_length": 353,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.029895544052124023,
          -0.008472953923046589,
          -0.021800672635436058,
          0.005251475144177675,
          0.016633624210953712,
          0.028260203078389168,
          0.02242417074739933,
          0.05915020406246185,
          -0.02891613356769085,
          -0.007374609354883432
        ]
      },
      {
        "chunk_index": 395,
        "relative_chunk_number": 38,
        "text": "This representation makes spatial and anatomical references explicit, simplifying downstream grounding between the text and the 3D image, and simplifying performance evaluation. An example is shown in Fig. 2. Figure 2. An example of our enhanced data format. Finally, to improve the model\u2019s understanding of global anatomy, we curate a complementary pretraining dataset.",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02625279687345028,
          0.031254757195711136,
          -0.017887555062770844,
          0.059897154569625854,
          0.023978011682629585,
          -0.006229735445231199,
          0.021359259262681007,
          0.01753976382315159,
          -0.03029436059296131,
          -0.0098764318972826
        ]
      },
      {
        "chunk_index": 396,
        "relative_chunk_number": 39,
        "text": "We apply TotalSegmentator to our CT volumes to generate approximately 5,000 segmentations across its 117 predefined anatomical classes, providing broad coverage of structural features for volumetric pretraining. Ours is the first publicly available multimodal PET/CT dataset with lesion-level granularity.",
        "text_length": 305,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.002775281434878707,
          -0.009630863554775715,
          -0.017453888431191444,
          0.048882752656936646,
          -0.019517196342349052,
          0.03960428759455681,
          0.0220482237637043,
          -0.006896012928336859,
          -0.03598292917013168,
          0.02531922236084938
        ]
      },
      {
        "chunk_index": 397,
        "relative_chunk_number": 40,
        "text": "As we highlight in Table 2, existing PET/CT datasets are typically limited in scope, focusing on specific organs, lacking text, lesion-level findings, or missing segmentation masks.",
        "text_length": 181,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.024618294090032578,
          0.021160783246159554,
          0.009250379167497158,
          0.05460081994533539,
          0.055772095918655396,
          0.033359020948410034,
          0.02247094362974167,
          0.02692469023168087,
          -0.03996088728308678,
          -0.000707015860825777
        ]
      },
      {
        "chunk_index": 398,
        "relative_chunk_number": 41,
        "text": "In contrast, our dataset, PETAR-11K, provides the first large-scale, whole- body PET/CT collection with aligned findings, 3D lesion masks, and textual descriptions, enabling comprehensive volumetric understanding and report generation. Table 2.",
        "text_length": 244,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.012104174122214317,
          0.010286250151693821,
          -0.016017358750104904,
          0.041214171797037125,
          -0.040142204612493515,
          0.02763335593044758,
          0.024159658700227737,
          0.02695608325302601,
          -0.039730943739414215,
          -0.018669327720999718
        ]
      },
      {
        "chunk_index": 399,
        "relative_chunk_number": 42,
        "text": "Comparison of PET/CT datasets available Dataset 3D CT 3D PET Findings Masks Whole body RIDER Lung PET-CT \u2713 \u2713 \u00d7 \u00d7 \u00d7 Head-Neck PET-CT \u2713 \u2713 \u00d7 \u00d7 \u00d7 Lung-PET-CT-Dx \u2713 \u2713 \u00d7 \u00d7 \u00d7 FDG-PET-CT-Lesions \u2713 \u2713 \u00d7 \u2713 \u2713 SegAnyPet \u00d7 \u2713 \u00d7 \u2713 \u2713 Pet2Rep \u00d7 \u2713 \u2713 \u00d7 \u2713 ViMed-PET \u2713 \u2713 \u2713 \u00d7 \u2713 PETAR-11K (ours) \u2713 \u2713 \u2713 \u2713 \u2713 At the time of this submission, Pet2Rep and ViMed-PET have not made public their datasets, and so we do not use them in our evaluations.",
        "text_length": 417,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.016661904752254486,
          -0.000519291905220598,
          -0.01591056026518345,
          0.04189848154783249,
          0.0005846513085998595,
          0.0013297757832333446,
          0.012830195017158985,
          0.04601145535707474,
          -0.013662283308804035,
          0.015132778324186802
        ]
      },
      {
        "chunk_index": 400,
        "relative_chunk_number": 43,
        "text": "4. Model Architecture In this section, we present the proposed architecture to achieve that goal and outline the inputs utilized to gener- ate detailed, localized findings for lesions. 4.1. Mask Aware Inputs Given a PET scan P \u2208RD\u00d7W \u00d7H, a CT scan C \u2208 RD\u00d7W \u00d7H, and a binary mask M \u2208{0, 1}D\u00d7W \u00d7H indicat- ing the region of interest, the objective is to generate detailed 3 Figure 3.",
        "text_length": 380,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01107097789645195,
          0.03776710852980614,
          -0.011334296315908432,
          0.046799734234809875,
          -0.03762105479836464,
          -0.022899942472577095,
          0.016736572608351707,
          0.00575936958193779,
          -0.027744684368371964,
          -0.010445540770888329
        ]
      },
      {
        "chunk_index": 401,
        "relative_chunk_number": 44,
        "text": "Overall architecture of the proposed method. The pipeline integrates PET, CT, and lesion mask inputs via convolutional layers and an M3D-CLIP encoder, followed by token fusion and spatial pooling. The pooled visual tokens are processed by a Phi-3B language model, which generates clinically grounded text reports conditioned on visual features and textual queries.",
        "text_length": 364,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03645254671573639,
          -0.0007482108194380999,
          -0.01103957835584879,
          0.023994937539100647,
          -0.051622483879327774,
          -0.03171709552407265,
          0.022132493555545807,
          -0.02202264405786991,
          -0.03061615116894245,
          0.01076943427324295
        ]
      },
      {
        "chunk_index": 402,
        "relative_chunk_number": 45,
        "text": "diagnostic findings restricted to the masked region. Let f\u03b8 denote our model parameterized by \u03b8. The formulation is as follows: y = f\u03b8(P , C, M) 4.2. Focal Prompt A core issue of lesion-level understanding is that lesions are often a very small part of a volume, and so standard approaches lead to a risk of information loss during common global processing steps, such as resizing or cropping.",
        "text_length": 393,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.04803645983338356,
          0.03617837652564049,
          -0.04322054982185364,
          0.025847649201750755,
          0.011232654564082623,
          -0.013925882056355476,
          0.02094070054590702,
          0.004030030220746994,
          -0.03547647222876549,
          -0.009970532730221748
        ]
      },
      {
        "chunk_index": 403,
        "relative_chunk_number": 46,
        "text": "For instance, in our dataset, the average lesion occupies less than 1% of the total scan volume. To address this, we introduce a 3D focal prompt to provide a localized, high-resolution view of the lesion, similar to prior work [8, 13]. We first center the crop around the mask M and extract a cubic subvolume encompassing the region of interest.",
        "text_length": 345,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.015562882646918297,
          0.0029556183144450188,
          -0.04037480056285858,
          0.027847500517964363,
          -0.030995076522231102,
          -0.0006365376757457852,
          0.021105876192450523,
          0.00791265070438385,
          -0.04346270486712456,
          0.0152010228484869
        ]
      },
      {
        "chunk_index": 404,
        "relative_chunk_number": 47,
        "text": "To improve robustness and avoid overfitting to fixed spatial positions, we apply random spatial perturbations to both the cube center and its side length, with deviations constrained to 20% of the original mask center and cube size. The crop is adjusted such that the mask remains fully contained within the subcube, ensuring complete lesion visibility in the focal prompt.",
        "text_length": 373,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0075621288269758224,
          -0.006506302859634161,
          -0.01010704692453146,
          0.03126620501279831,
          0.017143402248620987,
          0.0035023062955588102,
          0.021664218977093697,
          -0.004398558754473925,
          -0.03250874578952789,
          0.002565175062045455
        ]
      },
      {
        "chunk_index": 405,
        "relative_chunk_number": 48,
        "text": "Let the lesion mask centroid be cM \u2208R3 and the desired crop size be r. We introduce small random perturbations to both the center and size to improve robustness: \u02dcc = cM + \u03b4c, \u02dcr = r + \u03b4r, \u03b4c, \u03b4r i.i.d. \u223cU(\u22120.2r, 0.2r).",
        "text_length": 219,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01097173523157835,
          -0.0026990463957190514,
          -0.040130019187927246,
          0.029265036806464195,
          -0.044873595237731934,
          -0.010523173958063126,
          0.019341934472322464,
          -0.020107921212911606,
          -0.02846464328467846,
          -0.018708964809775352
        ]
      },
      {
        "chunk_index": 406,
        "relative_chunk_number": 49,
        "text": "The focal PET, CT, and mask crops are then extracted as: FP , FC, FM = Crop(P , C, M; \u02dcc, \u02dcr), where FP, FC, FM represent the PET, CT, and mask focal crops, and Crop(\u00b7) denotes the operation that extracts the 3D subvolume. 4.3. Shared Visual Encoding We adopt a shared 3D vision transformer to encode both PET and CT volumes.",
        "text_length": 325,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.003190012648701668,
          0.0083984499797225,
          -0.010625987313687801,
          0.048325128853321075,
          -0.09180671721696854,
          -0.012334336526691914,
          0.021958792582154274,
          0.020680833607912064,
          -0.03183259442448616,
          -0.014067396521568298
        ]
      },
      {
        "chunk_index": 407,
        "relative_chunk_number": 50,
        "text": "Specifically, to prepare the sequential in- puts for the shared transformer, we split the PET and CT vol- umes into non-overlapping 3D patches of size (sD, sW , sH), which are then flattened into a sequence of K patches, de- noted as Ppatch, Cpatch \u2208RK\u00d7sD\u00d7sW \u00d7sH. We then linearly project the patches into the token embeddings.",
        "text_length": 327,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.040146250277757645,
          0.011577988043427467,
          -0.011373835615813732,
          0.045893389731645584,
          0.035198360681533813,
          0.010485381819307804,
          0.02126159518957138,
          -0.036954618990421295,
          -0.02831925079226494,
          -0.007089673541486263
        ]
      },
      {
        "chunk_index": 408,
        "relative_chunk_number": 51,
        "text": "Zp = PatchEmbed(P ) \u2208RK\u00d7d (1) ZC = PatchEmbed(C) \u2208RK\u00d7d (2) We further encode the PET and CT patches using the shared 3D vision transformer T , while incorporating the lesion mask via additive conditioning: XPET = T \u0000ZP + MaskEmbed(M) \u0001 , XCT = T \u0000ZC \u0001 . (3) The two representations are fused by concatenating both the CT and mask-aware PET tokens along the embedding dimension: X = Concat(XPET, XCT).",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.015345625579357147,
          0.019428595900535583,
          -0.04018567129969597,
          0.0334509052336216,
          -0.007918181829154491,
          0.023865126073360443,
          0.020916016772389412,
          -0.038009822368621826,
          -0.02589491195976734,
          -0.0316782146692276
        ]
      },
      {
        "chunk_index": 409,
        "relative_chunk_number": 52,
        "text": "The same process is applied to focal crops (FP , FC, FM), producing focal embeddings \u02dcX. Global and focal features are then combined via element-wise addition and reduced via spatial pooling [1]: T = \u0000X + \u02dcX \u0001 \u2208RK\u00d7dv. These pooled tokens are linearly projected into the language model space: V = Proj(SpatialPooler(T)).",
        "text_length": 319,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.027262646704912186,
          0.018213262781500816,
          -0.015673434361815453,
          0.02313915267586708,
          0.002178024733439088,
          -0.02868078462779522,
          0.020432081073522568,
          0.032413411885499954,
          -0.028983481228351593,
          0.014757715165615082
        ]
      },
      {
        "chunk_index": 410,
        "relative_chunk_number": 53,
        "text": "4 Finally, the projected tokens V and a lesion-description query q are passed to the language model decoder to generate localized findings: y = LMDecoder(q, V). 4.4. Training We adopt a four-stage training pipeline designed to progres- sively align mask-aware PET/CT representations with the language model for lesion-focused report generation. Stage 1: Pretraining.",
        "text_length": 366,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.005332332570105791,
          0.017582561820745468,
          -0.02760964445769787,
          -0.0023648992646485567,
          -0.051431119441986084,
          0.04792802035808563,
          0.022788047790527344,
          0.035406917333602905,
          -0.03460266441106796,
          -0.018699392676353455
        ]
      },
      {
        "chunk_index": 411,
        "relative_chunk_number": 54,
        "text": "We begin by pretraining the model on the curated TotalSegmentator dataset, framing the region classification task as a text generation problem.",
        "text_length": 143,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.029540007933974266,
          0.026241328567266464,
          0.0011280179023742676,
          0.0396149642765522,
          0.03267878293991089,
          0.04383855685591698,
          0.023838620632886887,
          0.028497101739048958,
          -0.035846736282110214,
          0.09032226353883743
        ]
      },
      {
        "chunk_index": 412,
        "relative_chunk_number": 55,
        "text": "Each sam- ple is presented as a question\u2013answer pair following the template: Question: \u201cWhat is the region highlighted by the mask?\u201d Answer: \u201cThe highlighted region is the < Region Class>.\u201d Multiple prompt variations are used to encourage linguistic diversity and generalization (details provided in the supplementary material). Stage 2: Mask Embedding Alignment.",
        "text_length": 363,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.013201254419982433,
          -0.0011901442194357514,
          5.507052264874801e-05,
          0.053228482604026794,
          -0.023258935660123825,
          0.010699154809117317,
          0.018371233716607094,
          0.0360831543803215,
          -0.03067048080265522,
          0.026518285274505615
        ]
      },
      {
        "chunk_index": 413,
        "relative_chunk_number": 56,
        "text": "In this stage, the mask embedding module is trained independently to align binary mask features with corresponding 3D anatomical structures. All other components remain frozen, allowing the model to learn robust mask-to-anatomy correspondences. Stage 3: Projector Alignment. The projection head is then optimized to map mask-aware visual embeddings into the language model\u2019s feature space.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.033350154757499695,
          0.016792017966508865,
          -0.012917906977236271,
          0.02833767607808113,
          -0.06664997339248657,
          -0.02841344103217125,
          0.020907945930957794,
          -0.024996906518936157,
          -0.029636017978191376,
          -0.0032288040965795517
        ]
      },
      {
        "chunk_index": 414,
        "relative_chunk_number": 57,
        "text": "During this phase, both the vision backbone and the LLM are frozen to ensure the projector learns semantically consistent cross-modal align- ment between visual and textual representations. Stage 4: Full Finetuning. Finally, the entire network is finetuned end-to-end on our region-aware PET/CT dataset.",
        "text_length": 303,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0032802922651171684,
          0.027736781165003777,
          -0.020156513899564743,
          -0.010343627072870731,
          -0.02013302780687809,
          -0.03310629352927208,
          0.02243220806121826,
          0.04484900087118149,
          -0.0378740094602108,
          0.03636224567890167
        ]
      },
      {
        "chunk_index": 415,
        "relative_chunk_number": 58,
        "text": "This stage enables the model to jointly refine all compo- nents for generating detailed, clinically grounded, and lesion- specific findings conditioned on the input mask. 4.5.",
        "text_length": 175,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014283141121268272,
          0.023010412231087685,
          -0.027241405099630356,
          0.05168876796960831,
          -0.04213208705186844,
          -0.01721341535449028,
          0.0225736852735281,
          -0.045414671301841736,
          -0.03709164261817932,
          7.789007941028103e-05
        ]
      },
      {
        "chunk_index": 416,
        "relative_chunk_number": 59,
        "text": "Training Objective The model is trained using an autoregressive negative log- likelihood objective conditioned on the visual inputs and preceding text tokens: L = \u2212 X D N X i=1 log p(yi | V, q, y<i), where D denotes the training dataset, N is the number of tokens in each generated sequence, V represents the visual tokens projected from the PET\u2013CT\u2013mask encoder, q is the lesion-description query, and yi is the ith language token in the generated report.",
        "text_length": 455,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02062470093369484,
          0.034977663308382034,
          -0.0031551748979836702,
          -0.00591083662584424,
          -0.0755835771560669,
          0.00018657413602340966,
          0.01698443479835987,
          0.015729408711194992,
          -0.02108873799443245,
          0.006931719835847616
        ]
      },
      {
        "chunk_index": 417,
        "relative_chunk_number": 60,
        "text": "5. Experiments and Analysis 5.1. Implementation details We adopt the vision encoder and language model from M3D, a popular baseline for medical VLMs as it has been trained on a large corpra of 3D medical images and corresponding text. All experiemnts are conducted on 2 NVIDIA L40S GPU. Stage 1 is trained for 5 epochs and all subsequent stages are trained for 10 eopchs.",
        "text_length": 371,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02171633020043373,
          0.019991880282759666,
          -0.014664456248283386,
          0.0030928628984838724,
          -0.07477471977472305,
          -0.02398649789392948,
          0.019984379410743713,
          -0.00649590790271759,
          -0.028099246323108673,
          -0.0407685749232769
        ]
      },
      {
        "chunk_index": 418,
        "relative_chunk_number": 61,
        "text": "Training in total takes \u223c 20 hours. A detailed list of hyperparameters can be found in our supplamnetary material. 5.2. Evaluation Compared Models We include 2D VLMs, 2D prompt-aware VLMs, 3D VLMs, and 3D prompt-aware VLMs to ensure broad modality and capability coverage.",
        "text_length": 272,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.023301010951399803,
          0.015098178759217262,
          -0.02999662049114704,
          0.008053760044276714,
          -0.02193475142121315,
          0.01445713173598051,
          0.021844327449798584,
          0.04421600326895714,
          -0.027157500386238098,
          0.0006754912901669741
        ]
      },
      {
        "chunk_index": 419,
        "relative_chunk_number": 62,
        "text": "For 2D VLMs, we select strong medical baselines including InternVL3- 8B, Hautuo-7B, MedGemma-4B, and Qwen3-8B, each trained on large-scale medical and general-domain corpora. As 2D prompt-aware baselines, we evaluate ViP-LLaVA, which integrates visual grounding with mask-conditioned prompting.",
        "text_length": 294,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.017035340890288353,
          0.002149561420083046,
          -0.02771078795194626,
          -0.000983221922069788,
          0.01155431754887104,
          0.01161462813615799,
          0.022947918623685837,
          0.037817467004060745,
          -0.025757335126399994,
          0.009635930880904198
        ]
      },
      {
        "chunk_index": 420,
        "relative_chunk_number": 63,
        "text": "Among 3D VLMs, we benchmark against M3D, Med3DVLM, and M3D-RAD, all capable of volumetric reasoning on CT. Finally, we compare against 3D prompt- aware frameworks such as Reg2RG and our proposed model, which explicitly incorporates mask-guided conditioning. We also finetune representative models using our dataset for a fairer comparison and validation of our model architecture.",
        "text_length": 380,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0312587134540081,
          0.028240442276000977,
          -0.011071877554059029,
          0.020849334076046944,
          -0.0029039536602795124,
          0.00016789586516097188,
          0.021852068603038788,
          0.0041784788481891155,
          -0.023465221747756004,
          -0.015880074352025986
        ]
      },
      {
        "chunk_index": 421,
        "relative_chunk_number": 64,
        "text": "We choose the best performing models in each category for further finetuning. Metrics. To comprehensively evaluate model perfor- mance, we adopt multiple categories of metrics: \u2022 NLP Metrics: BLEU, ROUGE, METEOR, and CIDEr. These metrics are widely used in prior works. They all measure n-gram overlap.",
        "text_length": 302,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.01747318170964718,
          0.0024259653873741627,
          -0.03604062274098396,
          0.01225175429135561,
          -0.03087245672941208,
          -0.00600283732637763,
          0.02480166032910347,
          -0.03698420897126198,
          -0.024896210059523582,
          -0.006444084458053112
        ]
      },
      {
        "chunk_index": 422,
        "relative_chunk_number": 65,
        "text": "\u2022 Semantic Metrics: BERTScore and BARTScore, com- puted using checkpoints fine-tuned on large radiology cor- pora, to capture latent contextual similarity between pre- dicted and reference reports. \u2022 Language Model Metrics: RaTEScore and GREEN, which assess clinical plausibility and reasoning quality via language-model\u2013based evaluation. 5.3.",
        "text_length": 343,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0160682275891304,
          0.013363110832870007,
          0.0011735311709344387,
          0.028762074187397957,
          0.037284087389707565,
          -0.008044104091823101,
          0.02451101504266262,
          -0.020456258207559586,
          -0.025855395942926407,
          -0.007768976967781782
        ]
      },
      {
        "chunk_index": 423,
        "relative_chunk_number": 66,
        "text": "Human Evaluation Automated evaluation metrics often fail to capture the nu- anced clinical reasoning and linguistic preferences of ra- diologists. To address this limitation, we conducted two rounds of human-centered evaluations tailored to PET report generation.",
        "text_length": 263,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.018749792128801346,
          0.03204458951950073,
          -0.0251011922955513,
          0.05508316308259964,
          -0.039711955934762955,
          -0.028323037549853325,
          0.022391457110643387,
          0.020526601001620293,
          -0.037872910499572754,
          0.010084315203130245
        ]
      },
      {
        "chunk_index": 424,
        "relative_chunk_number": 67,
        "text": "In the first round, 100 pairs of ground-truth and model- generated findings were rated by board-certified physicians using our standardized scoring scheme. We then computed the Spearman\u2019s correlation between the automated metrics and the average human scores. In the second round, we performed a larger physician-led evaluation on a stratified subset of the test set.",
        "text_length": 367,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02692985348403454,
          0.025537360459566116,
          -0.012330519035458565,
          0.0439201258122921,
          -0.006980791222304106,
          0.0029964628629386425,
          0.02149224281311035,
          -0.05502196028828621,
          -0.035317447036504745,
          -0.04816579446196556
        ]
      },
      {
        "chunk_index": 425,
        "relative_chunk_number": 68,
        "text": "A team of five board-certified nuclear medicine physicians assessed each report for interpretation correctness, localization fidelity, 5 Model BLEU ROUGE-L METEOR CIDEr BART BERT RaTE GREEN 2D VLMs Gemma 0.124 0.352 0.358 0.027 -4.92 0.690 0.540 0.011 Hautuo 0.130 0.350 0.357 0.024 -4.85 0.695 0.584 0.015 InternVL3 0.137 0.355 0.356 0.035 -4.72 0.694 0.614 0.030 Qwen3 0.116 0.342 0.364 0.035 -4.83 0.696 0.531 0.013 2D mask-aware VLMs ViP-LLaVA 0.064 0.301 0.308 0.009 -5.96 0.651 0.503 0.006 3D VLMs Med3DVLM 0.004 0.080 0.066 0.005 -5.7 0.511 0.285 0.002 M3D 0.327 0.276 0.323 0.013 -5.32 0.634 0.505 0.000 M3D-RAD 0.343 0.300 0.340 0.016 -5.23 0.658 0.518 0.003 M3D-RAD (finetuned) 0.485 0.446 0.501 0.132 -4.34 0.750 0.627 0.071 3D mask-aware VLMs Reg2RG 0.044 0.060 0.108 0.001 -5.540 0.518 0.363 0.002 Reg2RG (finetuned) 0.478 0.416 0.487 0.055 -4.58 0.732 0.532 0.031 PETAR-4B (Ours) 0.531 0.519 0.557 0.415 -4.04 0.791 0.708 0.246 Table 3.",
        "text_length": 950,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010198338888585567,
          -0.010853209532797337,
          0.007553501985967159,
          0.00793931819498539,
          -0.020673129707574844,
          0.0028229476884007454,
          0.007254717871546745,
          0.028780082240700722,
          -0.004780754912644625,
          0.054876700043678284
        ]
      },
      {
        "chunk_index": 426,
        "relative_chunk_number": 69,
        "text": "Comparison of selected models on medical imaging modalities using multiple evaluation metrics.",
        "text_length": 94,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02221648395061493,
          0.004102054052054882,
          -0.03082418441772461,
          0.026369882747530937,
          -0.01768103986978531,
          -0.0019242584239691496,
          0.01914721354842186,
          -0.013375268317759037,
          -0.015378163196146488,
          0.01878485269844532
        ]
      },
      {
        "chunk_index": 427,
        "relative_chunk_number": 70,
        "text": "Mask CT Focal TS BLEU ROUGE METEOR CIDEr BERTScore BARTScore RaTEScore GREEN \u00d7 \u00d7 \u00d7 \u00d7 - - - - - - - - \u00d7 \u00d7 \u00d7 \u2713 0.485 0.446 0.501 0.132 0.750 -4.34 0.627 0.071 \u2713 \u00d7 \u00d7 \u2713 0.480 0.445 0.498 0.137 0.748 -4.33 0.626 0.088 \u00d7 \u2713 \u00d7 \u2713 0.477 0.436 0.499 0.134 0.746 -4.31 0.622 0.060 \u00d7 \u00d7 \u2713 \u2713 0.528 0.518 0.550 0.397 0.787 -4.05 0.698 0.226 \u2713 \u00d7 \u2713 \u2713 0.525 0.518 0.550 0.381 0.787 -4.05 0.700 0.232 \u00d7 \u2713 \u2713 \u2713 0.517 0.507 0.540 0.428 0.784 -4.08 0.587 0.234 \u2713 \u2713 \u2713 \u2713 0.531 0.519 0.557 0.415 0.791 -4.04 0.708 0.246 Table 4.",
        "text_length": 501,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0029615978710353374,
          -0.02018243819475174,
          -0.009500091895461082,
          0.011872324161231518,
          -0.009033090434968472,
          -0.030822530388832092,
          0.0024667843244969845,
          -0.014810149557888508,
          0.003994718659669161,
          0.05892013758420944
        ]
      },
      {
        "chunk_index": 428,
        "relative_chunk_number": 71,
        "text": "Ablation study of our model showing the effect of each component on multiple evaluation metrics. Metric Spearman\u2019s \u03c1 GREEN 0.608 ROUGE 0.470 RaTEScore 0.447 BERTScore 0.398 CIDEr 0.384 METEOR 0.362 BARTScore 0.313 BLEU 0.304 Table 5. Spearman\u2019s correlation between automated metrics and human evaluation scores. Higher correlation indicates stronger alignment with expert judgment.",
        "text_length": 381,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.013263607397675514,
          0.014011899009346962,
          -0.021036043763160706,
          0.06460314989089966,
          -0.011837794445455074,
          0.0020114595536142588,
          0.019783737137913704,
          -0.038602884858846664,
          -0.025120809674263,
          -0.02005062997341156
        ]
      },
      {
        "chunk_index": 429,
        "relative_chunk_number": 72,
        "text": "and clinical utility. Each expert evaluated a sample of 16 lesions from 8 patients, providing detailed qualitative feed- back alongside quantitative ratings. Details for the criteria used for scoring can be found in our supplementary material. 5.4.",
        "text_length": 248,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.01891947351396084,
          0.023813521489501,
          0.004610572010278702,
          0.049117401242256165,
          0.017348214983940125,
          0.052508000284433365,
          0.02205348201096058,
          -0.048381540924310684,
          -0.04506680741906166,
          -0.06817076355218887
        ]
      },
      {
        "chunk_index": 430,
        "relative_chunk_number": 73,
        "text": "PETAR-Bench To facilitate standardized and reproducible evaluation, we introduce PETAR-Bench\u2014a large-scale benchmark com- prising 1,170 lesions and their corresponding PET/CT find- ings. It includes ground-truth findings, corresponding le- sion masks, and cross-modality PET/CT data, enabling fine- grained evaluation across linguistic, semantic, and clinical dimensions.",
        "text_length": 371,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006820413749665022,
          0.0313509926199913,
          0.006785474251955748,
          0.028390690684318542,
          -0.00024339086667168885,
          -0.008649677038192749,
          0.023539751768112183,
          -0.031131979078054428,
          -0.028724703937768936,
          0.005383306182920933
        ]
      },
      {
        "chunk_index": 431,
        "relative_chunk_number": 74,
        "text": "We employ PETAR-Bench to benchmark both 2D and 3D vision\u2013language models under the full set of metrics described above, including human evaluation results. This benchmark establishes the first unified framework for evalu- ating localized, clinically grounded finding report generation on PET/CT data. 5.5. Main Results Tab.",
        "text_length": 323,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.021392256021499634,
          0.04355698078870773,
          -0.006742366123944521,
          0.0289502814412117,
          -0.044402871280908585,
          -0.013125917874276638,
          0.021767618134617805,
          -0.02587090991437435,
          -0.02375904656946659,
          -0.00787425972521305
        ]
      },
      {
        "chunk_index": 432,
        "relative_chunk_number": 75,
        "text": "3 compares PETAR-4B with a range of 2D and 3D vision\u2013language models across multiple evaluation metrics. Among all models, PETAR-4B (Ours) achieves the highest performance across nearly every category, demonstrating its strong capability for localized, clinically coherent report generation from PET/CT volumes.",
        "text_length": 311,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.050250161439180374,
          0.04393751174211502,
          -0.03821833059191704,
          0.030998632311820984,
          -0.0381804034113884,
          -0.006953114178031683,
          0.024299951270222664,
          0.036440085619688034,
          -0.03028017468750477,
          0.0016561785014346242
        ]
      },
      {
        "chunk_index": 433,
        "relative_chunk_number": 76,
        "text": "Compared to the best-performing 2D model, InternVL3, PETAR-4B improves BLEU-4 by 29%, ROUGE-L by 46%, and METEOR by 56%, highlighting the benefit of volu- metric reasoning and multimodal fusion. When compared to 3D baselines such as M3D and M3D-RAD, PETAR- 4B yields substantial gains in all metrics, most notably in CIDEr (+0.283) and GREEN (+0.175), which measure con- 6 Figure 4.",
        "text_length": 382,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03726421296596527,
          0.021845581009984016,
          -0.055866390466690063,
          0.0018478481797501445,
          -0.028679994866251945,
          0.006638741586357355,
          0.020122772082686424,
          -0.02848884090781212,
          -0.018901661038398743,
          -0.01723494566977024
        ]
      },
      {
        "chunk_index": 434,
        "relative_chunk_number": 77,
        "text": "Some examples showing reporting from our model and the next best performing model. tent diversity and clinical reasoning quality. The model also surpasses the finetuned M3D-RAD baseline, improving BERTScore from 0.7498 to 0.7914 and RaTEScore from 0.6271 to 0.7084, indicating stronger semantic alignment and factual accuracy.",
        "text_length": 326,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010995998978614807,
          0.02300400659441948,
          -0.05405101925134659,
          0.03819727152585983,
          -0.025648953393101692,
          -0.009635147638618946,
          0.02269315905869007,
          -0.013653757981956005,
          -0.031745851039886475,
          -0.04546002298593521
        ]
      },
      {
        "chunk_index": 435,
        "relative_chunk_number": 78,
        "text": "These improvements demonstrate that incorporating mask-aware volumetric conditioning and focal prompting en- ables PETAR-4B to effectively capture both global anatomi- cal context and fine-grained lesion characteristics, setting a new state of the art in PET/CT report generation. 5.6.",
        "text_length": 285,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.06329277157783508,
          0.0186174213886261,
          -0.028702622279524803,
          0.030674243345856667,
          -0.006567830685526133,
          0.015376055613160133,
          0.02530662715435028,
          0.012868855148553848,
          -0.03290019556879997,
          0.04242191091179848
        ]
      },
      {
        "chunk_index": 436,
        "relative_chunk_number": 79,
        "text": "Ablation Studies To assess the contribution of each component in PETAR- 4B, we conduct ablation experiments by selectively en- abling or disabling the mask input, CT modality, and focal prompt (Tab. 4). The baseline configuration without any of these components performs moderately well, but sequen- tially adding each module leads to consistent improvements across all metrics.",
        "text_length": 378,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.012687426060438156,
          0.021980199962854385,
          -0.019884426146745682,
          0.043236054480075836,
          -0.06373005360364914,
          0.010342461057007313,
          0.02311476692557335,
          -0.045039743185043335,
          -0.03068958967924118,
          -0.0020234391558915377
        ]
      },
      {
        "chunk_index": 437,
        "relative_chunk_number": 80,
        "text": "Introducing the mask input improves region-level local- ization, reflected in gains for CIDEr and RaTEScore, as the model learns to better ground descriptions spatially. In- corporating the CT modality further enhances structural understanding and anatomical coherence, yielding higher BERTScore and GREEN.",
        "text_length": 306,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.013704074546694756,
          0.02289174310863018,
          -0.031627632677555084,
          0.0411214642226696,
          -0.029112469404935837,
          -0.02289082668721676,
          0.021682141348719597,
          -0.05434871092438698,
          -0.017869649454951286,
          0.009354415349662304
        ]
      },
      {
        "chunk_index": 438,
        "relative_chunk_number": 81,
        "text": "The addition of the focal prompt produces the most pronounced gains, boosting CIDEr from 0.1317 to 0.3965 and GREEN from 0.0705 to 0.2260, by allowing the model to focus on small, high-resolution lesion regions. The full configuration\u2014combining all three compo- nents\u2014achieves the best overall results, with peak scores of 0.5312 (BLEU-4), 0.5566 (METEOR), and 0.7084 (RaTEScore).",
        "text_length": 380,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04585934802889824,
          -0.013375293463468552,
          -0.05563873052597046,
          0.03199077770113945,
          -0.056098997592926025,
          -0.036351241171360016,
          0.016535868868231773,
          -0.041495371609926224,
          -0.028676319867372513,
          0.04002528265118599
        ]
      },
      {
        "chunk_index": 439,
        "relative_chunk_number": 82,
        "text": "This demonstrates that each component con- tributes synergistically, and that PETAR-4B\u2019s design effec- tively balances global volumetric reasoning with fine-grained lesion grounding for accurate, clinically meaningful PET/CT report generation. 6.",
        "text_length": 246,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01508350670337677,
          0.03742189705371857,
          -0.03177349641919136,
          0.02803530916571617,
          -0.022234946489334106,
          -0.0017492749029770494,
          0.026318121701478958,
          0.00458592688664794,
          -0.03431154415011406,
          0.007840786129236221
        ]
      },
      {
        "chunk_index": 440,
        "relative_chunk_number": 83,
        "text": "Discussion The results showed that metrics emphasizing radiology- specific relevance and semantic similarity correlated most strongly with human judgment, underscoring their clinical validity. This is just a piece of the PETAR framework.",
        "text_length": 237,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.025454161688685417,
          0.020605914294719696,
          -0.006946495734155178,
          0.0492599792778492,
          0.0352412648499012,
          0.04111379012465477,
          0.02338704839348793,
          0.046593692153692245,
          -0.019497830420732498,
          0.002180645242333412
        ]
      },
      {
        "chunk_index": 441,
        "relative_chunk_number": 84,
        "text": "ROIs would be generated by one of a number of existing AI models that have been specifically designed to detect and segment PET lesions for a number of different PET radiotracers. These models would serve as the first stage of PETAR, or, altner- atively, physicians can generate ROIs using existing FDA- cleared single-mouse-click PET lesion segmentation tools.",
        "text_length": 361,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.026399917900562286,
          0.013651536777615547,
          -0.028421644121408463,
          0.03339407593011856,
          -0.02118414081633091,
          0.0566549189388752,
          0.021903017535805702,
          0.04148783162236214,
          -0.0346868559718132,
          0.047274716198444366
        ]
      },
      {
        "chunk_index": 442,
        "relative_chunk_number": 85,
        "text": "In both cases, PETAR would use these ROIs to make criti- cal numerical measurements (size, SUV), and automatically 7 populate the report with a list of findings and measurements based on the ROIs. We envision the list of findings would then be converted into a templated, free-text report using LLMs, such as automatic impression generators. This re- mains future work. 7.",
        "text_length": 372,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00011265215289313346,
          0.03021610714495182,
          -0.0059001329354941845,
          0.04057692736387253,
          -0.00669006397947669,
          0.029134545475244522,
          0.02344631776213646,
          0.02969197928905487,
          -0.036785613745450974,
          -0.05069132521748543
        ]
      },
      {
        "chunk_index": 443,
        "relative_chunk_number": 86,
        "text": "Conclusion We introduced PETAR, a 3D mask-aware vision\u2013language framework for PET/CT report generation, along with the first large-scale dataset linking lesion-level PET/CT findings to descriptive clinical text.",
        "text_length": 211,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.051793910562992096,
          0.02682015672326088,
          -0.007695838343352079,
          0.022194240242242813,
          -0.021833784878253937,
          0.025307118892669678,
          0.0206748079508543,
          0.0015217607142403722,
          -0.036007896065711975,
          -0.03081260435283184
        ]
      },
      {
        "chunk_index": 444,
        "relative_chunk_number": 87,
        "text": "By combining metabolic and structural cues through mask-guided volumetric reasoning and focal prompting, PETAR achieves state-of-the-art perfor- mance across linguistic, semantic, and clinically grounded metrics. Our human evaluations further demonstrate strong alignment with expert radiologist judgment, highlighting the model\u2019s capacity to produce accurate and clinically useful findings. 8.",
        "text_length": 394,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.005736073479056358,
          0.025647567585110664,
          0.0005500840488821268,
          0.025644974783062935,
          0.006272719707340002,
          0.00851674284785986,
          0.021695053204894066,
          0.0085685970261693,
          -0.021291524171829224,
          -0.011355985887348652
        ]
      },
      {
        "chunk_index": 445,
        "relative_chunk_number": 88,
        "text": "Acknowledgments Research reported in this publication was also supported by the National Institute Of Biomedical Imaging And Bioengi- neering of the National Institutes of Health under Award Number R01EB033782. References [1] Fan Bai, Yuxin Du, Tiejun Huang, Max Q.-H. Meng, and Bo Zhao. M3d: Advancing 3d medical image analysis with multi-modal large language models.",
        "text_length": 368,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.009486839175224304,
          0.017982322722673416,
          -0.02090480551123619,
          0.048399511724710464,
          -0.05349938943982124,
          0.03268938511610031,
          0.019164172932505608,
          0.0023298149462789297,
          -0.02598925679922104,
          -0.022502213716506958
        ]
      },
      {
        "chunk_index": 446,
        "relative_chunk_number": 89,
        "text": "arXiv preprint arXiv:2404.00578, 2024. 2, 4 [2] Shruthi Bannur, Kenza Bouzid, Daniel C. Castro, Anton Schwaighofer, Sam Bond-Taylor, Maximilian Ilse, Fernando P\u00b4erez-Garc\u00b4\u0131a, Valentina Salvatelli, Harshita Sharma, Fe- lix Meissen, Mercy Ranjit, Shaury Srivastav, Julia Gong, Fabian Falck, Ozan Oktay, Anja Thieme, Matthew P. Lun- gren, Maria Teodora Wetscherek, Javier Alvarez-Valle, and Stephanie L.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0100317457690835,
          0.0009607496904209256,
          -0.005843593273311853,
          0.013774290680885315,
          0.07237926870584488,
          0.017472680658102036,
          0.007939880713820457,
          -0.02380828931927681,
          -0.019176652655005455,
          0.005527640227228403
        ]
      },
      {
        "chunk_index": 447,
        "relative_chunk_number": 90,
        "text": "Hyland. Maira-2: Grounded radiology report generation. arXiv preprint arXiv:2406.04449, 2024.",
        "text_length": 93,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0009121575276367366,
          0.01780519261956215,
          -0.02331089973449707,
          0.049236975610256195,
          0.01088782399892807,
          -0.026187879964709282,
          0.023463595658540726,
          0.013830665498971939,
          -0.03282138705253601,
          -0.038896940648555756
        ]
      },
      {
        "chunk_index": 448,
        "relative_chunk_number": 91,
        "text": "2 [3] Louis Blankemeier, Joseph Paul Cohen, Ashwin Ku- mar, Dave Van Veen, Syed Jamal Safdar Gardezi, Mag- dalini Paschali, Zhihong Chen, Jean-Benoit Delbrouck, Ed- uardo Reis, Cesar Truyts, Christian Bluethgen, Malte En- gmann Kjeldskov Jensen, Sophie Ostmeier, Maya Varma, Jeya Maria Jose Valanarasu, Zhongnan Fang, Zepeng Huo, Zaid Nabulsi, Diego Ardila, Wei-Hung Weng, Edson Amaro Junior, Neera Ahuja, Jason Fries, Nigam H.",
        "text_length": 427,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.013286683708429337,
          0.023500671610236168,
          -0.00729682482779026,
          -0.02273908443748951,
          0.03154291957616806,
          -0.01185171864926815,
          0.011399507522583008,
          -0.02339758723974228,
          -0.01801501400768757,
          0.030116591602563858
        ]
      },
      {
        "chunk_index": 449,
        "relative_chunk_number": 92,
        "text": "Shah, Andrew Johnston, Robert D. Boutin, Andrew Wentland, Curtis P. Lan- glotz, Jason Hom, Sergios Gatidis, and Akshay S. Chaudhari. Merlin: A vision language foundation model for 3d computed tomography. arXiv preprint arXiv:2406.06512, 2024. 2 [4] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee.",
        "text_length": 357,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004533782601356506,
          -0.0010466494131833315,
          0.004870063625276089,
          0.008158334530889988,
          -0.013085261918604374,
          -0.019890516996383667,
          0.007824942469596863,
          0.011367715895175934,
          -0.013294373638927937,
          0.021703071892261505
        ]
      },
      {
        "chunk_index": 450,
        "relative_chunk_number": 93,
        "text": "Vip- llava: Making large multimodal models understand arbitrary visual prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12914\u201312923, 2024. 2 [5] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 2 [6] Z. Chen, Y.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00806424394249916,
          0.013897974975407124,
          0.02010580338537693,
          0.037109050899744034,
          0.007007861975580454,
          -0.001315183937549591,
          0.018372006714344025,
          -0.007811445277184248,
          -0.02480526827275753,
          -0.005785488989204168
        ]
      },
      {
        "chunk_index": 451,
        "relative_chunk_number": 94,
        "text": "Bie, H. Jin, and H. Chen. Large language model with region-guided referring and grounding for CT report generation. IEEE Transactions on Medical Imaging, 44(8): 3139\u20133150, 2025. 2 [7] Ibrahim Ethem Hamamci, Sezgin Er, and Bjoern Menze. Ct2rep: Automated radiology report generation for 3d med- ical imaging. In Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2024, 2024.",
        "text_length": 390,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.039273396134376526,
          0.012050864286720753,
          -0.00865023210644722,
          0.04203621670603752,
          -0.01776696741580963,
          0.013704550452530384,
          0.01663168892264366,
          -0.007055505178868771,
          -0.0322602316737175,
          0.0027416085358709097
        ]
      },
      {
        "chunk_index": 452,
        "relative_chunk_number": 95,
        "text": "2 [8] Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Zhifei Zhang, Yilin Wang, Jianming Zhang, and Jiebo Luo. Finecaption: Compositional image captioning focusing on wherever you want at any granularity, 2024. 2, 4 [9] Shih-Cheng Huang, Kevin Lee, Kimberly Dodelzon, and et al. Efficiency and quality of generative ai\u2013assisted radiograph reporting. JAMA Network Open, 8(10):e2334943, 2025.",
        "text_length": 387,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004302109591662884,
          0.02493351325392723,
          -0.00014583727170247585,
          0.030556300655007362,
          -0.028317686170339584,
          -0.021647078916430473,
          0.017927849665284157,
          0.029248720034956932,
          -0.02887907810509205,
          0.010112736374139786
        ]
      },
      {
        "chunk_index": 453,
        "relative_chunk_number": 96,
        "text": "1 [10] Zachary Huemann, Samuel Church, Joshua D. Warner, Daniel Tran, Xin Tie, Alan B McMillan, Junjie Hu, Steve Y. Cho, Meghan Lubner, and Tyler J. Bradshaw. Vision-language modeling in pet/ct for visual grounding of positive findings, 2025. 3 [11] Stephanie L.",
        "text_length": 262,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.020527560263872147,
          0.003746247384697199,
          -0.0041890572756528854,
          0.006433388683944941,
          0.022251056507229805,
          -0.012862815521657467,
          0.013745272532105446,
          0.030012641102075577,
          -0.03052222728729248,
          0.05125034973025322
        ]
      },
      {
        "chunk_index": 454,
        "relative_chunk_number": 97,
        "text": "Hyland, Shruthi Bannur, Kenza Bouzid, Daniel Coelho de Castro, Mercy Ranjit, Anton Schwaighofer, Fernando P\u00b4erez-Garc\u00b4\u0131a, Valentina Salvatelli, Shaury Sri- vastav, Anja Thieme, Noel Codella, Matthew P. Lungren, Maria Teodora Wetscherek, Ozan Oktay, and Javier Alvarez- Valle. Maira-1: A specialised large multimodal model for radiology report generation.",
        "text_length": 354,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.028874872252345085,
          0.02202724479138851,
          0.008442438207566738,
          0.005804641172289848,
          -0.003949794918298721,
          -0.008364880457520485,
          0.007106808014214039,
          -0.004775779787451029,
          -0.015787910670042038,
          0.009935819543898106
        ]
      },
      {
        "chunk_index": 455,
        "relative_chunk_number": 98,
        "text": "Technical Report MSR-TR-2023- 47, Microsoft Research, 2023. 2 [12] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and- vision assistant for biomedicine in one day. In Advances in Neural Information Processing Systems, 2023.",
        "text_length": 342,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01675901561975479,
          0.025265557691454887,
          -0.008817337453365326,
          -0.005408388562500477,
          -0.006874971557408571,
          0.032886482775211334,
          0.017420433461666107,
          -0.01024546567350626,
          -0.022601040080189705,
          -0.021538786590099335
        ]
      },
      {
        "chunk_index": 456,
        "relative_chunk_number": 99,
        "text": "2 [13] Long Lian, Yifan Ding, Yunhao Ge, Sifei Liu, Hanzi Mao, Boyi Li, Marco Pavone, Ming-Yu Liu, Trevor Darrell, Adam Yala, and Yin Cui. Describe anything: Detailed localized image and video captioning. In International Conference on Computer Vision (ICCV), 2025. 4 [14] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.",
        "text_length": 360,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.032921817153692245,
          0.02622145786881447,
          -0.02089725248515606,
          0.012183570303022861,
          -0.029286688193678856,
          -0.002707619220018387,
          0.013496873900294304,
          0.06282505393028259,
          -0.02181096561253071,
          -0.003656700486317277
        ]
      },
      {
        "chunk_index": 457,
        "relative_chunk_number": 100,
        "text": "Kosmos-2: Grounding multimodal large language models to the world, 2023. 2 [15] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha- clip: A clip model focusing on wherever you want. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13019\u201313029, 2024.",
        "text_length": 360,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.005558754317462444,
          0.010379952378571033,
          -0.005045657977461815,
          0.022811878472566605,
          -0.007411486003547907,
          -0.032437123358249664,
          0.018444208428263664,
          -0.01575690135359764,
          -0.024995237588882446,
          0.05502653494477272
        ]
      },
      {
        "chunk_index": 458,
        "relative_chunk_number": 101,
        "text": "2 [16] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. arXiv preprint arXiv:2308.02463, 2023. 2 8 [17] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning.",
        "text_length": 369,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.027599947527050972,
          0.037977948784828186,
          -0.01667398028075695,
          0.015244723297655582,
          -0.02934941276907921,
          -0.012624519877135754,
          0.01819484494626522,
          0.01700439117848873,
          -0.019595280289649963,
          -0.023585336282849312
        ]
      },
      {
        "chunk_index": 459,
        "relative_chunk_number": 102,
        "text": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28202\u201328211, 2024. 2 [18] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023. 2 9",
        "text_length": 338,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01665486954152584,
          0.010543725453317165,
          0.013529623858630657,
          0.03816236928105354,
          -0.015376344323158264,
          0.013279270380735397,
          0.017964249476790428,
          -0.009504868648946285,
          -0.020427949726581573,
          0.006353845354169607
        ]
      }
    ]
  },
  {
    "paper_id": "2510.27679v1",
    "total_chunks": 64,
    "index_range": {
      "start": 460,
      "end": 523
    },
    "chunks": [
      {
        "chunk_index": 460,
        "relative_chunk_number": 1,
        "text": "Dark-Field X-Ray Imaging Significantly Improves Deep- Learning based Detection of Synthetic Early-Stage Lung Tumors in Preclinical Models Joyoni Dey1, Hunter C. Meyer1, Murtuza S.",
        "text_length": 179,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.037865687161684036,
          -0.0062136296182870865,
          -0.04505021497607231,
          0.01068923156708479,
          -0.02764568291604519,
          -0.06503898650407791,
          0.021743914112448692,
          0.018027979880571365,
          -0.03130760043859482,
          -0.024772699922323227
        ]
      },
      {
        "chunk_index": 461,
        "relative_chunk_number": 2,
        "text": "Taqi1 1Department of Physics and Astronomy, Louisiana State University, Baton Rouge, LA, 70808 Abstract Background: Low-dose computed tomography (LDCT) is the current standard for lung cancer screening, yet its adoption and accessibility remain limited.",
        "text_length": 253,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02098487876355648,
          -0.005537893623113632,
          -0.020599041134119034,
          -0.010029543191194534,
          -0.01626342162489891,
          -0.024154601618647575,
          0.021792367100715637,
          0.012953747063875198,
          -0.022566236555576324,
          -0.026374105364084244
        ]
      },
      {
        "chunk_index": 462,
        "relative_chunk_number": 3,
        "text": "Many regions lack LDCT infrastructure, and even among those screened, early-stage cancer detection often yield false positives, as shown in the National Lung Screening Trial (NLST) with a sensitivity of 93.8% and a specificity of 73.4% (or a false-positive rate of 26.6%) Purpose: To investigate whether X-ray dark-field imaging (DFI) radiograph\u2014a technique sensitive to small-angle scatter from alveolar microstructure and less susceptible to organ shadowing \u2014can significantly improve early-stage lung tumor detection when coupled with deep-learning segmentation.",
        "text_length": 565,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0058864508755505085,
          -0.0033278281334787607,
          -0.03477362170815468,
          0.010092983953654766,
          0.0071768988855183125,
          -0.010722723789513111,
          0.019780892878770828,
          0.06233114004135132,
          -0.025512894615530968,
          -0.021298645064234734
        ]
      },
      {
        "chunk_index": 463,
        "relative_chunk_number": 4,
        "text": "Methods: Using paired attenuation (ATTN) and DFI radiograph images of euthanized mouse lungs, we generated realistic synthetic tumors with irregular boundaries and intensity profiles consistent with physical lung contrast. A U-Net segmentation network was trained on small patches using either ATTN, DFI, or combined ATTN + DFI channels.",
        "text_length": 337,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.002890026429668069,
          0.0047308458015322685,
          -0.02899187244474888,
          0.0119600435718894,
          0.02351934090256691,
          -0.0782240778207779,
          0.02486875094473362,
          0.06155851110816002,
          -0.02875080518424511,
          -0.021667392924427986
        ]
      },
      {
        "chunk_index": 464,
        "relative_chunk_number": 5,
        "text": "Results: The DFI-only model achieved a true-positive detection rate of 83.7%, compared with 51% for ATTN-only, while maintaining comparable specificity (90.5% vs. 92.9%). The combined ATTN + DFI input achieved 79.6% sensitivity and 97.6% specificity.",
        "text_length": 250,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.019388476386666298,
          0.02782873995602131,
          -0.07553750276565552,
          0.01571091078221798,
          -0.0017985637532547116,
          -0.0734831690788269,
          0.017672903835773468,
          -0.02509218454360962,
          -0.03432731702923775,
          -0.0023957749363034964
        ]
      },
      {
        "chunk_index": 465,
        "relative_chunk_number": 6,
        "text": "Conclusion: DFI substantially improves early-tumor detectability in comparison to standard attenuation radiography and shows potential as an accessible, low-dose alternative for pre-clinical or limited-resource screening where LDCT is unavailable. 1.",
        "text_length": 250,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.0075685023330152035,
          0.018871203064918518,
          -0.04431305080652237,
          0.017776954919099808,
          -0.017254013568162918,
          -0.04635811224579811,
          0.022518882527947426,
          0.028216900303959846,
          -0.030563613399863243,
          -0.004854835104197264
        ]
      },
      {
        "chunk_index": 466,
        "relative_chunk_number": 7,
        "text": "Introduction Early detection of lung cancer remains one of the most critical determinants of survival, yet the global implementation of low-dose computed tomography (LDCT) screening is far from uniform. For example, a recent study found LDCT utilization rate of only 18.4% among eligible subjects [1]. Another earlier study found screening uptake <6% [2].",
        "text_length": 355,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007924912497401237,
          -0.013068721629679203,
          -0.05100312829017639,
          0.018622735515236855,
          0.0001341441966360435,
          -0.025043174624443054,
          0.02228490263223648,
          -0.01737232692539692,
          -0.02888202853500843,
          -0.07558058202266693
        ]
      },
      {
        "chunk_index": 467,
        "relative_chunk_number": 8,
        "text": "Although LDCT is the clinical standard of care, its adoption remains low due to high infrastructure costs, access to rural populations, so on [1-6]. Moreover, the specificity is lower compared to chest radiography resulting in unnecessary follow-up visits and biopsies.",
        "text_length": 269,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02032274194061756,
          0.012762104161083698,
          -0.027038173750042915,
          0.022418497130274773,
          0.031197642907500267,
          -0.00041864963714033365,
          0.02072359435260296,
          0.017563994973897934,
          -0.030666792765259743,
          -0.012960555963218212
        ]
      },
      {
        "chunk_index": 468,
        "relative_chunk_number": 9,
        "text": "For example, the sensitivity and specificity were 93.8% and 73.4% for low-dose CT and 73.5% and 91.3% for chest radiography, respectively [7]. Conventional chest radiography remains the most widely available imaging modality but is inherently limited in detecting small or low-contrast pulmonary nodules due to the overlapping structures of ribs, heart, and mediastinum.",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0375998318195343,
          0.005031321197748184,
          -0.042316507548093796,
          0.03840779513120651,
          -0.03306540474295616,
          -0.03283077850937843,
          0.02118976041674614,
          -0.008007999509572983,
          -0.03867640346288681,
          0.04105740413069725
        ]
      },
      {
        "chunk_index": 469,
        "relative_chunk_number": 10,
        "text": "X-ray dark-field imaging (DFI) provides a fundamentally different contrast mechanism by detecting small-angle scattering from the alveolar microstructure. This enables visualization of subtle tissue-density variations and pathological changes that are invisible in attenuation-based images.",
        "text_length": 290,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.005593698937445879,
          0.0001092455568141304,
          -0.04769796133041382,
          0.018930060788989067,
          -0.007827593944966793,
          -0.10511340200901031,
          0.02330124005675316,
          0.030684154480695724,
          -0.027864055708050728,
          0.017476297914981842
        ]
      },
      {
        "chunk_index": 470,
        "relative_chunk_number": 11,
        "text": "Here, we present a pre-clinical deep-learning framework combining attenuation and dark-field imaging to improve detection of early-stage lung tumors. Using experimentally acquired mouse dark-field and attenuation radiographs, we generated realistic synthetic tumors spanning 0.75\u2013 1.5 mm and trained a patch-based U-Net model to segment tumor regions.",
        "text_length": 351,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0022728608455508947,
          0.01594369485974312,
          -0.009535280987620354,
          0.003486949484795332,
          0.030447378754615784,
          -0.092190220952034,
          0.026610180735588074,
          0.0017254361882805824,
          -0.023436052724719048,
          -0.0007296430412679911
        ]
      },
      {
        "chunk_index": 471,
        "relative_chunk_number": 12,
        "text": "We compare single- channel (ATTN-only and DFI-only) and dual-channel (ATTN + DFI) networks to quantify the diagnostic value of DFI. The results demonstrate that DFI dramatically enhances sensitivity without increasing false positives, supporting its potential for affordable and early lung-cancer screening in settings where LDCT is impractical. 2.",
        "text_length": 348,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02285047620534897,
          0.02647814154624939,
          -0.05278531089425087,
          0.005764728412032127,
          0.016616974025964737,
          -0.060433901846408844,
          0.021399134770035744,
          0.05821811407804489,
          -0.026217661798000336,
          -0.006875945255160332
        ]
      },
      {
        "chunk_index": 472,
        "relative_chunk_number": 13,
        "text": "Method The methods and algorithms are described below. Imaging The imaging experiments were performed by Talbot-Lau X-ray interferometry (TLXI) system at the Pennington Biomedical Research Center, Louisiana State University. The set up and details are given in our prior work [8]. 7 euthanized C57BL/6J (WT) mouse were imaged using the TLXI.",
        "text_length": 341,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04307951033115387,
          0.0014052788028493524,
          -0.02344188466668129,
          -0.017114048823714256,
          0.0024943831376731396,
          -0.02635662816464901,
          0.022076550871133804,
          0.02464916929602623,
          -0.03203600272536278,
          -0.011232534423470497
        ]
      },
      {
        "chunk_index": 473,
        "relative_chunk_number": 14,
        "text": "All animal-related procedures were approved by the Pennington Biomedical Research Center Institutional Animal Care and Use Committee (IACUC) and were carried out in strict adherence to the guidelines and regulations set by the NIH Office of Laboratory Animal Welfare. The mouse was euthanized via CO2 inhalation, transported to the imaging lab, mounted, and imaged by TLXI.",
        "text_length": 373,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02569647878408432,
          0.006668400950729847,
          0.017761562019586563,
          0.01239295955747366,
          0.024723542854189873,
          -0.01688661240041256,
          0.02172055095434189,
          0.05292123183608055,
          -0.03274068608880043,
          -0.028835387900471687
        ]
      },
      {
        "chunk_index": 474,
        "relative_chunk_number": 15,
        "text": "Lung Segmentation in Dark-field (DFI) We segment the lungs on the dark-field (DFI) image and use the attenuation image only as a side-by-side reference during drawing, as shown in Figure 1.",
        "text_length": 189,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0035764682106673717,
          -0.006939183920621872,
          -0.02383953146636486,
          0.03421102464199066,
          -0.013581558130681515,
          -0.05926704779267311,
          0.024669844657182693,
          0.06034397706389427,
          -0.03797699138522148,
          0.026357071474194527
        ]
      },
      {
        "chunk_index": 475,
        "relative_chunk_number": 16,
        "text": "Since the lung is the major source of small-angle scatter, the entire lung parenchyma is easier to visualize in the DFI images rather than the attenuation (ATTN) image, in which regions of the lungs occluded by ribs, spine, and the heart. As seen in Fig.",
        "text_length": 254,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.022055478766560555,
          0.03435986116528511,
          -0.025538451969623566,
          0.016013190150260925,
          -0.01672770082950592,
          -0.06960076838731766,
          0.02058511972427368,
          0.04419803246855736,
          -0.02991536632180214,
          -0.020242372527718544
        ]
      },
      {
        "chunk_index": 476,
        "relative_chunk_number": 17,
        "text": "1(b), although the lower lungs appear completely covered by the other organs in ATTN, they remain visible in DFI Fig 1(a) as low-intensity but detectable regions.",
        "text_length": 162,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0058041526935994625,
          0.028237834572792053,
          -0.010707193054258823,
          0.03757745027542114,
          0.0031336802057921886,
          -0.046849239617586136,
          0.018125494942069054,
          0.043083809316158295,
          -0.03413190320134163,
          0.008641419000923634
        ]
      },
      {
        "chunk_index": 477,
        "relative_chunk_number": 18,
        "text": "Therefore, the DFI image is used as the primary imaging domain for contouring, yielding more anatomically complete lung masks, while the attenuation image is displayed side-by-side to assist contouring in regions where lung boundaries are clearly visible.",
        "text_length": 255,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01193456444889307,
          -0.0013663286808878183,
          -0.023037292063236237,
          0.03436844050884247,
          -0.07086504995822906,
          -0.0942436084151268,
          0.023211218416690826,
          0.04915573447942734,
          -0.024017546325922012,
          0.03729070723056793
        ]
      },
      {
        "chunk_index": 478,
        "relative_chunk_number": 19,
        "text": "Before annotation, the moir\u00e9 artifacts are removed based on our prior work [8], both ATTN and DFI images are clipped to the [0.1, 99.9] percentile range to make outlines easier to see. Example contours are shown in Figure 1(b), which are filled to create robust L/R lung ROIs that are identical across both the DFI and ATTN channels.",
        "text_length": 333,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01580861769616604,
          0.0050837029702961445,
          -0.04281201958656311,
          0.02533056028187275,
          -0.0489831306040287,
          -0.06437525153160095,
          0.020730091258883476,
          0.05484887212514877,
          -0.03504684567451477,
          0.012419713661074638
        ]
      },
      {
        "chunk_index": 479,
        "relative_chunk_number": 20,
        "text": "The tumor-insertion step described next uses these masks to place multiple non-overlapping synthetic lesions with approximately spherical projection profiles and irregular boundaries and noise, in both lungs.",
        "text_length": 208,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.015083640813827515,
          0.022372832521796227,
          0.017770137637853622,
          0.04154101759195328,
          -0.00998174212872982,
          -0.0213630348443985,
          0.021512815728783607,
          -0.02812626026570797,
          -0.03384491801261902,
          -0.032569073140621185
        ]
      },
      {
        "chunk_index": 480,
        "relative_chunk_number": 21,
        "text": "Realistic tumor insertion Multiple non-overlapping synthetic tumors were inserted into both the left and right lung regions of mouse images (segmented by the above step in the dark-field images), based on their respective lung masks. Tumors were placed in identical locations in attenuation and dark-field images to maintain channel correspondence.",
        "text_length": 348,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.004938231315463781,
          0.015474437735974789,
          0.0002698033058550209,
          0.003009124891832471,
          0.05470222607254982,
          -0.07002730667591095,
          0.02099844440817833,
          0.05112053081393242,
          -0.0321383997797966,
          -0.013350516557693481
        ]
      },
      {
        "chunk_index": 481,
        "relative_chunk_number": 22,
        "text": "The central intensity of each tumor is adjusted for attenuation and darkfield, following expected behavior of attenuation and dark-field contrast. Lung tumors have higher attenuation than (a) (b) (c) (d) Figure 1. (a) Darkfield (DFI) image (b) Attenuation (ATTN) image.",
        "text_length": 269,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006624165922403336,
          0.004714365117251873,
          -0.020125113427639008,
          0.027662688866257668,
          0.025288814678788185,
          -0.0814671441912651,
          0.020252827554941177,
          0.02498648688197136,
          -0.028174227103590965,
          0.008666525594890118
        ]
      },
      {
        "chunk_index": 482,
        "relative_chunk_number": 23,
        "text": "In the attenuation image, several regions of the lungs are partially obscured by the cardiac volume and other organs, whereas in the DFI image these areas remain visible as regions of reduce, but still detectable small-angle scattering intensity. In particular the ribs or cardiac regions emit little or no scatter compared to the porous lungs.",
        "text_length": 344,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.018219484016299248,
          0.01952395960688591,
          -0.03399078920483589,
          0.04297256097197533,
          -0.01062428392469883,
          -0.09422081708908081,
          0.018947847187519073,
          0.059416648000478745,
          -0.022915663197636604,
          0.02598419412970543
        ]
      },
      {
        "chunk_index": 483,
        "relative_chunk_number": 24,
        "text": "(c) and (d) show the contouring performed on the DFI image, with the ATTN image on the side to assist in regions where the lungs are clearly visible in it. surrounding lung tissue and appear brighter in the standard X-ray attenuation image.",
        "text_length": 240,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02465059421956539,
          0.00018779514357447624,
          -0.021608129143714905,
          0.04614368826150894,
          -0.05213650315999985,
          -0.1116945818066597,
          0.021605653688311577,
          0.06914272904396057,
          -0.025562426075339317,
          0.015340199694037437
        ]
      },
      {
        "chunk_index": 484,
        "relative_chunk_number": 25,
        "text": "This was implemented by an enhancement factor (1.05) of the mean upper-lung attenuation, explicitly avoiding lower lung area, to avoid bias of overlapping heart.",
        "text_length": 161,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01733635924756527,
          0.042969007045030594,
          0.005150456912815571,
          0.009967692196369171,
          -0.037781715393066406,
          -0.037296127527952194,
          0.01919609308242798,
          0.004013456404209137,
          -0.04163476452231407,
          0.020250963047146797
        ]
      },
      {
        "chunk_index": 485,
        "relative_chunk_number": 26,
        "text": "DFI tumors on the other hand are darker than the lung dark-field scattering, so the intensity of the tumor is reduced by a random fraction chosen from (0.5 to 0.9) of the local DFI mean, consistent with reduced small-angle scattering inside denser tumor regions.",
        "text_length": 262,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.016329729929566383,
          0.0011116372188553214,
          -0.035674262791872025,
          0.019474878907203674,
          -0.004755508154630661,
          -0.07364795356988907,
          0.017256198450922966,
          0.06705419719219208,
          -0.03383766859769821,
          -0.02432294748723507
        ]
      },
      {
        "chunk_index": 486,
        "relative_chunk_number": 27,
        "text": "Then, each tumor was applied gray scale shading in intensity by as a modified projection of a sphere \u2014whose thickness profile follows \ud835\udc61(\ud835\udc5f) = \u221a1 \u2212 \ud835\udc5f2 \ud835\udc452 where R = nominal radius of the tumor, and a parametric gamma function to further modify the tumor morphology to control the steepness or flatness of the dome. A profile-gamma of 1.5 makes the center steeper for attenuation.",
        "text_length": 376,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.0113209318369627,
          -0.009473834186792374,
          -0.00630231061950326,
          0.017587050795555115,
          0.019463350996375084,
          -0.022779162973165512,
          0.02025742456316948,
          -0.04847289249300957,
          -0.021926751360297203,
          -0.004454003646969795
        ]
      },
      {
        "chunk_index": 487,
        "relative_chunk_number": 28,
        "text": "A complementary exponent was used for DFI, so that higher the profile-gamma, the center reduction is more pronounced. Profile-gamma parameters <1 would make the tumors flatter in ATTN or DFI. Each tumor boundary is randomized with a \u00b125% radial jitter generated using smoothed Gaussian noise over 64 angular samples (can be varied).",
        "text_length": 332,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007857239805161953,
          -0.0006384241278283298,
          -0.02040625549852848,
          0.013121855445206165,
          0.09119483083486557,
          -0.06754671037197113,
          0.01983542926609516,
          -0.004951557610183954,
          -0.03990674763917923,
          0.001685412134975195
        ]
      },
      {
        "chunk_index": 488,
        "relative_chunk_number": 29,
        "text": "This avoids perfect circular edges and creates irregular, realistic tumor perimeters consistent with biological variability. To prevent sharp artificial borders, the transition from tumor to lung was weighted by a smooth step function. Provisions are also there for local signal-to-noise-matched Gaussian noise with tunable spatial correlation.",
        "text_length": 344,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03245598450303078,
          0.02117013745009899,
          0.005202119238674641,
          0.022266078740358353,
          -0.01831902377307415,
          -0.0364152230322361,
          0.024051589891314507,
          -0.0003412449441384524,
          -0.030800865963101387,
          -0.048730023205280304
        ]
      },
      {
        "chunk_index": 489,
        "relative_chunk_number": 30,
        "text": "A tumor density parameter controls how many synthetic lesions are inserted per lung, scaled to its total pixel area. This ensures that smaller lungs contain proportionally fewer tumors and larger ones slightly more, maintaining visual realism. Figures 2-4 provides representative examples of tumor placement.",
        "text_length": 308,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.04482491314411163,
          0.008040199987590313,
          -0.025036588311195374,
          0.02405376173555851,
          -0.00045797580969519913,
          -0.06519632041454315,
          0.019408579915761948,
          0.023540431633591652,
          -0.03741009533405304,
          -0.0015462543815374374
        ]
      },
      {
        "chunk_index": 490,
        "relative_chunk_number": 31,
        "text": "The original darkfield is shown for reference and to show some inherent dark spots in the dfi image for some mice (Fig. 2(c). All the cases maybe visualized by running Tumor_insertion.m .",
        "text_length": 187,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008163523860275745,
          0.004974404349923134,
          -0.019832028076052666,
          0.033608343452215195,
          0.009674875065684319,
          -0.03152075782418251,
          0.020901357755064964,
          0.050057779997587204,
          -0.03713414445519447,
          0.02176104299724102
        ]
      },
      {
        "chunk_index": 491,
        "relative_chunk_number": 32,
        "text": "The tumor density value can be tuned so that when making patches (description follows), the number of positive (tumor-containing) and negative (tumor-free) patches are roughly balanced, allowing the subsequent deep-learning classifier to train on comparable datasets without class imbalance.",
        "text_length": 291,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007148916367441416,
          0.042166806757450104,
          -0.02773614600300789,
          0.0331076942384243,
          0.022476883605122566,
          -0.039541687816381454,
          0.0215059332549572,
          -0.02083520218729973,
          -0.03302416950464249,
          -0.011090760119259357
        ]
      },
      {
        "chunk_index": 492,
        "relative_chunk_number": 33,
        "text": "Making patches Patches (32 \u00d7 32 pixels) were extracted from the augmented attenuation and dark-field images within the lung regions to generate paired training inputs for the neural network training and testing. Each patch contained two channels (ATTN and DFI). A neutral network could be fed a single channel or both. Each patch could contain no tumor or one or more tumors.",
        "text_length": 375,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0454619936645031,
          0.02799297496676445,
          -0.025886133313179016,
          0.004705830942839384,
          0.0033478527329862118,
          -0.050304483622312546,
          0.01904255524277687,
          0.035499826073646545,
          -0.030358057469129562,
          0.033378131687641144
        ]
      },
      {
        "chunk_index": 493,
        "relative_chunk_number": 34,
        "text": "The tumors could be fully enclosed or partially intersecting the patch boundary. A patch is labeled as positive if it included any tumor pixels, even partially, or negative if it contained none. Only patches with at least 50% lung coverage were retained to exclude background and mediastinal regions.",
        "text_length": 300,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.036686427891254425,
          0.020848877727985382,
          -0.011276675388216972,
          0.04242676496505737,
          0.02182070165872574,
          -0.02608792670071125,
          0.019481902942061424,
          0.027115462347865105,
          -0.030072854831814766,
          0.041412632912397385
        ]
      },
      {
        "chunk_index": 494,
        "relative_chunk_number": 35,
        "text": "Intensity normalization has per-patch or per-mouse options using robust percentile scaling within the lungs to maintain consistent contrast. In our case per-patch normalization was performed. Figure 2. (a) Contoured attenuation (ATTN) image with inserted tumors.",
        "text_length": 262,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0354355089366436,
          0.009244904853403568,
          -0.029423309490084648,
          0.022299788892269135,
          -0.004010696429759264,
          -0.05149925500154495,
          0.0247805193066597,
          -0.038061752915382385,
          -0.023451389744877815,
          0.053264833986759186
        ]
      },
      {
        "chunk_index": 495,
        "relative_chunk_number": 36,
        "text": "The tumors exhibit slightly higher attenuation than the surrounding lung tissue, making them visible in the upper lung regions but nearly indistinguishable in the lower sections, where overlapping organs project higher attenuation. (b) Corresponding contoured dark-field (DFI) image with tumors, where the lesions appear as localized reductions in small-angle scattering.",
        "text_length": 371,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.021229812875390053,
          0.013098614290356636,
          -0.004867235664278269,
          0.032808784395456314,
          -0.00867386907339096,
          -0.09307657182216644,
          0.017830805853009224,
          0.046370647847652435,
          -0.032908979803323746,
          0.03317473828792572
        ]
      },
      {
        "chunk_index": 496,
        "relative_chunk_number": 37,
        "text": "(c) Original dark-field image shown for reference. Figure 3. (a) Contoured attenuation (ATTN) image with inserted tumors. The tumors exhibit slightly higher attenuation than the surrounding lung tissue, making them visible in the upper lung regions but nearly indistinguishable in the lower sections, where overlapping organs project higher attenuation.",
        "text_length": 353,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02165289781987667,
          0.009460345841944218,
          -0.012611648067831993,
          0.03787688910961151,
          -0.004122375510632992,
          -0.09923770278692245,
          0.021377241238951683,
          -0.002748754108324647,
          -0.03681733086705208,
          0.05318809300661087
        ]
      },
      {
        "chunk_index": 497,
        "relative_chunk_number": 38,
        "text": "(b) Corresponding contoured dark-field (DFI) image with tumors, where the lesions appear as localized reductions in small-angle scattering. (c) Original dark-field image shown for reference.",
        "text_length": 190,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.056395214051008224,
          -0.0028335326351225376,
          -0.00960699375718832,
          0.043787647038698196,
          0.035410232841968536,
          -0.11591757088899612,
          0.019431116059422493,
          0.03600826486945152,
          -0.03532358258962631,
          0.023537052795290947
        ]
      },
      {
        "chunk_index": 498,
        "relative_chunk_number": 39,
        "text": "UNET architecture Training/Testing 2-channel UNET (ATTN and DFI): A two-dimensional U-Net architecture was used for patch- based tumor segmentation using dual-channel input comprising attenuation (ATTN) and dark- field (DFI) images.",
        "text_length": 232,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.06403568387031555,
          0.02032633312046528,
          -0.024179499596357346,
          0.012183132581412792,
          0.03710351511836052,
          -0.0684201568365097,
          0.02517763338983059,
          -0.009484443813562393,
          -0.025685176253318787,
          0.038383472710847855
        ]
      },
      {
        "chunk_index": 499,
        "relative_chunk_number": 40,
        "text": "Each 32\u00d732 patch for ATTN and corresponding patch for DFI was fed as a 3D tensor of dimension [H x W x C], where H=W=32 and C=2, to jointly exploit the contrast from both imaging modalities. The encoder consisted of two levels, each containing two 3\u00d73 convolutional layers with ReLU activations followed by 2\u00d72 max pooling.",
        "text_length": 323,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04599772021174431,
          0.028672805055975914,
          -0.034662362188100815,
          0.019464420154690742,
          -0.01830967515707016,
          -0.06958865374326706,
          0.020489895716309547,
          0.00066621188307181,
          -0.02925579436123371,
          -0.02306229993700981
        ]
      },
      {
        "chunk_index": 500,
        "relative_chunk_number": 41,
        "text": "The number of feature channels doubled at each down-sampling stage (16\u201332\u201364). The decoder mirrored the encoder using 2\u00d72 transposed convolutions for up-sampling and skip connections from corresponding encoder layers to preserve spatial detail. A 1\u00d71 convolution generated two output logits (background and tumor), which were converted to foreground probabilities via a sigmoid or softmax activation.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.027856124565005302,
          0.03200117126107216,
          0.0023540668189525604,
          0.02530248835682869,
          -0.07531565427780151,
          -0.04065024107694626,
          0.024824827909469604,
          -0.02681325189769268,
          -0.03491989150643349,
          0.00968480296432972
        ]
      },
      {
        "chunk_index": 501,
        "relative_chunk_number": 42,
        "text": "Training loss was a hybrid binary cross-entropy + Dice loss, Adam optimizer (learning rate = 5\u00d710\u207b\u2074, \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999), global-norm gradient clipping (1.0), and early stopping based on validation loss. Random flips and 90\u00b0 rotations were used for data augmentation.",
        "text_length": 269,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0011364914244040847,
          0.02722666785120964,
          -0.028150910511612892,
          0.004808962810784578,
          -0.047041624784469604,
          -0.0454392209649086,
          0.019998623058199883,
          0.01115697156637907,
          -0.02305624820291996,
          0.036179300397634506
        ]
      },
      {
        "chunk_index": 502,
        "relative_chunk_number": 43,
        "text": "1-channel UNET (DFI or Attn): Two single-input variant of the same U-Net architecture were trained using only the dark-field (DFI) image or the attenuation (ATTN) channel as input. The Figure 4. (a) Contoured attenuation (ATTN) image with inserted tumors.",
        "text_length": 255,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03367391228675842,
          0.023203492164611816,
          -0.031408362090587616,
          0.014356808736920357,
          0.03046535514295101,
          -0.08659141510725021,
          0.024216577410697937,
          -0.0054094986990094185,
          -0.021817881613969803,
          0.02063468098640442
        ]
      },
      {
        "chunk_index": 503,
        "relative_chunk_number": 44,
        "text": "The tumors exhibit slightly higher attenuation than the surrounding lung tissue, making them visible in the upper lung regions but nearly indistinguishable in the lower sections, where overlapping organs project higher attenuation. (b) Corresponding contoured dark-field (DFI) image with tumors, where the lesions appear as localized reductions in small-angle scattering.",
        "text_length": 371,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.021229812875390053,
          0.013098614290356636,
          -0.004867235664278269,
          0.032808784395456314,
          -0.00867386907339096,
          -0.09307657182216644,
          0.017830805853009224,
          0.046370647847652435,
          -0.032908979803323746,
          0.03317473828792572
        ]
      },
      {
        "chunk_index": 504,
        "relative_chunk_number": 45,
        "text": "(c) Original dark-field image shown for reference. network preserved the same two-level encoder\u2013decoder structure (16\u201332\u201364 feature channels) and output configuration but operated on single-channel 32\u00d732 patches. This design tested the independent discriminative power of DFI contrast or ATTN contrast for tumor detection, isolating its contribution relative to combined ATTN + DFI inputs.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03747272491455078,
          0.02972159907221794,
          -0.0359358936548233,
          0.016202030703425407,
          -0.014064035378396511,
          -0.07704897224903107,
          0.02344011701643467,
          0.0007231039344333112,
          -0.032570358365774155,
          0.011307486332952976
        ]
      },
      {
        "chunk_index": 505,
        "relative_chunk_number": 46,
        "text": "Training and optimization parameters, data augmentations, and loss functions were identical to the two-channel model for consistent comparison. The testing and validation spits were identical as well for better comparison. 3. Results There were 446 total patches with tumors, some with multiple tumors) and 453 with no tumors.",
        "text_length": 326,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008213533088564873,
          0.03854585811495781,
          -0.030087359249591827,
          0.013806720264256,
          0.023858407512307167,
          -0.03452947735786438,
          0.018614666536450386,
          0.008039767853915691,
          -0.028883257880806923,
          -0.017359735444188118
        ]
      },
      {
        "chunk_index": 506,
        "relative_chunk_number": 47,
        "text": "These were split at random 80%-10%-10% into 719 training, 89 validation and 91 testing sets. These same sets were used for training, validation and testing of the three architectures. Figure 5. For the single-channel ATTN-only UNET model, the training and validation losses are shown across epochs.",
        "text_length": 298,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.035129208117723465,
          0.05519520118832588,
          -0.017578044906258583,
          0.022238358855247498,
          0.023399176076054573,
          -0.016427865251898766,
          0.01945672370493412,
          -0.009828319773077965,
          -0.023615499958395958,
          0.024375343695282936
        ]
      },
      {
        "chunk_index": 507,
        "relative_chunk_number": 48,
        "text": "The maximum epoch count was set to 100, but the training stopped early due to oscillations in the validation loss, possibly indicating the onset of overfitting. Figure 6. For the single-channel DFI-only UNET model, the training and validation losses are shown across epochs.",
        "text_length": 274,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.002668278757482767,
          0.05686371028423309,
          -0.0324828140437603,
          0.01332898624241352,
          0.03828271105885506,
          -0.0517127588391304,
          0.02111327089369297,
          -0.0025469684042036533,
          -0.029904088005423546,
          -0.005310601554811001
        ]
      },
      {
        "chunk_index": 508,
        "relative_chunk_number": 49,
        "text": "The maximum epoch count was set to 100, but training stopped early due to oscillations in the validation loss, possibly indicating the onset of overfitting. The training results of the single-channel UNET with ATTN-only and DFI-only are shown in Fig. 5-6. Fig. 7 shows that of 2-channel architecture of ATTN+DFI.",
        "text_length": 312,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02512579970061779,
          0.047611575573682785,
          -0.03133203089237213,
          0.010549917817115784,
          0.01699448749423027,
          -0.060495320707559586,
          0.01853880286216736,
          -0.0010773218236863613,
          -0.02612447366118431,
          0.004033793229609728
        ]
      },
      {
        "chunk_index": 509,
        "relative_chunk_number": 50,
        "text": "The Table 1 shows that the true-positive sensitivity improved with dark-field only patches to 83.75 from 51% with just attenuation patches. The Specificity was slightly better with attenuation 92.9% (ATTN-only) versus 90.5% with DFI-ONLY. The ATTN+DFI has intermediate results of Sensitivity of 79.6% and improved Specificity to 97.6%. 4.",
        "text_length": 338,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00548924133181572,
          -0.005243806634098291,
          -0.07768087834119797,
          0.0012512137182056904,
          -0.010082501918077469,
          -0.041996121406555176,
          0.01832699216902256,
          -0.016282372176647186,
          -0.022137083113193512,
          -0.003714616410434246
        ]
      },
      {
        "chunk_index": 510,
        "relative_chunk_number": 51,
        "text": "Discussions and Conclusion The DFI-only showed tremendous improvement of detection sensitivity compared to attenuation radiographs. The false-positive (1-sensitivity) was comparable if slightly lower for darkfield. The tumor range was 0.75-1.5mm in this project. Figure 7. For the two-channel ATTN+DFI UNET model, the training and validation losses are shown across epochs.",
        "text_length": 373,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.012641849927604198,
          0.030142799019813538,
          -0.03692499175667763,
          0.007020436692982912,
          0.032808661460876465,
          -0.06931325048208237,
          0.022546978667378426,
          0.013412193395197392,
          -0.024554289877414703,
          -0.00992649421095848
        ]
      },
      {
        "chunk_index": 511,
        "relative_chunk_number": 52,
        "text": "The maximum epoch count was set to 100, but the training stopped early due to oscillations in the validation loss, possibly indicating the onset of overfitting. Table 1. Sensitivity (True Positive %) and Specificity (100-False Positive%), both patch-wise, (tumor present/absent in patch). Precision and Recall (pixel-wise classification of each patch).",
        "text_length": 352,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.0017773190047591925,
          0.024709420278668404,
          -0.04759403318166733,
          -0.0017440107185393572,
          0.035731635987758636,
          0.030639132484793663,
          0.018896905705332756,
          -0.027696756646037102,
          -0.021705495193600655,
          -0.02073625661432743
        ]
      },
      {
        "chunk_index": 512,
        "relative_chunk_number": 53,
        "text": "ATTN- only DFI- only ATTN+DFI Sensitivity (%TP) (patch-wise) 51% 83.7% 79.6% Specificity (100-%FP) (patch-wise) 92.9% 90.5% 97.6% Precision (pixel-wise) 85.9% 87.8% 94.2% Recall (pixel-wise) 44.7% 85.5% 78.2% Attn Pred GT DFI Pred GT ATTN+DFI Pred GT Attn-only DFI-only ATTN+DFI Attn Pred GT DFI Pred GT ATTN DFI Pred GT Attn-only DFI-only ATTN+DFI Figure 8.",
        "text_length": 358,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04934386536478996,
          -0.010687734000384808,
          -0.02512170374393463,
          0.02077329158782959,
          0.018348751589655876,
          -0.03214511275291443,
          0.01547817699611187,
          0.015811869874596596,
          -0.016962522640824318,
          0.028041880577802658
        ]
      },
      {
        "chunk_index": 513,
        "relative_chunk_number": 54,
        "text": "The test panels show the patch(es), predicted mask and the ground truth (GT) masks for Attn-only, DFI-only and ATTN+DFI UNET results. The top and bottom panels show patches where the ATTN-only misses a tumor but DFI-only or ATTN+DFI finds the tumor.",
        "text_length": 249,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.009663445875048637,
          0.010162902995944023,
          -0.025948401540517807,
          0.007705846801400185,
          -0.020214630290865898,
          -0.04641592130064964,
          0.02081400901079178,
          -0.006175796966999769,
          -0.027241546660661697,
          -0.01121436432003975
        ]
      },
      {
        "chunk_index": 514,
        "relative_chunk_number": 55,
        "text": "If these results translate to the clinic, attenuation and dark-field radiography along with a AI interpreter may well full-fill a gap in clinics where Low-Dose-CT is not accessible or for patients are not covered. The dark-field performance can be potentially further improved with denoising. More data will allow larger range of tumor sizes and intensities both in the dark-field and attenuation. 5.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.041312023997306824,
          0.0038057598285377026,
          -0.022241177037358284,
          0.029181724414229393,
          -0.08555968105792999,
          -0.020176827907562256,
          0.022052403539419174,
          0.010197003372013569,
          -0.030140355229377747,
          0.012000925838947296
        ]
      },
      {
        "chunk_index": 515,
        "relative_chunk_number": 56,
        "text": "Acknowledgments We wish to thank Dr. Leslie Butler and Dr. Kyungmin Ham for their help in upgrading and automating the Keck System at Pennington Biomedical Research Center. We wish to thank Dr. Christopher Morrison for providing the deceased mice that was imaged at the Keck System and used for this project.",
        "text_length": 308,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.021714122965931892,
          -0.007174987345933914,
          -0.012546524405479431,
          0.029015500098466873,
          -0.017232483252882957,
          -0.03163963183760643,
          0.021857034415006638,
          -0.020382443442940712,
          -0.03809010609984398,
          -0.00798889808356762
        ]
      },
      {
        "chunk_index": 516,
        "relative_chunk_number": 57,
        "text": "Author Contributions: JD is responsible for designing the tumor inserts and AI architecture. HCM and MST and JD acquired the mice data. Data Availability: The data, code and instructions for this project can be obtained at https://github.com/deyj/dfi-atn_unet-lung-tumor_det/ or asking the first author. 6. Reference [1] Tseng TS, Li CC, Lin HY, Witmeier KN, Zeng C, Chiu YW, Celestin MD, Trapido EJ.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.06315150856971741,
          0.018589073792099953,
          -0.01384278479963541,
          0.0010142602259293199,
          0.025850612670183182,
          -0.055264074355363846,
          0.017123468220233917,
          0.024964135140180588,
          -0.03269891440868378,
          -0.02209063246846199
        ]
      },
      {
        "chunk_index": 517,
        "relative_chunk_number": 58,
        "text": "Multi-level factors associated with low dose computed tomography lung cancer screening in the United States. J Natl Med Assoc. 2025 Aug;117(4):225-234. doi: 10.1016/j.jnma.2025.05.002. Epub 2025 May 29. PMID: 40447527; PMCID: PMC12286424. [2] Yong PC, Sigel K, Rehmani S, Wisnivesky J, Kale MS. Lung Cancer Screening Uptake in the United States. Chest. 2020 Jan;157(1):236-238.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008767692372202873,
          -0.004920857958495617,
          0.005835915915668011,
          0.03270838409662247,
          0.0242602601647377,
          -0.003609431441873312,
          0.01361302100121975,
          0.04093321040272713,
          -0.018557703122496605,
          0.02458411268889904
        ]
      },
      {
        "chunk_index": 518,
        "relative_chunk_number": 59,
        "text": "doi: 10.1016/j.chest.2019.08.2176. Erratum in: Chest. 2020 Oct;158(4):1797. doi: 10.1016/j.chest.2020.08.2063. PMID: 31916962; PMCID: PMC7609956. [3] Michelle Palokas, Elizabeth Hinton, Roy Duhe, Robin Christian, Deirdre Rogers, Manvi Sharma, Michael Stefanek.",
        "text_length": 260,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.05112571269273758,
          -0.015804525464773178,
          0.0028871188405901194,
          0.014295929111540318,
          0.03958786278963089,
          0.024344906210899353,
          0.014715464785695076,
          0.030091822147369385,
          -0.015002656728029251,
          0.013903360813856125
        ]
      },
      {
        "chunk_index": 519,
        "relative_chunk_number": 60,
        "text": "\u201cBarriers and Facilitators for Low-Dose Computed Tomography Lung Cancer Screening in Rural Populations in the United States: A Scoping Review.\u201d JBI Evidence Synthesis. 2022;20(11):2727-2733. DOI:10.11124/JBIES-21-00337 [4] Lewis JA, Petty WJ, Tooze JA, Miller DP, Chiles C, Miller AA, Bellinger C, Weaver KE.",
        "text_length": 308,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003003871301189065,
          -0.016225744038820267,
          -0.007613039575517178,
          0.02434094250202179,
          -0.0230378620326519,
          0.017407558858394623,
          0.018168963491916656,
          0.03556964918971062,
          -0.0307737085968256,
          -0.03963765129446983
        ]
      },
      {
        "chunk_index": 520,
        "relative_chunk_number": 61,
        "text": "Low-Dose CT Lung Cancer Screening Practices and Attitudes among Primary Care Providers at an Academic Medical Center. Cancer Epidemiol Biomarkers Prev. 2015 Apr;24(4):664-70. doi: 10.1158/1055- 9965.EPI-14-1241. Epub 2015 Jan 22. PMID: 25613118; PMCID: PMC4383689. [5] Field, JK., de Koning, H., Oudkerk, M., Anwar, S., Mulshine, J., Pastorino, U., Eberhardt, W., & Prosch, H.",
        "text_length": 376,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.037333063781261444,
          -0.0014940410619601607,
          0.008372337557375431,
          0.04043129086494446,
          0.028392471373081207,
          0.008091774769127369,
          0.014400627464056015,
          0.01888476498425007,
          -0.024376189336180687,
          0.007407498080283403
        ]
      },
      {
        "chunk_index": 521,
        "relative_chunk_number": 62,
        "text": "(2019).Implementation of lung cancer screening in Europe: challenges and potential solutions: summary of a multidisciplinary roundtable discussion. ESMO Open, 4(5), Article UNSP e000577. https://doi.org/10.1136/esmoopen-2019-000577 [6] Haddad DN, Sandler KL, Henderson LM, Rivera MP, Aldrich MC. Disparities in Lung Cancer Screening: A Review. Ann Am Thorac Soc. 2020 Apr;17(4):399-405.",
        "text_length": 386,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010697930119931698,
          0.012627053074538708,
          -0.015987666323781013,
          0.05367013439536095,
          -0.010755450464785099,
          -0.026805860921740532,
          0.014804080128669739,
          0.08083001524209976,
          -0.02913009002804756,
          -0.017216607928276062
        ]
      },
      {
        "chunk_index": 522,
        "relative_chunk_number": 63,
        "text": "doi: 10.1513/AnnalsATS.201907- 556CME. PMID: 32017612; PMCID: PMC7175982. [7] The National Lung Screening Trial Research Team. Results of Initial Low-Dose Computed Tomographic Screening for Lung Cancer. N Engl J Med. 2013;368(21):1980-1991. doi:10.1056/NEJMoa1209120 [8] Meyer, H. C., Dey, J., Dooley, C. B., Taqi, M. S., Gala, V. R., Morrison, C., Fontenot, V. L., Ham, K., Butler, L. G., & Noel, A.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.024687593802809715,
          -0.031878627836704254,
          -0.00807129219174385,
          0.02371387369930744,
          -0.009223517961800098,
          -0.0082593634724617,
          0.01198576670140028,
          0.0033549286890774965,
          -0.020829906687140465,
          0.0006057810969650745
        ]
      },
      {
        "chunk_index": 523,
        "relative_chunk_number": 64,
        "text": "(2025, September 20). Moire artifact reduction in grating interferometry using multiple harmonics and total variation regularization (arXiv preprint arXiv:2509.16503). https://doi.org/10.48550/arXiv.2509.16503",
        "text_length": 209,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.048590995371341705,
          0.007216739002615213,
          -0.011580266989767551,
          0.047915972769260406,
          -0.015939861536026,
          -0.02552448771893978,
          0.021251041442155838,
          -0.02890927903354168,
          -0.02639826573431492,
          -0.027748875319957733
        ]
      }
    ]
  },
  {
    "paper_id": "2510.27675v1",
    "total_chunks": 179,
    "index_range": {
      "start": 524,
      "end": 702
    },
    "chunks": [
      {
        "chunk_index": 524,
        "relative_chunk_number": 1,
        "text": "On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection Md Abdul Hannan\u2217 Colorado State University ma.hannan@colostate.edu Ronghao Ni\u2217 Carnegie Mellon University ronghaon@andrew.cmu.edu Chi Zhang Carnegie Mellon University chiz5@andrew.cmu.edu Limin Jia Carnegie Mellon University liminjia@andrew.cmu.edu Ravi Mangal Colorado State University ravi.mangal@colostate.edu Corina S.",
        "text_length": 396,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009610333479940891,
          0.01644795387983322,
          -0.017575951293110847,
          0.008592698723077774,
          -0.030076418071985245,
          -0.010755506344139576,
          0.016444912180304527,
          -0.005068770609796047,
          -0.018296679481863976,
          0.01317957416176796
        ]
      },
      {
        "chunk_index": 525,
        "relative_chunk_number": 2,
        "text": "Pasareanu Carnegie Mellon University pcorina@andrew.cmu.edu Abstract Large language models (LLMs) have demonstrated impressive capa- bilities for many coding tasks, including summarization, translation, completion, and code generation. However, detecting code vulner- abilities remains a challenging task for LLMs.",
        "text_length": 314,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.028741229325532913,
          0.004317184444516897,
          -0.01874544657766819,
          -0.015784043818712234,
          0.02476244419813156,
          0.00982519518584013,
          0.019229138270020485,
          0.017266100272536278,
          -0.024188194423913956,
          0.017695987597107887
        ]
      },
      {
        "chunk_index": 526,
        "relative_chunk_number": 3,
        "text": "An effective way to improve LLM performance is in-context learning (ICL) \u2013 pro- viding few-shot examples similar to the query, along with correct answers, can improve an LLM\u2019s ability to generate correct solu- tions. However, choosing the few-shot examples appropriately is crucial to improving model performance.",
        "text_length": 313,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.06760673224925995,
          0.0005600305157713592,
          -0.05299060419201851,
          -0.0014675454003736377,
          -0.0039002448320388794,
          0.0194633100181818,
          0.020067255944013596,
          0.03922280669212341,
          -0.02726377174258232,
          0.002832248341292143
        ]
      },
      {
        "chunk_index": 527,
        "relative_chunk_number": 4,
        "text": "In this paper, we explore two criteria for choosing few-shot examples for ICL used in the code vulnerability detection task. The first criterion considers if the LLM (consistently) makes a mistake or not on a sample with the intuition that LLM performance on a sample is informative about its usefulness as a few-shot example.",
        "text_length": 326,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03726808354258537,
          0.023298099637031555,
          -0.03659967705607414,
          0.013375372625887394,
          -0.03843788057565689,
          0.03610705956816673,
          0.0219047199934721,
          0.02750468999147415,
          -0.02278645522892475,
          -0.006570387631654739
        ]
      },
      {
        "chunk_index": 528,
        "relative_chunk_number": 5,
        "text": "The other criterion consid- ers similarity of the examples with the program under query and chooses few-shot examples based on the k-nearest neighbors to the given sample. We perform evaluations to determine the benefits of these criteria individually as well as under various combinations, using open-source models on multiple datasets.",
        "text_length": 337,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01860770769417286,
          0.04558626934885979,
          -0.024047009646892548,
          0.06680826097726822,
          -0.057274311780929565,
          0.030681641772389412,
          0.02246924862265587,
          -0.02682643197476864,
          -0.041326917707920074,
          -0.006030935328453779
        ]
      },
      {
        "chunk_index": 529,
        "relative_chunk_number": 6,
        "text": "Keywords LLMs, Few-shot examples, Code vulnerability detection 1 Introduction Large language models (LLMs) have demonstrated impressive capa- bilities for coding tasks including detection of software defects and vulnerabilities (such as resource leaks, use-after-free of dynamic memory during program operation, and denial-of-service attacks), variable misuse detection (i.e., programmers used wrong variables), code summarization (i.e., represent a code segment with a single word), and code generation and code completion [23, 24, 53, 57, 67].",
        "text_length": 545,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.022840259596705437,
          0.008517426438629627,
          -0.03738987818360329,
          0.0008302307687699795,
          -0.028844429180026054,
          0.009461709298193455,
          0.020602693781256676,
          0.03851112350821495,
          -0.022165169939398766,
          0.016836604103446007
        ]
      },
      {
        "chunk_index": 530,
        "relative_chunk_number": 7,
        "text": "In this paper we explore the applicability of LLMs for vulnerability detection. This is a particularly important application, as software vulnerabilities are prevalent in many software systems, posing seri- ous risks such as compromising sensitive data and system failures.",
        "text_length": 273,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03471840173006058,
          0.019030146300792694,
          -0.05250384658575058,
          -0.0066653285175561905,
          -0.015448210760951042,
          0.006718245800584555,
          0.019855108112096786,
          0.04287508875131607,
          -0.01882161758840084,
          -0.03375565633177757
        ]
      },
      {
        "chunk_index": 531,
        "relative_chunk_number": 8,
        "text": "There are limited studies in previous work that show that state- of-the-art LLMs with few-shot-learning capabilities can achieve competitive results in detecting software vulnerabilities compared to previous machine learning techniques [33, 66]. However, the performance (measured in terms of precision, recall, F1 score) of \u2217Equal contribution. Preprint. Under review.",
        "text_length": 369,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04837740585207939,
          0.01982559636235237,
          -0.04937102273106575,
          -0.018388744443655014,
          -0.016642320901155472,
          0.006692573893815279,
          0.020717093721032143,
          0.03958304598927498,
          -0.02977958507835865,
          -0.008262607268989086
        ]
      },
      {
        "chunk_index": 532,
        "relative_chunk_number": 9,
        "text": "these models remains low [11, 26] preventing the deployment of these models in realistic settings. In this paper, we explore black-box, prompt-based methods for improving the performance of LLMs on vulnerability detection.",
        "text_length": 222,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.048396140336990356,
          0.016844358295202255,
          -0.024667149409651756,
          0.016240188851952553,
          -0.014625415205955505,
          -0.05071917921304703,
          0.020919861271977425,
          0.0013807713985443115,
          -0.03959449753165245,
          -0.0005327193066477776
        ]
      },
      {
        "chunk_index": 533,
        "relative_chunk_number": 10,
        "text": "Our idea is based on improving the efficacy of in-context learning (ICL)\u2014the well-known phenomenon of LLM\u2019s exhibiting improved performance when the provided prompt includes a set of examples (referred to as the few-shot set) that demonstrate the task to be performed\u2014for the purpose of vulnerability detection.",
        "text_length": 311,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.038526054471731186,
          0.009887457825243473,
          -0.026733344420790672,
          0.0022360330913215876,
          0.0005751822609454393,
          0.009738080203533173,
          0.01965097337961197,
          0.022130673751235008,
          -0.028135996311903,
          0.019004065543413162
        ]
      },
      {
        "chunk_index": 534,
        "relative_chunk_number": 11,
        "text": "The impact of ICL on model performance is highly sensitive to the specific examples chosen and therefore selecting the appropriate few-shot examples is crucial to improving model performance [35, 42, 61]. For vulnerability detection, the few-shot set comprises of programs along with the ground-truth label indicating whether they have a vulnerability or not.",
        "text_length": 359,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.04653403162956238,
          0.034055888652801514,
          -0.020187783986330032,
          0.04231656715273857,
          0.008219383656978607,
          0.00029175414238125086,
          0.02317485399544239,
          0.008122914470732212,
          -0.025893816724419594,
          -0.016689440235495567
        ]
      },
      {
        "chunk_index": 535,
        "relative_chunk_number": 12,
        "text": "We propose new methods for choosing the example programs that comprise the few-shot set used in the code vulnerability detec- tion task. Our first contribution is to present a pair of algorithmic primitives for choosing the few-shot examples.",
        "text_length": 242,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.005011831410229206,
          0.04171951487660408,
          -0.011377726681530476,
          0.024705903604626656,
          0.0003199305501766503,
          -0.030743297189474106,
          0.02134748548269272,
          -0.0001634438376640901,
          -0.04620502516627312,
          -0.0060175457037985325
        ]
      },
      {
        "chunk_index": 536,
        "relative_chunk_number": 13,
        "text": "The first primitive, Learn-from-Mistakes (LFM), is based on the idea that LLM perfor- mance on a sample is informative about its usefulness as a few-shot example. Therefore, LFM first queries the model (multiple times) on a potential example and records if the model is consistently mistaken (or correct) on the example.",
        "text_length": 320,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.024005575105547905,
          0.03541659936308861,
          -0.02715180069208145,
          0.003714563325047493,
          -0.046690184623003006,
          0.019000539556145668,
          0.021389814093708992,
          0.04656708240509033,
          -0.02753695845603943,
          0.015485193580389023
        ]
      },
      {
        "chunk_index": 537,
        "relative_chunk_number": 14,
        "text": "An example is added to the few-shot set only if the the model consistently makes mistakes on it. The intuition is that the model does not know well how to rea- son about that example therefore adding it in the few-shot set will likely help rectify the model behavior, while a randomly chosen example may not be as useful.",
        "text_length": 321,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014268976636230946,
          0.04282510653138161,
          -0.03349252790212631,
          0.05621233209967613,
          0.021254466846585274,
          -0.008952147327363491,
          0.017299635335803032,
          0.032495345920324326,
          -0.030633045360445976,
          0.006128185894340277
        ]
      },
      {
        "chunk_index": 538,
        "relative_chunk_number": 15,
        "text": "We also explore alternate versions of LFM where an example is added only if the model is consistently correct on it. The intuition here is that adding examples where the model is consistently correct can help reinforce the desired behavior and improve performance.",
        "text_length": 264,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.034149836748838425,
          0.049843329936265945,
          -0.060301780700683594,
          0.04321978613734245,
          -0.027364635840058327,
          0.035564567893743515,
          0.018817193806171417,
          0.052370019257068634,
          -0.042177196592092514,
          -0.013085168786346912
        ]
      },
      {
        "chunk_index": 539,
        "relative_chunk_number": 16,
        "text": "The second primitive, Learn- from-Nearest-Neighbors (LFNN), is based on the intuition that the semantic similarity between an example and the program under test can be a helpful guide in deciding if the example should be added to the few-shot set. In particular, LFNN adds the nearest neighbors to the set.",
        "text_length": 306,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.005735921673476696,
          0.04772169888019562,
          -0.02823071926832199,
          0.007713907863944769,
          -0.056573111563920975,
          0.009972439147531986,
          0.018477072939276695,
          0.05976705253124237,
          -0.025135429576039314,
          0.06439835578203201
        ]
      },
      {
        "chunk_index": 540,
        "relative_chunk_number": 17,
        "text": "To compute semantic similarity, LFNN relies on code embedding models that map programs to embedding vec- tors. Similarity of vectors can be computed using metrics such as cosine similarity.",
        "text_length": 189,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.025106333196163177,
          0.03291315212845802,
          -0.01951800286769867,
          0.019752828404307365,
          -0.04276997596025467,
          -0.0155429532751441,
          0.026010554283857346,
          0.01845337264239788,
          -0.022620433941483498,
          0.038004275411367416
        ]
      },
      {
        "chunk_index": 541,
        "relative_chunk_number": 18,
        "text": "Our second contribution is to propose three different methods for combining LFM and LFNN to yield few-shot sets that are both semantically similar to the program under test and provide a context 1 arXiv:2510.27675v1 [cs.SE] 31 Oct 2025 Md Abdul Hannan, Ronghao Ni, Chi Zhang, Limin Jia, Ravi Mangal, and Corina S. Pasareanu summarizing the model\u2019s past mistakes.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01953621581196785,
          0.05418878793716431,
          -0.004311153665184975,
          0.03201552852988243,
          -0.011044669896364212,
          0.005313828121870756,
          0.013876021839678288,
          0.07118038088083267,
          -0.022716466337442398,
          0.0003340107505209744
        ]
      },
      {
        "chunk_index": 542,
        "relative_chunk_number": 19,
        "text": "Each combination yields a unique algorithm to construct the few-shot set for the vulnerability detection task and represents a specific trade-off between the two criteria\u2014model performance and semantic similarity\u2014for selecting examples.",
        "text_length": 236,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00436854874715209,
          0.052440181374549866,
          0.011159480549395084,
          0.02749430015683174,
          -0.01062910445034504,
          0.007568439934402704,
          0.022079726681113243,
          -0.002829990815371275,
          -0.030436983332037926,
          0.05573030188679695
        ]
      },
      {
        "chunk_index": 543,
        "relative_chunk_number": 20,
        "text": "We evaluate our methods using two popular open-source models that are known to perform well on coding tasks, namely Qwen2.5- Coder-7B-Instruct and Gemma-3-4B-it, as well as a closed- source model, GPT-5-mini. We use four challenging datasets for our evaluation that include programs from a variety of languages such as C/C++, Python, and JavaScript.",
        "text_length": 349,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.019246527925133705,
          0.0058973608538508415,
          -0.02212447114288807,
          0.0017345508094877005,
          -0.012825962156057358,
          0.0006325590657070279,
          0.017701830714941025,
          0.019426871091127396,
          -0.03119032271206379,
          -0.000503256160300225
        ]
      },
      {
        "chunk_index": 544,
        "relative_chunk_number": 21,
        "text": "Our experimental results in- dicate that individual strategies, particularly LFNN, can significantly enhance the models\u2019 baseline vulnerability detection capabilities. LFM introduces a strong inductive bias, consistently skewing pre- dictions toward the positive class.",
        "text_length": 269,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.021635103970766068,
          0.034957095980644226,
          -0.03382149338722229,
          0.019938874989748,
          -0.04539884626865387,
          -0.04539639502763748,
          0.020334593951702118,
          0.04842629283666611,
          -0.02373228594660759,
          0.04553248733282089
        ]
      },
      {
        "chunk_index": 545,
        "relative_chunk_number": 22,
        "text": "However, our proposed combined methods demonstrate a more robust and balanced performance pro- file, effectively optimizing both accuracy and F1-score across the diverse range of models and datasets evaluated. 2 Related Work Vulnerability Detection. Vulnerability detection is a long stand- ing challenge in software security.",
        "text_length": 326,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006816272158175707,
          0.04267466813325882,
          -0.01774301938712597,
          0.018191516399383545,
          -0.020473914220929146,
          0.014190590009093285,
          0.019682127982378006,
          -0.004327649716287851,
          -0.04349285364151001,
          0.059774789959192276
        ]
      },
      {
        "chunk_index": 546,
        "relative_chunk_number": 23,
        "text": "Over the years, a wide range of techniques have been developed to detect vulnerabilities, rang- ing from static program analysis [34, 39] to dynamic testing and fuzzing [7, 8, 38], and from these traditional methods to more recent machine learning based approaches [21, 28, 31, 41]. Machine learning based approaches aim to detect vulnerabilities by learning patterns directly from data.",
        "text_length": 387,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.015184679068624973,
          0.019590936601161957,
          -0.0239467304199934,
          -0.01889016106724739,
          -0.024593941867351532,
          -0.020953644067049026,
          0.01859629526734352,
          0.01273271068930626,
          -0.027096666395664215,
          0.05076927691698074
        ]
      },
      {
        "chunk_index": 547,
        "relative_chunk_number": 24,
        "text": "Early work extracted hand- crafted features such as token frequencies, code complexity metrics, or dependency patterns, and trained classifiers to separate vulner- able from non-vulnerable code [40, 43].",
        "text_length": 203,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01250031404197216,
          0.012999063357710838,
          0.001658210065215826,
          0.011573269963264465,
          -0.025567250326275826,
          0.0018124148482456803,
          0.024823985993862152,
          -0.06448110938072205,
          -0.038122911006212234,
          0.03368816524744034
        ]
      },
      {
        "chunk_index": 548,
        "relative_chunk_number": 25,
        "text": "More recent methods use representation learning to automatically encode code or abstract representations of code into vector spaces that possibly capture both syntax and semantics, enabling models to recognize patterns that are difficult to design manually.",
        "text_length": 257,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0005984437884762883,
          0.02752182073891163,
          -0.043805815279483795,
          0.01444998662918806,
          -0.08180568367242813,
          -0.015123254619538784,
          0.020151637494564056,
          -0.01636003702878952,
          -0.024686219170689583,
          0.07045628130435944
        ]
      },
      {
        "chunk_index": 549,
        "relative_chunk_number": 26,
        "text": "These approaches have shown promising results [21, 29, 52, 65, 68], but their effectiveness is often constrained by dataset quality and the challenge of generalizing across diverse projects [10, 30, 50, 62]. LLM-based Vulnerability Detection. Recent attempts to ap- ply large models (LM) to vulnerability detection generally fall into two broad approaches.",
        "text_length": 356,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.032864298671483994,
          0.006747722625732422,
          -0.03667215257883072,
          -0.00970360916107893,
          0.019188668578863144,
          -0.009239924140274525,
          0.01854531466960907,
          0.008165057748556137,
          -0.025958789512515068,
          -0.022960776463150978
        ]
      },
      {
        "chunk_index": 550,
        "relative_chunk_number": 27,
        "text": "The first line of work treats LMs as embed- ding models, extracting vector representations of code and then either training lightweight classifiers on top of these embeddings or fine tuning the LM itself for the downstream task [19, 21, 51].",
        "text_length": 241,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.042458727955818176,
          0.04246125742793083,
          -0.0346803218126297,
          0.01487858034670353,
          -0.04162479564547539,
          0.00026310415705665946,
          0.022835442796349525,
          0.027643008157610893,
          -0.03624338656663895,
          0.005326214246451855
        ]
      },
      {
        "chunk_index": 551,
        "relative_chunk_number": 28,
        "text": "Many of these methods also incorporate a program\u2019s abstract rep- resentations such as Abstract Syntax Trees (ASTs), Control Flow Graphs (CFGs), or data flow features to complement the raw code input [32, 52, 65, 68].",
        "text_length": 216,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.015994969755411148,
          0.009124701842665672,
          -0.023254496976733208,
          0.02749466896057129,
          -0.0042065284214913845,
          -0.009222729131579399,
          0.02112526260316372,
          -0.034143172204494476,
          -0.03220907226204872,
          -0.009536538273096085
        ]
      },
      {
        "chunk_index": 552,
        "relative_chunk_number": 29,
        "text": "Early research in this category has often fo- cused on models like CodeBERT [16] and GraphCodeBERT [17], with more recent work exploring larger foundation models such as Qwen [3, 55, 63, 64] and LLaMA [14, 47, 56]. The second line of work leverages the generative capabilities of modern large language models (LLM) more directly.",
        "text_length": 329,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.009911741130053997,
          0.0010759889846667647,
          -0.04858227074146271,
          0.0006222358788363636,
          0.016423385590314865,
          0.0032569256145507097,
          0.020145082846283913,
          0.04534260556101799,
          -0.026746608316898346,
          -0.02511298842728138
        ]
      },
      {
        "chunk_index": 553,
        "relative_chunk_number": 30,
        "text": "Instead of relying only on embeddings, these approaches test the ability of LLMs to reason about and classify vulnerabilities through code understanding and natural language generation. Some studies eval- uate models in their pretrained form, while others fine tune the models to improve task performance [67]. Several recent efforts re- flect this trend. Du et al.",
        "text_length": 365,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.030709631741046906,
          0.05124830827116966,
          -0.031593747437000275,
          -0.000632239505648613,
          0.019395681098103523,
          0.014391080476343632,
          0.025218257680535316,
          0.00666642002761364,
          -0.028169576078653336,
          0.010320328176021576
        ]
      },
      {
        "chunk_index": 554,
        "relative_chunk_number": 31,
        "text": "[12] introduce VulLLM, a multi-task LLM framework that integrates vulnerability interpretation and data augmentation to significantly improve code vulnerability detection. Farr et al. [15] leverage few-shot prompting to enhance out-of- the-box LLMs for vulnerability detection. Du et al.",
        "text_length": 287,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.022746458649635315,
          0.03239262476563454,
          -0.008630005642771721,
          -0.01843835599720478,
          0.024623867124319077,
          -0.02285013347864151,
          0.023219957947731018,
          0.03982715681195259,
          -0.021701006218791008,
          -0.009151224978268147
        ]
      },
      {
        "chunk_index": 555,
        "relative_chunk_number": 32,
        "text": "[13] proposes knowledge-level retrieval-augmented generation (RAG) for code vulnerability detection and reports sizable gains over baselines. Our work falls into this second category. Specifically, we focus on evaluating LLMs out of the box in a few-shot classification setting. In-Context Learning and Example Selection.",
        "text_length": 321,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007406828925013542,
          0.03917423635721207,
          -0.015618946403265,
          0.0010998437646776438,
          0.0196462981402874,
          -0.035256337374448776,
          0.017690114676952362,
          0.004862719215452671,
          -0.023055490106344223,
          0.010190064087510109
        ]
      },
      {
        "chunk_index": 556,
        "relative_chunk_number": 33,
        "text": "In-context learning (ICL) [6] enables a model to adapt to a new task by condi- tioning on a small number of input output examples provided in the prompt. These examples serve as implicit supervision, enabling the model to infer the task format and desired output style without ex- pensive fine tuning.",
        "text_length": 301,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.011785346083343029,
          0.02053319476544857,
          -0.044300973415374756,
          0.0581267885863781,
          0.002127093030139804,
          0.013244588859379292,
          0.020761920139193535,
          -0.005238876212388277,
          -0.031274035573005676,
          0.05439779534935951
        ]
      },
      {
        "chunk_index": 557,
        "relative_chunk_number": 34,
        "text": "ICL has been widely studied in natural language and code related tasks, where it has been shown to substantially improve model performance over zero-shot prompting [4, 6, 45, 59]. A central question in ICL is how to select the few examples that are most useful for the model.",
        "text_length": 275,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.024599336087703705,
          0.011420164257287979,
          -0.043579138815402985,
          0.020382912829518318,
          -0.028747044503688812,
          0.02937796711921692,
          0.022649288177490234,
          -0.005607640370726585,
          -0.0384361557662487,
          0.0014780433848500252
        ]
      },
      {
        "chunk_index": 558,
        "relative_chunk_number": 35,
        "text": "Selection strategies can range from simple random sampling to more sophisticated methods that consider similarity between the test input and candidate examples, prior evidence about LLM behavior, or task specific heuristics [25, 35, 42, 48, 61]. Liu et al. [35] retrieve examples that are semantically- similar to a test sample to formulate its corresponding prompt. Rubin et al.",
        "text_length": 379,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009244500659406185,
          -0.008671595714986324,
          -0.008196917362511158,
          0.030189963057637215,
          0.04368225485086441,
          0.023330701515078545,
          0.0183732807636261,
          0.05418526753783226,
          -0.0273028202354908,
          0.03423094004392624
        ]
      },
      {
        "chunk_index": 559,
        "relative_chunk_number": 36,
        "text": "[48] train an efficient dense retriever to select training examples as prompts at test time. Xu et al. [61] employ in-context learning to create expert profiles that condition LLM responses. In our setting, we investigate new strategies for selecting few shot examples and study how they influence the performance of LLMs on vulnerability detection. Prompt Optimization.",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.05692240223288536,
          0.013275239616632462,
          -0.025193989276885986,
          0.006813927553594112,
          -0.01751464232802391,
          0.015123056247830391,
          0.019406717270612717,
          0.035620663315057755,
          -0.03696712851524353,
          -0.02758071944117546
        ]
      },
      {
        "chunk_index": 560,
        "relative_chunk_number": 37,
        "text": "Prompt optimization broadly refers to techniques that improve how tasks are presented to large language models so that they yield more reliable outputs [44, 49]. Beyond manual design, recent work has begun to explore automated prompt optimization. DSPy [27] introduces a declarative framework that compiles language model pipelines into self-improving programs.",
        "text_length": 361,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.012363996356725693,
          0.0131551343947649,
          -0.00014574368833564222,
          0.06085406616330147,
          -0.026684075593948364,
          0.01605106331408024,
          0.01887584663927555,
          0.026606936007738113,
          -0.034756168723106384,
          0.036501843482255936
        ]
      },
      {
        "chunk_index": 561,
        "relative_chunk_number": 38,
        "text": "GEPA [2] uses reflective natural-language feedback and evolution- ary search to iteratively refine prompts. Maestro [58] extends this line of work to multi-agent settings by jointly optimizing node configurations and the structure of agent graphs to better miti- gate structural failure modes.",
        "text_length": 293,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004327978938817978,
          0.016976676881313324,
          0.0004066761175636202,
          0.030483752489089966,
          0.012964735738933086,
          0.005350296385586262,
          0.025823574513196945,
          -0.045263588428497314,
          -0.03682997450232506,
          0.0025655580684542656
        ]
      },
      {
        "chunk_index": 562,
        "relative_chunk_number": 39,
        "text": "These advances connect closely to the problem of example selection in ICL, as the choice and ar- rangement of input\u2013output demonstrations influences model per- formance [1, 5, 18, 37]. In our work, we treat the construction of the few-shot example set as a form of prompt optimization and study how different selection strategies affect the performance of LLMs for vulnerability detection.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.038861267268657684,
          0.016659807413816452,
          -0.01479006465524435,
          0.035839829593896866,
          -0.04440861567854881,
          0.010008728131651878,
          0.0209190733730793,
          0.022893108427524567,
          -0.031658779829740524,
          0.016701163724064827
        ]
      },
      {
        "chunk_index": 563,
        "relative_chunk_number": 40,
        "text": "3 Two Algorithms: LFM and LFNN We explore two algorithms for choosing the examples presented in the LLM\u2019s context when performing vulnerability detection.",
        "text_length": 154,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04885994270443916,
          0.05285128951072693,
          -0.03896348923444748,
          0.005599366966634989,
          -0.06702176481485367,
          -0.01497450191527605,
          0.02273392491042614,
          0.03995458036661148,
          -0.030087944120168686,
          0.04905104637145996
        ]
      },
      {
        "chunk_index": 564,
        "relative_chunk_number": 41,
        "text": "The output of both these algorithms is a set of few-shot examples 2 On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection Algorithm 1: Learn from Mistakes (LFM) Input: (1) Vulnerability detection model \ud835\udc53\ud835\udc49\ud835\udc37; (2) Training dataset \ud835\udc37with \ud835\udc5alabeled samples; (3) Initial few-shot set S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61with \ud835\udc5flabeled samples; (4) Output few-shot set size \ud835\udc5b; (5) Boolean \ud835\udc60\ud835\udc61indicating stacked or unstacked version; (6) Number of iterations \ud835\udc58; (7) Option \ud835\udc5c\ud835\udc5d\ud835\udc61choosing between incorrect (\ud835\udc3c), correct (\ud835\udc36), and gray (\ud835\udc3a) Output: Few-shot set S // Default values of parameters are \ud835\udc60\ud835\udc61= TRUE,\ud835\udc58= 1,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3c 1 S\ud835\udc36\u2190\ud835\udc37, S\ud835\udc3c\u2190\ud835\udc37; 2 foreach \ud835\udc56\ud835\udc51\ud835\udc65\u2208{1, .",
        "text_length": 625,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0313691571354866,
          0.0292417760938406,
          -0.0167138259857893,
          0.030696365982294083,
          -0.008069970645010471,
          -0.0025962202344089746,
          0.017848841845989227,
          0.028545139357447624,
          -0.026337508112192154,
          0.029031457379460335
        ]
      },
      {
        "chunk_index": 565,
        "relative_chunk_number": 42,
        "text": ",\ud835\udc58} do 3 S\ud835\udc50\ud835\udc61\ud835\udc65\ud835\udc61\u2190S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61, S\ud835\udc36\ud835\udc56\u2190\u2205, S\ud835\udc3c\ud835\udc56\u2190\u2205; 4 foreach (\ud835\udc65,\ud835\udc66) \u2208\ud835\udc37\\ S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61do 5 Compute prediction \u02c6\ud835\udc66= \ud835\udc53\ud835\udc49\ud835\udc37(S\ud835\udc50\ud835\udc61\ud835\udc65\ud835\udc61;\ud835\udc65); 6 if \u02c6\ud835\udc66\u2260\ud835\udc66then 7 S\ud835\udc3c\ud835\udc56\u2190S\ud835\udc3c\ud835\udc56\u222a{(\ud835\udc65,\ud835\udc66)}; 8 if \ud835\udc60\ud835\udc61then S\ud835\udc50\ud835\udc61\ud835\udc65\ud835\udc61\u2190S\ud835\udc50\ud835\udc61\ud835\udc65\ud835\udc61\u222a{(\ud835\udc65,\ud835\udc66)}; 9 else 10 S\ud835\udc36\ud835\udc56\u2190S\ud835\udc36\ud835\udc56\u222a{(\ud835\udc65,\ud835\udc66)}; 11 end 12 S\ud835\udc36\u2190S\ud835\udc36\u2229S\ud835\udc36\ud835\udc56; 13 S\ud835\udc3c\u2190S\ud835\udc3c\u2229S\ud835\udc3c\ud835\udc56; 14 end 15 S\ud835\udc3a\u2190\ud835\udc37\\ (S\ud835\udc36\u222aS\ud835\udc3c); 16 if \ud835\udc60\ud835\udc61then 17 S \u2190S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61; 18 S\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc51\u2190 Uniformly draw (\ud835\udc5b\u2212|S|) samples from \ud835\udc46\ud835\udc5c\ud835\udc5d\ud835\udc61\\ S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61; 19 S \u2190S \u222aS\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc51; 20 else 21 S \u2190Uniformly draw \ud835\udc5bsamples from \ud835\udc46\ud835\udc5c\ud835\udc5d\ud835\udc61; 22 return S; where each example is a program with a yes/no label indicating if a vulnerability is present or not.",
        "text_length": 519,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0009388222242705524,
          0.009877821430563927,
          -0.03487209230661392,
          0.030105173587799072,
          -0.002045645611360669,
          0.01905278116464615,
          0.011468116194009781,
          0.032135628163814545,
          -0.01525304839015007,
          0.03642257675528526
        ]
      },
      {
        "chunk_index": 566,
        "relative_chunk_number": 43,
        "text": "3.1 Learn-from-Mistakes (LFM) Algorithm 1 chooses few-shot examples based on the intuition that the correctness of the LLM response on an example is informative about its usefulness as a few-shot example. The algorithm makes a linear scan over a labeled dataset\u2014for each sample in the dataset, it queries an LLM asking it to predict if the sample program has a vulnerability or not.",
        "text_length": 382,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04264989495277405,
          0.032161541283130646,
          -0.037088021636009216,
          0.017855310812592506,
          -0.014856615103781223,
          0.0036610641982406378,
          0.020483287051320076,
          0.028222788125276566,
          -0.03317342698574066,
          -0.01022210344672203
        ]
      },
      {
        "chunk_index": 567,
        "relative_chunk_number": 44,
        "text": "This information is used to construct a few- shot set from this dataset (which we refer to as the training dataset). In its default version, the algorithm, which we call Learn-from- Mistakes (LFM), operates under the assumption that the examples on which the LLM makes mistakes are more informative than ran- dom examples and should be added to the few-shot set.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01839265041053295,
          0.03356175869703293,
          -0.01710083894431591,
          -0.004514658357948065,
          7.617440860485658e-05,
          0.013987472280859947,
          0.02322453260421753,
          0.032258983701467514,
          -0.027356978505849838,
          0.013057894073426723
        ]
      },
      {
        "chunk_index": 568,
        "relative_chunk_number": 45,
        "text": "It iteratively updates the few-shot set based on incorrect predictions. However, the algorithm can be configured to run under various other settings that we describe next.",
        "text_length": 171,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.009222883731126785,
          0.05140328034758568,
          0.016263356432318687,
          0.0543740876019001,
          0.009463506750762463,
          -0.004829635377973318,
          0.018259266391396523,
          0.0004016555321868509,
          -0.04504186660051346,
          0.006252548191696405
        ]
      },
      {
        "chunk_index": 569,
        "relative_chunk_number": 46,
        "text": "The primary inputs to the algorithm are the LLM \ud835\udc53\ud835\udc49\ud835\udc37, a labeled dataset \ud835\udc37with \ud835\udc5apairs of programs and their corresponding yes/no label indicating the presence or absence of a vulnerability, an initial set S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61of few-shot examples, and the desired number of examples \ud835\udc5bin the few-shot set returned by the algorithm. Note that the set S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61can be empty.",
        "text_length": 349,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01957464963197708,
          0.04076780378818512,
          -0.018737399950623512,
          0.010995692573487759,
          -0.014264601282775402,
          -0.02133253403007984,
          0.021618831902742386,
          0.055667899549007416,
          -0.025607114657759666,
          0.032496947795152664
        ]
      },
      {
        "chunk_index": 570,
        "relative_chunk_number": 47,
        "text": "The remaining inputs to the algorithm are used to configure it. The boolean input \ud835\udc60\ud835\udc61, which stands for stacked, indicates whether the context used while querying the LLM during a run of the LFM algorithm should be iteratively refined or not. The input\ud835\udc58dictates the number of linear scans over the dataset. Multiple scans help deal with the non-determinism of LLM responses.",
        "text_length": 373,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.02874934673309326,
          0.02430947683751583,
          -0.03636227175593376,
          0.006096548400819302,
          0.004892221186310053,
          0.01679082214832306,
          0.019602308049798012,
          0.06380388885736465,
          -0.01722058281302452,
          0.07716158777475357
        ]
      },
      {
        "chunk_index": 571,
        "relative_chunk_number": 48,
        "text": "Finally, the input \ud835\udc5c\ud835\udc5d\ud835\udc61controls if the examples that are added to the few- shot set are the ones where the model makes a mistake or the ones where it is correct. The algorithm begins by initializing sets S\ud835\udc36and S\ud835\udc3cwith the entire training dataset \ud835\udc37(line 1). The S\ud835\udc36tracks the examples from \ud835\udc37where the LLM is correct while S\ud835\udc3ctracks the examples where it is incorrect.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.0075902631506323814,
          0.021109018474817276,
          -0.03767320141196251,
          0.04567798599600792,
          0.0056443531066179276,
          0.011958012357354164,
          0.022141722962260246,
          0.02573492005467415,
          -0.02843620255589485,
          0.0014909649034962058
        ]
      },
      {
        "chunk_index": 572,
        "relative_chunk_number": 49,
        "text": "The algorithm then enters a loop (lines 2-14) and makes a linear scan over the dataset in each loop iteration. Before starting a scan, the algorithm initializes the few-shot set S\ud835\udc50\ud835\udc61\ud835\udc65\ud835\udc61that will be used when querying the LLM during the linear scan. It also initializes sets S\ud835\udc36\ud835\udc56and S\ud835\udc3c\ud835\udc56that track the correctly and incorrectly labeled examples during each iteration.",
        "text_length": 362,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014126616530120373,
          0.0236405897885561,
          -0.020697610452771187,
          0.026819050312042236,
          0.011055572889745235,
          0.007260070648044348,
          0.021593943238258362,
          0.015474381856620312,
          -0.03635895997285843,
          0.03564196825027466
        ]
      },
      {
        "chunk_index": 573,
        "relative_chunk_number": 50,
        "text": "In each linear scan (lines 4-11), the LLM \ud835\udc53\ud835\udc49\ud835\udc37is queried on each of the examples in the dataset (except the ones in the initial set S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61). Depending on whether the prediction is correct or not, the example is added to the set S\ud835\udc36\ud835\udc56(line 10) or S\ud835\udc3c\ud835\udc56(line 7), respectively.",
        "text_length": 268,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.03306758776307106,
          0.028374461457133293,
          -0.06275499612092972,
          0.048876721411943436,
          0.025822559371590614,
          0.023956919088959694,
          0.020594675093889236,
          0.05520515888929367,
          -0.03170541673898697,
          0.00024366845900658518
        ]
      },
      {
        "chunk_index": 574,
        "relative_chunk_number": 51,
        "text": "Moreover, if the boolean input \ud835\udc60\ud835\udc61is set to TRUE, the few-shot set S\ud835\udc50\ud835\udc61\ud835\udc65\ud835\udc61used to query the LLM during the current linear scan is updated whenever the model makes a mistake (line 8). At the end of each iteration, the sets S\ud835\udc36and S\ud835\udc3cof correctly and incorrectly labeled examples are updated (lines 12-13).",
        "text_length": 299,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.025902600958943367,
          0.03313760459423065,
          -0.03995148837566376,
          0.021919548511505127,
          0.057302214205265045,
          0.017171252518892288,
          0.023459626361727715,
          0.00703905476257205,
          -0.027000408619642258,
          -0.02120550535619259
        ]
      },
      {
        "chunk_index": 575,
        "relative_chunk_number": 52,
        "text": "Note that these sets track the examples that are consistently labeled correctly of incorrectly by the LLM across the different iterations. This is enforced by the set intersection operations in lines 12 and 13.",
        "text_length": 210,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014255281537771225,
          -0.0007386323995888233,
          -0.05792783200740814,
          0.04731304198503494,
          0.010416222736239433,
          0.0636398121714592,
          0.022339830175042152,
          0.05013975128531456,
          -0.04463902488350868,
          -0.03674613684415817
        ]
      },
      {
        "chunk_index": 576,
        "relative_chunk_number": 53,
        "text": "After all the linear scans are over and a final version of the sets S\ud835\udc36and S\ud835\udc3chave been constructed, the algorithm also constructs a set S\ud835\udc3aof examples where the LLM is not consistently correct or incorrect (\ud835\udc3astands for gray). Finally, the few-shot set S to be returned by the algorithm is computed (lines 16-21).",
        "text_length": 310,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01717728190124035,
          0.018905527889728546,
          -0.04018862172961235,
          0.041088320314884186,
          0.0461183525621891,
          0.02657879889011383,
          0.0240946002304554,
          0.0173044353723526,
          -0.028016649186611176,
          -0.011354231275618076
        ]
      },
      {
        "chunk_index": 577,
        "relative_chunk_number": 54,
        "text": "If the flag \ud835\udc60\ud835\udc61is set to TRUE, the examples in the initial set S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61are included in the set S. The remaining samples in the set S are chosen from the appropriate sets S\ud835\udc36, S\ud835\udc3c, and S\ud835\udc3a (denoted by S\ud835\udc5c\ud835\udc5d\ud835\udc61) as dictated by the value of the \ud835\udc5c\ud835\udc5d\ud835\udc61input (lines 18 and 21).",
        "text_length": 258,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008848808705806732,
          -0.017808347940444946,
          -0.03877833113074303,
          0.05093415454030037,
          0.05855221673846245,
          0.08154423534870148,
          0.023148560896515846,
          0.03770802170038223,
          -0.03222990781068802,
          0.009370166808366776
        ]
      },
      {
        "chunk_index": 578,
        "relative_chunk_number": 55,
        "text": "3.2 Learn-from-Nearest-Neighbors (LFNN) Algorithm 2, which we call the Learn-from-Nearest-Neighbors (LFNN) algorithm, chooses few-shot examples based on the intu- ition that the program samples most similar to the program under query are the most helpful in improving LLM performance.",
        "text_length": 284,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.022344883531332016,
          0.040567900985479355,
          -0.0363379642367363,
          -0.004444862715899944,
          -0.06327493488788605,
          0.008860585279762745,
          0.020645126700401306,
          0.042531754821538925,
          -0.022917622700333595,
          0.040016621351242065
        ]
      },
      {
        "chunk_index": 579,
        "relative_chunk_number": 56,
        "text": "Al- though similar ideas have been proposed in the context of other applications [60], in this work, we use this idea to improve the performance of LLMs for the vulnerability detection task.",
        "text_length": 190,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.053404808044433594,
          0.03921821340918541,
          -0.04042673483490944,
          0.010626895353198051,
          -0.015547409653663635,
          0.01773837022483349,
          0.019133366644382477,
          0.015127637423574924,
          -0.05553619936108589,
          0.020893188193440437
        ]
      },
      {
        "chunk_index": 580,
        "relative_chunk_number": 57,
        "text": "The inputs to the LFNN algorithm are a labeled dataset \ud835\udc37with \ud835\udc5a pairs of programs and their corresponding yes/no label indicating the presence or absence of a vulnerability, the program \ud835\udc65under 3 Md Abdul Hannan, Ronghao Ni, Chi Zhang, Limin Jia, Ravi Mangal, and Corina S.",
        "text_length": 271,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.028470110148191452,
          0.058805931359529495,
          0.0012829003389924765,
          0.022584743797779083,
          -0.04440048336982727,
          -0.03937423974275589,
          0.01733335107564926,
          0.016467034816741943,
          -0.021754303947091103,
          0.009223472326993942
        ]
      },
      {
        "chunk_index": 581,
        "relative_chunk_number": 58,
        "text": "Pasareanu Algorithm 2: Learn from Nearest Neighbors (LFNN) Input: (1) Training dataset \ud835\udc37with \ud835\udc5alabeled samples; (2) Query instance \ud835\udc65; (3) Number of nearest neighbors \ud835\udc5b; (4) Encoder model \ud835\udc52\ud835\udc5b\ud835\udc50 Output: Nearest neighbor set NN\ud835\udc65for the given query instance \ud835\udc65 // Part 1: General pre-computation 1 \ud835\udc3e\u2190\u2205; 2 foreach (\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56) \u2208\ud835\udc37do 3 \ud835\udc3e\u2190\ud835\udc3e\u222a(\ud835\udc56,\ud835\udc52\ud835\udc5b\ud835\udc50(\ud835\udc65\ud835\udc56)); 4 end // Part 2: Instance-specific computation 5 \ud835\udc5e\u2190\ud835\udc52\ud835\udc5b\ud835\udc50(\ud835\udc65), \ud835\udc36\u2190\u2205; 6 foreach (\ud835\udc56,\ud835\udc58) \u2208\ud835\udc3edo 7 \ud835\udc36\u2190\ud835\udc36\u222a(\ud835\udc56,\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52(\ud835\udc58,\ud835\udc5e)); 8 end // \ud835\udc47\ud835\udc5c\ud835\udc5d2 \ud835\udc5b(\ud835\udc36) returns the \ud835\udc5bpairs from \ud835\udc36with the largest second components 9 NN\ud835\udc65\u2190{\ud835\udc37[\ud835\udc56] | (\ud835\udc56,\ud835\udc58) \u2208\ud835\udc47\ud835\udc5c\ud835\udc5d2 \ud835\udc5b(\ud835\udc36)}; 10 return NN\ud835\udc65; query, the number \ud835\udc5bof nearest neighbors of \ud835\udc65(i.e., the size of the few-shot set S) returned by the algorithm, and the encoder model \ud835\udc52\ud835\udc5b\ud835\udc50to be used.",
        "text_length": 718,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009005597792565823,
          0.027464576065540314,
          -0.022183798253536224,
          0.0170671958476305,
          -0.04174058139324188,
          -0.0026008544955402613,
          0.012595901265740395,
          -0.000683337333612144,
          -0.03157231584191322,
          0.027538271620869637
        ]
      },
      {
        "chunk_index": 582,
        "relative_chunk_number": 59,
        "text": "\ud835\udc52\ud835\udc5b\ud835\udc50is used to compute embedding vectors for pro- grams which are then used for the nearest neighbor computation. The algorithm begins with a query-agnostic phase. First, the set of embedding vectors \ud835\udc3eis initialized to be an empty set (line 1).",
        "text_length": 243,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014609311707317829,
          -3.2012074370868504e-05,
          -0.014778271317481995,
          0.06342367082834244,
          -0.044500984251499176,
          -0.007946684025228024,
          0.024687182158231735,
          -0.09292862564325333,
          -0.031214097514748573,
          0.01795097626745701
        ]
      },
      {
        "chunk_index": 583,
        "relative_chunk_number": 60,
        "text": "The algorithm then makes a pass over the dataset \ud835\udc37, computes the embedding vector corresponding to each sample program in the dataset, and stores the pair of program index \ud835\udc56and the correspond- ing embedding vector in \ud835\udc3e(lines 2-4). This part is independent of the query \ud835\udc65and can be computed in advance. The key vectors are stored for later use with any query instance \ud835\udc65.",
        "text_length": 369,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014858897775411606,
          0.03831065073609352,
          -0.02620333805680275,
          0.021722136065363884,
          0.015498246066272259,
          0.00677719758823514,
          0.0193827785551548,
          -0.010641445405781269,
          -0.029989510774612427,
          -0.010083629749715328
        ]
      },
      {
        "chunk_index": 584,
        "relative_chunk_number": 61,
        "text": "The next phase of the algorithm is query-specific. Given a query instance \ud835\udc65, the algorithm first computes the embedding vector \ud835\udc5efor \ud835\udc65(line 5). It also initializes a set \ud835\udc36to record the similarity between \ud835\udc65and the programs in the dataset \ud835\udc37.",
        "text_length": 238,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00107856176327914,
          0.03827056661248207,
          -0.0003871661610901356,
          0.04140626639127731,
          -0.007501968648284674,
          0.00954290758818388,
          0.018727673217654228,
          0.02154337242245674,
          -0.029810091480612755,
          0.019165489822626114
        ]
      },
      {
        "chunk_index": 585,
        "relative_chunk_number": 62,
        "text": "For each vector \ud835\udc58in the set of embedding vectors \ud835\udc3e, the algorithm calculates the cosine similarity between the embedding of \ud835\udc65and \ud835\udc58(lines 6-8), which measures the similarity between the query embedding and each program embedding from the dataset. The set \ud835\udc36records the indices of \ud835\udc58and the corresponding cosine similarity with \ud835\udc5e.",
        "text_length": 326,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0189363956451416,
          0.034262076020240784,
          -0.019073912873864174,
          0.037105634808540344,
          -0.017511511221528053,
          -0.007915021851658821,
          0.02042241580784321,
          -0.03728552907705307,
          -0.020273130387067795,
          0.0055273533798754215
        ]
      },
      {
        "chunk_index": 586,
        "relative_chunk_number": 63,
        "text": "Finally, the algorithm selects the\ud835\udc5bprograms from \ud835\udc37that have the highest cosine similarities, identifying the most relevant neighbors to the query (Line 9). These selected programs form the nearest neighbor set NN\ud835\udc65, which serves as the output of this algorithm. Note that the general pre-computation phase of the algorithm need not be run for each query.",
        "text_length": 353,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.011438134126365185,
          0.015151266939938068,
          -0.012363359332084656,
          0.014611529186367989,
          -0.11383219063282013,
          -0.009149480611085892,
          0.019231323152780533,
          0.02887415699660778,
          -0.031916648149490356,
          0.015263278037309647
        ]
      },
      {
        "chunk_index": 587,
        "relative_chunk_number": 64,
        "text": "Instead, the embedding vectors can be stored and then reused for each new query. 4 Combining the Two Algorithms We explore three different strategies for combining LFM and LFNN to enhance the model\u2019s performance in detecting code vulnerabili- ties (Algorithm 3).",
        "text_length": 262,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.07148578763008118,
          0.08245072513818741,
          -0.01521984115242958,
          0.015608243644237518,
          -0.03492940589785576,
          -0.011111685074865818,
          0.019823580980300903,
          0.05838286876678467,
          -0.023964006453752518,
          -0.01611177809536457
        ]
      },
      {
        "chunk_index": 588,
        "relative_chunk_number": 65,
        "text": "The output of each of these combinations is a Algorithm 3: Combined Methods Input: (1) Vulnerability detection model \ud835\udc53\ud835\udc49\ud835\udc37; (2) Training dataset \ud835\udc37with \ud835\udc5alabeled samples; (3) Initial few-shot set S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61with \ud835\udc5flabeled samples; (4) LFM output few-shot set sizes \ud835\udc5b1 and \ud835\udc5b2; (5) Boolean \ud835\udc60\ud835\udc61 indicating stacked or unstacked version; (6) Number of iterations \ud835\udc58; (7) Option \ud835\udc5c\ud835\udc5d\ud835\udc61choosing between incorrect (\ud835\udc3c), correct (\ud835\udc36), and gray (\ud835\udc3a); (8) Query instance \ud835\udc65; (9) Number of nearest neighbors \ud835\udc5b3; (10) Encoder model \ud835\udc52\ud835\udc5b\ud835\udc50 Output: Few-shot set S 1 def \ud835\udc5a\ud835\udc52\ud835\udc61\u210e\ud835\udc5c\ud835\udc511(): 2 S\ud835\udc3f\ud835\udc39\ud835\udc40\u2190\ud835\udc3f\ud835\udc39\ud835\udc40(\ud835\udc53\ud835\udc49\ud835\udc37, \ud835\udc37, S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61,\ud835\udc5b1,\ud835\udc60\ud835\udc61,\ud835\udc58,\ud835\udc5c\ud835\udc5d\ud835\udc61); 3 S\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41\u2190\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41(\ud835\udc37,\ud835\udc65,\ud835\udc5b3,\ud835\udc52\ud835\udc5b\ud835\udc50); 4 return S\ud835\udc3f\ud835\udc39\ud835\udc40\u222aS\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41; 5 def \ud835\udc5a\ud835\udc52\ud835\udc61\u210e\ud835\udc5c\ud835\udc512(): 6 S\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41\u2190\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41(\ud835\udc37,\ud835\udc65,\ud835\udc5b3,\ud835\udc52\ud835\udc5b\ud835\udc50); 7 S\ud835\udc3f\ud835\udc39\ud835\udc40\u2190\ud835\udc3f\ud835\udc39\ud835\udc40(\ud835\udc53\ud835\udc49\ud835\udc37, \ud835\udc37, S\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41,\ud835\udc5b1,\ud835\udc60\ud835\udc61,\ud835\udc58,\ud835\udc5c\ud835\udc5d\ud835\udc61); 8 return S\ud835\udc3f\ud835\udc39\ud835\udc40; 9 def \ud835\udc5a\ud835\udc52\ud835\udc61\u210e\ud835\udc5c\ud835\udc513(): 10 S\ud835\udc3f\ud835\udc39\ud835\udc40\u2190\ud835\udc3f\ud835\udc39\ud835\udc40(\ud835\udc53\ud835\udc49\ud835\udc37, \ud835\udc37, S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61,\ud835\udc5b2,\ud835\udc60\ud835\udc61,\ud835\udc58,\ud835\udc5c\ud835\udc5d\ud835\udc61); 11 S\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41\u2190\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41(\ud835\udc37,\ud835\udc65,\ud835\udc5b3,\ud835\udc52\ud835\udc5b\ud835\udc50); 12 return \ud835\udc3f\ud835\udc39\ud835\udc40(\ud835\udc53\ud835\udc49\ud835\udc37, S\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41, S\ud835\udc3f\ud835\udc39\ud835\udc40,\ud835\udc5b1,\ud835\udc60\ud835\udc61,\ud835\udc58,\ud835\udc5c\ud835\udc5d\ud835\udc61) query-specific few-shot set of examples that is then used to predict the label for the query.",
        "text_length": 947,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03351416438817978,
          0.009878743439912796,
          -0.005548426881432533,
          0.023092521354556084,
          -0.04427985101938248,
          -0.0024663973599672318,
          0.011622520163655281,
          0.04373018443584442,
          -0.011336873285472393,
          0.008167264983057976
        ]
      },
      {
        "chunk_index": 589,
        "relative_chunk_number": 66,
        "text": "Note that while there can be other ways of combining LFM and LFNN, we believe the three combinations we explore in this work represent the most natural starting point. Method 1. In this method (lines 1-4), we combine the few-shot set from the LFM with the nearest neighbors of the query instance \ud835\udc65computed by LFNN.",
        "text_length": 314,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.05374789610505104,
          0.05177320912480354,
          -0.03462120518088341,
          -0.012033961713314056,
          -0.10813548415899277,
          -0.022401053458452225,
          0.0210255179554224,
          0.06037968397140503,
          -0.04074671119451523,
          0.01226845197379589
        ]
      },
      {
        "chunk_index": 590,
        "relative_chunk_number": 67,
        "text": "This method is the most straightforward and cost-effective compared to the subsequent two approaches. We begin by constructing a few-shot set S\ud835\udc3f\ud835\udc39\ud835\udc40with a total of \ud835\udc5b1 samples. Note that this set is agnostic of the query \ud835\udc65, so it just needs to be computed once for all the queries. Next, for each query instance \ud835\udc65, we generate a unique few-shot set S\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41with the \ud835\udc5b3 nearest neighbors of \ud835\udc65.",
        "text_length": 386,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.025206541642546654,
          0.04901405796408653,
          -0.01721334271132946,
          0.021701138466596603,
          -0.027472060173749924,
          -0.010989597998559475,
          0.015613696537911892,
          0.04272838681936264,
          -0.03116907738149166,
          -0.03151457756757736
        ]
      },
      {
        "chunk_index": 591,
        "relative_chunk_number": 68,
        "text": "The final few-shot set is obtained by taking the union of the general few-shot set S\ud835\udc3f\ud835\udc39\ud835\udc40with the nearest neighbors S\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41of \ud835\udc65. Typically, in practice, the final few-shot set is composed of an equal number of samples from both the sources. Method 2. In this method (lines 5-8), we use the nearest neigh- bors of the query instance \ud835\udc65as initial few-shot examples S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61for LFM.",
        "text_length": 371,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.028623173013329506,
          0.015091135166585445,
          -0.025234296917915344,
          0.027853218838572502,
          0.013636200688779354,
          0.01657826453447342,
          0.018991129472851753,
          0.03168128803372383,
          -0.03864964842796326,
          -0.02045457437634468
        ]
      },
      {
        "chunk_index": 592,
        "relative_chunk_number": 69,
        "text": "Our intuition is that compared to using no initial few-shot examples or using random examples with LFM, the use of near- est neighbors provides the model with starting knowledge that is closely related to the query instance \ud835\udc65.",
        "text_length": 226,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01808818057179451,
          0.044067226350307465,
          -0.0367981493473053,
          0.03402477130293846,
          -0.06711503863334656,
          0.014348472468554974,
          0.020858071744441986,
          0.02928449958562851,
          -0.032250579446554184,
          0.014921789988875389
        ]
      },
      {
        "chunk_index": 593,
        "relative_chunk_number": 70,
        "text": "As a result, the few-shot set constructed by LFM is specifically tailored to the program \ud835\udc65under query and therefore, can be more effective at improving the vulner- ability detection capabilities of the model.",
        "text_length": 208,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03516283631324768,
          0.05539671704173088,
          -0.02098429948091507,
          0.009212433360517025,
          0.02500382624566555,
          -0.0377877838909626,
          0.021581411361694336,
          0.03271738067269325,
          -0.031295210123062134,
          -0.004782483447343111
        ]
      },
      {
        "chunk_index": 594,
        "relative_chunk_number": 71,
        "text": "Note that, in contrast to Method 1, we alter the order of applying the two algorithms such that both the calls (lines 6 and 7) generate distinct few-shot sets tailored for each query instance \ud835\udc65. In other words, we are not able to reuse any computation across the different queries.",
        "text_length": 281,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.006302743684500456,
          0.05804639682173729,
          -0.019856153056025505,
          0.02975889854133129,
          -0.039266299456357956,
          -0.018132003024220467,
          0.017640413716435432,
          0.026103008538484573,
          -0.031729806214571,
          0.018212726339697838
        ]
      },
      {
        "chunk_index": 595,
        "relative_chunk_number": 72,
        "text": "Although 4 On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection this approach is more resource-intensive, we hypothesize that this customized few-shot set could enhance model performance. Method 3. In contrast to methods 1 and 2 that both invoke LFM and LFNN just once, method 3 (lines 9-12) invokes LFM twice.",
        "text_length": 331,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.030734114348888397,
          0.04070838913321495,
          -0.04568721354007721,
          0.020071664825081825,
          -0.032394345849752426,
          -0.030851079151034355,
          0.022642754018306732,
          0.05193191021680832,
          -0.032952867448329926,
          -0.019203834235668182
        ]
      },
      {
        "chunk_index": 596,
        "relative_chunk_number": 73,
        "text": "This method first invokes LFM (line 10) in a manner similar to method 1, generating a few-shot set S\ud835\udc3f\ud835\udc39\ud835\udc40of size \ud835\udc5b2. This first call to LFM is query-agnostic and therefore, only needs to made once. Next, the method invokes LFNN (line 11), again in a manner similar to method 1 and generates a set of size \ud835\udc5b3. As usual, the call to LFNN is query-specific and needs to be repeated for each query.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.004536821041256189,
          0.04575275629758835,
          -0.012028238736093044,
          0.019412478432059288,
          -0.06526774168014526,
          -0.004660214297473431,
          0.020357515662908554,
          0.10005258023738861,
          -0.02612999640405178,
          -0.030841106548905373
        ]
      },
      {
        "chunk_index": 597,
        "relative_chunk_number": 74,
        "text": "Next, and unlike the other methods, LFM is invoked a second time. For this invocation, instead of using \ud835\udc37as the dataset, the few-shot set S\ud835\udc3f\ud835\udc39\ud835\udc41\ud835\udc41computed by LFNN is used as the dataset. This enables inclusion of only those examples in the final few-shot set that are most similar to the query while also accounting for the model correctness on these examples.",
        "text_length": 357,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.026177749037742615,
          0.04114567115902901,
          -0.023886295035481453,
          0.014812425710260868,
          -0.02520269900560379,
          -0.026965709403157234,
          0.021889176219701767,
          0.059405453503131866,
          -0.028049293905496597,
          -0.0020178710110485554
        ]
      },
      {
        "chunk_index": 598,
        "relative_chunk_number": 75,
        "text": "Moreover, the second call to LFM uses the few-shot set S\ud835\udc3f\ud835\udc39\ud835\udc40computed on line 10 as the initial set. The intuition here is that initializing LFM with these examples can make LFM aware of the examples on which the model makes a mistake (or is correct, depending on the \ud835\udc5c\ud835\udc5d\ud835\udc61parameter) and thus, enable LFM to pick more effective examples for the final few-shot set.",
        "text_length": 360,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.021269921213388443,
          0.04334764927625656,
          -0.022688210010528564,
          0.04388149082660675,
          -0.013250678777694702,
          0.018635882064700127,
          0.02000303380191326,
          0.05616907402873039,
          -0.03435783088207245,
          -0.048878371715545654
        ]
      },
      {
        "chunk_index": 599,
        "relative_chunk_number": 76,
        "text": "Note that the second call to LFM is also query-specific. 5 Experiments In this section, we report on our experiments with the proposed methods, using open source models. We aim to answer the following research questions. Research Questions.",
        "text_length": 240,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.0446658618748188,
          0.0454171821475029,
          -0.0023075162898749113,
          0.016901496797800064,
          -0.046515509486198425,
          -0.042996957898139954,
          0.02102401666343212,
          0.04973658546805382,
          -0.04503636807203293,
          -0.09919115900993347
        ]
      },
      {
        "chunk_index": 600,
        "relative_chunk_number": 77,
        "text": "(1) How do the proposed algorithms compare individually with baselines (zero shot and few shot settings) in helping large language models find vulnerabilities in code? (2) How do different strategies for combining LFM and LFNN influence the overall performance of the model and how do they compare to using either strategy in isolation?",
        "text_length": 336,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.03039894625544548,
          0.06164568290114403,
          -0.01030614785850048,
          0.02395373396575451,
          -0.03138141334056854,
          0.01648218370974064,
          0.023684872314333916,
          0.0418706052005291,
          -0.03136736899614334,
          -0.0011615757830440998
        ]
      },
      {
        "chunk_index": 601,
        "relative_chunk_number": 78,
        "text": "(3) Are the performance improvements introduced by LFM and LFNN consistent across different large language models, or are they model-specific? (4) Does the programming language or other linguistic charac- teristics of the dataset influence the effectiveness of LFM, LFNN, and their combinations? Datasets and Models.",
        "text_length": 316,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.03151477873325348,
          0.03707846626639366,
          -0.03373386710882187,
          0.019299978390336037,
          -0.0199025459587574,
          -0.022545738145709038,
          0.02270362712442875,
          0.06734126061201096,
          -0.022923124954104424,
          -0.025725992396473885
        ]
      },
      {
        "chunk_index": 602,
        "relative_chunk_number": 79,
        "text": "For datasets, we consider established benchmarks such as PrimeVul, DiverseVul, SVEN [9, 11, 20]. These are well-curated datasets, including pairs of code samples (vulnera- ble vs. non-vulnerable). We experiment with adding both vulnerable and non-vulnerable few-shot examples to better gauge the perfor- mance of the LLMs on the vulnerability detection task.",
        "text_length": 358,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.027424195781350136,
          0.019117673859000206,
          -0.014056804589927197,
          0.009732145816087723,
          0.006133500020951033,
          0.02529146708548069,
          0.02200297825038433,
          0.006076806224882603,
          -0.023824909701943398,
          -0.016838891431689262
        ]
      },
      {
        "chunk_index": 603,
        "relative_chunk_number": 80,
        "text": "We also leverage recent work on vulnerabilities in JavaScript programs [7, 8] and obtained a copy of the dataset generated by their tools directly from the authors. We refer to this dataset as NodeMedic. PrimeVul consists of a training set with 7578 samples, comprising 3789 pairs, and a test set with 870 samples. It has 112 unique CWEs.",
        "text_length": 338,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.016005655750632286,
          0.0041890996508300304,
          0.024217640981078148,
          0.018623948097229004,
          -0.014786784537136555,
          0.01712357997894287,
          0.016229063272476196,
          0.022281842306256294,
          -0.028242796659469604,
          0.021625837311148643
        ]
      },
      {
        "chunk_index": 604,
        "relative_chunk_number": 81,
        "text": "To mitigate computational overhead, the test set was downsampled to a representative subset of 200 examples. This sampling process was conducted based on the intersection of Common Weakness Enumerations (CWEs) found in both the training and test data. The final sampled test dataset is balanced, containing 100 vulnerable and non-vulnerable pairs, and 58 unique CWEs.",
        "text_length": 367,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03251152113080025,
          0.021780136972665787,
          -0.01795479841530323,
          0.03735477477312088,
          -0.026771847158670425,
          0.04103340208530426,
          0.020475411787629128,
          -0.060534581542015076,
          -0.02970353700220585,
          0.020270900800824165
        ]
      },
      {
        "chunk_index": 605,
        "relative_chunk_number": 82,
        "text": "DiverseVul comprises 330492 unpaired samples, and it has 150 unique CWEs. To ensure computational tractability, the dataset was first partitioned into a primary training and a primary test set, following an 80:20 ratio which were unbalanced. Subsequently, based on the intersection of CWEs present in both splits, a final balanced sample was created.",
        "text_length": 350,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.012436574324965477,
          0.010023055598139763,
          -0.027147183194756508,
          0.03853489086031914,
          -0.029717911034822464,
          -0.019474521279335022,
          0.015044193714857101,
          -0.013078986667096615,
          -0.019481008872389793,
          0.014986095950007439
        ]
      },
      {
        "chunk_index": 606,
        "relative_chunk_number": 83,
        "text": "This resulted in a training set of 200 examples and a test set of 300 examples. Both of these sampled sets have 114 unique and comon CWEs. SVEN comprises 1440 training and 166 validation samples. This dataset was partitioned into two distinct subsets based on the pro- gramming language of the functions. The C/C++ subset, designated SVENC, consists of 756 training and 90 validation samples.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.036272406578063965,
          0.009564520791172981,
          -0.025986628606915474,
          0.038358189165592194,
          -0.02078867144882679,
          0.028392985463142395,
          0.018921954557299614,
          0.01610473170876503,
          -0.021921580657362938,
          0.03844263032078743
        ]
      },
      {
        "chunk_index": 607,
        "relative_chunk_number": 84,
        "text": "The Python subset, SVENP, is composed of 684 training and 76 val- idation samples. SVENC and SVENP has 7 and 4 unique CWEs respectively (both train and validation set). NodeMedic dataset was provided by the authors of NodeMedic- FINE [8], a dynamic analysis tool that detects taint flows from package APIs to dangerous sinks that may enable arbitrary com- mand injection or code execution.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003538633929565549,
          0.001971209654584527,
          -0.0103753125295043,
          0.040346212685108185,
          -0.04369903355836868,
          0.00519468542188406,
          0.023398421704769135,
          0.044541142880916595,
          -0.023979848250746727,
          0.05281893163919449
        ]
      },
      {
        "chunk_index": 608,
        "relative_chunk_number": 85,
        "text": "The dataset is divided into 1,506 training and 189 test samples, each corresponding to a Node.js pack- age with potentially vulnerable dataflows reported by the tool. All reports are either automatically confirmed by NodeMedic-FINE [8] or manually verified by its authors.",
        "text_length": 272,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.016980264335870743,
          0.013331165537238121,
          -0.011491619050502777,
          0.02992611564695835,
          0.0038647421170026064,
          0.0058722104877233505,
          0.01930258423089981,
          0.014414500445127487,
          -0.04430198669433594,
          0.034720394760370255
        ]
      },
      {
        "chunk_index": 609,
        "relative_chunk_number": 86,
        "text": "To facilitate semantic code retrieval, we employed a specialized encoder model from the Salesforce SFR family: \"Salesforce/SFR- Embedding-Code-400M_R\" [36]. These vector embeddings are used in the nearest neighbor search, which utilizes cosine distance to identify the closest matches for any given query.",
        "text_length": 305,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.011954513378441334,
          0.021699126809835434,
          -0.03231142833828926,
          0.04486412554979324,
          -0.013941976241767406,
          0.026848068460822105,
          0.025197619572281837,
          0.028266863897442818,
          -0.02416403964161873,
          -0.03530104458332062
        ]
      },
      {
        "chunk_index": 610,
        "relative_chunk_number": 87,
        "text": "We conducted experiments on two open-source models, \"Qwen- 2.5-Coder-7B-Instruct\" [22] and \"gemma-3-4b-it\" [54], as well as the closed-source GPT-5-mini (the gpt-5-mini-2025-08-07 snapshot), to assess the generalizability of our proposed techniques.",
        "text_length": 249,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.005717221647500992,
          0.013566480949521065,
          -0.005480977240949869,
          0.018551066517829895,
          -0.004938899539411068,
          0.0007320520817302167,
          0.020360086113214493,
          0.0008528395555913448,
          -0.03425883501768112,
          -0.05354089289903641
        ]
      },
      {
        "chunk_index": 611,
        "relative_chunk_number": 88,
        "text": "The Qwen model was selected for its strong, well-documented proficiency on coding tasks and very large context window, while the Gemma model was chosen for its computational efficiency and competitive performance given its parameter count. GPT-5-mini serves as a high- quality closed-source baseline to contextualize the behavior of the open models.",
        "text_length": 349,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03231767937541008,
          0.012776519171893597,
          -0.012498960830271244,
          -0.0039032127242535353,
          0.0579620860517025,
          -0.03220950439572334,
          0.022127710282802582,
          0.005968596786260605,
          -0.026305314153432846,
          0.0156389307230711
        ]
      },
      {
        "chunk_index": 612,
        "relative_chunk_number": 89,
        "text": "The combination of these models allows evaluation across differing trade-offs of capability and resource requirements, while staying within our available computational budget. Experiment Setup.",
        "text_length": 193,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0034364671446383,
          0.0411553792655468,
          -0.006565683521330357,
          0.03153122961521149,
          -0.03712055832147598,
          -0.012544023804366589,
          0.017523979768157005,
          -0.040320366621017456,
          -0.030864663422107697,
          -0.014787979423999786
        ]
      },
      {
        "chunk_index": 613,
        "relative_chunk_number": 90,
        "text": "For our baselines, we consider a zero-shot setting, i.e., no examples in context, and a random few-shot setting where twenty vulnerable and non-vulnerable examples are drawn at random, from our train datasets. To address RQ1, we create several variants of the LFM algorithm with different parameter settings.",
        "text_length": 308,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.013999554328620434,
          0.06880809366703033,
          -0.015601812861859798,
          0.020231839269399643,
          -0.007945921272039413,
          -0.034492332488298416,
          0.021525410935282707,
          0.0212977584451437,
          -0.03467710688710213,
          -0.016886666417121887
        ]
      },
      {
        "chunk_index": 614,
        "relative_chunk_number": 91,
        "text": "However, since the performance differences are minimal, we only report the results for the following configurations: \ud835\udc5b= 20,\ud835\udc60\ud835\udc61= TRUE,\ud835\udc58= 1,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3cand an initial few-shot set S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61with an empty set. We run the LFNN algorithm with parameter settings \ud835\udc5b= 20.",
        "text_length": 251,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.00636065099388361,
          0.03636376932263374,
          -0.04965803772211075,
          0.017752746120095253,
          -0.03596455976366997,
          0.023006048053503036,
          0.022664641961455345,
          0.014847190119326115,
          -0.03787152096629143,
          0.014797818847000599
        ]
      },
      {
        "chunk_index": 615,
        "relative_chunk_number": 92,
        "text": "To answer RQ2, we run the combined methods with the following parameter settings (for all three combinations, the initial few-shot set S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61has 5 examples that are randomly drawn from the train dataset): 5 Md Abdul Hannan, Ronghao Ni, Chi Zhang, Limin Jia, Ravi Mangal, and Corina S.",
        "text_length": 283,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012480364181101322,
          0.01977161504328251,
          -0.024992577731609344,
          0.0378541573882103,
          0.0018733714241534472,
          -0.02180394157767296,
          0.017437037080526352,
          0.019926173612475395,
          -0.0346408411860466,
          -0.04928092285990715
        ]
      },
      {
        "chunk_index": 616,
        "relative_chunk_number": 93,
        "text": "Pasareanu Table 1: Results for all approaches (ZS = zero-shot, R-FS = random few-shot, LFM = Learn-from-Mistakes, LFNN = Learn-from- Nearest-Neighbors, CM = Combined Method) across three models. Gemma and Qwen results are mean over five runs; GPT results are based on one run. A dash (\u2018\u2013\u2019) indicates that the metric is undefined in at least one of the runs due to division by zero.",
        "text_length": 381,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.037872832268476486,
          0.032869916409254074,
          -0.01427766028791666,
          0.026793017983436584,
          -0.016782494261860847,
          -0.0005762770888395607,
          0.020330000668764114,
          0.014661498367786407,
          -0.03317694365978241,
          0.0005043083801865578
        ]
      },
      {
        "chunk_index": 617,
        "relative_chunk_number": 94,
        "text": "Dataset Approach Gemma-3-4b-it GPT-5-mini Qwen-2.5-Coder Acc Prec Recall F1 Acc Prec Recall F1 Acc Prec Recall F1 DiverseVul ZS 0.619 0.770 0.340 0.472 0.593 0.588 0.627 0.607 0.497 0.000 0.000 0.000 R-FS 0.545 0.571 0.467 0.480 0.560 0.598 0.367 0.455 0.599 0.741 0.303 0.411 LFM 0.525 0.840 0.061 0.114 0.617 0.647 0.513 0.573 0.500 0.500 1.000 0.667 LFNN 0.512 0.511 0.537 0.524 0.590 0.624 0.453 0.525 0.659 0.632 0.759 0.690 CM 1 0.548 0.530 0.868 0.658 0.587 0.610 0.480 0.537 0.638 0.606 0.792 0.686 CM 2 0.515 0.508 0.973 0.667 0.560 0.573 0.473 0.518 0.597 0.557 0.949 0.702 CM 3 0.499 0.499 0.985 0.663 0.570 0.585 0.480 0.527 0.531 0.517 0.988 0.678 NodeMedic ZS 0.506 0.765 0.460 0.574 0.757 0.786 0.912 0.845 0.375 0.870 0.162 0.273 R-FS 0.687 0.726 0.914 0.804 0.794 0.845 0.876 0.860 0.632 0.748 0.750 0.739 LFM 0.725 0.725 1.000 0.840 0.767 0.789 0.927 0.852 0.720 0.723 0.993 0.837 LFNN 0.758 0.759 0.975 0.854 0.788 0.849 0.861 0.855 0.713 0.768 0.866 0.814 CM 1 0.701 0.755 0.870 0.808 0.788 0.839 0.876 0.857 0.751 0.776 0.923 0.843 CM 2 0.565 0.719 0.657 0.686 0.794 0.840 0.883 0.861 0.735 0.748 0.958 0.840 CM 3 0.722 0.728 0.984 0.837 0.794 0.846 0.883 0.864 0.720 0.724 0.991 0.837 PrimeVul ZS 0.599 0.723 0.358 0.472 0.535 0.524 0.750 0.617 0.500 \u2013 0.002 0.004 R-FS 0.522 0.553 0.373 0.422 0.560 0.556 0.600 0.577 0.503 0.528 0.232 0.304 LFM 0.516 0.726 0.437 0.336 0.550 0.557 0.490 0.521 0.500 0.500 1.000 0.667 LFNN 0.527 0.520 0.648 0.571 0.555 0.549 0.620 0.582 0.483 0.390 0.060 0.104 CM 1 0.543 0.527 0.885 0.659 0.575 0.577 0.600 0.588 0.473 0.462 0.336 0.383 CM 2 0.516 0.508 0.964 0.666 0.555 0.549 0.610 0.578 0.455 0.445 0.370 0.404 CM 3 0.500 0.500 0.982 0.663 0.555 0.546 0.650 0.594 0.499 0.499 0.986 0.663 SVENC ZS 0.478 0.474 0.400 0.434 0.533 0.523 0.756 0.618 0.500 \u2013 0.000 0.000 R-FS 0.506 0.503 0.778 0.564 0.600 0.579 0.733 0.647 0.491 \u2013 0.422 0.345 LFM 0.500 0.500 1.000 0.667 0.544 0.543 0.556 0.549 0.500 0.500 1.000 0.667 LFNN 0.471 0.428 0.187 0.258 0.567 0.554 0.689 0.614 0.504 0.567 0.040 0.075 CM 1 0.520 0.513 0.813 0.629 0.578 0.566 0.667 0.612 0.520 0.527 0.387 0.442 CM 2 0.536 0.520 0.947 0.671 0.589 0.574 0.689 0.626 0.493 0.495 0.636 0.556 CM 3 0.500 0.500 0.938 0.652 0.611 0.596 0.689 0.639 0.496 0.498 0.982 0.661 SVENP ZS 0.587 0.597 0.537 0.565 0.763 0.727 0.842 0.780 0.705 0.965 0.426 0.591 R-FS 0.558 0.561 0.632 0.584 0.789 0.806 0.763 0.784 0.616 0.707 0.432 0.514 LFM 0.500 0.500 0.905 0.644 0.776 0.744 0.842 0.790 0.500 0.500 1.000 0.667 LFNN 0.634 0.641 0.611 0.625 0.895 0.941 0.842 0.889 0.792 0.845 0.716 0.775 CM 1 0.553 0.546 0.616 0.579 0.882 0.892 0.868 0.880 0.787 0.793 0.779 0.785 CM 2 0.632 0.639 0.605 0.622 0.895 0.917 0.868 0.892 0.697 0.660 0.816 0.729 CM 3 0.521 0.512 0.942 0.663 0.803 0.795 0.816 0.805 0.600 0.558 0.963 0.707 \u2022 Combined Method 1: \ud835\udc5b1 = 10,\ud835\udc60\ud835\udc61= TRUE,\ud835\udc58= 1,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3c,\ud835\udc5b3 = 10 \u2022 Combined Method 2: \ud835\udc5b1 = 20,\ud835\udc60\ud835\udc61= TRUE,\ud835\udc58= 1,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3c,\ud835\udc5b3 = 5 \u2022 Combined Method 3: \ud835\udc5b1 = 10,\ud835\udc5b2 = 15,\ud835\udc60\ud835\udc61= TRUE,\ud835\udc58= 1,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3c,\ud835\udc5b3 = 20 In our experiments, all dataset sampling is performed with a fixed random seed to ensure reproducibility.",
        "text_length": 3110,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003522126702591777,
          0.0065992651507258415,
          0.008849668316543102,
          0.027833441272377968,
          0.02422192320227623,
          -0.01194076705724001,
          0.0031953707803040743,
          0.02235097624361515,
          0.0031436041463166475,
          0.05854145064949989
        ]
      },
      {
        "chunk_index": 618,
        "relative_chunk_number": 95,
        "text": "For the Qwen and Gemma models, deterministic algorithms are enabled, and all random num- ber generators in Python and PyTorch are explicitly seeded to en- sure deterministic behavior under our hardware and software setup. For GPT-5-Mini, we set the API seed parameter to a fixed integer to encourage deterministic generation.",
        "text_length": 325,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.019832568243145943,
          0.01443055085837841,
          -0.019772933796048164,
          0.025792479515075684,
          -0.044933948665857315,
          -0.011423682793974876,
          0.023065632209181786,
          0.03923168033361435,
          -0.03506985679268837,
          -0.0022597787901759148
        ]
      },
      {
        "chunk_index": 619,
        "relative_chunk_number": 96,
        "text": "However, as documented by OpenAI1, setting this seed does not guarantee consistent outputs because changes to the backend system, such as model version updates, can still affect the generations. Results.",
        "text_length": 203,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010159322060644627,
          0.05454526096582413,
          -0.032109811902046204,
          0.019499395042657852,
          -0.019854648038744926,
          0.05259200185537338,
          0.02088913880288601,
          0.038221318274736404,
          -0.039307013154029846,
          -0.03643012046813965
        ]
      },
      {
        "chunk_index": 620,
        "relative_chunk_number": 97,
        "text": "We present a detailed analysis of the experimental results from our evaluation of three LLMs from the Gemma, GPT, and Qwen families across the four datasets (Results are presented 1https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_ parameter 6 On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection separately for SVENC and SVENP).",
        "text_length": 372,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04240594059228897,
          0.00922286044806242,
          -0.03053971566259861,
          0.023876860737800598,
          -0.021384501829743385,
          0.0058753048069775105,
          0.02274422161281109,
          0.050331536680459976,
          -0.031722236424684525,
          -0.0007971127051860094
        ]
      },
      {
        "chunk_index": 621,
        "relative_chunk_number": 98,
        "text": "All findings are summarized in Table 1, which reports the mean performance metrics over five independent runs for the Gemma and Qwen models using five different random seeds. The standard deviations between runs are minimal, so they are not included in the table because of space constraints. For the GPT model, we report the results from a single run.",
        "text_length": 352,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.016196226701140404,
          -0.006620828993618488,
          -0.005672718398272991,
          0.03704703599214554,
          -0.03629479184746742,
          -0.004711700137704611,
          0.021008433774113655,
          0.004148395732045174,
          -0.03951381519436836,
          -0.016502372920513153
        ]
      },
      {
        "chunk_index": 622,
        "relative_chunk_number": 99,
        "text": "Our analysis is structured on a per-dataset basis to highlight the varying effectiveness of each few-shot selection approach under different data distributions and model capabilities. On the NodeMedic dataset, LFM and LFNN demonstrate sub- stantial performance gains over the zero-shot (ZS) and random few-shot (R-FS) baselines across all models.",
        "text_length": 346,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.020051971077919006,
          0.03632689639925957,
          -0.037627384066581726,
          0.03995198756456375,
          -0.0100519098341465,
          -0.05169404670596123,
          0.019527003169059753,
          0.011939025484025478,
          -0.03186628594994545,
          -0.015573058277368546
        ]
      },
      {
        "chunk_index": 623,
        "relative_chunk_number": 100,
        "text": "For the Gemma model, LFNN achieves the highest F1-score of 0.854, a significant improve- ment from the ZS baseline of 0.574. The LFM approach pushed the model to a perfect recall of 1.000 and achieved an F1-score of 0.840. The Qwen model shows a similar trend, with LFM and LFNN improving the F1-score to 0.837 and 0.814, respectively.",
        "text_length": 335,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02569306269288063,
          0.024685699492692947,
          -0.05682957544922829,
          -0.020561298355460167,
          -0.008004398085176945,
          -0.05975242331624031,
          0.022494925186038017,
          0.017562370747327805,
          -0.023732269182801247,
          0.026015613228082657
        ]
      },
      {
        "chunk_index": 624,
        "relative_chunk_number": 101,
        "text": "GPT-5-mini, a more powerful model, exhibits strong performance even with the R-FS baseline (F1-score of 0.860), but our adaptive methods still provide a slight edge. Overall, there was a good balance between accuracy and F1-score with combined method 1 and 3 , and LFNN (accuracy: 0.701, 0.722, 0.758 respectively).",
        "text_length": 315,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.018178604543209076,
          0.02814706414937973,
          -0.06130729243159294,
          0.01762380078434944,
          0.00027281418442726135,
          -0.02975655160844326,
          0.01728014647960663,
          -0.013371223583817482,
          -0.024538900703191757,
          -0.011718099005520344
        ]
      },
      {
        "chunk_index": 625,
        "relative_chunk_number": 102,
        "text": "The SVENP dataset also yields strong results, but with a key difference: the ZS baseline is notably more effective here, especially for Qwen (0.965 Precision) and GPT-5-mini (0.780 F1-score). This indicates that the patterns in SVENP align well with the models\u2019 pre-trained knowledge or maybe the models are in general better at analyzing python source code.",
        "text_length": 358,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.004292874131351709,
          0.007136840373277664,
          -0.03054099716246128,
          0.028447939082980156,
          0.03866693004965782,
          0.04203721135854721,
          0.021235300227999687,
          -0.006500984542071819,
          -0.023436974734067917,
          -0.023762453347444534
        ]
      },
      {
        "chunk_index": 626,
        "relative_chunk_number": 103,
        "text": "Despite the strong baseline, LFNN improves the performance by achieving top-tier F1-scores of 0.889 (GPT) and 0.775 (Qwen). For the Gemma model, LFNN boosts the F1-score from 0.565 (ZS) to 0.625, and accuracy from 0.587 (ZS) to 0.634. A critical observation is the behavior of LFM with the Qwen model; it again defaults to predicting the positive class for all instances (1.000 Recall).",
        "text_length": 386,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01949913240969181,
          0.01707393117249012,
          -0.055971045047044754,
          -0.008031724952161312,
          -0.009244824759662151,
          -0.03931509330868721,
          0.02008313499391079,
          0.024286875501275063,
          -0.021087417379021645,
          0.030343875288963318
        ]
      },
      {
        "chunk_index": 627,
        "relative_chunk_number": 104,
        "text": "This highlights LFM\u2019s tendency to act as a powerful bias amplifier, which is effective when correcting false negatives but can be overly simplistic and increase false positives.",
        "text_length": 177,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.038252267986536026,
          0.03805816173553467,
          -0.04165706783533096,
          0.009509568102657795,
          -0.04565037414431572,
          -0.0344417430460453,
          0.02056175097823143,
          0.0383475162088871,
          -0.04235124960541725,
          -0.008771812543272972
        ]
      },
      {
        "chunk_index": 628,
        "relative_chunk_number": 105,
        "text": "The Combined Methods (CM), particularly on GPT-5-mini, achieve the highest overall F1-scores (e.g., 0.892 for CM 2), demonstrating that integrating both mistake-based and similarity-based signals is optimal when the baseline performance is already high. The DiverseVul dataset presents a more complex challenge.",
        "text_length": 311,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.030602721497416496,
          0.00807688944041729,
          -0.036607589572668076,
          0.009348788298666477,
          -0.022901766002178192,
          -0.05945330485701561,
          0.017240893095731735,
          -0.044075146317481995,
          -0.0253179669380188,
          -0.04439342021942139
        ]
      },
      {
        "chunk_index": 629,
        "relative_chunk_number": 106,
        "text": "Here, the ZS baseline for Qwen completely fails, predicting the negative class for all samples and resulting in an F1-score of 0.000. In contrast, the Gemma ZS baseline is more reasonable, with an F1-score of 0.472. For the Qwen model, LFNN is the most effective individual strategy, raising the F1-score to 0.690. However, the most compelling finding on this dataset comes from the Gemma model.",
        "text_length": 395,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0007720887078903615,
          0.019747843965888023,
          -0.053797900676727295,
          0.0029874839819967747,
          0.038582295179367065,
          -0.027380969375371933,
          0.023426475003361702,
          -0.014131150208413601,
          -0.025037731975317,
          -0.02077830210328102
        ]
      },
      {
        "chunk_index": 630,
        "relative_chunk_number": 107,
        "text": "While LFM performs poorly (0.114 F1-score) and LFNN offers only a modest improvement (0.524 F1-score), the Combined Methods deliver the best performance. CM 1, CM 2, and CM 3 achieve F1- scores of 0.658, 0.667, and 0.663, with corresponding accuracies of 0.548, 0.515, and 0.499, respectively.",
        "text_length": 293,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.005761933047324419,
          0.0438825748860836,
          -0.05238671973347664,
          0.010176601819694042,
          -0.06857199966907501,
          -0.04718324914574623,
          0.015587784349918365,
          -0.048539455980062485,
          -0.023289041593670845,
          0.0030183708295226097
        ]
      },
      {
        "chunk_index": 631,
        "relative_chunk_number": 108,
        "text": "The robustness of the Combined Methods indicates that a blended approach is necessary to navigate the diverse patterns present in the data. The PrimeVul and SVENC datasets underscore the importance of F1-score over accuracy.",
        "text_length": 224,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010710552334785461,
          0.032823655754327774,
          0.0005257260054349899,
          0.025487512350082397,
          0.00020485975255724043,
          -0.03760126605629921,
          0.019930537790060043,
          -0.010570545680820942,
          -0.02311035431921482,
          0.011282198131084442
        ]
      },
      {
        "chunk_index": 632,
        "relative_chunk_number": 109,
        "text": "On both datasets, the Qwen model\u2019s ZS baseline fails by uniformly predicting the negative class (0.000 F1-score), while the LFM approach predictably does the opposite, classifying all samples as positive (0.667 F1-score). In both cases, the accuracy is a misleading 0.500, masking these divergent failure modes.",
        "text_length": 311,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.006834316533058882,
          0.0356660857796669,
          -0.02926446497440338,
          0.019292790442705154,
          -0.023982685059309006,
          -0.029582154005765915,
          0.022905349731445312,
          0.007512315176427364,
          -0.024119902402162552,
          -0.025278467684984207
        ]
      },
      {
        "chunk_index": 633,
        "relative_chunk_number": 110,
        "text": "A crucial observation from these two datasets is the signifi- cant degradation of the LFNN method. For Qwen, LFNN yields very low F1-scores of 0.104 on PrimeVul and 0.075 on SVENC. This is a stark contrast to its success on NodeMedic and SVENP. This fail- ure implies that for these datasets, the nearest neighbors examples are not helpful.",
        "text_length": 340,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.011210260912775993,
          0.03189585730433464,
          -0.026086153462529182,
          0.0028509271796792746,
          -0.03810500353574753,
          -0.038598161190748215,
          0.02160806953907013,
          0.0520000085234642,
          -0.020063843578100204,
          0.042198680341243744
        ]
      },
      {
        "chunk_index": 634,
        "relative_chunk_number": 111,
        "text": "Once again, the Combined Methods demonstrate greater resilience, particularly for the Gemma model. On SVENC, CM 2 elevates the F1-score to 0.671, the highest for that model. This pattern reinforces the conclusion that when simpler adaptive heuristics like LFNN fail, a more robust, multi-faceted example selection strategy is required to achieve better performance.",
        "text_length": 365,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.01477228943258524,
          0.0396096296608448,
          -0.04513809084892273,
          0.008398695848882198,
          -0.020300939679145813,
          -0.085931695997715,
          0.018940391018986702,
          0.0017691694665700197,
          -0.02279186248779297,
          0.03186136111617088
        ]
      },
      {
        "chunk_index": 635,
        "relative_chunk_number": 112,
        "text": "Finally, one interesting insight was hard examples (LFM) in con- text try to bias the model towards greater recall and lower precision. As CM3 applies LFM on LFNN examples keeping first LFM\u2019s result as context, it also keep that trend mostly. Quality of Vulnerability Detection Datasets. Ding et al.",
        "text_length": 299,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.052281130105257034,
          0.04100341349840164,
          -0.05637870728969574,
          0.023946380242705345,
          -0.024362655356526375,
          -0.002770259277895093,
          0.02248387597501278,
          0.05991503596305847,
          -0.025996968150138855,
          0.038289863616228104
        ]
      },
      {
        "chunk_index": 636,
        "relative_chunk_number": 113,
        "text": "[11] recently highlighted key challenges in existing vulnerability de- tection datasets, including label noise, data duplication, and data leakage. To mitigate these concerns, we incorporate the PrimeVul dataset in our experiments, as it was carefully curated to address such issues, along with several other commonly used datasets.",
        "text_length": 332,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01251025777310133,
          0.006773196626454592,
          0.010319802910089493,
          0.025684231892228127,
          0.03833876922726631,
          0.008642974309623241,
          0.019475707784295082,
          0.018830809742212296,
          -0.031814780086278915,
          -0.001600703806616366
        ]
      },
      {
        "chunk_index": 637,
        "relative_chunk_number": 114,
        "text": "We also include an additional unpublished dataset (NodeMedic) to demonstrate the generality of our approach. Beyond the noisiness of current vulnerability datasets, we acknowledge that function- level vulnerability detection has inherent limitations compared to repository-level detection [46].",
        "text_length": 294,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.023409593850374222,
          0.025232408195734024,
          -0.009404805488884449,
          0.034576281905174255,
          -0.017548099160194397,
          -0.024886082857847214,
          0.022195715457201004,
          0.004386684857308865,
          -0.0425015427172184,
          0.015496961772441864
        ]
      },
      {
        "chunk_index": 638,
        "relative_chunk_number": 115,
        "text": "However, we believe that function- level datasets and detectors provide a valuable first step toward addressing vulnerability detection in broader contexts. 6 Conclusion This work studies the effectiveness of different few-shot selection methods for large language models in code vulnerability detection.",
        "text_length": 304,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.015167436562478542,
          0.04692321643233299,
          -0.019088005647063255,
          0.01730559766292572,
          0.014121669344604015,
          -0.01014496386051178,
          0.02262420393526554,
          0.021113261580467224,
          -0.03788062185049057,
          0.0031526752281934023
        ]
      },
      {
        "chunk_index": 639,
        "relative_chunk_number": 116,
        "text": "We evaluated several common techniques and introduced combined methods built upon them, showing that while open-source models perform worse under baseline few-shot settings, they achieve sub- stantially greater improvements with our combined methods and can, in some cases, approach the performance of the closed-source model GPT-5-Mini. References [1] R. Agarwal, A. Singh, L. Zhang, B. Bohnet, L.",
        "text_length": 398,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.001025623525492847,
          0.018289029598236084,
          -0.004915174562484026,
          0.03725302964448929,
          0.01680617406964302,
          -0.03537904843688011,
          0.015575400553643703,
          -0.020403537899255753,
          -0.02486075833439827,
          -0.06545685231685638
        ]
      },
      {
        "chunk_index": 640,
        "relative_chunk_number": 117,
        "text": "Rosias, S. Chan, B. Zhang, A. Anand, Z. Abbas, A. Nova, et al. Many-shot in-context learning. NeurIPS, 37:76930\u201376966, 2024. [2] L. A. Agrawal, S. Tan, D. Soylu, N. Ziems, R. Khare, K. Opsahl-Ong, A. Singhvi, H. Shandilya, M. J. Ryan, M. Jiang, C. Potts, K. Sen, A. G. Dimakis, I. Stoica, D. Klein, M. Zaharia, and O. Khattab.",
        "text_length": 326,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0017861271044239402,
          0.0014168571215122938,
          -0.01974235288798809,
          0.04552212357521057,
          0.042105525732040405,
          -0.00279944553039968,
          0.008638529106974602,
          -0.014098494313657284,
          -0.012251040898263454,
          0.03379521518945694
        ]
      },
      {
        "chunk_index": 641,
        "relative_chunk_number": 118,
        "text": "Gepa: Reflective prompt evolution can outperform reinforcement learning, 2025. URL https://arxiv.org/abs/2507.19457. [3] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] T. Bansal, R. Jha, and A. McCallum. Learning to few-shot learn across diverse natural language classification tasks.",
        "text_length": 393,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00918584130704403,
          0.005373829044401646,
          -0.017266063019633293,
          0.030505359172821045,
          0.03568191081285477,
          -0.046882279217243195,
          0.014434798620641232,
          -0.007556961849331856,
          -0.026232004165649414,
          0.011181103996932507
        ]
      },
      {
        "chunk_index": 642,
        "relative_chunk_number": 119,
        "text": "arXiv preprint arXiv:1911.03863, 2019. [5] R. A. Bhope, P. Venkateswaran, K. Jayaram, V. Isahagian, V. Muthusamy, and N. Venkatasubramanian. Optiseq: Ordering examples on-the-fly for in-context learning. arXiv preprint arXiv:2501.15030, 2025. [6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakan- tan, P. Shyam, G. Sastry, A. Askell, et al.",
        "text_length": 371,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014729609712958336,
          0.0042543490417301655,
          -0.03273025155067444,
          0.054387014359235764,
          0.010940652340650558,
          0.019110489636659622,
          0.014793026261031628,
          0.010119832120835781,
          -0.02328796125948429,
          0.04017212986946106
        ]
      },
      {
        "chunk_index": 643,
        "relative_chunk_number": 120,
        "text": "Language models are few-shot learners. NeurIPS, 33:1877\u20131901, 2020. 7 Md Abdul Hannan, Ronghao Ni, Chi Zhang, Limin Jia, Ravi Mangal, and Corina S. Pasareanu [7] D. Cassel, W. T. Wong, and L. Jia. Nodemedic: End-to-end analysis of node. js vulnerabilities with provenance graphs. In IEEE S&P, 2023. [8] D. Cassel, N. Sabino, M.-C. Hsu, R. Martins, and L. Jia.",
        "text_length": 359,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0033073045779019594,
          -0.0070957522839307785,
          -0.0004055126046296209,
          0.030172061175107956,
          -0.034558575600385666,
          -0.027358882129192352,
          0.013719315640628338,
          0.0010172594338655472,
          -0.022957200184464455,
          0.04881695285439491
        ]
      },
      {
        "chunk_index": 644,
        "relative_chunk_number": 121,
        "text": "Nodemedic-fine: Automatic detection and exploit synthesis for node. js vulnerabilities. In Proceedings of NDSS\u201925, volume 10, 2025. [9] Y. Chen, Z. Ding, L. Alowain, X. Chen, and D. Wagner. Diversevul: A new vulnerable source code dataset for deep learning based vulnerability detection, 2023. URL https://arxiv.org/abs/2304.00409. [10] R. Croft, M. A. Babar, and M. M. Kholoosi.",
        "text_length": 379,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.018405688926577568,
          -0.0035472377203404903,
          -0.0004202821583021432,
          0.012134629301726818,
          -0.04954971745610237,
          -0.07657313346862793,
          0.01640840247273445,
          0.024223413318395615,
          -0.019872641190886497,
          0.06979986280202866
        ]
      },
      {
        "chunk_index": 645,
        "relative_chunk_number": 122,
        "text": "Data quality for software vulnerability datasets. In ICSE, 2023. [11] Y. Ding, Y. Fu, O. Ibrahim, C. Sitawarin, X. Chen, B. Alomair, D. Wagner, B. Ray, and Y. Chen. Vulnerability detection with code language models: How far are we? In ICSE, 2024. [12] X. Du, M. Wen, J. Zhu, Z. Xie, B. Ji, H. Liu, X. Shi, and H. Jin.",
        "text_length": 317,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.006765855010598898,
          0.024336092174053192,
          -0.031222544610500336,
          0.035792168229818344,
          -0.011973745189607143,
          0.023538673296570778,
          0.01303204707801342,
          0.00950994249433279,
          -0.025861063972115517,
          -0.012722034007310867
        ]
      },
      {
        "chunk_index": 646,
        "relative_chunk_number": 123,
        "text": "Generalization- enhanced code vulnerability detection via multi-task instruction fine-tuning. arXiv preprint arXiv:2406.03718, 2024. [13] X. Du, G. Zheng, K. Wang, Y. Zou, Y. Wang, W. Deng, J. Feng, M. Liu, B. Chen, X. Peng, et al. Vul-rag: Enhancing llm-based vulnerability detection via knowledge-level rag. arXiv preprint arXiv:2406.11147, 2024. [14] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.022239627316594124,
          0.014621889218688011,
          -0.01412790548056364,
          0.0043905796483159065,
          0.01252436451613903,
          -0.0057060145772993565,
          0.014228381216526031,
          -0.009676172398030758,
          -0.025570256635546684,
          0.001773769035935402
        ]
      },
      {
        "chunk_index": 647,
        "relative_chunk_number": 124,
        "text": "Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [15] D. Farr, K. Talty, A. Farr, J. Stockdale, I. Cruickshank, and J. West. Expert-in-the- loop systems with cross-domain and in-domain few-shot learning for software vulnerability detection. arXiv preprint arXiv:2506.10104, 2025. [16] Z. Feng, D. Guo, D. Tang, N.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.003189492505043745,
          0.003723845351487398,
          -0.03624666482210159,
          0.012653583660721779,
          0.03389505669474602,
          0.006424091290682554,
          0.012919525615870953,
          0.013082316145300865,
          -0.01963500678539276,
          -0.01186804287135601
        ]
      },
      {
        "chunk_index": 648,
        "relative_chunk_number": 125,
        "text": "Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155, 2020. [17] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu, et al. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366, 2020. [18] Q. Guo, L.",
        "text_length": 396,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.017171258106827736,
          0.023896286264061928,
          0.009527378715574741,
          -0.00133209815248847,
          0.02394114062190056,
          0.010602721013128757,
          0.014925586991012096,
          -0.00465437164530158,
          -0.018483806401491165,
          -0.01151791401207447
        ]
      },
      {
        "chunk_index": 649,
        "relative_chunk_number": 126,
        "text": "Wang, Y. Wang, W. Ye, and S. Zhang. What makes a good order of examples in in-context learning. In ACL, pages 14892\u201314904, 2024. [19] H. Hanif and S. Maffeis. Vulberta: Simplified source code pre-training for vul- nerability detection. In 2022 International joint conference on neural networks (IJCNN), pages 1\u20138. IEEE, 2022. [20] J. He and M. Vechev.",
        "text_length": 351,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0019636733923107386,
          0.019516119733452797,
          -0.02241370640695095,
          0.010862176306545734,
          -0.0208392646163702,
          0.036792099475860596,
          0.018482951447367668,
          -0.01805252581834793,
          -0.03019717149436474,
          0.023758891969919205
        ]
      },
      {
        "chunk_index": 650,
        "relative_chunk_number": 127,
        "text": "Large language models for code: Security hardening and adversarial testing. In CCS, CCS \u201923, page 1865\u20131879, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400700507. doi: 10.1145/ 3576915.3623175. URL https://doi.org/10.1145/3576915.3623175. [21] D. Hin, A. Kan, H. Chen, and M. A. Babar. Linevd: Statement-level vulnerability detection using graph neural networks.",
        "text_length": 390,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02212551049888134,
          -0.0075264619663357735,
          -0.024388547986745834,
          -0.0009638601914048195,
          -0.028869908303022385,
          0.01918279565870762,
          0.01558366883546114,
          -0.031049417331814766,
          -0.017980175092816353,
          0.010116745717823505
        ]
      },
      {
        "chunk_index": 651,
        "relative_chunk_number": 128,
        "text": "In Proceedings of the 19th international conference on mining software repositories, pages 596\u2013607, 2022. [22] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [23] R. A. Husein, H. Aburajouh, and C. Catal. Large language models for code completion: A systematic literature review.",
        "text_length": 397,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.01745002157986164,
          0.014558402821421623,
          0.0009743119589984417,
          0.0251383688300848,
          -0.01947406306862831,
          0.014103734865784645,
          0.013628470711410046,
          -0.03820188343524933,
          -0.025379126891493797,
          -0.03851155564188957
        ]
      },
      {
        "chunk_index": 652,
        "relative_chunk_number": 129,
        "text": "Computer Standards & Interfaces, 92: 103917, 2025. [24] J. Jiang, F. Wang, J. Shen, S. Kim, and S. Kim. A survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. [25] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8: 423\u2013438, 2020. [26] A. Khare, S. Dutta, Z.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.018805913627147675,
          0.009563514031469822,
          -0.015219474211335182,
          0.01602533645927906,
          0.013047191314399242,
          0.0024779997766017914,
          0.012654918245971203,
          -0.028846537694334984,
          -0.02627875655889511,
          0.007098940666764975
        ]
      },
      {
        "chunk_index": 653,
        "relative_chunk_number": 130,
        "text": "Li, A. Solko-Breslin, R. Alur, and M. Naik. Understanding the effectiveness of large language models in detecting security vulnerabilities. In ICST 2025, pages 103\u2013114. IEEE, 2025. [27] O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. Vardhamanan, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts.",
        "text_length": 344,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.021080372855067253,
          0.009772919118404388,
          -0.0237016249448061,
          0.016435081139206886,
          0.005193550605326891,
          0.02626211568713188,
          0.008327153511345387,
          -0.027832459658384323,
          -0.025779113173484802,
          -0.011563227511942387
        ]
      },
      {
        "chunk_index": 654,
        "relative_chunk_number": 131,
        "text": "Dspy: Compiling declarative language model calls into self-improving pipelines, 2023. URL https://arxiv.org/abs/2310.03714. [28] P. Li, S. Yao, J. S. Korich, C. Luo, J. Yu, Y. Cao, and J. Yang. Automated static vulnerability detection via a holistic neuro-symbolic approach. arXiv preprint arXiv:2504.16057, 2025. [29] Y. Li, S. Wang, and T. N. Nguyen.",
        "text_length": 352,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02660767175257206,
          0.0039037757087498903,
          -0.015015463344752789,
          -0.0028749010525643826,
          -0.02444896474480629,
          0.0002278718602610752,
          0.01444904413074255,
          5.1572009397204965e-05,
          -0.02207321673631668,
          0.03779299929738045
        ]
      },
      {
        "chunk_index": 655,
        "relative_chunk_number": 132,
        "text": "Vulnerability detection with fine-grained interpretations. In FSE, pages 292\u2013303, 2021. [30] Y. Li, N. T. Bui, T. Zhang, M. Weyssow, C. Yang, X. Zhou, J. Jiang, J. Chen, H. Huang, H. H. Nguyen, et al. Out of distribution, out of luck: How well can llms trained on vulnerability datasets detect top 25 cwe weaknesses? arXiv preprint arXiv:2507.21817, 2025. [31] Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03507166728377342,
          0.015264343470335007,
          -0.03742149472236633,
          0.010781673714518547,
          -0.008325731381773949,
          0.024983029812574387,
          0.014619770459830761,
          0.005805869121104479,
          -0.02367984689772129,
          0.03493848815560341
        ]
      },
      {
        "chunk_index": 656,
        "relative_chunk_number": 133,
        "text": "Wang, Z. Deng, and Y. Zhong. VulDeeP- ecker: A deep learning-based system for vulnerability detection. arXiv preprint arXiv:1801.01681, 2018. [32] Z. Li, D. Zou, S. Xu, H. Jin, Y. Zhu, and Z. Chen. Sysevr: A framework for using deep learning to detect software vulnerabilities. IEEE Transactions on Dependable and Secure Computing, 19(4):2244\u20132258, 2021. [33] J. Lin and D. Mohaisen.",
        "text_length": 383,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00030064547900110483,
          0.005934274289757013,
          -0.007451366633176804,
          -0.008632274344563484,
          -0.014598237350583076,
          -0.023993942886590958,
          0.017439670860767365,
          -0.015873797237873077,
          -0.010126536712050438,
          -0.04005344957113266
        ]
      },
      {
        "chunk_index": 657,
        "relative_chunk_number": 134,
        "text": "From large to mammoth: A comparative evaluation of large language models in vulnerability detection. In NDSS, 2025. [34] S. Lipp, S. Banescu, and A. Pretschner. An empirical study on the effectiveness of static c code analyzers for vulnerability detection. In ISSTA, 2022. [35] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes good in-context examples for gpt-3?",
        "text_length": 384,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01969260349869728,
          0.010512709617614746,
          -0.026878200471401215,
          0.03154313191771507,
          0.018896685913205147,
          -0.002783725969493389,
          0.019040687009692192,
          -0.03496064245700836,
          -0.029372213408350945,
          0.017890792340040207
        ]
      },
      {
        "chunk_index": 658,
        "relative_chunk_number": 135,
        "text": "arXiv preprint arXiv:2101.06804, 2021. [36] Y. Liu, R. Meng, S. Jot, S. Savarese, C. Xiong, Y. Zhou, and S. Yavuz. Codexembed: A generalist embedding model family for multiligual and multi-task code retrieval. arXiv preprint arXiv:2411.12644, 2024. [37] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp.",
        "text_length": 311,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010137097910046577,
          0.018389739096164703,
          -5.964034698990872e-06,
          0.02782469056546688,
          0.03358624875545502,
          -0.0019211482722312212,
          0.015166097320616245,
          -0.0185700636357069,
          -0.018570737913250923,
          -0.018310515210032463
        ]
      },
      {
        "chunk_index": 659,
        "relative_chunk_number": 136,
        "text": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In ACL, pages 8086\u20138098, 2022. [38] V. J. Man\u00e8s, H. Han, C. Han, S. K. Cha, M. Egele, E. J. Schwartz, and M. Woo. The art, science, and engineering of fuzzing: A survey. IEEE Transactions on Software Engineering, 47(11):2312\u20132331, 2021. doi: 10.1109/TSE.2019.2946563. [39] F. Marques, M. Ferreira, A.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01613869145512581,
          0.011727741919457912,
          -0.007605325896292925,
          0.049667518585920334,
          0.036150239408016205,
          -0.0302229393273592,
          0.011659796349704266,
          0.016810134053230286,
          -0.03064849227666855,
          0.030799394473433495
        ]
      },
      {
        "chunk_index": 660,
        "relative_chunk_number": 137,
        "text": "Nascimento, M. E. Coimbra, N. Santos, L. Jia, and J. Fragoso Santos. Automated exploit generation for node. js packages. Proceed- ings of the ACM on Programming Languages, 9(PLDI):1341\u20131366, 2025. [40] N. Medeiros, N. Ivaki, P. Costa, and M. Vieira. Vulnerable code detection using software metrics and machine learning. IEEE Access, 8:219174\u2013219198, 2020. [41] W. Melicher, C. Fung, L. Bauer, and L.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.017316533252596855,
          -0.021233459934592247,
          0.003042000811547041,
          0.0267492588609457,
          -0.021555015817284584,
          -0.005074869841337204,
          0.011191741563379765,
          -0.003832802874967456,
          -0.029833434149622917,
          0.05308365449309349
        ]
      },
      {
        "chunk_index": 661,
        "relative_chunk_number": 138,
        "text": "Jia. Towards a lightweight, hybrid approach for detecting dom xss vulnerabilities with machine learning. In Proceedings of the Web Conference 2021, WWW \u201921, 2021. [42] S. Min, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Noisy channel language model prompting for few-shot text classification. In ACL, pages 5316\u20135330, 2022. [43] J. Nam and S. Kim. Clami: Defect prediction on unlabeled datasets (t).",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.014533454552292824,
          -0.005225983448326588,
          -0.004796010907739401,
          0.0232286024838686,
          -0.011189958080649376,
          -0.004296588245779276,
          0.01591440476477146,
          0.02402164414525032,
          -0.03129793331027031,
          0.014681655913591385
        ]
      },
      {
        "chunk_index": 662,
        "relative_chunk_number": 139,
        "text": "In ASE, pages 452\u2013463. IEEE, 2015. [44] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng. Automatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023. [45] G. Ramesh, M. Sahil, S. A. Palan, D. Bhandary, T. A. Ashok, J. Shreyas, and N. Sowjanya. A review on nlp zero-shot and few-shot learning: methods and applications.",
        "text_length": 378,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.0074939364567399025,
          -0.01251547783613205,
          0.00447121262550354,
          0.035323262214660645,
          -0.03414361551403999,
          -0.02913273498415947,
          0.01204971969127655,
          -0.007626584265381098,
          -0.027635417878627777,
          -0.03162118420004845
        ]
      },
      {
        "chunk_index": 663,
        "relative_chunk_number": 140,
        "text": "Discover Applied Sciences, 7(9):1\u201322, 2025. [46] N. Risse, J. Liu, and M. B\u00f6hme. Top score on the wrong exam: On benchmarking in machine learning for vulnerability detection. (ISSTA):388\u2013410, 2025. [47] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code Llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.",
        "text_length": 396,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.045618508011102676,
          -8.336523751495406e-05,
          -0.007976213470101357,
          -0.019617155194282532,
          0.00032515914062969387,
          0.016634918749332428,
          0.012746531516313553,
          0.010219174437224865,
          -0.021100690588355064,
          0.02959519997239113
        ]
      },
      {
        "chunk_index": 664,
        "relative_chunk_number": 141,
        "text": "[48] O. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learning. In ACL, pages 2655\u20132671, 2022. [49] A. Sabbatella, A. Ponti, I. Giordani, A. Candelieri, and F. Archetti. Prompt optimization in large language models. Mathematics, 12(6):929, 2024. [50] R. Safdar, D. Mateen, S. T. Ali, M. U. Ashfaq, and W. Hussain.",
        "text_length": 344,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.017960935831069946,
          -0.0027883686125278473,
          -0.005741629283875227,
          0.04447474703192711,
          0.027888089418411255,
          -0.007845544256269932,
          0.010731598362326622,
          -0.013058249838650227,
          -0.027145758271217346,
          0.015620903111994267
        ]
      },
      {
        "chunk_index": 665,
        "relative_chunk_number": 142,
        "text": "Data and context matter: Towards generalizing ai-based software vulnerability detection. arXiv preprint arXiv:2508.16625, 2025. [51] B. Steenhoek, M. M. Rahman, R. Jiles, and W. Le. An empirical study of deep learning models for vulnerability detection. In ICSE, 2023. [52] B. Steenhoek, H. Gao, and W. Le. Dataflow analysis-inspired deep learning for efficient vulnerability detection.",
        "text_length": 386,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0036712181754410267,
          0.021820392459630966,
          -0.0362727977335453,
          0.016939425840973854,
          -0.024175425991415977,
          -0.03344695270061493,
          0.019837111234664917,
          -0.002737615490332246,
          -0.029098540544509888,
          0.016260048374533653
        ]
      },
      {
        "chunk_index": 666,
        "relative_chunk_number": 143,
        "text": "In ICSE, 2024. [53] W. Sun, Y. Miao, Y. Li, H. Zhang, C. Fang, Y. Liu, G. Deng, Y. Liu, and Z. Chen. Source code summarization in the era of large language models. In ICSE, 2025. [54] G. Team. Gemma 3. 2025. URL https://goo.gle/Gemma3Report. [55] Q. Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [56] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.008413583040237427,
          0.015380325727164745,
          -0.02216561883687973,
          0.037646304816007614,
          0.010106900706887245,
          0.0036648879759013653,
          0.01184911746531725,
          -0.010430407710373402,
          -0.02735275961458683,
          -0.014178656041622162
        ]
      },
      {
        "chunk_index": 667,
        "relative_chunk_number": 144,
        "text": "Lacroix, B. Roz- i\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [57] Y. Wan, Z. Bi, Y. He, J. Zhang, H. Zhang, Y. Sui, G. Xu, H. Jin, and P. Yu. Deep learning for code intelligence: Survey, benchmark and toolkit. ACM Computing Surveys, 56(12):1\u201341, 2024. [58] W. Wang, P. Kattakinda, and S. Feizi.",
        "text_length": 387,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007288024760782719,
          -0.01428882498294115,
          -0.02635527215898037,
          0.00858516339212656,
          -0.020008910447359085,
          -0.00011361236829543486,
          0.011842591688036919,
          0.0032901400700211525,
          -0.020231196656823158,
          -0.02536650374531746
        ]
      },
      {
        "chunk_index": 668,
        "relative_chunk_number": 145,
        "text": "Maestro: Joint graph & config optimization for reliable ai agents, 2025. URL https://arxiv.org/abs/2509.04642. [59] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 35:24824\u201324837, 2022. [60] B. Xu, Q. Wang, Z. Mao, Y. Lyu, Q. She, and Y. Zhang.",
        "text_length": 360,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.011703850701451302,
          0.013547929003834724,
          -0.006777968257665634,
          0.03274444490671158,
          -0.002969395602121949,
          -0.008619710803031921,
          0.01194817665964365,
          -0.011812114156782627,
          -0.01881732977926731,
          -0.004376663826406002
        ]
      },
      {
        "chunk_index": 669,
        "relative_chunk_number": 146,
        "text": "\ud835\udc58nn prompting: Beyond- context learning with calibration-free nearest neighbor inference. arXiv preprint arXiv:2303.13824, 2023. [61] B. Xu, A. Yang, J. Lin, Q. Wang, C. Zhou, Y. Zhang, and Z. Mao. Expertprompting: Instructing large language models to be distinguished experts. arXiv preprint arXiv:2305.14688, 2023. [62] A. S. Yadav and J. N. Wilson.",
        "text_length": 351,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.027346622198820114,
          0.018185103312134743,
          -0.020342223346233368,
          0.052002377808094025,
          -0.01745351403951645,
          0.022693417966365814,
          0.017045961692929268,
          -0.015784243121743202,
          -0.028076190501451492,
          0.0552753247320652
        ]
      },
      {
        "chunk_index": 670,
        "relative_chunk_number": 147,
        "text": "R+ r: Security vulnerability dataset quality is critical. In ACSAC, pages 1047\u20131061. IEEE, 2024. [63] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [64] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, et al. Qwen3 technical report.",
        "text_length": 380,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.021246880292892456,
          0.03554776683449745,
          0.010091032832860947,
          0.02473553642630577,
          0.008327179588377476,
          -0.02420204132795334,
          0.014686957001686096,
          -0.0076384334824979305,
          -0.017124393954873085,
          -0.00022622168762609363
        ]
      },
      {
        "chunk_index": 671,
        "relative_chunk_number": 148,
        "text": "arXiv preprint arXiv:2505.09388, 2025. [65] A. Z. Yang, H. Tian, H. Ye, R. Martins, and C. L. Goues. Security vulnerability detection with multitask self-instructed fine-tuning of large language models. arXiv preprint arXiv:2406.05892, 2024. 8 On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection [66] T. Zheng, H. Liu, H. Xu, X. Chen, P. Yi, and Y. Wu.",
        "text_length": 374,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009347278624773026,
          0.0009803209686651826,
          -0.040641143918037415,
          0.006351475138217211,
          -0.0012079916195943952,
          0.002379097742959857,
          0.017443306744098663,
          -0.0028745944146066904,
          -0.020974164828658104,
          0.01548431720584631
        ]
      },
      {
        "chunk_index": 672,
        "relative_chunk_number": 149,
        "text": "Few-vuld: A few-shot learning framework for software vulnerability detection. Computers & Security, 144: 103992, 2024. [67] X. Zhou, S. Cao, X. Sun, and D. Lo. Large language model for vulnerability detection and repair: Literature review and the road ahead. ACM Transactions on Software Engineering and Methodology, 34(5):1\u201331, 2025. [68] Y. Zhou, S. Liu, J. Siow, X. Du, and Y. Liu.",
        "text_length": 384,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.004753647837787867,
          0.02205674909055233,
          0.001241411198861897,
          0.0018735979683697224,
          0.03637225180864334,
          -0.05005783587694168,
          0.012930504977703094,
          -0.010150710120797157,
          -0.016630833968520164,
          -0.0014716873411089182
        ]
      },
      {
        "chunk_index": 673,
        "relative_chunk_number": 150,
        "text": "Devign: Effective vulnerability identifi- cation by learning comprehensive program semantics via graph neural networks. NeurIPS, 32, 2019. A Prompt Used for Vulnerability Detection The prompt shown here is used when running LFM as well as for evaluating the vulnerability detection capabilities of the LLM for the experiments reported in Section 5.",
        "text_length": 348,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.011040943674743176,
          0.0010582514805719256,
          -0.015870902687311172,
          -0.0007266808534041047,
          -0.06397098302841187,
          -0.025902342051267624,
          0.024323858320713043,
          0.01576041989028454,
          -0.02385098673403263,
          0.0220508873462677
        ]
      },
      {
        "chunk_index": 674,
        "relative_chunk_number": 151,
        "text": "To instruct the model clearly, we provided a concise and explicit system instruction, guiding the model to behave strictly as a security expert and to output responses in a standardized format. The exact system-level instruction used to prime the LLM is defined as follows: You are a security expert that is good at static program analysis.",
        "text_length": 340,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.024672064930200577,
          0.01846030168235302,
          -0.022592922672629356,
          0.007130065001547337,
          0.015186003409326077,
          0.03585664555430412,
          0.02291908673942089,
          -0.002807761076837778,
          -0.035649336874485016,
          -0.021331721916794777
        ]
      },
      {
        "chunk_index": 675,
        "relative_chunk_number": 152,
        "text": "First, you will be given some examples of vulnerable and non-vulnerable codes indicated through Yes and No. There can be no examples too.You will be given a piece of code. Your task is to analyze whether it contains a security vulnerability. Please only reply with one of the following options: (1) YES: A security vulnerability detected. (2) NO: No security vulnerability.",
        "text_length": 373,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.016554297879338264,
          0.05395795404911041,
          -0.007765650749206543,
          0.03640976920723915,
          -0.007979716174304485,
          -0.006419896613806486,
          0.019799362868070602,
          -0.012064780108630657,
          -0.030386867001652718,
          0.002239574445411563
        ]
      },
      {
        "chunk_index": 676,
        "relative_chunk_number": 153,
        "text": "Only reply with one of the options above. Do not include any further information. For each code snippet \ud835\udc65, we first construct a few-shot set S con- taining representative examples of code snippets, each annotated as either vulnerable or non-vulnerable. These labeled examples serve as context to guide the model\u2019s decision by explicitly illustrating the desired behavior.",
        "text_length": 371,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.016346869990229607,
          0.04906916990876198,
          -0.006048187613487244,
          0.06436581909656525,
          0.06761179864406586,
          0.000167185440659523,
          0.02116229385137558,
          0.004305946175009012,
          -0.032595302909612656,
          0.017122404649853706
        ]
      },
      {
        "chunk_index": 677,
        "relative_chunk_number": 154,
        "text": "The few-shot prompt we use is as follows (System, User, Assistant refer to the roles used for prompting the model): System: (System Instruction from above) User: Code: <Example 1: Vulnerable code snippet> Answer: Assistant: YES User: Code: <Example 2: Non-vulnerable code snippet> Answer: Assistant: NO ...",
        "text_length": 306,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.017652759328484535,
          0.011950606480240822,
          -0.01731487363576889,
          0.03207974508404732,
          -0.004455273039638996,
          -0.00025616740458644927,
          0.02039690688252449,
          0.0011053882772102952,
          -0.031195806339383125,
          -0.05618996545672417
        ]
      },
      {
        "chunk_index": 678,
        "relative_chunk_number": 155,
        "text": "(remaining few-shot examples) User: Code: {code} Answer: B Full Results with Standard Deviation An extended version of Table 1 with standard deviation is shown in Table 2. A key observation is the general consistency of the methods\u2014most exhibit low standard deviations. Table 2 reinforces the reliability of the mean results discussed in the main text.",
        "text_length": 352,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.017607908695936203,
          0.015224938280880451,
          -0.04517349228262901,
          0.04461447522044182,
          0.006332057062536478,
          0.001929288962855935,
          0.01864667795598507,
          0.049931760877370834,
          -0.034232884645462036,
          -0.06196906790137291
        ]
      },
      {
        "chunk_index": 679,
        "relative_chunk_number": 156,
        "text": "As anticipated, the R-FS approach consistently displayed the highest variance, underscoring its sensitivity to the random selec- tion of examples. This is particularly evident on datasets like SVENC with the Qwen model, where the Recall was 0.422 (\u00b10.373). In con- trast, the failure cases of certain baselines and methods proved to be remarkably deterministic.",
        "text_length": 361,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.039989717304706573,
          0.04146948084235191,
          -0.06109068542718887,
          0.022425979375839233,
          0.009668756276369095,
          -0.06158623471856117,
          0.020017538219690323,
          0.04164557904005051,
          -0.022061480209231377,
          0.028957750648260117
        ]
      },
      {
        "chunk_index": 680,
        "relative_chunk_number": 157,
        "text": "For instance, the LFM method\u2019s tendency to uniformly predict the positive class on datasets like NodeMedic is confirmed by its perfect recall with zero deviation (1.000 \u00b10.000) for both Gemma and Qwen models. Furthermore, the stability of our more successful methods lends additional credence to their effectiveness.",
        "text_length": 316,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009029367938637733,
          0.029965946450829506,
          -0.06351937353610992,
          -0.0014603468589484692,
          -0.06712021678686142,
          -0.02743406593799591,
          0.019312022253870964,
          0.0414067767560482,
          -0.02885274961590767,
          -0.013653242029249668
        ]
      },
      {
        "chunk_index": 681,
        "relative_chunk_number": 158,
        "text": "The LFNN approach on NodeMedic not only achieved a high F1-score (0.854 for Gemma) but did so with minimal variance (\u00b10.002). Similarly, the Combined Methods on the DiverseVul dataset demonstrated lower variance than R-FS, suggesting that their improved performance is not an artifact of random chance.",
        "text_length": 302,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0033673152793198824,
          0.021673180162906647,
          -0.04304591938853264,
          0.02780473791062832,
          -0.07647240161895752,
          -0.037444934248924255,
          0.019214898347854614,
          0.024798430502414703,
          -0.024945644661784172,
          0.0026622244622558355
        ]
      },
      {
        "chunk_index": 682,
        "relative_chunk_number": 159,
        "text": "C Different Variants of Learn-from-Mistakes (LFM) To explore the impact of different LFM variants, we run LFM with the following different parameter settings: \u2022 S-1\u00d7: \ud835\udc5b= 20,\ud835\udc60\ud835\udc61= TRUE,\ud835\udc58= 1,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3c \u2022 U-1\u00d7: \ud835\udc5b= 20,\ud835\udc60\ud835\udc61= FALSE,\ud835\udc58= 1,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3c \u2022 U-m\u00d7 (inc.): \ud835\udc5b= 20,\ud835\udc60\ud835\udc61= FALSE,\ud835\udc58= 5,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3c \u2022 U-m\u00d7 (corr.):\ud835\udc5b= 20,\ud835\udc60\ud835\udc61= FALSE,\ud835\udc58= 5,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc36 \u2022 U-m\u00d7 (gray):\ud835\udc5b= 20,\ud835\udc60\ud835\udc61= FALSE,\ud835\udc58= 5,\ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3a For all the unstacked variations, the initial few-shot set S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61has 20 examples that are randomly drawn from the train dataset while S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61is the empty set for the stacked version.",
        "text_length": 537,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.028117138892412186,
          0.024851134046912193,
          -0.053114183247089386,
          0.01738453283905983,
          0.017950618639588356,
          0.011018902994692326,
          0.017199313268065453,
          0.052897945046424866,
          -0.03215949982404709,
          0.00757101783528924
        ]
      },
      {
        "chunk_index": 683,
        "relative_chunk_number": 160,
        "text": "Table 3 shows a comprehensive breakdown of performance across different LFM configurations. It is evident that the method\u2019s effectiveness is highly sensitive to its parameterization and the specific characteristics of the dataset. First, we have already stated that the default LFM method used in our main experiments, S-1x (inc.), consistently induces a strong bias towards positive predictions.",
        "text_length": 396,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.06368951499462128,
          0.04414597153663635,
          -0.047747332602739334,
          0.005809285212308168,
          -0.04455249384045601,
          -0.04590185731649399,
          0.01687454804778099,
          0.01328488439321518,
          -0.03385235741734505,
          -0.02065642923116684
        ]
      },
      {
        "chunk_index": 684,
        "relative_chunk_number": 161,
        "text": "This is most evident with the Qwen model, where it achieves a perfect 1.000 recall four times. However, this perfect recall on balanced datasets results in an uninformative accuracy of 0.500 and a misleadingly high F1-score of 0.667. For Gemma, this method shows highly divergent performance. It is effective on NodeMedic, achieving both high accuracy (0.725) and a high F1-score (0.840).",
        "text_length": 388,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02646714448928833,
          0.007306311745196581,
          -0.06554828584194183,
          -0.004527134820818901,
          -0.016085775569081306,
          -0.04654739797115326,
          0.021487904712557793,
          0.01019955426454544,
          -0.029416007921099663,
          0.018900418654084206
        ]
      },
      {
        "chunk_index": 685,
        "relative_chunk_number": 162,
        "text": "Yet, on SVENC, it shows the biased trend: 0.667 F1-score with 0.500 accuracy. On DiverseVul and PrimeVul, it fails on both metrics, with F1-scores of 0.114 and 0.336, respectively. This confirms that it often trades all precision for recall.",
        "text_length": 241,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.031211981549859047,
          0.01720012165606022,
          -0.05642656236886978,
          0.017223665490746498,
          0.010557926259934902,
          -0.04505511745810509,
          0.019872713834047318,
          0.0058915382251143456,
          -0.02206607721745968,
          0.037031710147857666
        ]
      },
      {
        "chunk_index": 686,
        "relative_chunk_number": 163,
        "text": "Second, the comparison between stacked (S-1x) and unstacked (U-1x) methods highlights the critical impact of the initial prompt 9 Md Abdul Hannan, Ronghao Ni, Chi Zhang, Limin Jia, Ravi Mangal, and Corina S. Pasareanu Table 2: Full results with standard deviation for all approaches across three models.",
        "text_length": 303,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.0031690362375229597,
          0.04792182892560959,
          -0.017617247998714447,
          0.04278269410133362,
          0.03412763401865959,
          -0.03515147790312767,
          0.01640170067548752,
          0.010069071315228939,
          -0.0325186550617218,
          -0.000287919508991763
        ]
      },
      {
        "chunk_index": 687,
        "relative_chunk_number": 164,
        "text": "Each cell reports the mean and standard deviation over five runs for Gemma and Qwen models, while GPT results are based on a single run and do not have standard deviation. For the approach names, the following abbreviations are used: ZS = zero-shot, R-FS = random few-shot, LFM = Learn-from-Mistakes, LFNN = Learn-from-Nearest-Neighbors, CM = Combined Method.",
        "text_length": 359,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010911465622484684,
          0.03400980308651924,
          -0.028423311188817024,
          0.02648742124438286,
          -0.02786923758685589,
          0.01199889462441206,
          0.0222418662160635,
          0.060363635420799255,
          -0.026332661509513855,
          -0.030779922381043434
        ]
      },
      {
        "chunk_index": 688,
        "relative_chunk_number": 165,
        "text": "A dash (\u2018\u2013\u2019) indicates that the metric is undefined in at least one of the runs due to division by zero.",
        "text_length": 104,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.05209682881832123,
          0.010395972058176994,
          -0.025324728339910507,
          0.040347881615161896,
          -0.026021210476756096,
          -0.06061352789402008,
          0.019923148676753044,
          -0.04066537320613861,
          -0.026541996747255325,
          0.008716832846403122
        ]
      },
      {
        "chunk_index": 689,
        "relative_chunk_number": 166,
        "text": "Dataset Approach Gemma-3-4b-it GPT-5-mini Qwen-2.5-Coder Acc Prec Recall F1 Acc Prec Recall F1 Acc Prec Recall F1 DiverseVul ZS 0.619 (\u00b10.003) 0.770 (\u00b10.009) 0.340 (\u00b10.000) 0.472 (\u00b10.002) 0.593 0.588 0.627 0.607 0.497 (\u00b10.000) 0.000 (\u00b10.000) 0.000 (\u00b10.000) 0.000 (\u00b10.000) R-FS 0.545 (\u00b10.053) 0.571 (\u00b10.081) 0.467 (\u00b10.210) 0.480 (\u00b10.132) 0.560 0.598 0.367 0.455 0.599 (\u00b10.045) 0.741 (\u00b10.027) 0.303 (\u00b10.139) 0.411 (\u00b10.150) LFM 0.525 (\u00b10.003) 0.840 (\u00b10.056) 0.061 (\u00b10.005) 0.114 (\u00b10.009) 0.617 0.647 0.513 0.573 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) LFNN 0.512 (\u00b10.004) 0.511 (\u00b10.004) 0.537 (\u00b10.009) 0.524 (\u00b10.006) 0.590 0.624 0.453 0.525 0.659 (\u00b10.009) 0.632 (\u00b10.007) 0.759 (\u00b10.014) 0.690 (\u00b10.009) CM 1 0.548 (\u00b10.020) 0.530 (\u00b10.015) 0.868 (\u00b10.035) 0.658 (\u00b10.007) 0.587 0.610 0.480 0.537 0.638 (\u00b10.008) 0.606 (\u00b10.011) 0.792 (\u00b10.039) 0.686 (\u00b10.011) CM 2 0.515 (\u00b10.002) 0.508 (\u00b10.001) 0.973 (\u00b10.000) 0.667 (\u00b10.001) 0.560 0.573 0.473 0.518 0.597 (\u00b10.006) 0.557 (\u00b10.003) 0.949 (\u00b10.008) 0.702 (\u00b10.004) CM 3 0.499 (\u00b10.002) 0.499 (\u00b10.001) 0.985 (\u00b10.007) 0.663 (\u00b10.002) 0.570 0.585 0.480 0.527 0.531 (\u00b10.021) 0.517 (\u00b10.012) 0.988 (\u00b10.015) 0.678 (\u00b10.007) NodeMedic ZS 0.506 (\u00b10.003) 0.765 (\u00b10.005) 0.460 (\u00b10.000) 0.574 (\u00b10.001) 0.757 0.786 0.912 0.845 0.375 (\u00b10.009) 0.870 (\u00b10.049) 0.162 (\u00b10.005) 0.273 (\u00b10.008) R-FS 0.687 (\u00b10.060) 0.726 (\u00b10.003) 0.914 (\u00b10.136) 0.804 (\u00b10.060) 0.794 0.845 0.876 0.860 0.632 (\u00b10.060) 0.748 (\u00b10.020) 0.750 (\u00b10.163) 0.739 (\u00b10.071) LFM 0.725 (\u00b10.000) 0.725 (\u00b10.000) 1.000 (\u00b10.000) 0.840 (\u00b10.000) 0.767 0.789 0.927 0.852 0.720 (\u00b10.000) 0.723 (\u00b10.000) 0.993 (\u00b10.000) 0.837 (\u00b10.000) LFNN 0.758 (\u00b10.004) 0.759 (\u00b10.003) 0.975 (\u00b10.004) 0.854 (\u00b10.002) 0.788 0.849 0.861 0.855 0.713 (\u00b10.011) 0.768 (\u00b10.004) 0.866 (\u00b10.014) 0.814 (\u00b10.008) CM 1 0.701 (\u00b10.005) 0.755 (\u00b10.009) 0.870 (\u00b10.018) 0.808 (\u00b10.004) 0.788 0.839 0.876 0.857 0.751 (\u00b10.008) 0.776 (\u00b10.004) 0.923 (\u00b10.020) 0.843 (\u00b10.007) CM 2 0.565 (\u00b10.008) 0.719 (\u00b10.005) 0.657 (\u00b10.008) 0.686 (\u00b10.006) 0.794 0.840 0.883 0.861 0.735 (\u00b10.006) 0.748 (\u00b10.004) 0.958 (\u00b10.003) 0.840 (\u00b10.003) CM 3 0.722 (\u00b10.007) 0.728 (\u00b10.003) 0.984 (\u00b10.014) 0.837 (\u00b10.005) 0.794 0.846 0.883 0.864 0.720 (\u00b10.000) 0.724 (\u00b10.001) 0.991 (\u00b10.003) 0.837 (\u00b10.000) PrimeVul ZS 0.599 (\u00b10.044) 0.723 (\u00b10.106) 0.358 (\u00b10.036) 0.472 (\u00b10.003) 0.535 0.524 0.750 0.617 0.500 (\u00b10.000) \u2013 0.002 (\u00b10.004) 0.004 (\u00b10.008) R-FS 0.522 (\u00b10.041) 0.553 (\u00b10.086) 0.373 (\u00b10.148) 0.422 (\u00b10.101) 0.560 0.556 0.600 0.577 0.503 (\u00b10.010) 0.528 (\u00b10.058) 0.232 (\u00b10.105) 0.304 (\u00b10.094) LFM 0.516 (\u00b10.013) 0.726 (\u00b10.186) 0.437 (\u00b10.459) 0.336 (\u00b10.270) 0.550 0.557 0.490 0.521 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) LFNN 0.527 (\u00b10.030) 0.520 (\u00b10.017) 0.648 (\u00b10.147) 0.571 (\u00b10.064) 0.555 0.549 0.620 0.582 0.483 (\u00b10.009) 0.390 (\u00b10.056) 0.060 (\u00b10.009) 0.104 (\u00b10.015) CM 1 0.543 (\u00b10.032) 0.527 (\u00b10.022) 0.885 (\u00b10.040) 0.659 (\u00b10.010) 0.575 0.577 0.600 0.588 0.473 (\u00b10.007) 0.462 (\u00b10.010) 0.336 (\u00b10.092) 0.383 (\u00b10.057) CM 2 0.516 (\u00b10.001) 0.508 (\u00b10.001) 0.964 (\u00b10.022) 0.666 (\u00b10.005) 0.555 0.549 0.610 0.578 0.455 (\u00b10.012) 0.445 (\u00b10.015) 0.370 (\u00b10.026) 0.404 (\u00b10.021) CM 3 0.500 (\u00b10.002) 0.500 (\u00b10.001) 0.982 (\u00b10.017) 0.663 (\u00b10.004) 0.555 0.546 0.650 0.594 0.499 (\u00b10.006) 0.499 (\u00b10.003) 0.986 (\u00b10.015) 0.663 (\u00b10.005) SVENC ZS 0.478 (\u00b10.000) 0.474 (\u00b10.000) 0.400 (\u00b10.000) 0.434 (\u00b10.000) 0.533 0.523 0.756 0.618 0.500 (\u00b10.000) \u2013 0.000 (\u00b10.000) 0.000 (\u00b10.000) R-FS 0.506 (\u00b10.010) 0.503 (\u00b10.005) 0.778 (\u00b10.347) 0.564 (\u00b10.174) 0.600 0.579 0.733 0.647 0.491 (\u00b10.013) \u2013 0.422 (\u00b10.373) 0.345 (\u00b10.274) LFM 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) 0.544 0.543 0.556 0.549 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) LFNN 0.471 (\u00b10.009) 0.428 (\u00b10.034) 0.187 (\u00b10.044) 0.258 (\u00b10.049) 0.567 0.554 0.689 0.614 0.504 (\u00b10.009) 0.567 (\u00b10.133) 0.040 (\u00b10.009) 0.075 (\u00b10.017) CM 1 0.520 (\u00b10.008) 0.513 (\u00b10.005) 0.813 (\u00b10.027) 0.629 (\u00b10.010) 0.578 0.566 0.667 0.612 0.520 (\u00b10.019) 0.527 (\u00b10.024) 0.387 (\u00b10.074) 0.442 (\u00b10.055) CM 2 0.536 (\u00b10.004) 0.520 (\u00b10.003) 0.947 (\u00b10.011) 0.671 (\u00b10.002) 0.589 0.574 0.689 0.626 0.493 (\u00b10.005) 0.495 (\u00b10.004) 0.636 (\u00b10.011) 0.556 (\u00b10.005) CM 3 0.500 (\u00b10.000) 0.500 (\u00b10.000) 0.938 (\u00b10.029) 0.652 (\u00b10.007) 0.611 0.596 0.689 0.639 0.496 (\u00b10.005) 0.498 (\u00b10.003) 0.982 (\u00b10.009) 0.661 (\u00b10.004) SVENP ZS 0.587 (\u00b10.006) 0.597 (\u00b10.007) 0.537 (\u00b10.013) 0.565 (\u00b10.009) 0.763 0.727 0.842 0.780 0.705 (\u00b10.011) 0.965 (\u00b10.028) 0.426 (\u00b10.020) 0.591 (\u00b10.019) R-FS 0.558 (\u00b10.038) 0.561 (\u00b10.044) 0.632 (\u00b10.117) 0.584 (\u00b10.035) 0.789 0.806 0.763 0.784 0.616 (\u00b10.027) 0.707 (\u00b10.071) 0.432 (\u00b10.144) 0.514 (\u00b10.102) LFM 0.500 (\u00b10.000) 0.500 (\u00b10.000) 0.905 (\u00b10.013) 0.644 (\u00b10.003) 0.776 0.744 0.842 0.790 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) LFNN 0.634 (\u00b10.005) 0.641 (\u00b10.004) 0.611 (\u00b10.011) 0.625 (\u00b10.007) 0.895 0.941 0.842 0.889 0.792 (\u00b10.005) 0.845 (\u00b10.002) 0.716 (\u00b10.011) 0.775 (\u00b10.007) CM 1 0.553 (\u00b10.012) 0.546 (\u00b10.009) 0.616 (\u00b10.043) 0.579 (\u00b10.023) 0.882 0.892 0.868 0.880 0.787 (\u00b10.023) 0.793 (\u00b10.034) 0.779 (\u00b10.021) 0.785 (\u00b10.019) CM 2 0.632 (\u00b10.000) 0.639 (\u00b10.000) 0.605 (\u00b10.000) 0.622 (\u00b10.000) 0.895 0.917 0.868 0.892 0.697 (\u00b10.019) 0.660 (\u00b10.016) 0.816 (\u00b10.029) 0.729 (\u00b10.018) CM 3 0.521 (\u00b10.013) 0.512 (\u00b10.008) 0.942 (\u00b10.045) 0.663 (\u00b10.009) 0.803 0.795 0.816 0.805 0.600 (\u00b10.018) 0.558 (\u00b10.012) 0.963 (\u00b10.027) 0.707 (\u00b10.010) 10 On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection Table 3: Performance comparison of different LFM variations.",
        "text_length": 5454,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.005582412704825401,
          0.010694683529436588,
          0.00015405405429191887,
          0.03257843852043152,
          0.024666670709848404,
          -0.015661805868148804,
          0.002975902985781431,
          0.02826300635933876,
          0.001342360395938158,
          0.06308723241090775
        ]
      },
      {
        "chunk_index": 690,
        "relative_chunk_number": 167,
        "text": "Each cell shows the mean and standard deviation over five runs. For the LFM variation names, the following abbreviations are used: S = stacked, U = unstacked, 1\u00d7 = one iteration, m\u00d7 = more than one iteration, inc. = incorrect, corr. = correct, and gray = gray cases. \u2018\u2013\u2019 indicates metrics that are undefined due to division by zero.",
        "text_length": 332,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00959768332540989,
          0.027549806982278824,
          -0.04205060377717018,
          0.012504588812589645,
          -0.03326389566063881,
          0.0232990775257349,
          0.022895434871315956,
          0.06435231864452362,
          -0.023723771795630455,
          -0.01488546933978796
        ]
      },
      {
        "chunk_index": 691,
        "relative_chunk_number": 168,
        "text": "Dataset LFM Variations Gemma-3-4b-it Qwen-2.5-Coder Accuracy Precision Recall F1 Score Accuracy Precision Recall F1 Score DiverseVul S-1\u00d7 (inc.) 0.525 (\u00b10.003) 0.840 (\u00b10.056) 0.061 (\u00b10.005) 0.114 (\u00b10.009) 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) U-1\u00d7 (inc.) 0.502 (\u00b10.013) 0.501 (\u00b10.007) 0.940 (\u00b10.025) 0.654 (\u00b10.010) 0.502 (\u00b10.016) 0.501 (\u00b10.010) 0.844 (\u00b10.063) 0.628 (\u00b10.022) U-m\u00d7 (inc.) 0.507 (\u00b10.010) 0.504 (\u00b10.005) 0.995 (\u00b10.008) 0.669 (\u00b10.003) 0.527 (\u00b10.039) 0.516 (\u00b10.025) 0.976 (\u00b10.045) 0.674 (\u00b10.009) U-m\u00d7 (corr.) 0.597 (\u00b10.053) 0.778 (\u00b10.056) 0.295 (\u00b10.189) 0.383 (\u00b10.204) 0.621 (\u00b10.065) 0.752 (\u00b10.027) 0.365 (\u00b10.199) 0.455 (\u00b10.199) U-m\u00d7 (gray) 0.513 (\u00b10.000) 0.667 (\u00b10.000) 0.053 (\u00b10.000) 0.099 (\u00b10.000) 0.598 (\u00b10.061) 0.713 (\u00b10.059) 0.341 (\u00b10.226) 0.415 (\u00b10.199) NodeMedic S-1\u00d7 (inc.) 0.725 (\u00b10.000) 0.725 (\u00b10.000) 1.000 (\u00b10.000) 0.840 (\u00b10.000) 0.720 (\u00b10.000) 0.723 (\u00b10.000) 0.993 (\u00b10.000) 0.837 (\u00b10.000) U-1\u00d7 (inc.) 0.273 (\u00b10.004) 0.500 (\u00b10.707) 0.001 (\u00b10.003) 0.003 (\u00b10.006) 0.274 (\u00b10.005) 0.333 (\u00b10.422) 0.004 (\u00b10.006) 0.009 (\u00b10.011) U-m\u00d7 (inc.) 0.277 (\u00b10.000) - 0.000 (\u00b10.000) 0.000 (\u00b10.000) 0.272 (\u00b10.003) \u2013 0.000 (\u00b10.000) 0.000 (\u00b10.000) U-m\u00d7 (corr.) 0.725 (\u00b10.000) 0.725 (\u00b10.000) 1.000 (\u00b10.000) 0.840 (\u00b10.000) 0.724 (\u00b10.002) 0.727 (\u00b10.001) 0.991 (\u00b10.003) 0.839 (\u00b10.001) U-m\u00d7 (gray) 0.506 (\u00b10.003) 0.765 (\u00b10.005) 0.460 (\u00b10.000) 0.574 (\u00b10.001) 0.467 (\u00b10.100) 0.748 (\u00b10.022) 0.412 (\u00b10.245) 0.493 (\u00b10.158) PrimeVul S-1\u00d7 (inc.) 0.516 (\u00b10.013) 0.726 (\u00b10.186) 0.437 (\u00b10.459) 0.336 (\u00b10.270) 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) U-1\u00d7 (inc.) 0.507 (\u00b10.010) 0.529 (\u00b10.054) 0.773 (\u00b10.352) 0.551 (\u00b10.212) 0.497 (\u00b10.027) 0.489 (\u00b10.042) 0.466 (\u00b10.137) 0.470 (\u00b10.092) U-m\u00d7 (inc.) 0.496 (\u00b10.009) 0.397 (\u00b10.221) 0.034 (\u00b10.022) 0.061 (\u00b10.038) 0.501 (\u00b10.005) 0.502 (\u00b10.005) 0.864 (\u00b10.219) 0.621 (\u00b10.077) U-m\u00d7 (corr.) 0.499 (\u00b10.002) 0.499 (\u00b10.001) 0.886 (\u00b10.223) 0.626 (\u00b10.079) 0.489 (\u00b10.010) 0.386 (\u00b10.090) 0.034 (\u00b10.010) 0.062 (\u00b10.018) U-m\u00d7 (gray) 0.499 (\u00b10.002) 0.496 (\u00b10.009) 0.366 (\u00b10.284) 0.357 (\u00b10.202) 0.498 (\u00b10.017) 0.529 (\u00b10.141) 0.246 (\u00b10.241) 0.267 (\u00b10.198) SVENC S-1\u00d7 (inc.) 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) U-1\u00d7 (inc.) 0.500 (\u00b10.000) - 0.000 (\u00b10.000) 0.000 (\u00b10.000) 0.500 (\u00b10.000) \u2013 0.000 (\u00b10.000) 0.000 (\u00b10.000) U-m\u00d7 (inc.) 0.500 (\u00b10.000) - 0.000 (\u00b10.000) 0.000 (\u00b10.000) 0.500 (\u00b10.000) \u2013 0.000 (\u00b10.000) 0.000 (\u00b10.000) U-m\u00d7 (corr.) 0.504 (\u00b10.009) 0.503 (\u00b10.006) 0.960 (\u00b10.069) 0.659 (\u00b10.013) 0.493 (\u00b10.009) 0.496 (\u00b10.005) 0.911 (\u00b10.056) 0.642 (\u00b10.016) U-m\u00d7 (gray) 0.478 (\u00b10.000) 0.474 (\u00b10.000) 0.400 (\u00b10.000) 0.434 (\u00b10.000) 0.502 (\u00b10.011) \u2013 0.080 (\u00b10.109) 0.114 (\u00b10.142) SVENP S-1\u00d7 (inc.) 0.500 (\u00b10.000) 0.500 (\u00b10.000) 0.905 (\u00b10.013) 0.644 (\u00b10.003) 0.500 (\u00b10.000) 0.500 (\u00b10.000) 1.000 (\u00b10.000) 0.667 (\u00b10.000) U-1\u00d7 (inc.) 0.558 (\u00b10.067) 0.571 (\u00b10.083) 0.600 (\u00b10.127) 0.573 (\u00b10.050) 0.590 (\u00b10.051) 0.607 (\u00b10.074) 0.547 (\u00b10.111) 0.567 (\u00b10.062) U-m\u00d7 (inc.) 0.540 (\u00b10.033) 0.567 (\u00b10.079) 0.447 (\u00b10.130) 0.483 (\u00b10.074) 0.571 (\u00b10.054) 0.569 (\u00b10.065) 0.721 (\u00b10.132) 0.624 (\u00b10.041) U-m\u00d7 (corr.) 0.524 (\u00b10.023) 0.522 (\u00b10.018) 0.621 (\u00b10.154) 0.557 (\u00b10.063) 0.629 (\u00b10.038) 0.712 (\u00b10.045) 0.437 (\u00b10.133) 0.528 (\u00b10.110) U-m\u00d7 (gray) 0.547 (\u00b10.062) 0.549 (\u00b10.064) 0.742 (\u00b10.145) 0.619 (\u00b10.034) 0.626 (\u00b10.038) 0.767 (\u00b10.120) 0.437 (\u00b10.175) 0.519 (\u00b10.106) 11 Md Abdul Hannan, Ronghao Ni, Chi Zhang, Limin Jia, Ravi Mangal, and Corina S.",
        "text_length": 3458,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00809477549046278,
          -0.0005409828154370189,
          -0.005177414044737816,
          0.012486354447901249,
          0.008689554408192635,
          -0.02018055133521557,
          0.0035270031075924635,
          0.020271487534046173,
          -3.97391922888346e-05,
          0.05390845611691475
        ]
      },
      {
        "chunk_index": 692,
        "relative_chunk_number": 169,
        "text": "Pasareanu S\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61. For the NodeMedic and SVENC datasets, the U-1x (inc.) vari- ant fails catastrophically. On NodeMedic, accuracy plummets to 0.273 for Gemma, and the F1-score becomes near-zero (0.003). On SVENC, it results in 0.000 recall, indicating a complete shift to negative-class bias.",
        "text_length": 290,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0052767288871109486,
          0.0007108728168532252,
          -0.044613316655159,
          0.043960072100162506,
          0.02964223548769951,
          -0.03794817253947258,
          0.02440309152007103,
          0.026452092453837395,
          -0.027367224916815758,
          0.04492134228348732
        ]
      },
      {
        "chunk_index": 693,
        "relative_chunk_number": 170,
        "text": "Conversely, on DiverseVul and PrimeVul with the Gemma model, where the stacked method failed, the unstacked version provides a better balance. On DiverseVul, it improves the F1-score from 0.114 to 0.654, though accuracy remains low at 0.502.",
        "text_length": 241,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.05313143879175186,
          0.0282716304063797,
          -0.02944965474307537,
          0.042934756726026535,
          0.005243917461484671,
          -0.027018653228878975,
          0.019729964435100555,
          -0.027460066601634026,
          -0.03167708218097687,
          0.01851530745625496
        ]
      },
      {
        "chunk_index": 694,
        "relative_chunk_number": 171,
        "text": "This indicates that while the random examples help, they do not solve the accuracy/F1 trade-off, instead achieving a high F1 (0.654) through high recall (0.940) and low precision (0.501). Third, a comparison of the unstacked, multi-iteration (U-mx) variants reveals dataset-dependent findings.",
        "text_length": 293,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.011296402662992477,
          0.022895267233252525,
          -0.05448739975690842,
          0.010984450578689575,
          -0.0018268636194989085,
          -0.019309284165501595,
          0.018731243908405304,
          0.012895333580672741,
          -0.03963031992316246,
          0.03950294107198715
        ]
      },
      {
        "chunk_index": 695,
        "relative_chunk_number": 172,
        "text": "For NodeMedic and SVENC, where learning from incorrect examples (opt=I) failed, learning from correct examples (opt=C) is highly effective. On NodeMedic, this U-mx (corr.) variant restores both high accuracy (0.725) and a high F1-score (0.840) for Gemma, mirroring the per- formance of the original stacked LFM. This demonstrates an ideal balance.",
        "text_length": 347,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00482537504285574,
          -0.0005610744701698422,
          -0.07877699285745621,
          0.023016761988401413,
          -0.025067172944545746,
          -0.04028390720486641,
          0.02226269245147705,
          0.0013174887280911207,
          -0.03565084934234619,
          0.022571463137865067
        ]
      },
      {
        "chunk_index": 696,
        "relative_chunk_number": 173,
        "text": "On SVENC, however, the same method only restores the high F1-score (0.659) while accuracy remains low (0.504), indicat- ing it learned to trade precision for high recall (0.960). For these same two datasets, the U-m\u00d7 (gray) variant was largely ineffective. Gemma\u2019s performance regressed to F1-scores of 0.574 (NodeMedic) and 0.434 (SVENC), close to the ZS baseline.",
        "text_length": 365,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.015080811455845833,
          0.018938196823000908,
          -0.04662289097905159,
          -0.006345562636852264,
          0.005629296880215406,
          -0.04987163096666336,
          0.019179174676537514,
          -0.0002540792047511786,
          -0.024224866181612015,
          0.03784919157624245
        ]
      },
      {
        "chunk_index": 697,
        "relative_chunk_number": 174,
        "text": "This pattern is completely reversed on the DiverseVul dataset. Here, learning from incorrect examples (opt=I) yields a high F1- score (0.669 for Gemma) but poor accuracy (0.507). In contrast, learning from correct examples (opt=C) achieves a much better accuracy (0.597) at the cost of a poor F1-score (0.383), presenting a clear trade-off.",
        "text_length": 340,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.027803681790828705,
          0.023678923025727272,
          -0.05634918063879013,
          0.03349299728870392,
          -0.036081187427043915,
          -0.004446851555258036,
          0.020258545875549316,
          -0.0019083135994151235,
          -0.027341175824403763,
          0.02059243619441986
        ]
      },
      {
        "chunk_index": 698,
        "relative_chunk_number": 175,
        "text": "The U-m\u00d7 (gray) variant was similarly ineffective on this dataset, achieving a very low F1-score of 0.099 for Gemma. The PrimeVul dataset presents the most significant finding: a direct con- tradiction between the models. Gemma achieves its best unstacked F1-score (0.626) with U-mx (corr.), though its accuracy remains low (0.499). It fails with U-mx (inc.) (0.061 F1).",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.041146717965602875,
          0.004862860776484013,
          -0.01741597056388855,
          0.01973799802362919,
          0.052031345665454865,
          -0.014748410321772099,
          0.02144704759120941,
          -0.026916563510894775,
          -0.028379524126648903,
          0.012524033896625042
        ]
      },
      {
        "chunk_index": 699,
        "relative_chunk_number": 176,
        "text": "Qwen\u2019s performance is the exact inverse, performing best with U-mx (inc.) (F1 0.621, Acc 0.501) and failing with U-mx (corr.) (F1 0.062, Acc 0.489). Here, the U-m\u00d7 (gray) variant for Gemma on PrimeVul landed in the middle, with a modest F1-score of 0.357. In summary, the LFM\u2019s behavior is not monolithic.",
        "text_length": 305,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.029422221705317497,
          0.012884211726486683,
          -0.03198491409420967,
          -0.0015082495519891381,
          0.043204888701438904,
          -0.04706116393208504,
          0.019926022738218307,
          -0.012927909381687641,
          -0.02769196219742298,
          0.007173634599894285
        ]
      },
      {
        "chunk_index": 700,
        "relative_chunk_number": 177,
        "text": "The stacked, single-iteration approach is a predictable bias-inducer, sacrificing accuracy for recall. The unstacked, multi-iteration approaches are highly contingent on the type of examples used for learning (i.e., whether \ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3c, \ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc36, or \ud835\udc5c\ud835\udc5d\ud835\udc61= \ud835\udc3a), and no single strategy has been proven universally superior.",
        "text_length": 312,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010255816392600536,
          0.0399453304708004,
          -0.026922186836600304,
          0.027266519144177437,
          0.03272856026887894,
          0.000709396437741816,
          0.017697593197226524,
          0.01028096117079258,
          -0.02451031468808651,
          0.05843689292669296
        ]
      },
      {
        "chunk_index": 701,
        "relative_chunk_number": 178,
        "text": "Furthermore, our analysis highlights a persistent tension between optimizing for F1-score and accuracy. On balanced datasets, in some cases, LFM variants achieve better F1-score by inducing a strong recall bias, which simultane- ously results in an accuracy score close to 0.500.",
        "text_length": 279,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03159281983971596,
          0.044929780066013336,
          -0.05139971151947975,
          -0.008420194499194622,
          -0.024628257378935814,
          -0.025549529120326042,
          0.02011265605688095,
          0.011662176810204983,
          -0.033199749886989594,
          0.045530691742897034
        ]
      },
      {
        "chunk_index": 702,
        "relative_chunk_number": 179,
        "text": "To summarize, the optimal approach is highly dependent on the specific model and dataset, as demonstrated by the contradictory results on PrimeVul. 12",
        "text_length": 150,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.027949467301368713,
          0.054641157388687134,
          -0.014257424511015415,
          0.04428857937455177,
          -0.017323536798357964,
          -0.017147675156593323,
          0.017554720863699913,
          -0.030523238703608513,
          -0.026096783578395844,
          -0.08348675817251205
        ]
      }
    ]
  },
  {
    "paper_id": "2510.27672v1",
    "total_chunks": 244,
    "index_range": {
      "start": 703,
      "end": 946
    },
    "chunks": [
      {
        "chunk_index": 703,
        "relative_chunk_number": 1,
        "text": "Culture Cartography: Mapping the Landscape of Cultural Knowledge Caleb Ziems William Held Jane Yu Amir Goldberg David Grusky Diyi Yang Stanford University Georgia Institute of Technology Meta AI {cziems, held, amirgo, grusky, diyi}@stanford.edu Abstract To serve global users safely and productively, LLMs need culture-specific knowledge that might not be learned during pre-training.",
        "text_length": 384,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004082806874066591,
          0.023157985880970955,
          -0.008053889498114586,
          0.014945455826818943,
          0.03588257357478142,
          0.015679044649004936,
          0.022359244525432587,
          0.01747247204184532,
          -0.03252017870545387,
          -0.00028528727125376463
        ]
      },
      {
        "chunk_index": 704,
        "relative_chunk_number": 2,
        "text": "How do we find knowledge that is (1) salient to in- group users, but (2) unknown to LLMs? The most common solutions are single-initiative: either researchers define challenging questions that users passively answer (traditional anno- tation), or users actively produce data that researchers structure as benchmarks (knowl- edge extraction).",
        "text_length": 340,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.013698128052055836,
          -0.0078050545416772366,
          -0.03878914564847946,
          0.005274544935673475,
          -0.028698958456516266,
          0.0007496433099731803,
          0.02278704009950161,
          -0.001415106700733304,
          -0.023144276812672615,
          -0.009877492673695087
        ]
      },
      {
        "chunk_index": 705,
        "relative_chunk_number": 3,
        "text": "The process would bene- fit from mixed-initiative collaboration, where users guide the process to meaningfully re- flect their cultures, and LLMs steer the pro- cess to meet the researcher\u2019s goals. We pro- pose CULTURE CARTOGRAPHY as a method- ology that operationalizes this mixed-initiative vision.",
        "text_length": 300,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.01686892844736576,
          0.02731371857225895,
          -0.00621950626373291,
          0.04276452586054802,
          0.0596434660255909,
          -0.0335085429251194,
          0.020559780299663544,
          -0.002973077818751335,
          -0.03087807446718216,
          0.029785310849547386
        ]
      },
      {
        "chunk_index": 706,
        "relative_chunk_number": 4,
        "text": "Here, an LLM initializes annotation with questions for which it has low-confidence answers, making explicit both its prior knowl- edge and the gaps therein. This allows a hu- man respondent to fill these gaps and steer the model towards salient topics through direct ed- its. We implement CULTURE CARTOGRAPHY as a tool called CULTURE EXPLORER.",
        "text_length": 343,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.015670685097575188,
          0.023607023060321808,
          -0.026535870507359505,
          0.0041385493241250515,
          -0.01741771213710308,
          -0.001018076902255416,
          0.020839141681790352,
          0.043144889175891876,
          -0.02826441265642643,
          -0.03979005664587021
        ]
      },
      {
        "chunk_index": 707,
        "relative_chunk_number": 5,
        "text": "Com- pared to a baseline where humans answer LLM- proposed questions, we find that CULTURE EX- PLORER more effectively produces knowledge that strong models like DeepSeek R1, Llama-4 and GPT-4o are missing, even with web search. Fine-tuning on this data boosts the accuracy of Llama models by up to 19.2% on related culture benchmarks.",
        "text_length": 335,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0036545649636536837,
          0.004034498706459999,
          -0.052089281380176544,
          0.013909728266298771,
          0.005156816449016333,
          -0.008652201853692532,
          0.022762415930628777,
          0.01354270987212658,
          -0.021812332794070244,
          -0.08902843296527863
        ]
      },
      {
        "chunk_index": 708,
        "relative_chunk_number": 6,
        "text": "1 Introduction Large Language Models (LLMs) can empower users to be more knowledgeable, productive, and creative (Carmichael and Stinson, 2024; Adiguzel et al., 2023; Yang et al., 2024b; Chen et al., 2021; Si et al., 2024), but this utility is often diminished for under-represented groups (Cao et al., 2023; Yong et al., 2024; Ziems et al., 2023b) and cultures (Myung et al., 2024; Shi et al., 2024; Chiu et al., Humans Actively Produce Data \u25cfQ1: \u2026 A1 x \u25cfQ2: \u2026 A2 x \u25cfQ3: \u2026 A3 { Mixed-Initiative (Jointly Produce) } Traditional Annotation \u25cfQ1: \u2026 x \u25cfQ2: \u2026 x \u25cfQ3: \u2026 Culture Cartography List any customs or traditions related to the presentation of Christmas Idul Fitrix gifts in Indonesian Javanese culture { Single-Initiative (Human or LLM Produces) } Knowledge Extraction LLM Formats a Benchmark xMay Notx xChallenge LLMsx LLM Produces Challenging Questions Humans Passively Answer Questions xMay Notx xRepresentx xCulturex Human Guides tox Salient Knowledgex Model Steers tox Challenging Questions x LLM LLM LLM \u25cfA1 x \u25cfA2 x \u25cfA3 Salam Tempel (Money Gifts).",
        "text_length": 1056,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0070691825821995735,
          0.003241799771785736,
          0.004747571889311075,
          0.009661862626671791,
          -0.0286247581243515,
          0.003495729761198163,
          0.009622456505894661,
          0.03663654625415802,
          -0.018280580639839172,
          0.0052611129358410835
        ]
      },
      {
        "chunk_index": 709,
        "relative_chunk_number": 7,
        "text": "Elders give small amounts of money to children and younger relatives. The money is usually presented in newly minted banknotes inside envelopes. Parcel Lebaran (Parcels and Hampers). Families, neighbors, or employees may exchange food parcels, often containing cookies, syrup, or traditional cakes.",
        "text_length": 298,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007330189924687147,
          0.025390662252902985,
          0.0044153607450425625,
          0.03064611181616783,
          -0.04683160036802292,
          -0.0062921373173594475,
          0.024378642439842224,
          -0.012473531998693943,
          -0.03667551279067993,
          -0.07129403948783875
        ]
      },
      {
        "chunk_index": 710,
        "relative_chunk_number": 8,
        "text": "Figure 1: CULTURE CARTOGRAPHY is a new method for identifying culturally-salient knowledge gaps in LLMs. Prior methods are single-initiative: either a human determines the distribution (Knowledge Extrac- tion), which may not challenge models, or an LLM de- cides on challenging questions (Traditional Annotation), which may not represent human interests.",
        "text_length": 354,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.002392203314229846,
          0.03140558302402496,
          -0.011275159195065498,
          0.021113982424139977,
          0.0065343016758561134,
          -0.03187507390975952,
          0.02208751067519188,
          -0.023052826523780823,
          -0.025216761976480484,
          0.030299486592411995
        ]
      },
      {
        "chunk_index": 711,
        "relative_chunk_number": 9,
        "text": "CULTURE CARTOGRAPHY is the first mixed-initiative method that combines four key ingredients: (1) an LLM pro- poses challenging questions; (2) a human proposes salient questions; (3) human edits constrain subse- quent LLM generations; and (4) the data forms a tree structure. Compared to prior methods, CULTURE CARTOGRAPHY identifies more LLM knowledge gaps.",
        "text_length": 357,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.028071915730834007,
          0.029238417744636536,
          -0.004066671244800091,
          0.009745747782289982,
          0.02666604518890381,
          -0.0297550056129694,
          0.020884646102786064,
          0.013750161975622177,
          -0.02603929676115513,
          0.03301704302430153
        ]
      },
      {
        "chunk_index": 712,
        "relative_chunk_number": 10,
        "text": "2024), due to data imbalances in pre-training (Li et al., 2024a) and post-training (Ryan et al., 2024).",
        "text_length": 103,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.007380801253020763,
          0.020755287259817123,
          -0.041559308767318726,
          0.004836557433009148,
          0.038086436688899994,
          -0.0005016324575990438,
          0.017129680141806602,
          0.01735672913491726,
          -0.010064540430903435,
          -0.04836537688970566
        ]
      },
      {
        "chunk_index": 713,
        "relative_chunk_number": 11,
        "text": "LLM agents that lack knowledge about their user\u2019s cultures can be less helpful personal assistants (Qiu et al., 2024), less relevant recommender systems arXiv:2510.27672v1 [cs.CL] 31 Oct 2025 (Casillo et al., 2023), and less engaging conversa- tion partners (Cao et al., 2024), while being more prone to generate harmful stereotypes (Cheng et al., 2023) and violate social norms (Santy et al., 2023).",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03513629734516144,
          0.028872163966298103,
          -0.009497515857219696,
          0.0012057098792865872,
          0.001680130255408585,
          -0.021221790462732315,
          0.019971271976828575,
          -0.029876524582505226,
          -0.024262964725494385,
          -0.005346317775547504
        ]
      },
      {
        "chunk_index": 714,
        "relative_chunk_number": 12,
        "text": "Problem. A principal challenge is identify- ing the cultural knowledge that is both necessary for language models to effectively serve in-group users, and also absent from current models\u2019 aware- ness. The most common methods for finding missing knowledge involve benchmarking with single-initiative datasets (Figure 1, top).",
        "text_length": 324,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012897278182208538,
          0.020690668374300003,
          -0.03610510751605034,
          0.01949416473507881,
          0.01436238456517458,
          -0.009630769491195679,
          0.020530041307210922,
          -0.0460658073425293,
          -0.022659961134195328,
          0.018988803029060364
        ]
      },
      {
        "chunk_index": 715,
        "relative_chunk_number": 13,
        "text": "These approaches generally follow one of two patterns: either researchers define questions that challenge LLMs (Li et al., 2023b) and have human anno- tators provide answers (identifying hard but po- tentially non-salient knowledge), or they convert existing cultural knowledge into benchmarks us- ing LLMs (capturing salient but potentially not challenging knowledge).",
        "text_length": 369,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04252079874277115,
          0.007836674340069294,
          -0.03829673305153847,
          0.003123445902019739,
          -0.001627563964575529,
          0.0056859832257032394,
          0.02548036351799965,
          0.008575213141739368,
          -0.028952639549970627,
          -0.007034634239971638
        ]
      },
      {
        "chunk_index": 716,
        "relative_chunk_number": 14,
        "text": "Ideally, users and current- LLMs should be involved in a mixed-initiative in- teraction (Horvitz, 1999) (Figure 1, bottom) where humans guide the process towards culturally salient knowledge, and LLMs guide the process towards knowledge missing from existing training data. Proposed Solution.",
        "text_length": 292,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.035125017166137695,
          0.026620754972100258,
          -0.024750608950853348,
          0.009342425502836704,
          0.04260348528623581,
          0.0015483815222978592,
          0.021988267078995705,
          0.047258105129003525,
          -0.027103347703814507,
          0.0038159270770847797
        ]
      },
      {
        "chunk_index": 717,
        "relative_chunk_number": 15,
        "text": "We propose CULTURE CARTOGRAPHY as the first mixed-initiative anno- tation method to satisfy the above desiderata with all of the following ingredients: 1. An LLM proposes challenging questions for which it has low confidence in its answers, thus exposing its knowledge gaps\u2014the do- main of interest for many researchers. 2.",
        "text_length": 323,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008283300325274467,
          0.03754814341664314,
          -0.012245039455592632,
          0.017667217180132866,
          0.027664273977279663,
          -0.020968925207853317,
          0.018825476989150047,
          0.026178376749157906,
          -0.03022453375160694,
          -0.009634788148105145
        ]
      },
      {
        "chunk_index": 718,
        "relative_chunk_number": 16,
        "text": "The human makes direct edits or proposes new questions that reflect their expertise and interests, thus introducing cultural salience. 3. Human edits will guide and constrain sub- sequent LLM generations, thus making the interaction truly mixed-initiative. 4. Knowledge is visualized in a tree data struc- ture, thus affording humans more control through parallel exploration.",
        "text_length": 376,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.013324915431439877,
          0.004751183092594147,
          -0.0348454974591732,
          0.005489917937666178,
          -0.0009713054751046002,
          -0.018350420519709587,
          0.021345604211091995,
          0.030736399814486504,
          -0.031536463648080826,
          0.027924872934818268
        ]
      },
      {
        "chunk_index": 719,
        "relative_chunk_number": 17,
        "text": "Figure 1 (bottom) exemplifies each of these ingredients. Here, the LLM proposes a low- confidence question about Christmas gift-giving, which does not align with the annotator\u2019s expertise. She is Muslim, like the vast majority of people who live on the Indonesian island of Java, so she edits the question to ask instead about Javanese gift-giving during Eid al-Fitr, an important Islamic holiday.",
        "text_length": 397,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.05561048910021782,
          0.038202427327632904,
          0.0062520867213606834,
          -0.017378363758325577,
          0.00479533988982439,
          0.01087577361613512,
          0.020400473847985268,
          0.017062613740563393,
          -0.035191040486097336,
          -0.09313838183879852
        ]
      },
      {
        "chunk_index": 720,
        "relative_chunk_number": 18,
        "text": "This directly informs the LLM\u2019s updated answer suggestions, which are structured as a tree. Research Tool. Annotation in such a broad and nebulous domain as culture could seem pro- hibitively inefficient, but we implement a tractable solution with an open-source web-tool called CUL- TURE EXPLORER (Figure 2).",
        "text_length": 309,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.003452964825555682,
          0.005973636172711849,
          0.008734669536352158,
          0.002772217383608222,
          -0.028442183509469032,
          -0.02068922482430935,
          0.017602797597646713,
          -0.006622052285820246,
          -0.0306536927819252,
          -0.0484212189912796
        ]
      },
      {
        "chunk_index": 721,
        "relative_chunk_number": 19,
        "text": "This tool not only solves the more mundane aspects of the annota- tion task, like boilerplate and text formatting, but also visually facilitates the mixed-initiative inter- action. Unlike constrained and linear chat-based interfaces, CULTURE EXPLORER affords users a more interactive interface to consider and edit a growing tree of knowledge.",
        "text_length": 343,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008242729119956493,
          0.008887318894267082,
          0.00790130440145731,
          0.019954416900873184,
          -0.0409778468310833,
          -0.020229917019605637,
          0.01960502192378044,
          -0.01396513544023037,
          -0.03640412539243698,
          0.011960458010435104
        ]
      },
      {
        "chunk_index": 722,
        "relative_chunk_number": 20,
        "text": "In this way, CULTURE EXPLORER empowers users with a sense of direct manipulation (Shneiderman, 1983). The user can make edits rapidly, reversibly, and iteratively, while the tool visually displays the ramifications of these edits by generating parallel follow-up questions.",
        "text_length": 273,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.005954340100288391,
          0.008427159860730171,
          -0.02035527117550373,
          0.017097260802984238,
          -0.05128293111920357,
          -0.029433315619826317,
          0.01781829260289669,
          -0.021695636212825775,
          -0.0263484138995409,
          -0.014589249156415462
        ]
      },
      {
        "chunk_index": 723,
        "relative_chunk_number": 21,
        "text": "By expanding and pruning branches, the user can consider multiple thematic directions and then fo- cus on what most interests them. Findings. We use CULTURE EXPLORER to build cultural knowledge banks for two multicul- tural and multiethnic countries: Nigeria and In- donesia.",
        "text_length": 275,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02887134626507759,
          0.0269395150244236,
          -0.0018149528186768293,
          0.01519957184791565,
          -0.022814860567450523,
          0.010089398361742496,
          0.020359324291348457,
          0.0006641343352384865,
          -0.03275178372859955,
          -0.01751202903687954
        ]
      },
      {
        "chunk_index": 724,
        "relative_chunk_number": 22,
        "text": "By design, we expect CULTURE EX- PLORER will outperform single-initiative meth- ods (Figure 1) at eliciting knowledge that is both salient and challenging. Indeed, compared to tradi- tional annotation, CULTURE EXPLORER identifies data that is at least 6% less likely to be known by DeepSeek R1, and up to 42% less likely to be known by other models.",
        "text_length": 349,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.014580903574824333,
          0.012853702530264854,
          -0.029569432139396667,
          0.03175215795636177,
          -0.023627934977412224,
          -0.0008051944896578789,
          0.02223460003733635,
          -0.010801201686263084,
          -0.024376386776566505,
          -0.058741744607686996
        ]
      },
      {
        "chunk_index": 725,
        "relative_chunk_number": 23,
        "text": "Furthermore, unlike knowledge extraction, CULTURE EXPLORER pro- duces data that is not easily discoverable online. We find search-enabled models do not outperform search-disabled models at recalling our data. Fi- nally, we demonstrate how our methodology is aligned with the objectives of the field via transfer learning experiments.",
        "text_length": 333,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004095027223229408,
          0.029748544096946716,
          -0.020370887592434883,
          0.013337917625904083,
          -0.03933057188987732,
          -0.021144350990653038,
          0.02283167466521263,
          -0.044925905764102936,
          -0.032568082213401794,
          -0.04552052542567253
        ]
      },
      {
        "chunk_index": 726,
        "relative_chunk_number": 24,
        "text": "By fine-tuning on data pro- duced with CULTURE EXPLORER, we can boost the downstream performance of LLMs on other culture benchmarks by up to 19.2% accuracy. Contributions. In summary, we propose CUL- TURE CARTOGRAPHY, a new methodological framework for eliciting culturally-salient knowl- edge gaps in LLMs. We implement the idea as CULTURE EXPLORER and demonstrate its utility over prior methods.",
        "text_length": 398,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.002435885602608323,
          0.031249240040779114,
          -0.018272606655955315,
          0.009183844551444054,
          0.03367885574698448,
          -0.026810232549905777,
          0.020365018397569656,
          -0.027363084256649017,
          -0.027985455468297005,
          -0.040632784366607666
        ]
      },
      {
        "chunk_index": 727,
        "relative_chunk_number": 25,
        "text": "We publicly release all arti- facts, including data, code, tooling, and models.1 1github.com/SALT-NLP/culture-cartography CultureExplorer gifts List any customs or traditions related to the presentation of Christmas Idul Fitrix gifts in Indonesian Javanese culture artistic expression national identity family dynamics sporting events wedding practices gift giving A Seed Topic B Question Nodes The use of e-wallets and QR Codes has become increasingly popular in Indonesia\u2019s urban areas.",
        "text_length": 488,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00675937719643116,
          0.0014219902222976089,
          0.022073425352573395,
          0.04272298887372017,
          -0.0033026505261659622,
          -0.011951584368944168,
          0.019440066069364548,
          0.018002411350607872,
          -0.042586907744407654,
          -0.004300693050026894
        ]
      },
      {
        "chunk_index": 728,
        "relative_chunk_number": 26,
        "text": "During weddings the traditional \u201chantaran\u201d (gift parcels) are now often curated by professional services. For Eid al-Fitr, giving money or \u201cduit lebaran\u201d in decorative envelopes has remained a tradition, but now the designs are influenced by popular online trends. C Answer Nodes 1 2 3 List the traditional methods of preparing and presenting \u201chantaran\u201d before the rise of professional services.",
        "text_length": 395,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.011892490088939667,
          0.03421764075756073,
          0.02361992374062538,
          0.033483490347862244,
          -0.020931536331772804,
          -0.0364619717001915,
          0.02406023070216179,
          -0.02722756750881672,
          -0.02685047686100006,
          -0.07409609854221344
        ]
      },
      {
        "chunk_index": 729,
        "relative_chunk_number": 27,
        "text": "List the reasons why families might choose professional services to curate their \u201chantaran\u201d for weddings List the occasions, other than weddings, where \u201chantaran\u201d might be a traditional practice in Indonesian culture.",
        "text_length": 217,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009815366938710213,
          0.06026722490787506,
          0.024244604632258415,
          0.06991422921419144,
          0.0006766035221517086,
          -0.0025090547278523445,
          0.02221982181072235,
          -0.003148432355374098,
          -0.038142211735248566,
          -0.022096676751971245
        ]
      },
      {
        "chunk_index": 730,
        "relative_chunk_number": 28,
        "text": "D Deeper Follow-Up Questions and Answers Time Profe proc signi Expe Profe stun refle auth Qua Item prof qual both can be edited, regenerated, or deleted can be ranked x uncertain answers randomly sampled; can be edited List any adaptations or changes in Indonesian gifting customs that have occurred over time due to social or technological advancements.",
        "text_length": 354,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.013298259116709232,
          0.025875570252537727,
          -0.0019211923936381936,
          0.019087377935647964,
          0.013607002794742584,
          0.0050514270551502705,
          0.021813908591866493,
          0.0583537258207798,
          -0.03673290088772774,
          0.03947163745760918
        ]
      },
      {
        "chunk_index": 731,
        "relative_chunk_number": 29,
        "text": "2 1 3 highlighted Figure 2: The CULTURE EXPLORER interface allows human experts to lead the annotation process, as they can Edit, Regenerate, or Delete nodes at any time. Cultural Knowledge annotation is initiated with A a seed topic (here: gifts), which the LLM uses to generate B Question nodes.",
        "text_length": 297,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.013268450275063515,
          -0.005088020581752062,
          -0.007574309129267931,
          0.016467738896608353,
          -0.03956348076462746,
          -0.017438191920518875,
          0.02229497581720352,
          0.012474119663238525,
          -0.031002098694443703,
          0.005994135048240423
        ]
      },
      {
        "chunk_index": 732,
        "relative_chunk_number": 30,
        "text": "Here, the annotator is editing the first Question node to make it more specific to her Islamic culture. Each Question will serve as a seed for the LLM to generate C Answer nodes. The user can then pick the questions and answers interests her, clarify through edits, or write her own from scratch, iteratively expanding the tree with D deeper follow-up questions and answers.",
        "text_length": 374,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01746344566345215,
          0.03271222114562988,
          -0.008787847124040127,
          0.018896985799074173,
          -0.08792773634195328,
          -0.02051933854818344,
          0.020328320562839508,
          0.059903811663389206,
          -0.03646945580840111,
          -0.01014372706413269
        ]
      },
      {
        "chunk_index": 733,
        "relative_chunk_number": 31,
        "text": "2 Related Work There are strong economic, social, and scientific motivations to build more culturally-competent lan- guage models that are capable of meaningfully engaging with users from different cultural back- grounds (Hershcovich et al., 2022).",
        "text_length": 248,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02195044793188572,
          0.03170708566904068,
          -0.025370381772518158,
          0.03815397992730141,
          0.04560936242341995,
          -0.047812480479478836,
          0.018774259835481644,
          -0.039468828588724136,
          -0.028679676353931427,
          -0.04128280282020569
        ]
      },
      {
        "chunk_index": 734,
        "relative_chunk_number": 32,
        "text": "For com- mon ground, effective language technologies need knowledge of the behavioral norms (Shi et al., 2024; Sky et al., 2023; Rao et al., 2024), linguis- tic conventions (Shaikh et al., 2023), values (Cao et al., 2023), and preferences (Kirk et al., 2024) that shape each user\u2019s interactions with the model.",
        "text_length": 310,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01375523954629898,
          0.0037978163454681635,
          -0.03862279653549194,
          0.0015131733380258083,
          0.026914963498711586,
          -0.004502921365201473,
          0.015668638050556183,
          -0.019648034125566483,
          -0.0215191300958395,
          -0.01885572075843811
        ]
      },
      {
        "chunk_index": 735,
        "relative_chunk_number": 33,
        "text": "This motivates new culture-specific training and evalua- tion datasets (Hershcovich et al., 2022).",
        "text_length": 98,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.026516903191804886,
          0.05177125334739685,
          -0.008212016895413399,
          0.05781660974025726,
          0.03991902992129326,
          0.01327494252473116,
          0.02339191362261772,
          -0.046446096152067184,
          -0.04488281533122063,
          0.010534240864217281
        ]
      },
      {
        "chunk_index": 736,
        "relative_chunk_number": 34,
        "text": "There are many datasets for advancing cultural competence in specific NLP tasks (Shode et al., 2023; Muhammad et al., 2023, 2022; Tonneau et al., 2024; Ilevbare et al., 2024; Vargas et al., 2024; Olamma et al., 2019; Adelani et al., 2022; Olatunji et al., 2023; Owodunni et al., 2024), but in the current task- agnostic paradigm, QA-style knowledge bench- marking is the most common (Adilazuarda et al., 2024).",
        "text_length": 410,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.004492117092013359,
          0.009701699949800968,
          -0.017228472977876663,
          0.02035227231681347,
          -0.0022833896800875664,
          0.0040411869995296,
          0.008556626737117767,
          0.027583414688706398,
          -0.01874462142586708,
          0.0369061678647995
        ]
      },
      {
        "chunk_index": 737,
        "relative_chunk_number": 35,
        "text": "Benchmarks are typically built via Knowl- edge Extraction or Traditional Annotation. Knowledge Extraction. To scale up evalua- tion, LLMs can distill knowledge from web sources (Nguyen et al., 2024, 2023) like Wikipedia (Fung et al., 2024; Li et al., 2024b; Naous et al., 2024), TV (Fung et al., 2023), or social media (Shi et al., 2024).",
        "text_length": 338,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.02576475776731968,
          -7.47494341339916e-05,
          -0.01824173331260681,
          0.016232425346970558,
          -0.07647458463907242,
          -0.013745339587330818,
          0.020886065438389778,
          -0.012758574448525906,
          -0.02161037176847458,
          0.001642170362174511
        ]
      },
      {
        "chunk_index": 738,
        "relative_chunk_number": 36,
        "text": "LLMs can also generate synthetic evalu- ation data without seed knowledge (Wang et al., 2024a; Liu et al., 2024), which human annota- tors prune and validate.",
        "text_length": 158,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03972875326871872,
          0.019059697166085243,
          -0.020778991281986237,
          0.018377210944890976,
          -0.0403100959956646,
          0.02990889549255371,
          0.024912172928452492,
          0.0310641061514616,
          -0.03669125586748123,
          -0.017714951187372208
        ]
      },
      {
        "chunk_index": 739,
        "relative_chunk_number": 37,
        "text": "But there are concerns of test set contamination (Oren et al., 2023) when distilled or synthetic data overlaps with LLM pre- training data, and such knowledge extraction is lim- ited only to higher-resource cultures that are well- represented on the web (Seth et al., 2024).",
        "text_length": 274,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.07121503353118896,
          0.02051485702395439,
          0.01701110601425171,
          0.018571391701698303,
          0.0015870904317125678,
          0.049358196556568146,
          0.02319481037557125,
          0.038695238530635834,
          -0.024993864819407463,
          -0.007188804913312197
        ]
      },
      {
        "chunk_index": 740,
        "relative_chunk_number": 38,
        "text": "Alterna- tively, unstructured (Zhang and Wildemuth, 2009) or semi-structured interviews (Karatsareas, 2022) can provide knowledge from under-represented cul- tures. However, these methods are not designed to target the gaps in models\u2019 knowledge. Traditional Annotation.",
        "text_length": 269,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0460825152695179,
          0.024103792384266853,
          -0.009235546924173832,
          0.019289636984467506,
          -0.020761890336871147,
          -0.02533179521560669,
          0.022025175392627716,
          -0.020003195852041245,
          -0.021796442568302155,
          0.024222424253821373
        ]
      },
      {
        "chunk_index": 741,
        "relative_chunk_number": 39,
        "text": "For the sake of effi- ciency, it is standard for annotators to respond to pre-determined questions that target gaps in mod- els\u2019 knowledge. Questions can derive from sec- ondary sources like templates (Yin et al., 2022; Myung et al., 2024), LLM generations (Liu et al., 2024; Ziems et al., 2023a), social media (Sky et al., 2023; Huang and Yang, 2023), or human experts (Masala et al., 2024).",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.028064412996172905,
          0.011807803995907307,
          0.016821878030896187,
          0.030565345659852028,
          -0.034468844532966614,
          0.023033438250422478,
          0.013724078424274921,
          0.009895565919578075,
          -0.020758431404829025,
          -0.009213894605636597
        ]
      },
      {
        "chunk_index": 742,
        "relative_chunk_number": 40,
        "text": "Games can serve as a more dynamic and interactive alternative to question- naires (Seth et al., 2024; Shaikh et al., 2023), but these also depend on fixed seed topics. In each case, the culture informer is not empowered to steer the topical distribution of the data they are providing. 3 CULTURE EXPLORER 3.1 Tool Design Cultural competence is not a generalizable objec- tive.",
        "text_length": 376,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.031271811574697495,
          0.010659942403435707,
          -0.012352646328508854,
          0.023321140557527542,
          0.024327727034687996,
          -0.06731946766376495,
          0.019561056047677994,
          0.004053513519465923,
          -0.02541734091937542,
          -0.030598457902669907
        ]
      },
      {
        "chunk_index": 743,
        "relative_chunk_number": 41,
        "text": "Like many other forms of alignment, the task is ambiguous (Tamkin et al., 2022; Li et al., 2023a), as its formalization depends on the culture and the specific users in question. The problem we aim to solve is mixed-initiative elicitation: guiding members of a cultural group to specify what cul- tural competence means to them, and to do so in an efficient manner.",
        "text_length": 365,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.018545735627412796,
          0.03375643864274025,
          -0.022935329005122185,
          0.0346364751458168,
          0.0027202279306948185,
          0.015292569994926453,
          0.015396660193800926,
          -0.005396035499870777,
          -0.03622943535447121,
          0.03178637474775314
        ]
      },
      {
        "chunk_index": 744,
        "relative_chunk_number": 42,
        "text": "An efficient solution will not waste human effort to reproduce what language models already know, but rather prioritize regions of knowledge that current models lack, following the literature on active learning (Cohn et al., 1994).",
        "text_length": 231,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.021677667275071144,
          0.035990502685308456,
          -0.03634992614388466,
          0.005004147998988628,
          0.014324049465358257,
          0.01675713062286377,
          0.02001100219786167,
          0.00463554821908474,
          -0.02555077336728573,
          0.059194356203079224
        ]
      },
      {
        "chunk_index": 745,
        "relative_chunk_number": 43,
        "text": "CULTURE EXPLORER is our proposed solution that balances flexibility with efficiency, empower- ing users to co-construct with the LLM a branching tree of cultural knowledge. As shown in Figure 2, this tree is composed of related questions and an- swers, and users can edit, add, or delete elements at any time. At each iteration, the LLM generates question and answer suggestions.",
        "text_length": 379,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02637469582259655,
          0.02942514792084694,
          -0.00944354198873043,
          -0.007265633437782526,
          -0.006161952391266823,
          -0.011010834015905857,
          0.020132195204496384,
          -0.0008828318677842617,
          -0.027656259015202522,
          -0.0014310494298115373
        ]
      },
      {
        "chunk_index": 746,
        "relative_chunk_number": 44,
        "text": "Similar to Gener- ative Active Task Elicitation (Li et al., 2023a), this allows CULTURE EXPLORER to act as a data scaf- folding and brainstorming tool that guides respon- dents towards critical knowledge gaps.",
        "text_length": 209,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01698681153357029,
          0.012929939664900303,
          -0.0031416243873536587,
          0.02208988554775715,
          0.04363754391670227,
          -0.013067159801721573,
          0.022546187043190002,
          -0.012668101117014885,
          -0.027948059141635895,
          0.01163619477301836
        ]
      },
      {
        "chunk_index": 747,
        "relative_chunk_number": 45,
        "text": "But moving beyond a linear chat, CULTURE EXPLORER pre- serves the respondents\u2019 creative freedom to both refine specific answers through edits and define the topic space by adding and removing nodes. The CULTURE EXPLORER interface is built on FARSIGHT2 (Wang et al., 2024b), and specially tuned for the relevant culture domains (for details on our specific prompting methodology, see Ap- pendix D).",
        "text_length": 397,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0165170319378376,
          0.01621444895863533,
          9.240928193321452e-05,
          0.027372155338525772,
          -0.006235072854906321,
          -0.028248516842722893,
          0.021442081779241562,
          -0.028507601469755173,
          -0.03107701614499092,
          -0.01032944954931736
        ]
      },
      {
        "chunk_index": 748,
        "relative_chunk_number": 46,
        "text": "We now explain the pipeline in more detail, in steps corresponding to Figure 2. Adding Knowledge. CULTURE EXPLORER is organized as a tree data structure, with nodes rep- resenting branching questions and their answers, which users expand recursively.",
        "text_length": 250,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0336291529238224,
          0.017532531172037125,
          0.013008150272071362,
          0.017091916874051094,
          -0.012291650287806988,
          -0.02346448041498661,
          0.01863984577357769,
          -0.02027297578752041,
          -0.032288528978824615,
          -0.008083797991275787
        ]
      },
      {
        "chunk_index": 749,
        "relative_chunk_number": 47,
        "text": "Annotation is ini- tiated by a pre-selected but editable seed topic3 Figure 2, A ), which the LLM uses to generate 2FARSIGHT was designed for visualizing AI harms. 3Seeds derive indirectly from Brown (2004) Human Uni- versal categories. We feed each Brown Universal into CUL- TURE EXPLORER, and generate for each seed a tree of depth 6 (3 rounds of questions and answers).",
        "text_length": 372,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01914818584918976,
          0.022553935647010803,
          -0.013679725117981434,
          0.02613307721912861,
          -0.07937323302030563,
          -0.0387243926525116,
          0.021708529442548752,
          0.02003333903849125,
          -0.035748329013586044,
          -0.01107273530215025
        ]
      },
      {
        "chunk_index": 750,
        "relative_chunk_number": 48,
        "text": "By semantically clus- tering the answers across 8 national cultures, we identify new universals to seed the user-facing version of CULTURE EX- PLORER. We cluster across: Argentina, Australia, Germany, India, Indonesia, Nigeria, Saudi Arabia, and the United States. up to 5 Question nodes ( B ). The user can then pick a question that interests her, or clarify through edits, or write her own.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.021219410002231598,
          0.007136407308280468,
          -0.01919860579073429,
          0.03648889809846878,
          0.004933042451739311,
          0.019420858472585678,
          0.01718362234532833,
          0.018809419125318527,
          -0.024497060105204582,
          0.014164254069328308
        ]
      },
      {
        "chunk_index": 751,
        "relative_chunk_number": 49,
        "text": "Once the user is satis- fied with a Question, the AI will generate as many as 5 Answer nodes ( C ). With Uncertainty Es- timation (Kivlichan et al., 2021), CULTURE EX- PLORER visually highlights answer nodes in which the LLM is not confident.",
        "text_length": 242,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.019194338470697403,
          0.013772659003734589,
          -0.03334055840969086,
          0.014991242438554764,
          -0.024669723585247993,
          -0.023709040135145187,
          0.018403638154268265,
          0.013756973668932915,
          -0.02978469431400299,
          0.010173816233873367
        ]
      },
      {
        "chunk_index": 752,
        "relative_chunk_number": 50,
        "text": "CULTURE EXPLORER measures model confidence by prompting the same model, \u201cDoes this answer the question correctly?\u201d It constrains the logits to True/False, and takes the probability of True as the answer confidence. An- swers with confidence below a threshold (\u22640.4) are marked as uncertain.",
        "text_length": 290,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.011281769722700119,
          0.012883367948234081,
          -0.029774734750390053,
          0.0003806237073149532,
          -0.005052267573773861,
          0.03039482608437538,
          0.02160133607685566,
          -0.06260573118925095,
          -0.025317084044218063,
          -0.05982191115617752
        ]
      },
      {
        "chunk_index": 753,
        "relative_chunk_number": 51,
        "text": "The user can edit, regenerate, or delete any node at any time, and in response, the tool will generate new follow-up questions ( D ), visually displaying the ramifications of the user\u2019s changes.",
        "text_length": 194,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01623593643307686,
          0.008911415003240108,
          -0.03008500672876835,
          0.002747928025200963,
          -0.020372914150357246,
          -0.0435900017619133,
          0.017497556284070015,
          0.046996187418699265,
          -0.03726113587617874,
          0.04539838060736656
        ]
      },
      {
        "chunk_index": 754,
        "relative_chunk_number": 52,
        "text": "Moreover, CULTURE EXPLORER explicitly encourages anno- tators to make significant edits and novel contribu- tions, as it sums the edit distance over all contribu- tions and computes a scalar monetary reward. Scoring Answers.",
        "text_length": 224,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04717160016298294,
          0.034076668322086334,
          0.002366757020354271,
          0.06999628990888596,
          0.021901894360780716,
          -0.014055692590773106,
          0.02073952555656433,
          -0.02754877880215645,
          -0.02882666513323784,
          -0.011407211422920227
        ]
      },
      {
        "chunk_index": 755,
        "relative_chunk_number": 53,
        "text": "A user may not choose to edit every answer given by the LLM, but users can still provide a valuable preference signal by scoring LLM answers for their relevance and personal ap- plicability. CULTURE EXPLORER asks users to score AI answers on a 0-3 Likert Scale, where 3 awards \u201cbest\u201d answers that can\u2019t be improved, and 0 marks \u201cbad\u201d or incorrect answers.",
        "text_length": 355,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.006995232775807381,
          0.02956271544098854,
          -0.01863805018365383,
          0.002669486915692687,
          -0.0278927069157362,
          -0.004318016581237316,
          0.022920940071344376,
          -0.029124347493052483,
          -0.028037002310156822,
          -0.003954012878239155
        ]
      },
      {
        "chunk_index": 756,
        "relative_chunk_number": 54,
        "text": "3.2 Data Collection CULTURE EXPLORER allows us to fill many of the gaps identified in Related Work (\u00a72). We can build a participatory, multilingual knowledge bank of lo- calized cultural knowledge that complements what LLMs already know. We focus our data collection on two culturally diverse yet under-resourced coun- tries: Nigeria and Indonesia.",
        "text_length": 348,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0175718292593956,
          0.013411875814199448,
          -0.0020700893364846706,
          0.014886434189975262,
          0.026362964883446693,
          -0.014015794731676579,
          0.021003656089305878,
          0.007531480398029089,
          -0.03403690084815025,
          -0.0006992439739406109
        ]
      },
      {
        "chunk_index": 757,
        "relative_chunk_number": 55,
        "text": "Each nation contains hundreds of distinct ethnolinguistic groups, with over 500 distinct indigenous languages in Nigeria (Campbell and Grondona, 2008), and over 600 eth- nic groups across Indonesia (Ananta et al., 2015). The following was approved by the Institutional Review Board at the authors\u2019 institution.",
        "text_length": 310,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.053023308515548706,
          0.015495308674871922,
          -0.011995872482657433,
          0.03273224085569382,
          -0.05991026759147644,
          0.023582635447382927,
          0.017079273238778114,
          0.04324830323457718,
          -0.03844311833381653,
          0.04337897524237633
        ]
      },
      {
        "chunk_index": 758,
        "relative_chunk_number": 56,
        "text": "We recruit annotators on Upwork, aiming for balance across ethnolinguistic groups (see Appendix A for details).",
        "text_length": 111,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010393290780484676,
          0.04424925521016121,
          0.019788004457950592,
          0.06309880316257477,
          -0.04814668372273445,
          0.023640070110559464,
          0.021442418918013573,
          -0.002431069500744343,
          -0.056690771132707596,
          0.02845681644976139
        ]
      },
      {
        "chunk_index": 759,
        "relative_chunk_number": 57,
        "text": "Annotations are collected in the national language for each respective country, since this language is shared across ethnolinguistic groups (Bahasa Indonesia for Indonesian annotators, and English Ethnolinguistic Synthetic Data Traditional Annotation CULTURE CARTOGRAPHY Country Groups Scored Answers Score ICC Avg. Score Fixed Answers Pref. Pairs Free Answers Pref.",
        "text_length": 366,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01534587237983942,
          0.03363484889268875,
          0.00014168121560942382,
          0.08784901350736618,
          -0.03813222423195839,
          0.012721432372927666,
          0.02186582051217556,
          0.01795009896159172,
          -0.04279640316963196,
          0.053207118064165115
        ]
      },
      {
        "chunk_index": 760,
        "relative_chunk_number": 58,
        "text": "Pairs Nigeria 7 1,913 0.58 2.6 / 3 944 757 521 262 Indonesia 13 3,412 0.55 2.5 / 3 1,081 468 586 1,196 Table 1: CULTURE CARTOGRAPHY Dataset Statistics demonstrate the size of the data we collected (\u223c1k answers to fixed questions; \u223c500 free answers with CULTURE EXPLORER) and its reliability (ICC \u22650.55 on Score annotations, which is moderate), as well as the cultural diversity of our annotator pool (7 distinct ethnolinguistic groups from Nigeria, and 13 distinct groups from Indonesia).",
        "text_length": 488,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.0010393272386863828,
          0.01858511008322239,
          -0.009504283778369427,
          0.05921512469649315,
          -0.00834426935762167,
          -0.016165275126695633,
          0.014222088269889355,
          -0.00012430442438926548,
          -0.02636181004345417,
          0.023325107991695404
        ]
      },
      {
        "chunk_index": 761,
        "relative_chunk_number": 59,
        "text": "Finally, we see that AI responses are quite reliable for these cultures, with quality scores that are better than \u201cgood\u201d on average (e.g., avg = 2.5/3.0 for Nigeria).",
        "text_length": 166,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.037430040538311005,
          0.04301335662603378,
          -0.015306640416383743,
          0.024697205051779747,
          -0.05939936637878418,
          0.010329929180443287,
          0.017820004373788834,
          -0.049774616956710815,
          -0.0344717875123024,
          4.011171404272318e-05
        ]
      },
      {
        "chunk_index": 762,
        "relative_chunk_number": 60,
        "text": "DeepSeek R1 GPT-4o Llama-4 Claude 3.5 o3-Mini Qwen2-72B Mixtral-8x22B 0.00 0.25 0.50 0.75 1.00 1.25 1.00 0.87 0.89 0.85 0.88 0.89 0.79 0.91 0.75 0.72 0.68 0.72 0.67 0.43 0.85 0.66 0.61 0.61 0.56 0.46 0.37 DeepSeek R1 GPT-4o Llama-4 Claude 3.5 o3-Mini Qwen2-72B Mixtral-8x22B 0.00 0.25 0.50 0.75 1.00 1.25 0.98 0.86 0.82 0.76 0.72 0.73 0.71 0.92 0.83 0.79 0.75 0.74 0.68 0.61 0.82 0.70 0.70 0.68 0.66 0.64 0.61 Recall@100 Indonesia Nigeria d=0.34* d=0.17* d=0.29* d=0.20** d=0.42**** d=0.24*** d=0.37* d=0.16** d=0.38** d=0.32**** d=0.49*** d=0.43**** ns d=0.29**** ns d=0.32**** ns d=0.21** ns ns ns d=0.16* ns ns Synthetic Data Traditional Annotation CultureCartography Figure 3: Performance on CULTURE CARTOGRAPHY.",
        "text_length": 716,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01237344741821289,
          -0.00787135399878025,
          0.00400312477722764,
          0.021129770204424858,
          -0.01290714181959629,
          -0.02925446256995201,
          0.00860368087887764,
          0.014816852286458015,
          -0.009939217008650303,
          0.026028770953416824
        ]
      },
      {
        "chunk_index": 763,
        "relative_chunk_number": 61,
        "text": "Powerful models like DeepSeek R1 can entirely solve Synthetic Data (R@100 \u226598%), and also perform well on Traditional Annotation data (R@100 \u226492%).",
        "text_length": 147,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04044080898165703,
          0.01223766803741455,
          -0.03997315838932991,
          0.03464560583233833,
          -0.018401488661766052,
          -0.06145310774445534,
          0.023370496928691864,
          -0.006872665602713823,
          -0.02922455035150051,
          -0.026466084644198418
        ]
      },
      {
        "chunk_index": 764,
        "relative_chunk_number": 62,
        "text": "Most importantly, CULTURE CARTOGRAPHY data is appreciably harder than these single-initiative data sources, with moderate and statistically significant effect sizes (ns = \u201cnot significant\u201d; \u2217p < 0.05; \u2217\u2217p < 0.01; \u2217\u2217\u2217p < 0.001; \u2217\u2217\u2217\u2217p < 0.0001) for both R1 (d = 0.17 Indonesia; d = 0.29 Nigeria) and GPT-4o (d = 0.20 Indonesia; d = 0.32 Nigeria). for Nigerian annotators).",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008442609570920467,
          0.018275661394000053,
          -0.012977263890206814,
          0.04313104227185249,
          -0.0013385394122451544,
          0.02119835652410984,
          0.015336585231125355,
          -3.177053440595046e-05,
          -0.025261713191866875,
          0.012626050971448421
        ]
      },
      {
        "chunk_index": 765,
        "relative_chunk_number": 63,
        "text": "To establish baselines for the CULTURE CARTOGRAPHY methodology and test our principal hypothesis that it is better than entirely model-driven approaches, we collect data in three distinct, non-overlapping subsets: (1) Synthetic Data: Humans validate the top-four answers given by the LLM to a pre-determined set of fixed questions about behavioral norms.4 For each question, annotators scored the AI answers for quality on the same 0-3 Likert Scale.",
        "text_length": 449,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0025269826874136925,
          0.026421550661325455,
          -0.026006601750850677,
          0.011304065585136414,
          0.013752981089055538,
          -0.015210526064038277,
          0.02163618803024292,
          -0.00813078973442316,
          -0.019779184833168983,
          0.012744002044200897
        ]
      },
      {
        "chunk_index": 766,
        "relative_chunk_number": 64,
        "text": "We retained 4We created this set of questions by running CULTURE EX- PLORER to a tree depth of 6 and filtering for behavioral norms with a FastText classifier that we distilled from GPT-4. For other aspects of data collection with CULTURE EXPLORER, the underlying LLM was gpt-3.5-turbo.",
        "text_length": 286,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010331866331398487,
          0.021793873980641365,
          -0.04149365797638893,
          0.006441948935389519,
          0.0046073272824287415,
          -0.02065110206604004,
          0.022726664319634438,
          0.013623056001961231,
          -0.024271657690405846,
          -0.04183085635304451
        ]
      },
      {
        "chunk_index": 767,
        "relative_chunk_number": 65,
        "text": "Thus we show the benefits of the CULTURE CARTOGRAPHY approach do not depend on the most recent advances in language modeling. only high-quality AI answers: for a given question, we identified all pairs of AI answers whose quality scores were different with statistical significance by t-test (\u03b1 = 0.05) and kept only the better answer from each pair.",
        "text_length": 350,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.006121686194092035,
          0.04636044427752495,
          -0.031834524124860764,
          0.008670959621667862,
          -0.02311728149652481,
          0.054536350071430206,
          0.019033456221222878,
          -0.03260260820388794,
          -0.03308878839015961,
          -0.008505682460963726
        ]
      },
      {
        "chunk_index": 768,
        "relative_chunk_number": 66,
        "text": "(2) Traditional Annotation: With the same fixed questions about behavioral norms, the respondents, having considered the top-four AI Responses above, then provided up to four new answers that com- plemented what the AI already gave. Annotators were explicitly encouraged to think of specific ex- amples from their most specific and local cultures about what AI wouldn\u2019t already know.",
        "text_length": 383,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0018430324271321297,
          0.03287205100059509,
          -0.018087349832057953,
          0.06199781969189644,
          -0.02474771998822689,
          -0.00131575099658221,
          0.017606742680072784,
          0.014116344042122364,
          -0.033799123018980026,
          0.045066338032484055
        ]
      },
      {
        "chunk_index": 769,
        "relative_chunk_number": 67,
        "text": "In this way, the Traditional Annotation directly mirrored the incentive structure of CULTURE CARTOGRAPHY to identify gaps in LLM knowledge. Importantly, in this subset, respondents could not edit questions or guide their topical distribution.",
        "text_length": 242,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.007479197345674038,
          0.03960537537932396,
          -0.014833308756351471,
          0.057724565267562866,
          0.015659736469388008,
          0.013058568350970745,
          0.019462667405605316,
          -0.001792033901438117,
          -0.0298414658755064,
          0.03956776484847069
        ]
      },
      {
        "chunk_index": 770,
        "relative_chunk_number": 68,
        "text": "(3) CULTURE CARTOGRAPHY: This repre- sents the set of all answers that humans wrote from scratch, working with the CULTURE EXPLORER in an unconstrained manner, where they could edit AI questions freely and iteratively. Annotators worked on the task for as long as they liked, and were paid a fair hourly rate, plus bonuses for their total edit distance on questions and answers.",
        "text_length": 378,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010478345677256584,
          0.031432218849658966,
          -0.014146857894957066,
          0.015827899798750877,
          -0.05188889801502228,
          0.0034244447015225887,
          0.018438832834362984,
          0.006195918656885624,
          -0.038655634969472885,
          0.03951925411820412
        ]
      },
      {
        "chunk_index": 771,
        "relative_chunk_number": 69,
        "text": "3.3 Dataset Summary Table 1 gives the summary statistics for the three annotated subsets of CULTURE CARTOGRAPHY. We worked with 19 Indonesian annotators from across 13 ethnolinguistic groups and 12 provinces, as well as 9 Nigerian annotators from 7 ethnolin- guistic groups across 5 states (see Tables 5 and 6 in Appendix A for more details).",
        "text_length": 342,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.024493757635354996,
          0.034954823553562164,
          -0.013087991625070572,
          0.07498768717050552,
          -0.038373447954654694,
          -0.017356468364596367,
          0.02026575617492199,
          0.009609821252524853,
          -0.041328925639390945,
          0.021841060370206833
        ]
      },
      {
        "chunk_index": 772,
        "relative_chunk_number": 70,
        "text": "From each pool of Nigerian and Indonesian annotators, we collected \u223c1k fixed answers with Traditional An- notation, and \u223c500 free answers with CULTURE CARTOGRAPHY. Annotators also scored > 5k LLM answers.",
        "text_length": 204,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0015724354889243841,
          0.035634852945804596,
          -0.01039278693497181,
          0.08470170944929123,
          -0.049323488026857376,
          -0.01547238789498806,
          0.02158508263528347,
          0.0022705316077917814,
          -0.04231305047869682,
          -0.02139998972415924
        ]
      },
      {
        "chunk_index": 773,
        "relative_chunk_number": 71,
        "text": "These score annotations are reliable, with a moderately high inter-annotator agreement of ICC \u22650.55, which is a moderate intraclass cor- relation (Shrout and Fleiss, 1979), and reasonable for the subjective nature of this task.",
        "text_length": 227,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03180399909615517,
          0.020352177321910858,
          -0.027370914816856384,
          0.059176743030548096,
          0.041689660400152206,
          0.019809521734714508,
          0.021717287600040436,
          -0.0023123417049646378,
          -0.03277844190597534,
          -0.016682347282767296
        ]
      },
      {
        "chunk_index": 774,
        "relative_chunk_number": 72,
        "text": "On the left side of Table 1, we see that the LLM\u2019s responses are quite reliable, with quality scores that are better than \u201cgood\u201d on average (e.g., avg = 2.5/3.0 for Nigeria). For all three countries, over 90% of answers were deemed at least passable (avg. score \u22651.0) \u2014 less than 10% of AI answers were deemed fully incorrect.",
        "text_length": 326,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0041624256409704685,
          0.031539253890514374,
          -0.040467265993356705,
          0.007417145185172558,
          -0.06095358729362488,
          0.025711553171277046,
          0.016338909044861794,
          -0.017244772985577583,
          -0.032724808901548386,
          -0.019238851964473724
        ]
      },
      {
        "chunk_index": 775,
        "relative_chunk_number": 73,
        "text": "One can interpret these results like precision metrics, suggesting that, when a well-prompted LLM answers its own pre- determined cultural questions about Nigerian and Indonesian cultures, the answers are reliable. 4 Evaluating CULTURE CARTOGRAPHY In this section, we test whether leading LLMs re- call less CULTURE CARTOGRAPHY data than they recall synthetic or traditionally annotated data.",
        "text_length": 392,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.020757168531417847,
          0.03469974920153618,
          0.0006937803118489683,
          0.029625480994582176,
          -0.023828327655792236,
          -0.005469517316669226,
          0.019569260999560356,
          0.03204994648694992,
          -0.03727444261312485,
          0.009547472931444645
        ]
      },
      {
        "chunk_index": 776,
        "relative_chunk_number": 74,
        "text": "We select seven flagship models to evaluate. There are three proprietary API models: GPT-4o (Hurst et al., 2024), o3-Mini5 (OpenAI, 2024), and Claude 3.5 Sonnet (Anthropic, 2024). The remaining four models are open-weight: DeepSeek R1 (Guo et al., 5We use the medium reasoning setting for o3-Mini. 2025), Llama-4-Maverick (Meta, 2025), Qwen 2- 72B (Yang et al., 2024a), and Mixtral-8x22B.",
        "text_length": 388,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0032033342868089676,
          -0.008477226831018925,
          -0.013181271031498909,
          0.019578907638788223,
          -0.021196631714701653,
          -0.03804931044578552,
          0.017444269731640816,
          0.017616186290979385,
          -0.015758680179715157,
          -0.04853513836860657
        ]
      },
      {
        "chunk_index": 777,
        "relative_chunk_number": 75,
        "text": "To scalably evaluate model awareness of gold answers, we rely on LLM-as-a-Judge evaluations with GPT- 4o to compute the Recall@K R@K = |{gold answers} \u2229{model answers @K}| |{gold answers}| Here, {model answers @K} refers to the set of all K answers produced by the model by iteratively prompting it: \u201cWe\u2019re looking different examples.",
        "text_length": 334,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03213503584265709,
          0.013513131998479366,
          -0.03519262745976448,
          0.047858282923698425,
          0.013044578954577446,
          -0.01652698405086994,
          0.023495860397815704,
          -0.026009635999798775,
          -0.018292078748345375,
          -0.017625348642468452
        ]
      },
      {
        "chunk_index": 778,
        "relative_chunk_number": 76,
        "text": "Without explanation, list 10 more examples.\u201d6 The LLM-as-a-Judge determines the overlap in the nu- merator by iterating through each gold answer and telling us: \u201cDoes any part of {model answers @K} contain the same information as the {gold answer}?",
        "text_length": 248,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.020505251362919807,
          0.026861580088734627,
          -0.01418614573776722,
          0.044100016355514526,
          0.000630214053671807,
          0.037374988198280334,
          0.019258365035057068,
          0.011264747940003872,
          -0.03451891615986824,
          0.015517796389758587
        ]
      },
      {
        "chunk_index": 779,
        "relative_chunk_number": 77,
        "text": "\u201d by answering \u201cYes\u201d or \u201cNo.\u201d For all subsequent experiments, we set K = 100 because baseline model performance on Synthetic Data plateaus here (see Figure 4 in Appendix B). Human validation also ensures that our LLM-as- a-Judge approach is reliable. One author blindly annotated a random sample of data, oversampling for the minority class.",
        "text_length": 341,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03599968180060387,
          0.03941577300429344,
          -0.03079996630549431,
          0.011503608897328377,
          -0.013813846744596958,
          0.022117067128419876,
          0.021740684285759926,
          -0.03687615320086479,
          -0.024838363751769066,
          -0.051133327186107635
        ]
      },
      {
        "chunk_index": 780,
        "relative_chunk_number": 78,
        "text": "We observed a substantial agreement of 85% between human and model judg- ments, with a Cohen\u2019s \u03ba = 0.66, which indicates substantial agreement (McHugh, 2012). Q1: Is CULTURE CARTOGRAPHY Data More Challenging? Figure 3 compares CULTURE CARTOGRAPHY data (in purple) against both base- lines: traditional annotation (in black), and syn- thetic data (in gray).",
        "text_length": 356,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04433543607592583,
          0.02639932930469513,
          -0.029343582689762115,
          0.06595228612422943,
          0.012020383961498737,
          0.0012545855715870857,
          0.019668132066726685,
          -0.0579642653465271,
          -0.028179090470075607,
          -0.010012657381594181
        ]
      },
      {
        "chunk_index": 781,
        "relative_chunk_number": 79,
        "text": "We see that, compared to tradi- tional annotation, Indonesian CULTURE CARTOG- RAPHY data is 6% less likely to be known by R1 (0.85 vs. 0.91), and Nigerian data is 10% less likely to be known by R1 (0.82 vs. 0.92). CULTURE CARTOGRAPHY is even less likely to be known by other models.",
        "text_length": 282,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.024704745039343834,
          0.03783044591546059,
          0.0089497659355402,
          0.06118730083107948,
          -0.03402652218937874,
          -0.006761793512851,
          0.020473560318350792,
          0.01939258724451065,
          -0.02907785214483738,
          0.030163997784256935
        ]
      },
      {
        "chunk_index": 782,
        "relative_chunk_number": 80,
        "text": "This difference between CULTURE CARTOGRAPHY and Traditional Annotation is sta- tistically significant by t-test (\u03b1 = 0.05) on both Indonesian and Nigerian data for each of the fol- lowing models: DeepSeek R1, GPT-4o, Llama-4, and o3-Mini. The effect sizes on R1 are Cohen\u2019s d =0.17 and d =0.29 for Indonesian and Nigerian performance gaps, indicating small or moderately- sized effects.",
        "text_length": 386,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006026444956660271,
          0.023020906373858452,
          -0.012562800198793411,
          0.04758938401937485,
          -0.04235254228115082,
          0.006880736444145441,
          0.01964435540139675,
          -0.006750927306711674,
          -0.01741497963666916,
          -0.00605040043592453
        ]
      },
      {
        "chunk_index": 783,
        "relative_chunk_number": 81,
        "text": "We conclude that CULTURE CAR- TOGRAPHY more readily produces challenging and long-tail cultural knowledge than does synthetic or traditional annotation. 6This parallels the instructions given to humans to find examples that AI wouldn\u2019t already know. Q2: How do LLMs Compare on CULTURE CARTOGRAPHY?",
        "text_length": 297,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.041505638509988785,
          0.025844885036349297,
          -0.009185087867081165,
          0.03609432280063629,
          -0.01187462080270052,
          4.453527071746066e-05,
          0.021153490990400314,
          0.00801265798509121,
          -0.04007399082183838,
          0.006645649205893278
        ]
      },
      {
        "chunk_index": 784,
        "relative_chunk_number": 82,
        "text": "Figure 3 also demonstrates that many LLMs cannot reach high levels of recall for CULTURE CARTOGRAPHY data, thus demon- strating gaps in their long-tail knowledge of plu- ralistic cultures.",
        "text_length": 188,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0016509518027305603,
          0.0464886836707592,
          -0.03696104511618614,
          0.005229572299867868,
          0.0785798579454422,
          -0.014566095545887947,
          0.02634473517537117,
          0.01345808245241642,
          -0.04618800804018974,
          -0.004417047835886478
        ]
      },
      {
        "chunk_index": 785,
        "relative_chunk_number": 83,
        "text": "DeepSeek R1 completely satu- rates the Synthetic subset, maintains a relatively high recall on Traditional Annotation data (91% Indonesia; 92% Nigeria), and achieves the highest overall performance on CULTURE CARTOGRAPHY (85% Indonesia; 82% Nigeria). In contrast to R1, Mixtral-8x22B fails to produce as much as half of the CULTURE CARTOGRAPHY.",
        "text_length": 344,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0031008401419967413,
          0.009725476615130901,
          -0.017175644636154175,
          0.055128976702690125,
          -0.02122384123504162,
          -0.04068688303232193,
          0.021587496623396873,
          -0.013781425543129444,
          -0.038663920015096664,
          0.018245812505483627
        ]
      },
      {
        "chunk_index": 786,
        "relative_chunk_number": 84,
        "text": "Most models are between these two extremes and attain mod- erate scores, with recall around 60-70% on the CULTURE CARTOGRAPHY subsets. When we sort models by their performance on CULTURE CAR- TOGRAPHY, we get the same relative order for both countries: DeepSeek R1 \u227bGPT-4o \u227bLlama-4 \u227b Claude 3.5 \u227bo3-Mini \u227bQwen2-72B \u227bMixtral- 8x22B.",
        "text_length": 331,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03019648790359497,
          0.02242153324186802,
          -0.04847940430045128,
          0.025589579716324806,
          0.02691819705069065,
          -0.024137698113918304,
          0.02039416879415512,
          -0.01353257056325674,
          -0.027531743049621582,
          0.027663618326187134
        ]
      },
      {
        "chunk_index": 787,
        "relative_chunk_number": 85,
        "text": "There is not a stark strong performance gap between API-based and open-weight models here, as both the best and the worst performing model are open-weight, and the runner-up model is the proprietary GPT-4o. Furthermore, reasoning mod- els do not unanimously win: o3-mini (\u223c200B pa- rameters) falls behind Claude 3.5 Sonnet (\u223c175B), a slightly smaller, non-reasoning model.",
        "text_length": 372,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01677826978266239,
          0.01196692418307066,
          -0.04562364146113396,
          0.023750191554427147,
          -0.0074247331358492374,
          -0.019747167825698853,
          0.019433455541729927,
          -0.025458376854658127,
          -0.016585877165198326,
          -0.020613864064216614
        ]
      },
      {
        "chunk_index": 788,
        "relative_chunk_number": 86,
        "text": "To con- clude, CULTURE CARTOGRAPHY allows produces stable evaluation results that reveal nuanced dif- ferences in model performance, not attributable to reasoning or model size alone. Q3: What Are the Knowledge Gaps? Even the best reasoning model, DeepSeek R1, fails to re- call 15-18% of CULTURE CARTOGRAPHY data. Now we investigate the topical distribution of this missing knowledge.",
        "text_length": 385,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.02407841943204403,
          0.04526117444038391,
          -0.02031363919377327,
          0.05434161052107811,
          0.0006790637853555381,
          -0.03638123720884323,
          0.021383386105298996,
          -0.03921990096569061,
          -0.02673685923218727,
          0.0046330722980201244
        ]
      },
      {
        "chunk_index": 789,
        "relative_chunk_number": 87,
        "text": "We do so by adapting LlooM (Lam et al., 2024), a concept induction algorithm. First, for each QA pair that DeepSeek R1 fails to recall, we prompt GPT-4o to summarize the QA pair with 3 bullet points of at most 30 words each.",
        "text_length": 224,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02322313003242016,
          0.01125088520348072,
          -0.03957311436533928,
          0.0010145788546651602,
          -0.030975185334682465,
          0.013548105023801327,
          0.02393759973347187,
          -0.01166801992803812,
          -0.03263073414564133,
          0.010609408840537071
        ]
      },
      {
        "chunk_index": 790,
        "relative_chunk_number": 88,
        "text": "Next we use k-means clustering over the full set of bullet point sentence embeddings (Reimers and Gurevych, 2019) to produce the top 10 semantic clusters each for the Nigerian and Indonesian sub- sets respectively. Then we prompt GPT-4o to per- form the LlooM Synthesize operator and summa- rize each bullet point cluster with two key concept patterns each.",
        "text_length": 357,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.027811605483293533,
          0.011946619488298893,
          -0.01994604617357254,
          0.04216624051332474,
          -0.030189041048288345,
          0.02473202347755432,
          0.023251160979270935,
          0.0023354929871857166,
          -0.049171749502420425,
          0.05742666497826576
        ]
      },
      {
        "chunk_index": 791,
        "relative_chunk_number": 89,
        "text": "A concept pattern consists of a text label and a corresponding prompt for classification. Finally, we use the concept prompts to classify the Nigerian CULTURE CARTOGRAPHY Data Concept Proportion 1. Community Engagement 79.2% 2. Cultural Preservation 77.1% 3. Family Roles 30.2% 4. Funeral Rituals 9.4% 5. Family and Community Integration 8.3% Indonesian CULTURE CARTOGRAPHY Data Concept Proportion 1.",
        "text_length": 400,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.011133522726595402,
          0.028599310666322708,
          0.004799604415893555,
          0.06363170593976974,
          0.009404062293469906,
          -0.04708750545978546,
          0.018222086131572723,
          -0.009908676147460938,
          -0.036967359483242035,
          0.053015585988759995
        ]
      },
      {
        "chunk_index": 792,
        "relative_chunk_number": 90,
        "text": "Cultural Adaptations 48.8% 2. Exclusive Cultural Practices 44.2% 3. Cultural Traditions 38.4% 4. Cultural Gatherings 38.4% 5. Community Engagement 31.4% Table 2: DeepSeek R1\u2019s top 5 most prevalent missing concepts for the Nigerian and Indonesian subsets of CULTURE CARTOGRAPHY data. Note that categories are not mutually exclusive, so proportions add to more than 100%.",
        "text_length": 369,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.00950951874256134,
          0.013304147869348526,
          -0.0014323742361739278,
          0.05590299889445305,
          -0.006596977356821299,
          -0.06598009169101715,
          0.018200596794486046,
          0.001941709197126329,
          -0.04847138375043869,
          -0.015237869694828987
        ]
      },
      {
        "chunk_index": 793,
        "relative_chunk_number": 91,
        "text": "knowledge that DeepSeek R1 originally missed. Table 2 lists DeepSeek R1\u2019s top 5 most prevalent missing concepts for Nigerian and Indonesian cul- tures respectively. DeepSeek R1\u2019s knowledge gaps here are not merely incidental trivia; they concern topics that are essential for preserving the social cohesion of families and larger communities.",
        "text_length": 342,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03122234344482422,
          0.01883288472890854,
          -0.0011179912835359573,
          0.04413527995347977,
          -0.003625132841989398,
          -0.01629289612174034,
          0.02467147260904312,
          0.0018254515016451478,
          -0.04365111514925957,
          -0.006562341470271349
        ]
      },
      {
        "chunk_index": 794,
        "relative_chunk_number": 92,
        "text": "We see Community Engagement appears in almost 80% of missing Nigerian knowledge, and almost a third of missing Indonesian knowledge. For example, R1 was unaware of the Bornean communal meal called baseprah in which people of different social status dine together, fostering the spirit of gotong royong or communal responsibility and unity. Q4: Is CULTURE CARTOGRAPHY Google- Proof?",
        "text_length": 381,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.013344528153538704,
          0.017378628253936768,
          0.0498894639313221,
          0.059562645852565765,
          -0.034129202365875244,
          -0.0490398146212101,
          0.021178031340241432,
          -0.007946343161165714,
          -0.04123162850737572,
          0.004651935771107674
        ]
      },
      {
        "chunk_index": 795,
        "relative_chunk_number": 93,
        "text": "If CULTURE CARTOGRAPHY can pro- duce data not found on the web, we can demon- strate another benefit of our methodology over knowledge extraction methods, or other bench- marks collected from the internet, which are prone to test set contamination (Oren et al., 2023). We ask, is CULTURE CARTOGRAPHY Google-Proof\u201d (Rein et al., 2024)?",
        "text_length": 334,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00864996388554573,
          0.022892596200108528,
          -0.00415616761893034,
          0.03589779883623123,
          -0.0015735318884253502,
          0.00922438781708479,
          0.020903995260596275,
          -0.01002710685133934,
          -0.03576663136482239,
          0.009235306642949581
        ]
      },
      {
        "chunk_index": 796,
        "relative_chunk_number": 94,
        "text": "That is, could the challenging questions from CULTURE CARTOGRAPHY be an- swered by a flagship LLM with retrieval access to the web. For direct comparison with our Figure 3 results, we evaluate GPT-4o with web search en- abled,7 and compare these results to GPT-4o\u2019s prior performance without search.",
        "text_length": 299,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.03987075388431549,
          0.03540680930018425,
          -0.006385055370628834,
          0.04170216619968414,
          -0.010571537539362907,
          -0.012286221608519554,
          0.020389849320054054,
          0.010156931355595589,
          -0.03644281625747681,
          -0.059043534100055695
        ]
      },
      {
        "chunk_index": 797,
        "relative_chunk_number": 95,
        "text": "We further estimate 7a.k.a., gpt-4o-search-preview GPT4o R@100 on CULTURE CARTOGRAPHY Culture ind nga no search 65.9 69.7 with search 61.9 54.8 Effect Size (d) 0.08 0.31 Significance ns p < 0.0001 Table 3: CULTURE CARTOGRAPHY is Google-Proof. GPT4o attains lower Recall@100 scores on CULTURE CARTOGRAPHY with web search enabled, vs. with- out search.",
        "text_length": 350,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0730624571442604,
          0.025813478976488113,
          -0.0059342216700315475,
          0.05058803781867027,
          0.034598808735609055,
          -0.024311186745762825,
          0.01952095329761505,
          -0.03088611550629139,
          -0.030407985672354698,
          0.005479210987687111
        ]
      },
      {
        "chunk_index": 798,
        "relative_chunk_number": 96,
        "text": "Since performance does not improve with search, we conclude that CULTURE CARTOGRAPHY is Google-Proof. performance for the most advanced frontier model currently available in Appendix C. Results in Table 3 show that web search fails to improve the performance of GPT4o on CULTURE CARTOGRAPHY. In fact, performance is worse with search (54.8% with, vs. 69.7% recall without search on nga; p < 0.0001).",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02337338775396347,
          0.01796533912420273,
          -0.02111206203699112,
          0.020204808562994003,
          -0.019129330292344093,
          -0.016049429774284363,
          0.02174469456076622,
          -0.025937922298908234,
          -0.03192305937409401,
          -0.0034277806989848614
        ]
      },
      {
        "chunk_index": 799,
        "relative_chunk_number": 97,
        "text": "The exact mechanism behind the performance drop is speculative, but it is conceivable that web search narrows the model\u2019s focus to the head of the knowledge distribution, since the model recalls less of the long tail. The principal conclusion we can draw is that CULTURE CARTOGRAPHY is Google-Proof.",
        "text_length": 299,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04198078066110611,
          0.02046765200793743,
          -0.016855735331773758,
          0.014938510954380035,
          -0.022393977269530296,
          -0.01201543863862753,
          0.017767157405614853,
          0.004623046610504389,
          -0.025939861312508583,
          -0.016782354563474655
        ]
      },
      {
        "chunk_index": 800,
        "relative_chunk_number": 98,
        "text": "Knowledge derived from this methodology is not easily retrieved from public web sources, including those used for Knowledge Extraction in \u00a72, like Wikipedia, television transcripts, and social media. This complements our previous findings, further demonstrating the benefits of CULTURE CARTOG- RAPHY over single-initiative data collection meth- ods.",
        "text_length": 349,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.006539067719131708,
          0.009868761524558067,
          -0.02027813345193863,
          0.03421223163604736,
          -0.03590048477053642,
          -0.004441993311047554,
          0.02182801440358162,
          -0.019719041883945465,
          -0.03938331827521324,
          0.009210126474499702
        ]
      },
      {
        "chunk_index": 801,
        "relative_chunk_number": 99,
        "text": "Such methods would be more easily solved by web search, as we will show in \u00a75. 5 Transfer Performance Section 4 showed that, compared to synthetic or traditional annotations, CULTURE CARTOGRAPHY data is more challenging, and unlike knowledge ex- traction, CULTURE CARTOGRAPHY helps circum- vent test set contamination, as it appears Google- Proof.",
        "text_length": 347,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01935305818915367,
          0.018679406493902206,
          -0.010395633988082409,
          0.031061116605997086,
          0.016048971563577652,
          -0.0014476511860266328,
          0.023568304255604744,
          -0.010494062677025795,
          -0.03217132389545441,
          0.00011023329716408625
        ]
      },
      {
        "chunk_index": 802,
        "relative_chunk_number": 100,
        "text": "Now we demonstrate that data produced with CULTURE CARTOGRAPHY quantifiably aligns with the objectives of prior efforts in culturally- aware NLP, since training on CULTURE CARTOG- RAPHY data can boost the downstream transfer per- formance of LLMs on related culture benchmarks. For completeness, we evaluate transfer perfor- mance on benchmarks that represent contrasting annotation paradigms.",
        "text_length": 393,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.03272559121251106,
          0.048459894955158234,
          -0.002792529994621873,
          0.02723809704184532,
          0.006370792631059885,
          -0.04860520735383034,
          0.023849457502365112,
          0.00996457040309906,
          -0.027895765379071236,
          0.0016484966035932302
        ]
      },
      {
        "chunk_index": 803,
        "relative_chunk_number": 101,
        "text": "First, we consider BLEnD (Myung et al., 2024), which represents a more traditional annotation approach where annotators responded to fixed questions from a set of 500 pre-defined question templates, resulting in the largest available benchmark for Indonesian and Nigerian cultural knowledge.",
        "text_length": 291,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010372976772487164,
          0.0032390255946666002,
          -0.017891185358166695,
          0.014959805645048618,
          -0.05430244281888008,
          -0.012897643260657787,
          0.02075306326150894,
          0.04945014417171478,
          -0.02189376763999462,
          0.019730323925614357
        ]
      },
      {
        "chunk_index": 804,
        "relative_chunk_number": 102,
        "text": "Next, we look at CulturalBench (Chiu et al., 2024), which was pro- duced with a red-teaming methodology similar to our mixed-initiative approach, where humans and LLMs jointly produce knowledge. In Cultur- alBench, humans propose social situations, and LLMs generate related MCQ questions; finally hu- mans modify the questions until they can stump the LLM.",
        "text_length": 357,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.014731918461620808,
          0.03881802037358284,
          -0.003168539609760046,
          0.012546072714030743,
          -0.006224770098924637,
          -0.06978544592857361,
          0.022482648491859436,
          -0.0382152758538723,
          -0.03479311987757683,
          0.0020988669712096453
        ]
      },
      {
        "chunk_index": 805,
        "relative_chunk_number": 103,
        "text": "The key difference between CulturalBench and CULTURE CARTOGRAPHY is that Cultural- Bench was produced in through a linear chat in- teraction rather than a tree-based exploration, and humans iteration did not impact the topical domain of LLM generations in real time. Training.",
        "text_length": 276,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.028496017679572105,
          0.03360015153884888,
          0.027186822146177292,
          0.027255214750766754,
          0.012657904997467995,
          -0.07698473334312439,
          0.02021041139960289,
          -0.016569338738918304,
          -0.034014925360679626,
          0.004833298735320568
        ]
      },
      {
        "chunk_index": 806,
        "relative_chunk_number": 104,
        "text": "Given compute limitations, we opted to train two relatively smaller models, Llama-3.1- 8B and Qwen-2-7B, on answers and preference pairs from either CULTURE CARTOGRAPHY or our Traditionally Annotated data from \u00a73.2. Training has two steps. First, we derive a set of high-quality preference pairs.",
        "text_length": 296,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.04875662177801132,
          0.050347257405519485,
          -0.00912557728588581,
          -0.00406104139983654,
          -0.04111770540475845,
          0.01059485413134098,
          0.02203272469341755,
          -0.005632359534502029,
          -0.030213292688131332,
          -0.024276306852698326
        ]
      },
      {
        "chunk_index": 807,
        "relative_chunk_number": 105,
        "text": "A preference pair is given for every set of AI answers whose scores are different with statistical significance by t-test with \u03b1 = 0.05. Following the method of Shaikh et al. (2025), we treat Human Responses as strictly preferable to any AI Responses. Then we train with SFT on the preferred answers, followed optionally by DPO on the derived preference pairs (see Appendix E for hyperparameters).",
        "text_length": 397,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0056039416231215,
          0.023933466523885727,
          -0.039856187999248505,
          0.009847331792116165,
          -0.03627968207001686,
          0.024312889203429222,
          0.021190330386161804,
          -0.035026345402002335,
          -0.024528779089450836,
          -0.044344641268253326
        ]
      },
      {
        "chunk_index": 808,
        "relative_chunk_number": 106,
        "text": "Results. Table 4 shows the transfer performance on the Nigerian (nga) and Indonesian (ind) sub- sets of BLeND and CulturalBench. We begin our discussion with the Llama results. Only CUL- TURE CARTOGRAPHY data (Cart.) helps Llama models significantly outperform vanilla models on both benchmarks.",
        "text_length": 295,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.020576104521751404,
          0.03211617469787598,
          -0.01584293507039547,
          0.02839815616607666,
          -0.014358943328261375,
          -0.010982475243508816,
          0.022873198613524437,
          0.019879471510648727,
          -0.040759433060884476,
          0.0033095460385084152
        ]
      },
      {
        "chunk_index": 809,
        "relative_chunk_number": 107,
        "text": "After SFT+DPO on CULTURE CARTOGRAPHY, Llama-3.1-8B achieves +6.5% accuracy on BLEnD-nga, and +7.1% accuracy on BLEnD-ind compared to vanilla (p < 0.0001 by paired t-test). CULTURE CARTOGRAPHY also sig- nificantly boosts vanilla models by +18.2% and +19.2% on CulturalBench-nga and CulturalBench- ind respectively (p < 0.05).",
        "text_length": 324,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.022389911115169525,
          0.015848031267523766,
          -0.05928162857890129,
          0.017895333468914032,
          0.010104612447321415,
          -0.006533850450068712,
          0.021969767287373543,
          0.015504221431910992,
          -0.024760732427239418,
          0.024337314069271088
        ]
      },
      {
        "chunk_index": 810,
        "relative_chunk_number": 108,
        "text": "This demonstrates how CULTURE CARTOGRAPHY produces data that aligns with prior benchmarking efforts. Training on CULTURE CARTOGRAPHY data also results in overall better downstream performance than training on the Traditionally Annotated data (Trad., Table 4). For example, on BLEnD-nga, per- formance is +3.2% better with Cart. than Trad (p < 0.0001).",
        "text_length": 351,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.03100953996181488,
          0.045612454414367676,
          -0.024796878919005394,
          0.01977572776377201,
          -0.014308943413197994,
          -0.011121034622192383,
          0.023610113188624382,
          -0.021024931222200394,
          -0.027643123641610146,
          0.010645574890077114
        ]
      },
      {
        "chunk_index": 811,
        "relative_chunk_number": 109,
        "text": "While these benefits are similar on CulturalBench, the results do not reach statistical significance with such a small test set (26 Indone- sian QA pairs in CulturalBench, vs. 18.5k pairs in BLEnD). This further the added utility of CUL- TURE CARTOGRAPHY for collecting knowledge that more richly reflects underlying cultures.",
        "text_length": 326,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00885750725865364,
          0.05154338851571083,
          -0.016058221459388733,
          0.03458058089017868,
          -0.005305389408022165,
          -0.01813538372516632,
          0.020801857113838196,
          -0.010120004415512085,
          -0.03027445822954178,
          -0.019787002354860306
        ]
      },
      {
        "chunk_index": 812,
        "relative_chunk_number": 110,
        "text": "The findings are directionally the same for the Qwen2-7B: training on CULTURE CARTOG- RAPHY data results in better downstream per- formance than training on Traditionally Anno- tated data. With SFT+DPO on data from CUL- TURE CARTOGRAPHY, Qwen2-7B achieves +3.9% accuracy CulturalBench-ind, +4.5% accuracy on CulturalBench-nga, and +0.6% accuracy on BLEnD-nga compared to the vanilla model.",
        "text_length": 389,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.016239628195762634,
          0.0439491868019104,
          -0.03418964147567749,
          0.023499982431530952,
          -0.009390359744429588,
          -0.035453133285045624,
          0.025314010679721832,
          0.01535510178655386,
          -0.027186036109924316,
          0.03054525889456272
        ]
      },
      {
        "chunk_index": 813,
        "relative_chunk_number": 111,
        "text": "These boosts are not as large, nor statistically significant as those observed with Llama-3.1-8B, but this is expected since vanilla Qwen-2 starts with higher baseline performance, more than 10% greater than vanilla Llama-3.1-8B on these benchmarks.",
        "text_length": 249,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.01065786462277174,
          0.03328901529312134,
          -0.0383288599550724,
          -0.008604485541582108,
          -0.026285085827112198,
          -0.02408456802368164,
          0.022395940497517586,
          0.0018417465034872293,
          -0.031863804906606674,
          0.01343291811645031
        ]
      },
      {
        "chunk_index": 814,
        "relative_chunk_number": 112,
        "text": "Furthermore, we see that fine-tuning with CUL- TURE CARTOGRAPHY helps close the performance gap between much smaller 8B open models and the much larger, proprietary GPT-4o model with search enabled (bottom of Table 4). In doing so, CULTURE CARTOGRAPHY lends itself to solutions for building more culturally-aware NLP systems.",
        "text_length": 325,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.024018218740820885,
          0.02560710906982422,
          -0.026820380240678787,
          0.026601407676935196,
          0.009705482050776482,
          -0.015390699729323387,
          0.02083688974380493,
          -0.004092324059456587,
          -0.028955142945051193,
          -0.0037097109016031027
        ]
      },
      {
        "chunk_index": 815,
        "relative_chunk_number": 113,
        "text": "Finally, these results exemplify the compar- ative advantage CULTURE CARTOGRAPHY has over single-initiative data collection. While web search failed to improve performance on the more challenging CULTURE CARTOGRAPHY data, search nearly saturates both BLEnD-nga and CulturalBench-ind, with accuracies above 90%.",
        "text_length": 310,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.027763819321990013,
          0.03280749171972275,
          -0.029258275404572487,
          0.03415963426232338,
          0.01960321143269539,
          -0.021501213312149048,
          0.022834530100226402,
          -0.035009995102882385,
          -0.033282458782196045,
          0.02005889266729355
        ]
      },
      {
        "chunk_index": 816,
        "relative_chunk_number": 114,
        "text": "The same search-enabled model achieved only 54.8% and 61.9% recall respectively on the Nige- rian and Indonesian subsets of our mixed-initiative CULTURE CARTOGRAPHY data (Table 3).",
        "text_length": 180,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0050248047336936,
          0.0416400320827961,
          -0.036025065928697586,
          0.04807979613542557,
          -0.014319165609776974,
          -0.028863275423645973,
          0.022441182285547256,
          -0.002632244722917676,
          -0.03800356760621071,
          0.02806067280471325
        ]
      },
      {
        "chunk_index": 817,
        "relative_chunk_number": 115,
        "text": "6 Conclusion Towards the development of culturally-competent language models, we contribute CULTURE EX- PLORER (\u00a73.1) as an interactive annotation tool that implements our mixed-initiative CULTURE CAR- BLEnD CulturalBench nga ind nga ind Llama-3.1-8B Vanilla 56.0 66.5 50.0 46.2 Trad. SFT 55.8 68.7 59.1 53.8 SFT+DPO 59.3 73.2\u2217\u2217\u2217 50.0 65.4 Cart.",
        "text_length": 345,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.040448613464832306,
          0.027401812374591827,
          -0.03178456798195839,
          0.03370611369609833,
          0.0037307317834347486,
          -0.007886339910328388,
          0.016086027026176453,
          0.0021112870890647173,
          -0.026984430849552155,
          -0.02910284884274006
        ]
      },
      {
        "chunk_index": 818,
        "relative_chunk_number": 116,
        "text": "SFT 57.9 69.1 68.2\u2217 61.5 SFT+DPO 62.5\u2217\u2217\u2217 73.6\u2217\u2217\u2217 63.6 65.4\u2217 Qwen2-7B Vanilla 64.9 79.8 68.2 69.2 Trad. SFT 63.7 79.6 63.6 65.4 SFT+DPO 63.6 77.4 68.2 65.4 Cart. SFT 63.8 79.4 63.6 65.4 SFT+DPO 65.5 78.7 72.7 73.1 GPT4o + search 76.8 91.3 95.5 84.6 Num.",
        "text_length": 252,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04547491669654846,
          0.006101266946643591,
          -0.05147426202893257,
          0.027902478352189064,
          0.016907429322600365,
          -0.007749391719698906,
          0.01441966462880373,
          0.03261907771229744,
          -0.01579427905380726,
          0.0020357249304652214
        ]
      },
      {
        "chunk_index": 819,
        "relative_chunk_number": 117,
        "text": "evals 16.4k 18.5k 22 26 Table 4: Zero-shot Cultural Awareness of Llama-3.1- 8B and Qwen-2- 7B after training (SFT) and optionally preference-tuning (+DPO) on either Traditional Anno- tation (Trad.) or CULTURE CARTOGRAPHY (Cart.), and evaluated on BLEnD and CulturalBench, with sta- tistically significant best performances among Llama models bolded (\u2217p < 0.05; \u2217\u2217\u2217p < 0.0001).",
        "text_length": 376,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.005894274450838566,
          0.009935879148542881,
          -0.038751982152462006,
          0.009803546592593193,
          -0.0007580326637253165,
          -0.03276294842362404,
          0.022274745628237724,
          0.0389300175011158,
          -0.028777219355106354,
          0.006130433641374111
        ]
      },
      {
        "chunk_index": 820,
        "relative_chunk_number": 118,
        "text": "CULTURE CARTOGRAPHY results in better downstream perfor- mance than Traditional Annotation data. Only CUL- TURE CARTOGRAPHY data results in significantly better performance than vanilla Llama models on both evalua- tion sets. Qwen results are directionally the same, with better downstream performance from CULTURE CAR- TOGRAPHY, but results are not statistically significant. TOGRAPHY methodology.",
        "text_length": 398,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.015619209967553616,
          0.04060671851038933,
          -0.012735378928482533,
          0.019611608237028122,
          0.007668857462704182,
          -0.022627653554081917,
          0.027170609682798386,
          -0.022671543061733246,
          -0.027945825830101967,
          0.014866545796394348
        ]
      },
      {
        "chunk_index": 821,
        "relative_chunk_number": 119,
        "text": "Compared to single- initiative annotation, CULTURE CARTOGRAPHY better satisfies two motivating desiderata: the data it generates is more challenging for models, while also being more representative of human interests.",
        "text_length": 217,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.05320362746715546,
          0.04369189962744713,
          -0.017911972478032112,
          0.04499003291130066,
          0.0170524250715971,
          -0.010773404501378536,
          0.019448529928922653,
          0.018424993380904198,
          -0.02805415727198124,
          0.006116898730397224
        ]
      },
      {
        "chunk_index": 822,
        "relative_chunk_number": 120,
        "text": "Six flagship LLMs attain \u226470% recall on CUL- TURE CARTOGRAPHY data, even when search is enabled, so we conclude that CULTURE CARTOG- RAPHY is Google-Proof, and thus less prone to test set contamination than single-initiative method- ologies.",
        "text_length": 241,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.012860680930316448,
          0.027150247246026993,
          -0.023703401908278465,
          0.030450861901044846,
          0.035071611404418945,
          -0.02162919193506241,
          0.024089248850941658,
          0.005327853839844465,
          -0.02843567542731762,
          -0.003492454532533884
        ]
      },
      {
        "chunk_index": 823,
        "relative_chunk_number": 121,
        "text": "Finally, we see that CULTURE CAR- TOGRAPHY aligns with prior efforts in culturally- aware NLP, since fine-tuning on this data boosts the downstream transfer performance of LLMs on prior benchmarks, and helps close the performance gap between larger and smaller models. To con- clude, CULTURE CARTOGRAPHY is a new mixed- initiative method for eliciting useful and represen- tative cultural knowledge.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.038869038224220276,
          0.026297492906451225,
          0.0045426227152347565,
          0.02833985909819603,
          0.02350960299372673,
          -0.040662869811058044,
          0.019332800060510635,
          -0.013391653075814247,
          -0.02530963346362114,
          0.03092985972762108
        ]
      },
      {
        "chunk_index": 824,
        "relative_chunk_number": 122,
        "text": "This may complement social science methodologies like surveys and semi- structured interviews, offering a new lens for study- ing cultural variation and heterogeneity. 7 Limitations Biases in Annotator Recruitment. It is difficult to recruit contributors from under-represented cul- tures.",
        "text_length": 289,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.003944747615605593,
          0.00979530904442072,
          -0.010224968194961548,
          0.06283653527498245,
          -0.04737725108861923,
          0.00522627355530858,
          0.023339830338954926,
          -0.0030082690063863993,
          -0.031051455065608025,
          0.01262572966516018
        ]
      },
      {
        "chunk_index": 825,
        "relative_chunk_number": 123,
        "text": "As a result, the majority of published knowl- edge banks are exclusively in English (Adilazuarda et al., 2024), and cover the culture groups most ac- cessible on Crowdwork platforms. In this work, we demonstrated a preliminary effort to construct mul- tilingual knowledge resources for low-resourced cultures, but this effort is far from complete.",
        "text_length": 347,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03783806785941124,
          0.025957925245165825,
          -0.006767944898456335,
          0.04686575382947922,
          -0.0062595997005701065,
          -0.0027968280483037233,
          0.01617230474948883,
          -0.009709961712360382,
          -0.03820501267910004,
          0.0015003310982137918
        ]
      },
      {
        "chunk_index": 826,
        "relative_chunk_number": 124,
        "text": "All of our annotator recruitment was performed through Upwork, which may introduce biases in annotator recruitment, including but not limited to imbalances in underlying annotator distribution on the platform, the ranking algorithm (S\u00fchr et al., 2021) or the system of worker reputations on which it is based (Thebault-Spieker et al., 2017).",
        "text_length": 341,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01584884151816368,
          0.03848886862397194,
          0.012672705575823784,
          0.04480991140007973,
          -0.042398352175951004,
          0.0192035473883152,
          0.018081363290548325,
          -0.008434079587459564,
          -0.03081461228430271,
          0.0428849533200264
        ]
      },
      {
        "chunk_index": 827,
        "relative_chunk_number": 125,
        "text": "Our recruitment strategy in \u00a73.2 was intended to ame- liorate some of these factors, as we took a roughly balanced stratified random sample across ethnolin- guistic groups for each country we studied.",
        "text_length": 200,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.007724655792117119,
          0.054646071046590805,
          -0.00795024260878563,
          0.03870847448706627,
          0.006724451202899218,
          0.012379337102174759,
          0.0206423532217741,
          0.001430998556315899,
          -0.05972544848918915,
          0.029802687466144562
        ]
      },
      {
        "chunk_index": 828,
        "relative_chunk_number": 126,
        "text": "Still, we were limited by time and the availability of an- notators, so the number of respondents from each group is relatively small, and as a result, the data we collected may not be fully representative of each group. More importantly, our study is limited to only the responses of those who have stable and reliable access to Upwork and the ability to commu- nicate in English.",
        "text_length": 381,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.034789085388183594,
          0.02213788405060768,
          -0.012254229746758938,
          0.05483696237206459,
          -0.0245111882686615,
          0.012475588358938694,
          0.014749255031347275,
          -0.0067385840229690075,
          -0.05226545035839081,
          0.014662085101008415
        ]
      },
      {
        "chunk_index": 829,
        "relative_chunk_number": 127,
        "text": "This is very likely to introduce biases in the worker pool (e.g., by education level, socioeconomic status, etc.). Culture is More Than Knowledge. This work builds on existing efforts in Cultural NLP, which focus primarily on benchmarking LLMs to identify critical knowledge gaps (Adilazuarda et al., 2024).",
        "text_length": 307,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0017422058153897524,
          0.01659303903579712,
          -0.014286324381828308,
          0.002175363013520837,
          -0.03737569972872734,
          -0.01615525409579277,
          0.02105569653213024,
          -0.02003864012658596,
          -0.027056632563471794,
          0.022564509883522987
        ]
      },
      {
        "chunk_index": 830,
        "relative_chunk_number": 128,
        "text": "Our current CULTURE EXPLORER implementation effectively generates a tree of cultural knowledge, which is useful for benchmarking LLMs. However, culture can be much broader than the domain of factual knowledge (Zhou et al., 2025), and there are other compelling applications for CULTURE CARTOGRAPHY more broadly which may be re- stricted in the current fact-based implementation of CULTURE EXPLORER.",
        "text_length": 398,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004152784124016762,
          0.015019001439213753,
          -0.025310680270195007,
          0.01411524135619402,
          -0.005773427430540323,
          -0.00889690313488245,
          0.021048612892627716,
          -0.02065976709127426,
          -0.031239762902259827,
          -0.01718060113489628
        ]
      },
      {
        "chunk_index": 831,
        "relative_chunk_number": 129,
        "text": "For example, social sci- entists may be interested in extending CULTURE EXPLORER to help build digital museums that pre- serve not only knowledge, but also stories, history, and cultural artifacts (Srinivasan and Huang, 2005).",
        "text_length": 226,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          2.0224879335728474e-05,
          -0.015732768923044205,
          -0.009116215631365776,
          0.016995510086417198,
          0.032453469932079315,
          -0.031306322664022446,
          0.018672611564397812,
          -0.01203431561589241,
          -0.03161042928695679,
          -0.04140530154109001
        ]
      },
      {
        "chunk_index": 832,
        "relative_chunk_number": 130,
        "text": "To support a more fluid ontology in this setting would require further engineering CULTURE EX- PLORER beyond its question-answer tree format, and to consider broader themes or abstractions: not only the highly-detailed features of daily life. 8 Ethics Responsible Research Ethics.",
        "text_length": 280,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0004973208997398615,
          0.043724846094846725,
          -0.026463674381375313,
          -0.0014496702933683991,
          0.018973467871546745,
          -0.012022123672068119,
          0.020295709371566772,
          0.01299317553639412,
          -0.029670067131519318,
          -0.0606866292655468
        ]
      },
      {
        "chunk_index": 833,
        "relative_chunk_number": 131,
        "text": "This study has been approved by the Institutional Review Board (IRB) at the researchers\u2019 institution, and partici- pant consent was obtained using the standard in- stitutional consent form. Annotators were also en- couraged to stop any time. They were paid a fair stipend of $20 per hour for their time. To protect annotators\u2019 privacy, all data was anonymized. Risks in Deployment.",
        "text_length": 381,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0010367118520662189,
          0.013899413868784904,
          -0.0195857435464859,
          0.040638405829668045,
          -0.09116385132074356,
          0.028286071494221687,
          0.018343351781368256,
          -0.01352243684232235,
          -0.03851824626326561,
          0.07830705493688583
        ]
      },
      {
        "chunk_index": 834,
        "relative_chunk_number": 132,
        "text": "Here we outline risks in deploying CULTURE EXPLORER. Since this tool is powered by a Large Language Model, it shares the risks of many other human-LLM interactions, which include the potential harms of offensive, stereotypical, or hateful outputs, and the risk of mis- information.",
        "text_length": 281,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.021055499091744423,
          0.028665682300925255,
          -0.013873147778213024,
          0.01806809939444065,
          -0.004503951407968998,
          -0.032288093119859695,
          0.02049672231078148,
          -0.0017542399000376463,
          -0.028574608266353607,
          -0.07340525835752487
        ]
      },
      {
        "chunk_index": 835,
        "relative_chunk_number": 133,
        "text": "These risks are mitigated by our task- specific prompts, which constrain the output distri- bution, and by our use of safety-aligned LLMs. Specifically, in this domain, LLMs may misin- terpret, flatten, or misrepresent nuanced cultural knowledge.",
        "text_length": 246,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.05347224324941635,
          0.01593955047428608,
          -0.025842318311333656,
          0.046742379665374756,
          0.045593950897455215,
          0.013500213623046875,
          0.02440951019525528,
          0.04743802919983864,
          -0.02457806095480919,
          -0.010814794339239597
        ]
      },
      {
        "chunk_index": 836,
        "relative_chunk_number": 134,
        "text": "If users do not carefully consider these risks, or overly rely on LLM suggestions, their work will be more prone to endorse dominant cul- tural narratives at the expense of authentic expres- sion. To mitigate this risk, we explicitly state in the instructions and onboarding video: \u201cPlease be critical! You know your culture better than AI.\u201d For more details on the onboarding, see Appendix F.2.",
        "text_length": 395,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01899571530520916,
          0.017119864001870155,
          -0.013517992570996284,
          0.02959098480641842,
          0.01135949231684208,
          -0.031290579587221146,
          0.021733475849032402,
          0.028122499585151672,
          -0.029324255883693695,
          -0.015089997090399265
        ]
      },
      {
        "chunk_index": 837,
        "relative_chunk_number": 135,
        "text": "Acknowledgments We are thankful to the members of SALT Lab and the Stanford NLP Group: particularly Camille Har- ris, Giuseppe Russo, Raj Sanjay Shah, Myra Cheng, Jared Moore, and Sunny Yu for their helpful feed- back on the draft. We also immensely appreciated regular feedback from Jing Huang, Julia Kruk, and Yanzhe Zhang in the earliest stages of this project.",
        "text_length": 364,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0075160968117415905,
          0.017489731311798096,
          0.00864110141992569,
          0.021627426147460938,
          -0.04006839543581009,
          0.020923759788274765,
          0.014436066150665283,
          0.014515049755573273,
          -0.027537012472748756,
          -0.01004456914961338
        ]
      },
      {
        "chunk_index": 838,
        "relative_chunk_number": 136,
        "text": "Caleb Ziems was supported by the NSF Gradu- ate Research Fellowship under Grant No. DGE- 2039655. The work was supported in part by a grant from Meta, and by a grant from the Stan- ford Institute for Human-Centered Artificial Intelli- gence (HAI).",
        "text_length": 247,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.029693029820919037,
          -0.010717838071286678,
          -0.008654516190290451,
          0.02502955123782158,
          0.0015111371176317334,
          -0.0039028720930218697,
          0.021420808508992195,
          -0.0014906743308529258,
          -0.02338467910885811,
          -0.031478531658649445
        ]
      },
      {
        "chunk_index": 839,
        "relative_chunk_number": 137,
        "text": "References David Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, et al. 2022. A few thousand translations go a long way! leveraging pre-trained models for african news trans- lation.",
        "text_length": 271,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.011733847670257092,
          0.031255777925252914,
          0.007150473538786173,
          0.03253829851746559,
          0.01622350700199604,
          0.012957903556525707,
          0.02003171481192112,
          0.019588248804211617,
          -0.030149882659316063,
          -0.027754582464694977
        ]
      },
      {
        "chunk_index": 840,
        "relative_chunk_number": 138,
        "text": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 3053\u20133070. Tufan Adiguzel, Mehmet Haldun Kaya, and Fatih K\u00fcr- sat Cansu. 2023. Revolutionizing education with ai: Exploring the transformative potential of chatgpt. Contemporary Educational Technology, 15(3).",
        "text_length": 363,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.012397573329508305,
          0.0054854643531143665,
          -0.020640887320041656,
          0.013264196924865246,
          -0.03586583584547043,
          -0.038609541952610016,
          0.02006261609494686,
          -0.018162159249186516,
          -0.04118426889181137,
          -0.01297334861010313
        ]
      },
      {
        "chunk_index": 841,
        "relative_chunk_number": 139,
        "text": "Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Shivdutt Singh, Al- ham Fikri Aji, Jacki O\u2019Neill, Ashutosh Modi, and Monojit Choudhury. 2024. Towards measuring and modeling \u201cculture\u201d in LLMs: A survey. In Proceed- ings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15763\u201315784, Miami, Florida, USA.",
        "text_length": 361,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007274947594851255,
          0.0021452198270708323,
          0.0085749551653862,
          0.006388016976416111,
          -0.024483097717165947,
          -0.0249728262424469,
          0.01706087216734886,
          -0.004720177501440048,
          -0.02783319354057312,
          -0.04462863877415657
        ]
      },
      {
        "chunk_index": 842,
        "relative_chunk_number": 140,
        "text": "Association for Computational Linguistics. Aris Ananta, Evi Nurvidya Arifin, M Sairi Hasbullah, Nur Budi Handayani, and Agus Pramono. 2015. De- mography of Indonesia\u2019s ethnicity. Institute of South- east Asian Studies. Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Donald E Brown. 2004. Human universals, human nature & human culture. Daedalus, 133(4):47\u201354.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.0228913314640522,
          0.016288377344608307,
          -0.007125870790332556,
          0.051695648580789566,
          -0.019368711858987808,
          -0.056474216282367706,
          0.015054747462272644,
          -0.04294506460428238,
          -0.03765743598341942,
          0.010130670852959156
        ]
      },
      {
        "chunk_index": 843,
        "relative_chunk_number": 141,
        "text": "Lyle Campbell and Ver\u00f3nica Grondona. 2008. Eth- nologue: Languages of the world. Language, 84(3):636\u2013641. Yong Cao, Min Chen, and Daniel Hershcovich. 2024. Bridging cultural nuances in dialogue agents through cultural value surveys. In Findings of the Associ- ation for Computational Linguistics: EACL 2024, pages 929\u2013945, St. Julian\u2019s, Malta. Association for Computational Linguistics.",
        "text_length": 386,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0012737690703943372,
          0.012405251152813435,
          -0.01203150488436222,
          0.01423068530857563,
          -0.03581137955188751,
          -0.021204199641942978,
          0.014975183643400669,
          -0.002815484069287777,
          -0.03212248533964157,
          0.02339453063905239
        ]
      },
      {
        "chunk_index": 844,
        "relative_chunk_number": 142,
        "text": "Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel Hershcovich. 2023. Assessing cross-cultural alignment between chatgpt and human societies: An empirical study. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 53\u201367. Michael Carmichael and J. Stinson. 2024.",
        "text_length": 318,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0023115703370422125,
          0.03444214165210724,
          -0.004445906262844801,
          0.04208789020776749,
          0.026429740712046623,
          -0.03656212240457535,
          0.01753712072968483,
          -0.012376366183161736,
          -0.03801562637090683,
          -0.03819296136498451
        ]
      },
      {
        "chunk_index": 845,
        "relative_chunk_number": 143,
        "text": "The ipsos ai monitor 2024: Changing attitudes and feelings about ai and the future it will bring. Ipsos Online. Mario Casillo, Francesco Colace, Dajana Conte, Marco Lombardi, Domenico Santaniello, and Carmine Valentino. 2023. Context-aware recommender sys- tems and cultural heritage: a survey. Journal of Ambient Intelligence and Humanized Computing, 14(4):3109\u20133127.",
        "text_length": 368,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.015147409401834011,
          -0.007927619852125645,
          -0.04861657693982124,
          0.019121579825878143,
          0.060725752264261246,
          -0.028084535151720047,
          0.015705659985542297,
          0.022031748667359352,
          -0.03036893904209137,
          0.025973286479711533
        ]
      },
      {
        "chunk_index": 846,
        "relative_chunk_number": 144,
        "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Ka- plan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Myra Cheng, Tiziano Piccardi, and Diyi Yang. 2023. Compost: Characterizing and evaluating caricature in llm simulations.",
        "text_length": 375,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.011864298954606056,
          0.02515341155230999,
          0.000775607768446207,
          0.014133415184915066,
          -1.3900204066885635e-05,
          -0.008379125967621803,
          0.02034052088856697,
          0.019199229776859283,
          -0.024492692202329636,
          -0.001877220580354333
        ]
      },
      {
        "chunk_index": 847,
        "relative_chunk_number": 145,
        "text": "In Proceedings of the 2023 Con- ference on Empirical Methods in Natural Language Processing, pages 10853\u201310875. Yu Ying Chiu, Liwei Jiang, Maria Antoniak, Chan Young Park, Shuyue Stella Li, Mehar Bha- tia, Sahithya Ravi, Yulia Tsvetkov, Vered Shwartz, and Yejin Choi. 2024. Culturalteaming: Ai- assisted interactive red-teaming for challenging llms\u2019(lack of) multicultural knowledge.",
        "text_length": 383,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0024455629754811525,
          0.024645017459988594,
          -0.013732648454606533,
          0.028163131326436996,
          -0.03731207922101021,
          -0.047626662999391556,
          0.01427171565592289,
          -0.028927447274327278,
          -0.02653021179139614,
          0.0063916705548763275
        ]
      },
      {
        "chunk_index": 848,
        "relative_chunk_number": 146,
        "text": "arXiv preprint arXiv:2404.06664. David Cohn, Les Atlas, and Richard Ladner. 1994. Im- proving generalization with active learning. Machine learning, 15:201\u2013221. Yi Fung, Tuhin Chakrabarty, Hao Guo, Owen Ram- bow, Smaranda Muresan, and Heng Ji. 2023. Norm- sage: Multi-lingual multi-cultural norm discovery from conversations on-the-fly.",
        "text_length": 336,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0021087017375975847,
          -0.002311879303306341,
          -0.037486638873815536,
          0.019276460632681847,
          -0.0009844352025538683,
          -0.03722132742404938,
          0.015580856241285801,
          -0.014804796315729618,
          -0.03271302580833435,
          0.06606738269329071
        ]
      },
      {
        "chunk_index": 849,
        "relative_chunk_number": 147,
        "text": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15217\u201315230. Yi Fung, Ruining Zhao, Jae Doo, Chenkai Sun, and Heng Ji. 2024. Massively multi-cultural knowledge acquisition & lm benchmarking. arXiv preprint arXiv:2402.09369. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.",
        "text_length": 398,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01397633459419012,
          0.025636594742536545,
          -0.012793970294296741,
          0.025552472099661827,
          -0.04917750880122185,
          -0.05301725119352341,
          0.0156931821256876,
          -0.017994821071624756,
          -0.028511464595794678,
          -0.019296811893582344
        ]
      },
      {
        "chunk_index": 850,
        "relative_chunk_number": 148,
        "text": "2025. Deepseek-r1: In- centivizing reasoning capability in llms via reinforce- ment learning. arXiv preprint arXiv:2501.12948. Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Pi- queras, Ilias Chalkidis, Ruixiang Cui, et al. 2022. Challenges and strategies in cross-cultural nlp.",
        "text_length": 367,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.028673887252807617,
          0.008364053443074226,
          -0.03874479606747627,
          0.031771689653396606,
          0.017754362896084785,
          -0.042453866451978683,
          0.012745846062898636,
          0.028519805520772934,
          -0.020742498338222504,
          0.0055520255118608475
        ]
      },
      {
        "chunk_index": 851,
        "relative_chunk_number": 149,
        "text": "In Proceedings of the 60th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 6997\u20137013. Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pages 159\u2013 166. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.",
        "text_length": 397,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.027915120124816895,
          0.007883326150476933,
          0.010843240655958652,
          0.04334017634391785,
          -0.0241080392152071,
          -0.042647868394851685,
          0.011345324106514454,
          -0.014925642870366573,
          -0.025211818516254425,
          0.008044957183301449
        ]
      },
      {
        "chunk_index": 852,
        "relative_chunk_number": 150,
        "text": "2021. Lora: Low-rank adaptation of large lan- guage models. In International Conference on Learn- ing Representations. Jing Huang and Diyi Yang. 2023. Culturally aware natural language inference. In Findings of the Associ- ation for Computational Linguistics: EMNLP 2023, pages 7591\u20137609.",
        "text_length": 288,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.04001551866531372,
          0.03806173801422119,
          -0.035108182579278946,
          0.03075203113257885,
          0.021780509501695633,
          -0.053169164806604385,
          0.02277170494198799,
          -0.025059759616851807,
          -0.02289579063653946,
          -0.009568827226758003
        ]
      },
      {
        "chunk_index": 853,
        "relative_chunk_number": 151,
        "text": "Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os- trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Comfort Ilevbare, Jesujoba Alabi, David Ifeoluwa Adelani, Firdous Bakare, Oluwatoyin Abiola, and Oluwaseyi Adeyemo. 2024.",
        "text_length": 327,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.015155010856688023,
          0.007916690781712532,
          -0.02178914286196232,
          0.006393699906766415,
          -0.04334481060504913,
          0.018074942752718925,
          0.01202315092086792,
          -0.026713814586400986,
          -0.0186008233577013,
          0.002506102668121457
        ]
      },
      {
        "chunk_index": 854,
        "relative_chunk_number": 152,
        "text": "Ekohate: Abusive lan- guage and hate speech detection for code-switched political discussions on nigerian twitter. In Proceed- ings of the 8th Workshop on Online Abuse and Harms (WOAH 2024), pages 28\u201337. Petros Karatsareas. 2022. Semi-structured interviews. Research methods in language attitudes, pages 99\u2013 113.",
        "text_length": 312,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.029301656410098076,
          0.019080355763435364,
          -0.004712162539362907,
          0.025869326665997505,
          -0.020865095779299736,
          -0.01745469868183136,
          0.02041098102927208,
          -0.03397751599550247,
          -0.022712891921401024,
          0.014806881546974182
        ]
      },
      {
        "chunk_index": 855,
        "relative_chunk_number": 153,
        "text": "Hannah Rose Kirk, Alexander Whitefield, Paul Rottger, Andrew M Bean, Katerina Margatina, Rafael Mosquera-Gomez, Juan Ciro, Max Bartolo, Adina Williams, He He, et al. 2024. The prism alignment dataset: What participatory, representative and indi- vidualised human feedback reveals about the subjec- tive and multicultural alignment of large language models.",
        "text_length": 356,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008928867988288403,
          -0.019517751410603523,
          -0.0045876153744757175,
          0.05964316800236702,
          0.02517835795879364,
          -0.009229309856891632,
          0.013492022641003132,
          0.02005458064377308,
          -0.029225381091237068,
          0.04480575770139694
        ]
      },
      {
        "chunk_index": 856,
        "relative_chunk_number": 154,
        "text": "Advances in Neural Information Processing Systems, 37:105236\u2013105344. Ian Kivlichan, Zi Lin, Jeremiah Liu, and Lucy Vasser- man. 2021. Measuring and improving model- moderator collaboration using uncertainty estimation. In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 36\u201353. Michelle S Lam, Janice Teoh, James A Landay, Jeffrey Heer, and Michael S Bernstein. 2024.",
        "text_length": 395,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.010165474377572536,
          -0.004098306410014629,
          -0.016547024250030518,
          0.039277248084545135,
          -0.027176083996891975,
          -0.021405259147286415,
          0.012036554515361786,
          -0.035192351788282394,
          -0.024903208017349243,
          0.023068010807037354
        ]
      },
      {
        "chunk_index": 857,
        "relative_chunk_number": 155,
        "text": "Concept in- duction: Analyzing unstructured text with high-level concepts using lloom. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pages 1\u201328. Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. 2023a. Eliciting human preferences with language models. arXiv preprint arXiv:2310.11589. Huihan Li, Arnav Goel, Keyu He, and Xiang Ren. 2024a.",
        "text_length": 383,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.005746612790971994,
          0.013533958233892918,
          -0.004601585678756237,
          0.025343209505081177,
          -0.04370112717151642,
          -0.05521218851208687,
          0.015013938769698143,
          -0.0061909812502563,
          -0.022130679339170456,
          0.022638307884335518
        ]
      },
      {
        "chunk_index": 858,
        "relative_chunk_number": 156,
        "text": "Attributing culture-conditioned generations to pre- training corpora. arXiv preprint arXiv:2412.20760. Jialin Li, Junli Wang, Junjie Hu, and Ming Jiang. 2024b. How well do llms identify cultural unity in diversity? arXiv preprint arXiv:2408.05102. Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy Chen, Zhengyuan Liu, and Diyi Yang. 2023b.",
        "text_length": 345,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0011421713279560208,
          0.011603519320487976,
          -0.011413553729653358,
          0.0329727940261364,
          0.03045850805938244,
          -0.02598550170660019,
          0.01695135608315468,
          -0.02198251709342003,
          -0.025250477716326714,
          0.005845512263476849
        ]
      },
      {
        "chunk_index": 859,
        "relative_chunk_number": 157,
        "text": "Coannotating: Uncertainty-guided work allocation between human and large language models for data annotation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Process- ing, pages 1487\u20131505. Yang Liu, Meng Xu, Shuo Wang, Liner Yang, Haoyu Wang, Zhenghao Liu, Cunliang Kong, Yun Chen, Maosong Sun, and Erhong Yang. 2024.",
        "text_length": 348,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.007635789457708597,
          0.02613498643040657,
          -0.006042773835361004,
          0.05672035738825798,
          -0.015685809776186943,
          -0.03473609313368797,
          0.01718679443001747,
          0.01780066266655922,
          -0.028253890573978424,
          0.015609282068908215
        ]
      },
      {
        "chunk_index": 860,
        "relative_chunk_number": 158,
        "text": "Omgeval: An open multilingual generative evaluation bench- mark for large language models. arXiv preprint arXiv:2402.13524. Mihai Masala, Denis C Ilie-Ablachim, Alexandru Dima, Dragos Corlatescu, Miruna Zavelca, Ovio Olaru, Simina Terian, Andrei Terian, Marius Leordeanu, Horia Velicu, et al. 2024.",
        "text_length": 298,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.016347071155905724,
          0.03536401316523552,
          -0.007296265102922916,
          0.03423947095870972,
          0.031539902091026306,
          -0.0123074259608984,
          0.013944532722234726,
          0.002867575967684388,
          -0.020542249083518982,
          0.03492032736539841
        ]
      },
      {
        "chunk_index": 861,
        "relative_chunk_number": 159,
        "text": "\" vorbe\\c {s} ti rom\\\u02c6 ane\\c {s} te?\" a recipe to train powerful roma- nian llms with english instructions. arXiv preprint arXiv:2406.18266. Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276\u2013282. Meta. 2025. The llama 4 herd: The beginning of a new era of natively multimodal ai innovation.",
        "text_length": 334,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03452863544225693,
          0.015115607529878616,
          -0.026251759380102158,
          0.014496910385787487,
          0.03343247249722481,
          0.038636915385723114,
          0.01961614564061165,
          -0.025936342775821686,
          -0.03065274842083454,
          0.004486199002712965
        ]
      },
      {
        "chunk_index": 862,
        "relative_chunk_number": 160,
        "text": "Shamsuddeen Muhammad, Idris Abdulmumin, Abinew Ayele, Nedjma Ousidhoum, David Adelani, Seid Yi- mam, Ibrahim Ahmad, Meriem Beloucif, Saif Mo- hammad, Sebastian Ruder, et al. 2023. Afrisenti: A twitter sentiment analysis benchmark for african lan- guages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13968\u201313981.",
        "text_length": 364,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.005540601909160614,
          0.00786108709871769,
          0.007801856845617294,
          0.024649277329444885,
          0.04024939611554146,
          -0.028976254165172577,
          0.013459371402859688,
          -0.003777215024456382,
          -0.02079024538397789,
          0.011331910267472267
        ]
      },
      {
        "chunk_index": 863,
        "relative_chunk_number": 161,
        "text": "Shamsuddeen Hassan Muhammad, David Ifeoluwa Ade- lani, Sebastian Ruder, Ibrahim Sa\u2019id Ahmad, Idris Abdulmumin, Bello Shehu-Bello, Monojit Choud- hury, Chris Chinenye Emezue, Saheed Salahudeen Abdullahi, Anuoluwapo Aremu, et al. 2022. Nai- jasenti: A nigerian twitter sentiment corpus for mul- tilingual sentiment analysis.",
        "text_length": 322,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02041558362543583,
          0.009453491307795048,
          -0.005947439931333065,
          0.004858802538365126,
          -0.00202188384719193,
          0.01571536995470524,
          0.019406631588935852,
          -0.012006789445877075,
          -0.019853129982948303,
          0.009504029527306557
        ]
      },
      {
        "chunk_index": 864,
        "relative_chunk_number": 162,
        "text": "In Proceedings of the Thirteenth Language Resources and Evaluation Con- ference, pages 590\u2013602. Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, et al. 2024. Blend: A benchmark for llms on everyday knowledge in diverse cultures and languages. arXiv preprint arXiv:2406.09948.",
        "text_length": 380,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.004004946444183588,
          -0.009061437100172043,
          -0.018602337688207626,
          0.024312876164913177,
          -0.01624421216547489,
          -0.00916992500424385,
          0.017012592405080795,
          -0.00022939049813430756,
          -0.031394049525260925,
          -0.0008916020160540938
        ]
      },
      {
        "chunk_index": 865,
        "relative_chunk_number": 163,
        "text": "Tarek Naous, Michael J Ryan, Alan Ritter, and Wei Xu. 2024. Having beer after prayer? measuring cultural bias in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 16366\u201316393, Bangkok, Thailand. Association for Computational Linguistics.",
        "text_length": 334,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.02884843572974205,
          0.027790900319814682,
          0.014251336455345154,
          0.021777529269456863,
          -0.022999266162514687,
          -0.012235937640070915,
          0.02015538327395916,
          -0.032983798533678055,
          -0.02816799469292164,
          0.0663263127207756
        ]
      },
      {
        "chunk_index": 866,
        "relative_chunk_number": 164,
        "text": "Tuan-Phong Nguyen, Simon Razniewski, Aparna Varde, and Gerhard Weikum. 2023. Extracting cultural com- monsense knowledge at scale. In Proceedings of the ACM Web Conference 2023, pages 1907\u20131917. Tuan-Phong Nguyen, Simon Razniewski, and Gerhard Weikum. 2024. Cultural commonsense knowledge for intercultural dialogues.",
        "text_length": 317,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0271306149661541,
          0.025847623124718666,
          0.007069641724228859,
          0.04399240389466286,
          -0.0347210094332695,
          -0.02475612238049507,
          0.02529965154826641,
          -0.0698150247335434,
          -0.03543265163898468,
          0.009768116287887096
        ]
      },
      {
        "chunk_index": 867,
        "relative_chunk_number": 165,
        "text": "In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 1774\u20131784. Ihenaetu Olamma, Michael Kingsley, and Sunday Ojo. 2019. Hidden markov-based part-of-speech tagger for igbo resource-scarce african language.",
        "text_length": 255,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01823364943265915,
          0.003610609332099557,
          -0.009027757681906223,
          0.028919650241732597,
          0.012087059207260609,
          0.06683127582073212,
          0.02349107898771763,
          0.026054665446281433,
          -0.03376117721199989,
          0.026275714859366417
        ]
      },
      {
        "chunk_index": 868,
        "relative_chunk_number": 166,
        "text": "In Pro- ceedings of the First International Workshop on NLP Solutions for Under Resourced Languages (NSURL 2019) co-located with ICNLSP 2019-Short Papers, pages 118\u2013123. Tobi Olatunji, Tejumade Afonja, Aditya Yadavalli, Chris Chinenye Emezue, Sahib Singh, Bonaven- ture FP Dossou, Joanne Osuchukwu, Salomey Osei, Atnafu Lambebo Tonja, Naome Etori, et al. 2023.",
        "text_length": 360,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          0.01612931303679943,
          0.009561710059642792,
          -0.009831630624830723,
          0.022937923669815063,
          -0.015819989144802094,
          0.04400227218866348,
          0.012067698873579502,
          0.04657542705535889,
          -0.02586330659687519,
          0.025737909600138664
        ]
      },
      {
        "chunk_index": 869,
        "relative_chunk_number": 167,
        "text": "Afrispeech-200: Pan-african accented speech dataset for clinical and general domain asr. Transactions of the Association for Computational Linguistics, 11:1669\u20131685. OpenAI. 2024. [link]. OpenAI. 2025. Introducing gpt-4.5. Yonatan Oren, Nicole Meister, Niladri S Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. 2023. Prov- ing test set contamination in black-box language models.",
        "text_length": 382,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.008899770677089691,
          0.01810821332037449,
          -0.0029477060306817293,
          0.020905934274196625,
          -0.00050054193707183,
          -0.003053679596632719,
          0.017539625987410545,
          -0.022234736010432243,
          -0.026273706927895546,
          0.014241276308894157
        ]
      },
      {
        "chunk_index": 870,
        "relative_chunk_number": 168,
        "text": "In The Twelfth International Conference on Learning Representations. Abraham Owodunni, Aditya Yadavalli, Chris Emezue, Tobi Olatunji, and Clinton Mbataku. 2024. Accent- fold: A journey through african accents for zero-shot asr adaptation to target accents. In Findings of the Association for Computational Linguistics: EACL 2024, pages 2146\u20132161.",
        "text_length": 346,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.015481814742088318,
          0.015917180106043816,
          -0.011704359203577042,
          0.01470620185136795,
          -0.050576865673065186,
          0.002060327911749482,
          0.017584869638085365,
          0.01578216813504696,
          -0.03174813464283943,
          0.02461744099855423
        ]
      },
      {
        "chunk_index": 871,
        "relative_chunk_number": 169,
        "text": "Haoyi Qiu, Alexander R Fabbri, Divyansh Agarwal, Kung-Hsiang Huang, Sarah Tan, Nanyun Peng, and Chien-Sheng Wu. 2024. Evaluating cultural and so- cial awareness of llm web agents. arXiv preprint arXiv:2410.23252. Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katha- rina Reinecke, and Maarten Sap. 2024. Normad: A benchmark for measuring the cultural adaptability of large language models.",
        "text_length": 386,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          1.1897085272494223e-07,
          0.016488412395119667,
          -0.017793983221054077,
          0.007329810876399279,
          -0.0249346774071455,
          -0.018230639398097992,
          0.015779828652739525,
          0.004625541623681784,
          -0.023220041766762733,
          -0.018405526876449585
        ]
      },
      {
        "chunk_index": 872,
        "relative_chunk_number": 170,
        "text": "arXiv e-prints, pages arXiv\u2013 2404. Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China.",
        "text_length": 356,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0016107367118820548,
          0.01000915840268135,
          -0.009542413987219334,
          0.04894634708762169,
          -0.01593671552836895,
          -0.02177179977297783,
          0.02296026237308979,
          -0.0029762741178274155,
          -0.030037084594368935,
          0.012996133416891098
        ]
      },
      {
        "chunk_index": 873,
        "relative_chunk_number": 171,
        "text": "Association for Com- putational Linguistics. David Rein, Betty Li Hou, Asa Cooper Stickland, Jack- son Petty, Richard Yuanzhe Pang, Julien Dirani, Ju- lian Michael, and Samuel R Bowman. 2024. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Michael J Ryan, William Held, and Diyi Yang. 2024. Unintended impacts of LLM alignment on global rep- resentation.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012930591590702534,
          0.015429151244461536,
          0.008994119241833687,
          -0.0038491261657327414,
          0.04918194189667702,
          0.012021095491945744,
          0.0171299297362566,
          0.026887258514761925,
          -0.029143407940864563,
          0.005786493886262178
        ]
      },
      {
        "chunk_index": 874,
        "relative_chunk_number": 172,
        "text": "In Proceedings of the 62nd Annual Meet- ing of the Association for Computational Linguis- tics (Volume 1: Long Papers), pages 16121\u201316140, Bangkok, Thailand. Association for Computational Linguistics. Sebastin Santy, Jenny Liang, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. 2023. Nlpositionality: Characterizing design biases of datasets and models.",
        "text_length": 357,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.026178989559412003,
          0.0008586151525378227,
          -0.0028583183884620667,
          0.02279805764555931,
          -0.009024759754538536,
          0.01136773731559515,
          0.020082103088498116,
          0.05179319530725479,
          -0.026783762499690056,
          0.06606926023960114
        ]
      },
      {
        "chunk_index": 875,
        "relative_chunk_number": 173,
        "text": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9080\u20139102. Agrima Seth, Sanchit Ahuja, Kalika Bali, and Sunayana Sitaram. 2024. Dosa: A dataset of social artifacts from different indian geographical subcultures. arXiv preprint arXiv:2403.14651.",
        "text_length": 318,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010544545017182827,
          -0.011877400800585747,
          0.006740115582942963,
          0.04981271177530289,
          0.037363454699516296,
          -0.009677650406956673,
          0.02224504016339779,
          0.02224867418408394,
          -0.037225376814603806,
          0.0006998764583840966
        ]
      },
      {
        "chunk_index": 876,
        "relative_chunk_number": 174,
        "text": "Omar Shaikh, Michelle S Lam, Joey Hejna, Yijia Shao, Hyundong Justin Cho, Michael S Bernstein, and Diyi Yang. 2025. Aligning language models with demon- strated feedback. In The Thirteenth International Conference on Learning Representations. Omar Shaikh, Caleb Ziems, William Held, Aryan Pari- ani, Fred Morstatter, and Diyi Yang. 2023.",
        "text_length": 337,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008082778193056583,
          0.0209469236433506,
          -0.018590856343507767,
          0.013542192056775093,
          -0.01735737919807434,
          0.0001703146699583158,
          0.018582461401820183,
          0.01917390339076519,
          -0.029861019924283028,
          -0.01765431836247444
        ]
      },
      {
        "chunk_index": 877,
        "relative_chunk_number": 175,
        "text": "Modeling cross-cultural pragmatic inference with codenames duet. In Findings of the Association for Computa- tional Linguistics: ACL 2023, pages 6550\u20136569. Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Sunny Yu, Raya Horesh, Rog\u00e9rio Abreu De Paula, and Diyi Yang. 2024. CultureBank: An online community-driven knowledge base towards cultur- ally aware language technologies.",
        "text_length": 376,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.009932322427630424,
          0.053450483828783035,
          0.0028838973958045244,
          0.02939719893038273,
          -0.014178358018398285,
          -0.014062931761145592,
          0.019998762756586075,
          0.0009017007541842759,
          -0.0291324220597744,
          -0.03263614699244499
        ]
      },
      {
        "chunk_index": 878,
        "relative_chunk_number": 176,
        "text": "In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 4996\u20135025, Miami, Florida, USA. Asso- ciation for Computational Linguistics. Ben Shneiderman. 1983. Direct manipulation: A step beyond programming languages. Computer, 16(08):57\u201369. Iyanuoluwa Shode, David Ifeoluwa Adelani, JIng Peng, and Anna Feldman. 2023.",
        "text_length": 338,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02087569609284401,
          -0.01459243893623352,
          -0.0002733357541728765,
          0.03062652237713337,
          -0.05881078541278839,
          -0.012580213136970997,
          0.016342241317033768,
          0.007945825345814228,
          -0.032943833619356155,
          0.0038104967679828405
        ]
      },
      {
        "chunk_index": 879,
        "relative_chunk_number": 177,
        "text": "NollySenti: Leveraging transfer learning and machine translation for Nige- rian movie sentiment classification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 986\u2013998, Toronto, Canada. Association for Computational Linguistics. Patrick E Shrout and Joseph L Fleiss. 1979.",
        "text_length": 347,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.007151349447667599,
          0.016938025131821632,
          0.0001094238759833388,
          0.029540564864873886,
          -0.013639063574373722,
          -0.020291166380047798,
          0.022798892110586166,
          0.012156031094491482,
          -0.03383518382906914,
          0.07193963974714279
        ]
      },
      {
        "chunk_index": 880,
        "relative_chunk_number": 178,
        "text": "Intraclass correlations: uses in assessing rater reliability. Psy- chological bulletin, 86(2):420. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024. Can llms generate novel research ideas? a large- scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109. CH-Wang Sky, Arkadiy Saakyan, Oliver Li, Zhou Yu, and Smaranda Muresan. 2023.",
        "text_length": 358,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0545278824865818,
          0.039178747683763504,
          -0.043224915862083435,
          0.0331498458981514,
          -0.014616379514336586,
          0.02101762220263481,
          0.017134755849838257,
          -0.04558081924915314,
          -0.024155326187610626,
          -0.02208702266216278
        ]
      },
      {
        "chunk_index": 881,
        "relative_chunk_number": 179,
        "text": "Sociocultural norm similarities and differences via situational alignment and explainable textual entailment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3548\u20133564. Ramesh Srinivasan and Jeffrey Huang. 2005. Fluid on- tologies for digital museums. International journal on digital libraries, 5:193\u2013204.",
        "text_length": 355,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.018441464751958847,
          0.05405813455581665,
          0.0010021304478868842,
          0.04961024597287178,
          -0.025915563106536865,
          -0.040396030992269516,
          0.018872348591685295,
          -0.025510987266898155,
          -0.03460114821791649,
          -0.016154831275343895
        ]
      },
      {
        "chunk_index": 882,
        "relative_chunk_number": 180,
        "text": "Tom S\u00fchr, Sophie Hilgard, and Himabindu Lakkaraju. 2021. Does fair ranking improve minority outcomes? understanding the interplay of human and algorithmic biases in online hiring. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 989\u2013999. Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah Goodman. 2022. Task ambiguity in humans and language models.",
        "text_length": 381,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0066089630126953125,
          0.009531423449516296,
          0.003973308019340038,
          0.02955244667828083,
          -0.018948273733258247,
          -0.028656549751758575,
          0.01817186363041401,
          0.01804709993302822,
          -0.029841812327504158,
          0.014248768799006939
        ]
      },
      {
        "chunk_index": 883,
        "relative_chunk_number": 181,
        "text": "In The Eleventh International Con- ference on Learning Representations. Jacob Thebault-Spieker, Daniel Kluver, Maximilian A Klein, Aaron Halfaker, Brent Hecht, Loren Terveen, and Joseph A Konstan. 2017. Simulation exper- iments on (the absence of) ratings bias in reputa- tion systems. Proceedings of the ACM on Human- Computer Interaction, 1(CSCW):1\u201325.",
        "text_length": 354,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.003561705816537142,
          0.026095576584339142,
          -0.008880156092345715,
          0.021527539938688278,
          0.00667931605130434,
          -0.006857221946120262,
          0.018676038831472397,
          -0.014617476612329483,
          -0.011694028042256832,
          0.06049678102135658
        ]
      },
      {
        "chunk_index": 884,
        "relative_chunk_number": 182,
        "text": "Manuel Tonneau, Pedro Quinta De Castro, Karim Lasri, Ibrahim Farouq, Lakshmi Subramanian, Victor Orozco-Olvera, and Samuel Fraiberger. 2024. Nai- jahate: Evaluating hate speech detection on nigerian twitter using representative data. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9020\u20139040.",
        "text_length": 366,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.017418600618839264,
          0.014134110882878304,
          0.006511719431728125,
          0.005157409701496363,
          -0.02303815819323063,
          -0.012877753004431725,
          0.014374903403222561,
          -0.014782115817070007,
          -0.027672965079545975,
          0.01570717990398407
        ]
      },
      {
        "chunk_index": 885,
        "relative_chunk_number": 183,
        "text": "Francielle Vargas, Samuel Guimar\u00e3es, Sham- suddeen Hassan Muhammad, Diego Alves, Ibrahim Sa\u2019id Ahmad, Idris Abdulmumin, Diallo Mohamed, Thiago Pardo, and Fabr\u00edcio Benevenuto. 2024. Hausahate: An expert annotated corpus for hausa hate speech detection. In Proceedings of the 8th Workshop on Online Abuse and Harms (WOAH 2024), pages 52\u201358.",
        "text_length": 338,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02948838658630848,
          0.006014309357851744,
          0.004792964551597834,
          0.02459554374217987,
          0.0284076277166605,
          -0.019528506323695183,
          0.0172863882035017,
          -0.013089763931930065,
          -0.028810864314436913,
          0.033259693533182144
        ]
      },
      {
        "chunk_index": 886,
        "relative_chunk_number": 184,
        "text": "Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gal- lou\u00e9dec. 2020. Trl: Transformer reinforcement learn- ing. https://github.com/huggingface/trl. Yuhang Wang, Yanxu Zhu, Chao Kong, Shuyu Wei, Xi- aoyuan Yi, Xing Xie, and Jitao Sang. 2024a.",
        "text_length": 333,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.0117372190579772,
          0.029544802382588387,
          -0.009478054940700531,
          0.012704591266810894,
          0.012010018341243267,
          -0.025397619232535362,
          0.012460824102163315,
          -0.00931395310908556,
          -0.026758216321468353,
          0.002837606705725193
        ]
      },
      {
        "chunk_index": 887,
        "relative_chunk_number": 185,
        "text": "CDEval: A benchmark for measuring the cultural dimensions of large language models. In Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP, pages 1\u201316, Bangkok, Thailand. Association for Computational Linguistics. Zijie J Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, and Michael Madaio. 2024b.",
        "text_length": 322,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.07335265725851059,
          0.04018658399581909,
          -0.011574732139706612,
          0.010108327493071556,
          -0.019138261675834656,
          -0.03052689880132675,
          0.017440851777791977,
          -0.035245757550001144,
          -0.018661851063370705,
          -0.03321389853954315
        ]
      },
      {
        "chunk_index": 888,
        "relative_chunk_number": 186,
        "text": "Far- sight: Fostering responsible ai awareness during ai application prototyping. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 1\u201340.",
        "text_length": 169,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.032749101519584656,
          0.022415252402424812,
          -0.011545537039637566,
          0.02416774444282055,
          -0.04882039502263069,
          -0.028128555044531822,
          0.020663686096668243,
          0.04050932824611664,
          -0.03651648387312889,
          0.008320274762809277
        ]
      },
      {
        "chunk_index": 889,
        "relative_chunk_number": 187,
        "text": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Hao- ran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke- qin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan.",
        "text_length": 750,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.010613544844090939,
          0.03805643320083618,
          -0.009844884276390076,
          0.0022253377828747034,
          0.0009451938676647842,
          -0.06140989065170288,
          0.010946547612547874,
          0.015500645153224468,
          -0.012000991962850094,
          0.005762902088463306
        ]
      },
      {
        "chunk_index": 890,
        "relative_chunk_number": 188,
        "text": "2024a. Qwen2 techni- cal report. Preprint, arXiv:2407.10671. Diyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael S Bernstein, and John Mitchell. 2024b. So- cial skill training with large language models. arXiv preprint arXiv:2404.04204. Da Yin, Hritik Bansal, Masoud Monajatipoor, Liu- nian Harold Li, and Kai-Wei Chang. 2022.",
        "text_length": 333,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01834292709827423,
          0.008565491065382957,
          -0.011751673184335232,
          -0.006706908345222473,
          -0.01601869985461235,
          -0.035621967166662216,
          0.016624892130494118,
          -0.0026777111925184727,
          -0.022757166996598244,
          0.007553687784820795
        ]
      },
      {
        "chunk_index": 891,
        "relative_chunk_number": 189,
        "text": "GeoM- LAMA: Geo-diverse commonsense probing on multi- lingual pre-trained language models. In Proceedings of the 2022 Conference on Empirical Methods in Nat- ural Language Processing, pages 2039\u20132055, Abu Dhabi, United Arab Emirates. Association for Com- putational Linguistics. Zheng Xin Yong, Cristina Menghini, and Stephen Bach. 2024. Low-resource languages jailbreak gpt-4.",
        "text_length": 377,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0021811833139508963,
          0.012628824450075626,
          0.011308091692626476,
          0.01260637491941452,
          -0.010722216218709946,
          0.025766562670469284,
          0.02230982296168804,
          -0.006104577798396349,
          -0.0266705472022295,
          0.0023179026320576668
        ]
      },
      {
        "chunk_index": 892,
        "relative_chunk_number": 190,
        "text": "In Socially Responsible Language Modelling Research. Yan Zhang and Barbara M Wildemuth. 2009. Unstruc- tured interviews. Applications of social research methods to questions in information and library sci- ence, 2:222\u2013231. Naitian Zhou, David Bamman, and Isaac L Bleaman. 2025. Culture is not trivia: Sociocultural theory for cultural nlp. arXiv preprint arXiv:2502.12057.",
        "text_length": 372,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.008689574897289276,
          0.0028728300239890814,
          -0.01958254911005497,
          0.01801827736198902,
          0.03291263431310654,
          -0.04767381399869919,
          0.018272824585437775,
          -0.03784845396876335,
          -0.03385445103049278,
          -0.05134095624089241
        ]
      },
      {
        "chunk_index": 893,
        "relative_chunk_number": 191,
        "text": "Caleb Ziems, Jane Dwivedi-Yu, Yi-Chia Wang, Alon Halevy, and Diyi Yang. 2023a. Normbank: A knowl- edge bank of situational social norms. In Proceed- ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7756\u20137776. Caleb Ziems, William Held, Jingfeng Yang, Jwala Dhamala, Rahul Gupta, and Diyi Yang. 2023b.",
        "text_length": 363,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.012008115649223328,
          0.05634092539548874,
          -0.017027972266077995,
          0.04880373552441597,
          0.010956017300486565,
          -0.0432623066008091,
          0.016137653961777687,
          -0.06866519153118134,
          -0.026209158822894096,
          0.011932652443647385
        ]
      },
      {
        "chunk_index": 894,
        "relative_chunk_number": 192,
        "text": "Multi- value: A framework for cross-dialectal english nlp. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 744\u2013768.",
        "text_length": 189,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.02492024563252926,
          0.0356905497610569,
          -0.0046087647788226604,
          0.01849200204014778,
          0.06130027025938034,
          -0.019707798957824707,
          0.0174289308488369,
          0.01980571448802948,
          -0.032679036259651184,
          -0.040457386523485184
        ]
      },
      {
        "chunk_index": 895,
        "relative_chunk_number": 193,
        "text": "state town ethnicity Akwa Ibom Uyo Ekid Anambra State Awka Igbo Federal Capital Territory Abuja Abawa Lagos State Lagos Yoruba Lagos Yoruba Ogun State Abeokuta Egba Abeokuta Ijebu Abeokuta Remo Ibadan Yoruba Table 5: Nigerian Annotator Demographics for 9 annotators from 7 ethnolinguistic groups across 5 states.",
        "text_length": 312,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.023014232516288757,
          0.025490226224064827,
          0.020733186975121498,
          0.04835981875658035,
          -0.062190305441617966,
          0.01681734435260296,
          0.018511641770601273,
          0.02808809094130993,
          -0.03693981468677521,
          0.05912573263049126
        ]
      },
      {
        "chunk_index": 896,
        "relative_chunk_number": 194,
        "text": "A Annotator Recruitment and Demographics We recruited all annotators on Upwork, offering $20 per hour, and specifying that applicants needed to be adult workers (18+) from Nigeria or Indone- sia who grew up in the local cultures. To ensure high quality work, we hired only workers with at least 90% Job Success as indicated by the platform.",
        "text_length": 340,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.00531348492950201,
          0.03563281148672104,
          0.0008225373458117247,
          0.05640451982617378,
          -0.04500504955649376,
          0.01732383854687214,
          0.019082704558968544,
          0.009055519476532936,
          -0.04522828012704849,
          0.051349345594644547
        ]
      },
      {
        "chunk_index": 897,
        "relative_chunk_number": 195,
        "text": "We also filtered by the following skills: (1) data annotation, (2) data entry, (3) writing, (4) cultural and ethnic studies, (5) arts and culture. Work- ers were hired following a roughly balanced strati- fied random sample across geography (provinces or states), as indicated by the locations in their Up- work profiles (see Tables 5 and 6 for the results).",
        "text_length": 358,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.02249663881957531,
          0.03234133869409561,
          0.00329016731120646,
          0.013901449739933014,
          0.029888104647397995,
          0.013068797998130322,
          0.018063640221953392,
          0.016066135838627815,
          -0.032311782240867615,
          0.04832862317562103
        ]
      },
      {
        "chunk_index": 898,
        "relative_chunk_number": 196,
        "text": "Then annotators were extensively onboarded by one of the authors of this study. The annotator first watched a 4-minute Instructional Demonstration Video on YouTube, explaining how the CULTURE EXPLORER tool works, as well as the goals of the study. The worker then completed a series of 1-5 staging rounds of annotation to ensure their under- standing of the task.",
        "text_length": 363,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.017278021201491356,
          0.020621443167328835,
          -0.02699270471930504,
          0.03399593383073807,
          -0.0812019631266594,
          0.02062547579407692,
          0.020067157223820686,
          -0.0019376911222934723,
          -0.04883964732289314,
          -0.02885357104241848
        ]
      },
      {
        "chunk_index": 899,
        "relative_chunk_number": 197,
        "text": "Each staging round entailed 15 minutes of work with the CULTURE EXPLORER. The recruiting author evaluated this work and gave extensive personalized feedback, answering ques- tions, and particularly encouraging workers to pri- oritize novel contributions around knowledge that AI doesn\u2019t already know, and to focus on their most local or regional cultures.",
        "text_length": 355,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0018166154623031616,
          0.03907223418354988,
          -0.011109886690974236,
          0.0035041251685470343,
          -0.019932778552174568,
          0.00417224271222949,
          0.01844630017876625,
          -0.019485032185912132,
          -0.03830574080348015,
          2.2818649085820653e-05
        ]
      },
      {
        "chunk_index": 900,
        "relative_chunk_number": 198,
        "text": "Once the annota- tor demonstrated understanding, work was scaled up to 5 hour blocks with random audits for quality. The same pool of annotators contributed to all three data subsets of \u00a73.2: Synthetic Data, Traditional Annotation, and CULTURE CARTOGRAPHY.",
        "text_length": 256,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.01995263062417507,
          0.020393729209899902,
          0.00013700041745323688,
          0.048661354929208755,
          -0.010348030366003513,
          -0.04466661810874939,
          0.027079371735453606,
          -0.019034137949347496,
          -0.04514770209789276,
          0.030391987413167953
        ]
      },
      {
        "chunk_index": 901,
        "relative_chunk_number": 199,
        "text": "province town ethnicity Aceh Banda Aceh Acehnese Banda Aceh Acehnese Bali Denpasar Balinese Ubud Balinese Banten Tangerang Selatan Bantenese Central Java Salatiga Javanese East Java Malang Javanese East Kalimantan Balikpapan Malay Samarinda Malay South Kalimantan Martapura Banjarese South Sulawesi Makassar Bugis Makassar Bugis West Java Bandung Sundanese Indramayu Indramayu Jakarta Javanese South Tangerang Javanese West Kalimantan Pontianak Dayak West Nusa Tenggara Mataram Sasak West Sumatra Padang Minangkabau Table 6: Indonesian Annotator Demographics for 19 annotators from 13 ethnolinguistic groups across 12 provinces.",
        "text_length": 628,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.0025845079217106104,
          0.04019833728671074,
          0.017131078988313675,
          0.025933686643838882,
          -0.10226934403181076,
          0.00989289116114378,
          0.012943202629685402,
          0.028305666521191597,
          -0.027883732691407204,
          0.06250166893005371
        ]
      },
      {
        "chunk_index": 902,
        "relative_chunk_number": 200,
        "text": "B Validating LLM-as-a-Judge We validate the LLM-as-a-Judge by uniformly sampling 50 evaluations of Nigerian CULTURE CARTOGRAPHY data from across all 7 models, plus 25 additional examples oversampled from the minority-predicted class (here: \u201cNo\u201d). One author blindly annotated these 75 datapoints. The author agreed with the model\u2019s judgment 39+25=64 times (85%), with a substantial Cohen\u2019s \u03ba = 0.66.",
        "text_length": 399,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.040342237800359726,
          0.03737987205386162,
          -0.016620179638266563,
          0.03631876781582832,
          0.005978498607873917,
          0.013551147654652596,
          0.020911196246743202,
          0.009317487478256226,
          -0.02641795203089714,
          0.008192388340830803
        ]
      },
      {
        "chunk_index": 903,
        "relative_chunk_number": 201,
        "text": "C Estimating GPT-4.5\u2019s Performance on CULTURE CARTOGRAPHY At the time of this study, the largest available model was GPT-4.5 (OpenAI, 2025). GPT-4.5 would be prohibitively expensive to evaluate on the entirety of CULTURE CARTOGRAPHY. If we use GPT-4o\u2019s total evaluation I/O of 66M tokens, then at $150 / 1M tokens, a full GPT-4.5 evaluation would cost $9,900.",
        "text_length": 359,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.033733535557985306,
          0.0065442658960819244,
          -0.03759405389428139,
          0.03245749697089195,
          0.02111509069800377,
          -0.013973597437143326,
          0.020546158775687218,
          -0.01488003320991993,
          -0.025687063112854958,
          0.002825129544362426
        ]
      },
      {
        "chunk_index": 904,
        "relative_chunk_number": 202,
        "text": "Instead, we estimate an optimistic upper bound on its performance. Given its demonstrated performance benefits over GPT-4o (OpenAI, 2025), and our interest in the optimistic upper-bound, we assume GPT-4.5 would correctly answer any ques- tion that GPT-4o could answer.",
        "text_length": 268,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.014574420638382435,
          0.03419893607497215,
          -0.011902830563485622,
          0.034387458115816116,
          -0.031133657321333885,
          0.02723640389740467,
          0.020825307816267014,
          -0.03501845523715019,
          -0.03253361955285072,
          -0.052328161895275116
        ]
      },
      {
        "chunk_index": 905,
        "relative_chunk_number": 203,
        "text": "We evaluated GPT- 4.5 directly on a random sample of 50 questions that GPT-4o got wrong; then we used GPT-4.5\u2019s accuracy on this set to interpolate between GPT- 4o\u2019s performance and perfect performance.",
        "text_length": 202,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.02729826606810093,
          0.005483043845742941,
          -0.060392942279577255,
          0.029357071965932846,
          -0.0034260950051248074,
          0.033541809767484665,
          0.02026398852467537,
          -0.03459502011537552,
          -0.03911704942584038,
          -0.0692489743232727
        ]
      },
      {
        "chunk_index": 906,
        "relative_chunk_number": 204,
        "text": "We randomly sample 50 questions for which GPT-4o 0 20 40 60 80 100 120 K 0.6 0.7 0.8 0.9 1.0 1.1 Recall@K DeepSeek R1 GPT-4o Qwen2-72B (a) On Synthetic Indonesian Data 0 20 40 60 80 100 120 K 0.6 0.7 0.8 0.9 1.0 1.1 Recall@K DeepSeek R1 GPT-4o Qwen2-72B (b) On Synthetic Nigerian Data Figure 4: Recall@K curves for DeepSeek R1, GPT-4o, and Qwen2-72B on Synthetic Data demonstrate that model performances either plateau or reach 100% by K = 100 failed to retrieve at least one human gold answer: 25 questions for Indonesia and 25 questions for Nigeria.",
        "text_length": 551,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0038829005789011717,
          -0.004288236610591412,
          -0.016880685463547707,
          0.055745016783475876,
          -0.04416358843445778,
          -0.04686667025089264,
          0.021023763343691826,
          -0.014086482115089893,
          -0.019897347316145897,
          -0.0014367199037224054
        ]
      },
      {
        "chunk_index": 907,
        "relative_chunk_number": 205,
        "text": "Using the ChatGPT web interface, one researcher manually evaluated GPT-4.5 on all 50 pairs in a manner that most optimistically estimated upper-bound performance. For each question, the researcher added clarifying details to the prompt that most effectively narrowed the retrieval space without explicitly giving the answer.",
        "text_length": 324,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.009472234174609184,
          0.027028484269976616,
          -0.017940888181328773,
          0.03949090465903282,
          -0.02505985088646412,
          -0.02033061720430851,
          0.020326273515820503,
          -0.04152142256498337,
          -0.03705278038978577,
          -0.030509235337376595
        ]
      },
      {
        "chunk_index": 908,
        "relative_chunk_number": 206,
        "text": "For example, the answer to one question on \u201ccoming of age rit- uals\u201d was Otu-Odu, an initiation specific to Igbo women, so the researcher clarified: \u201cWe\u2019re looking different examples of initiation rituals for women in Igbo culture.\u201d Then the researcher manually judged the results in the manner of \u00a74.",
        "text_length": 301,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.009229717776179314,
          0.026540443301200867,
          -0.008334781043231487,
          0.036600567400455475,
          0.006564141716808081,
          0.0046031661331653595,
          0.026474326848983765,
          0.009051562286913395,
          -0.04507772997021675,
          0.0005308914114721119
        ]
      },
      {
        "chunk_index": 909,
        "relative_chunk_number": 207,
        "text": "In doing so, we computed R(4.5)\u03c3failure@K \u2014 the recall of GPT-4.5 on the failure set \u03c3failure \u2014 as 72% and 48% for Indonesia and Nigeria respec- tively.",
        "text_length": 152,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0000001192092896,
        "vector_sample_first_10": [
          -0.03070290945470333,
          0.03403404727578163,
          -0.015458015725016594,
          0.057526443153619766,
          0.006435727700591087,
          0.0037608977872878313,
          0.02054738812148571,
          0.0004778869915753603,
          -0.02133425883948803,
          -0.008389792405068874
        ]
      },
      {
        "chunk_index": 910,
        "relative_chunk_number": 208,
        "text": "Then we estimated d R(4.5)\u03c3full@K, the recall of GPT-4.5 on the full CULTURE EXPLORER set as d R(4.5)\u03c3full@K = R(4o)\u03c3full@K + (1 \u2212R(4o)\u03c3full@K)(R(4.5)\u03c3failure@K) Here we assumed (as an upper bound) that GPT- 4.5 would successfully recall anything that GPT-4o recalled, then estimated 4.5\u2019s improvement on 4o\u2019s failure set according to R(4.5)\u03c3failure@K.",
        "text_length": 352,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01896655559539795,
          0.009403816424310207,
          -0.028513720259070396,
          0.040244750678539276,
          -0.003805076237767935,
          0.00429793493822217,
          0.01669500023126602,
          -0.02145087905228138,
          -0.017183475196361542,
          -0.03047803044319153
        ]
      },
      {
        "chunk_index": 911,
        "relative_chunk_number": 209,
        "text": "Table 7 shows the GPT-4.5 upper bound performance es- timates of 90% recall for Indonesia and 84% for Nigeria. We see that GPT-4.5 can exceed DeepSeek R1\u2019s performance (as given in Figure 3), but GPT- 4.5 does not fully solve these challenging subsets.",
        "text_length": 252,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01654091291129589,
          0.016468871384859085,
          -0.03636014461517334,
          0.04098551347851753,
          -0.002748750615864992,
          0.002705318620428443,
          0.0205682385712862,
          -0.014462284743785858,
          -0.03106839396059513,
          -0.03349173441529274
        ]
      },
      {
        "chunk_index": 912,
        "relative_chunk_number": 210,
        "text": "On CULTURE CARTOGRAPHY, GPT-4.5 lags be- Culture R@100 95% CI ind 0.90 [0.84, 0.96] nga 0.84 [0.79, 0.90] Table 7: Expected Performance of GPT-4.5 on the CULTURE CARTOGRAPHY still lag behind best model performances on Fixed Questionnaires, further demonstrating the utility of CULTURE EXPLORER co- annotation for producing challenging and long-tail cul- tural knowledge.",
        "text_length": 370,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0593920461833477,
          0.023691140115261078,
          -0.023956291377544403,
          0.045979373157024384,
          0.029552936553955078,
          -0.028775978833436966,
          0.022060973569750786,
          -0.039168987423181534,
          -0.02382214367389679,
          -0.029721694067120552
        ]
      },
      {
        "chunk_index": 913,
        "relative_chunk_number": 211,
        "text": "hind even R1\u2019s performance on Traditional Annota- tions (91% and 92%), which further demonstrates the \u201cGoogle-Proof-ness\u201d of our approach (Q3). This also further reinforces our \u00a74 conclusion that CULTURE CARTOGRAPHY produces more chal- lenging data than synthetic or traditional annota- tion.",
        "text_length": 292,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.025867536664009094,
          0.022899523377418518,
          -0.03398926183581352,
          0.058224596083164215,
          -0.055449217557907104,
          -0.02726253867149353,
          0.021611522883176804,
          -0.01145746186375618,
          -0.03799532353878021,
          0.00845422875136137
        ]
      },
      {
        "chunk_index": 914,
        "relative_chunk_number": 212,
        "text": "D Prompts We build CULTURE EXPLORER on the Farsight (Wang et al., 2024b) codebase, which is licenced under a Creative Commons Attribution Interna- tional 4.0 License, which permits us to trans- form and build upon the material for any purpose. Among other changes, we transform the prompts to fit our domain, and these prompts are given below.",
        "text_length": 343,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03108874335885048,
          0.01919972151517868,
          0.004608870483934879,
          0.022852811962366104,
          -0.0024003603029996157,
          -0.017972905188798904,
          0.0203107800334692,
          0.024915512651205063,
          -0.04222332686185837,
          -0.06083264946937561
        ]
      },
      {
        "chunk_index": 915,
        "relative_chunk_number": 213,
        "text": "Nigerian Questions: You are an anthropologist who is good at asking important questions about Nigerian culture. Given a description of an abstract cultural concept, please brainstorm 5 specific questions about this concept in Nigerian culture. Put each question in the XML tags: <question></question>. Each question should be written in a way that starts with the word \u2019List\u2019.",
        "text_length": 376,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.017677441239356995,
          0.022472398355603218,
          -0.007026503328233957,
          0.044630490243434906,
          -0.03062235563993454,
          -0.05155796930193901,
          0.022132372483611107,
          0.04411572590470314,
          -0.04530519247055054,
          0.02434004284441471
        ]
      },
      {
        "chunk_index": 916,
        "relative_chunk_number": 214,
        "text": "cultural concept:Gifts examples: <question>List any customs or traditions related to the preparation and presentation of gifts in Nigerian culture.</question> <question>List the etiquette and expectations surrounding gift-giving and receiving in Nigerian culture.</question> <question>List the differences in gifting practices between various regions or social groups within Nigerian culture.</question> <question>List the occasions when gifts are traditionally exchanged in Nigerian culture.</question> <question>List any adaptations or changes in Nigerian gifting customs that have occurred over time due to social or technological advancements.</question> cultural concept:{{concept}} examples: Indonesian Questions: Anda seorang antropolog yang pandai mengajukan pertanyaan penting tentang budaya Indonesia.",
        "text_length": 811,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.007073516957461834,
          0.015705019235610962,
          0.014256095513701439,
          0.08439556509256363,
          -0.0016559616196900606,
          -0.041127655655145645,
          0.015875427052378654,
          0.005053812172263861,
          -0.03445984423160553,
          0.03056672029197216
        ]
      },
      {
        "chunk_index": 917,
        "relative_chunk_number": 215,
        "text": "Dengan deskripsi konsep budaya abstrak, silakan buat 5 pertanyaan spesifik tentang konsep ini dalam budaya Indonesia. Masukkan setiap pertanyaan dalam tag XML: <question></question>. Setiap pertanyaan harus ditulis dengan cara yang dimulai dengan kata \u2019Buat\u2019.",
        "text_length": 259,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          -0.044218529015779495,
          0.02455384097993374,
          -0.025301141664385796,
          0.05185898393392563,
          0.043930184096097946,
          0.008243301883339882,
          0.019772596657276154,
          0.014891977421939373,
          -0.026476822793483734,
          0.0388011559844017
        ]
      },
      {
        "chunk_index": 918,
        "relative_chunk_number": 216,
        "text": "konsep budaya:Hadiah contoh: <question>Buat daftar kebiasaan atau tradisi yang terkait dengan persiapan dan pemberian hadiah dalam budaya Indonesia.</question> <question>Buat daftar etiket dan harapan seputar pemberian dan penerimaan hadiah dalam budaya Indonesia.</question> <question>Buat daftar perbedaan dalam praktik pemberian hadiah antara berbagai daerah atau kelompok sosial dalam budaya Indonesia.</question> <question>Buat daftar kesempatan saat hadiah secara tradisional dipertukarkan dalam budaya Indonesia.</question> <question>Buat daftar adaptasi atau perubahan dalam kebiasaan pemberian hadiah Indonesia yang telah terjadi dari waktu ke waktu karena kemajuan sosial atau teknologi.</question> konsep budaya:{{concept}} contoh: Nigerian Answers: You are an observant Nigerian person who is good at recalling diverse and accurate traditions, practices, and norms in your culture.",
        "text_length": 893,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.05429834872484207,
          0.03574980050325394,
          0.009015710093080997,
          0.03522839769721031,
          0.058206405490636826,
          -0.023876242339611053,
          0.01354102324694395,
          0.009195436723530293,
          -0.015692437067627907,
          0.032017678022384644
        ]
      },
      {
        "chunk_index": 919,
        "relative_chunk_number": 217,
        "text": "Given a question, please brainstorm 5 more specific answers from Nigerian culture. Put each answer in one of the XML tags: <universal> if the answer applies to many cultures, or <local> if the answer applies only to a few related cultures, or <unique> if the answer applies only to Nigerian culture. cultural question: List some Nigerian wedding traditions and what they signify.",
        "text_length": 379,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003085752483457327,
          0.03471674025058746,
          0.002774306107312441,
          0.07276314496994019,
          0.005832941271364689,
          -0.051577311009168625,
          0.015358895063400269,
          0.052991874516010284,
          -0.03834926709532738,
          0.0010469678090885282
        ]
      },
      {
        "chunk_index": 920,
        "relative_chunk_number": 218,
        "text": "examples:<unique>Alaga: A Nigerian wedding ceremony officiant whose job is to heckle the groom and his friends as entertainment for the wedding guests.",
        "text_length": 151,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01163471769541502,
          0.04297773167490959,
          0.032052259892225266,
          0.04864725098013878,
          -0.035387251526117325,
          0.00123960105702281,
          0.020482195541262627,
          -0.017478875815868378,
          -0.04916655272245407,
          0.044073499739170074
        ]
      },
      {
        "chunk_index": 921,
        "relative_chunk_number": 219,
        "text": "This keeps guests engaged during the hour-long ceremony.</unique> <unique>Aso-Ebi: Nigerian couples choose what their guests wear by assigning a color for the bride\u2019s family and a separate color for the groom\u2019s family.</unique> <local>No Guest List: Nigerian weddings won\u2019t have a guest list.",
        "text_length": 292,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.024773484095931053,
          0.033724814653396606,
          0.02875141054391861,
          0.06884195655584335,
          -0.08819600194692612,
          -0.008093943819403648,
          0.01999692991375923,
          -0.002092593815177679,
          -0.03955300897359848,
          0.029363121837377548
        ]
      },
      {
        "chunk_index": 922,
        "relative_chunk_number": 220,
        "text": "The entire community is welcome and an abundance of food and drink is available in case you end up with your entire community in attendance.</local> <local>Gele: Nigerian brides wear a traditional, ornate headpiece called a Gele.",
        "text_length": 229,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0007003879291005433,
          0.0036032204516232014,
          0.06131806597113609,
          0.031123926863074303,
          -0.040134552866220474,
          -0.005382355768233538,
          0.017665676772594452,
          -0.053619466722011566,
          -0.03850121796131134,
          -0.005468827206641436
        ]
      },
      {
        "chunk_index": 923,
        "relative_chunk_number": 221,
        "text": "The bridesmaids and families also follow suit and wear a Gele to honor the cultural traditions of the day.</local> cultural concept: {{concept}} examples: Indonesian Answers: Anda seorang antropolog yang pandai mengajukan pertanyaan penting tentang budaya Indonesia. Dengan deskripsi konsep budaya abstrak, silakan buat 5 pertanyaan spesifik tentang konsep ini dalam budaya Indonesia.",
        "text_length": 384,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.028760813176631927,
          0.029733512550592422,
          0.005486006382852793,
          0.06673969328403473,
          -0.001721779117360711,
          -0.0138978511095047,
          0.01456646528095007,
          -0.03990471735596657,
          -0.031766150146722794,
          0.023488184437155724
        ]
      },
      {
        "chunk_index": 924,
        "relative_chunk_number": 222,
        "text": "Masukkan setiap pertanyaan dalam tag XML: <question></question>. Setiap pertanyaan harus ditulis dengan cara yang dimulai dengan kata \u2019Buat\u2019.",
        "text_length": 141,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.055320855230093,
          0.0294968169182539,
          -0.013280856423079967,
          0.034893766045570374,
          0.04442325606942177,
          0.001523488899692893,
          0.019884999841451645,
          0.018080709502100945,
          -0.02837616764008999,
          -0.0010756324045360088
        ]
      },
      {
        "chunk_index": 925,
        "relative_chunk_number": 223,
        "text": "konsep budaya: Hadiah contoh: <question>Buat daftar kebiasaan atau tradisi yang terkait dengan persiapan dan pemberian hadiah dalam budaya Indonesia.</question> <question>Buat daftar etiket dan harapan seputar pemberian dan penerimaan hadiah dalam budaya Indonesia.</question> <question>Buat daftar perbedaan dalam praktik pemberian hadiah antara berbagai daerah atau kelompok sosial dalam budaya Indonesia.</question> <question>Buat daftar kesempatan saat hadiah secara tradisional dipertukarkan dalam budaya Indonesia.</question> <question>Buat daftar adaptasi atau perubahan dalam kebiasaan pemberian hadiah Indonesia yang telah terjadi dari waktu ke waktu karena kemajuan sosial atau teknologi.</question> <question>Buat daftar norma budaya untuk mengakui dan menanggapi hadiah dalam budaya Indonesia.</question> <question>Buat daftar signifikansi ukuran, nilai, dan pembungkusan hadiah dalam budaya Indonesia.</question> <question>Buat daftar makna atau simbolisme khusus yang terkait dengan jenis hadiah tertentu di Indonesia budaya.</question> <question>Buat daftar peran hadiah dalam membangun hubungan dan mengekspresikan emosi dalam budaya Indonesia.</question> <question>Buat daftar berbagai jenis hadiah yang umum diberikan dalam budaya Indonesia.</question> konsep budaya:concept contoh: E Models & Hyperparameters We fine-tune the base version of Llama-3.1-8B on NVIDIA RTX 6000 Ada GPUs with LoRA (Hu et al., 2021) (rank 8; \u03b1 = 16; dropout of 0.1) for 4 epochs of SFT, followed optionally by 4 epochs of DPO using the Huggingface Trainer and the TRL library (von Werra et al., 2020).",
        "text_length": 1598,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.06500677764415741,
          0.020060375332832336,
          0.009229831397533417,
          0.030707895755767822,
          0.048642247915267944,
          -0.014493921771645546,
          0.012672098353505135,
          0.013833899982273579,
          -0.007801610976457596,
          0.039357125759124756
        ]
      },
      {
        "chunk_index": 926,
        "relative_chunk_number": 224,
        "text": "We set a batch size of 1, a learning rate of 2e \u22124, and an AdamW-8bit optimizer. F Instructions Given to Participants F.1 Recruitment Information Description: You are invited to participate in a research study whose goal is to help people partner with AI to collect diverse, culture-specific knowl- edge, that is organized in a way that makes sense to you.",
        "text_length": 356,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.00037034001434221864,
          0.03377467021346092,
          0.0031206661369651556,
          0.018927183002233505,
          -0.028465181589126587,
          0.024193469434976578,
          0.017452683299779892,
          0.006448485888540745,
          -0.039763543754816055,
          0.017713762819767
        ]
      },
      {
        "chunk_index": 927,
        "relative_chunk_number": 225,
        "text": "You will also be asked to provide your own knowledge, answering our open-ended questions by listing examples from your local culture. You will also be asked to check the correctness of ex- isting data that has been generated automatically using Generative AI, and if these are wrong, you will have the ability to edit, delete, or regenerate knowledge.",
        "text_length": 351,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.013884813524782658,
          0.012111149728298187,
          -0.006592762190848589,
          0.008754165843129158,
          -0.030833635479211807,
          0.03423171117901802,
          0.01774626597762108,
          -0.003437158651649952,
          -0.052397847175598145,
          0.05577468499541283
        ]
      },
      {
        "chunk_index": 928,
        "relative_chunk_number": 226,
        "text": "Eligibility: you must be an adult vol- unteer (18+) from the country we have specified. Payments: You will receive $20 USD per hour as payment for your participation. Risks: The are no significant risks associated with this study. Study data will be stored securely, minimizing the risk of confidentiality breach.",
        "text_length": 313,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.011175230145454407,
          0.03514548018574715,
          -0.020547106862068176,
          0.017700187861919403,
          0.014587133191525936,
          -0.011557552963495255,
          0.014599781483411789,
          -0.029372544959187508,
          -0.03897811844944954,
          0.0678587481379509
        ]
      },
      {
        "chunk_index": 929,
        "relative_chunk_number": 227,
        "text": "Your individual privacy will be maintained during the research and in all published and written data resulting from the study. F.2 Onboarding Video Transcript Welcome to CULTURE EXPLORER, a tool for help- ing you to visualize and share your own cultural- specific knowledge. You can work with AI to fill in the missing pieces in a growing tree of knowledge. When you\u2019re ready, click Yes to start.",
        "text_length": 396,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003973906394094229,
          0.019653912633657455,
          0.011577961035072803,
          0.0076350378803908825,
          -0.031700387597084045,
          -0.02694343961775303,
          0.015513775870203972,
          -0.0344938226044178,
          -0.038692258298397064,
          -0.016886906698346138
        ]
      },
      {
        "chunk_index": 930,
        "relative_chunk_number": 228,
        "text": "To get you thinking, the tool starts with a random seed topic, like here is Weddings. Feel free to change this seed to anything that interests you! According to the instructions, we need to first ask at least 3 interesting questions about our cul- ture. The tool will automatically generate 5 ideas for you, but please be critical! You know your cul- ture better than AI.",
        "text_length": 371,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.009466033428907394,
          0.012219245545566082,
          -0.011562144383788109,
          0.034723199903964996,
          -0.05672444403171539,
          -0.02374337427318096,
          0.017542453482747078,
          0.03588501363992691,
          -0.053107861429452896,
          0.044713448733091354
        ]
      },
      {
        "chunk_index": 931,
        "relative_chunk_number": 229,
        "text": "An interesting question should specifically distinguish your culture from others. The question here about planning customs isn\u2019t very specific, so let\u2019s see what we can do from the instructions: we can either edit, regenerate, add our own, or delete questions. Let\u2019s click regen- erate here until we find a better one. This one looks pretty good.",
        "text_length": 346,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.06482870876789093,
          0.044126689434051514,
          0.0031912268605083227,
          0.0331597663462162,
          0.02135566435754299,
          -0.022343281656503677,
          0.013416045345366001,
          0.023976709693670273,
          -0.03780970722436905,
          0.009143699891865253
        ]
      },
      {
        "chunk_index": 932,
        "relative_chunk_number": 230,
        "text": "The question about etiquette is great because it leads to practical advice that would help an outsider fit in, so we\u2019ll keep that. Finally, an interesting question should be on a topic that is cherished or important to community members.",
        "text_length": 237,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.016964685171842575,
          0.04236453026533127,
          -0.019861534237861633,
          0.019662680104374886,
          0.009158316999673843,
          -0.0013844026252627373,
          0.015912562608718872,
          -0.0018000284908339381,
          -0.04222650080919266,
          -0.04936649277806282
        ]
      },
      {
        "chunk_index": 933,
        "relative_chunk_number": 231,
        "text": "Americans care about money, but none of these questions ask about finances, so we will add one by double clicking the bottom node and typing di- rectly into the box. <types: List some budgetary considerations for American weddings.> You see this follows instructions, since questions should start with the word \u201cList.\u201d In the end, we need 3 validated questions.",
        "text_length": 361,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.027815477922558784,
          0.01234260480850935,
          0.0077623240649700165,
          0.03264356777071953,
          0.0105290487408638,
          -0.024286257103085518,
          0.016760600730776787,
          0.04115862771868706,
          -0.040556471794843674,
          0.036152180284261703
        ]
      },
      {
        "chunk_index": 934,
        "relative_chunk_number": 232,
        "text": "We mark interesting ques- tions by clicking the validation button in the bottom right. Notice how the footer keeps track of how many we have. Once we have all three validated, we are told to hover over them and click add answer suggestions. After that, we can click Next on the instructions. AI starts us off with 5 answers, which we can again edit or regenerate.",
        "text_length": 363,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.01848963461816311,
          0.013775396160781384,
          -0.04700923338532448,
          0.005924175027757883,
          -0.014110791496932507,
          0.04720645025372505,
          0.01562260091304779,
          0.025108912959694862,
          -0.03769202530384064,
          0.05150843411684036
        ]
      },
      {
        "chunk_index": 935,
        "relative_chunk_number": 233,
        "text": "Some of these, like dress appropriately, are too vague. Instead of deleting them outright, we can specify: do not dress in casual attire at a formal ceremony. Some like do not wear white may be generalized to: do not wear any clothing that upstages the bride or groom. Note that each time we make edits, the counter in the right footer updates the bonuses we\u2019ve earned!",
        "text_length": 369,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01387487631291151,
          -0.00215564901009202,
          0.022411616519093513,
          0.030099960044026375,
          -0.037288371473550797,
          -0.03445124998688698,
          0.01631736010313034,
          -0.006003915332257748,
          -0.029106762260198593,
          -0.0023539604153484106
        ]
      },
      {
        "chunk_index": 936,
        "relative_chunk_number": 234,
        "text": "Bonuses are based on character-level contributions. The goal is to contribute as much knowledge as you can that is also distinct from what is already in the tree \u2013 distinct from what AI already knows. You will work for a fixed amount of time, and you will be paid bonuses for any new knowledge you con- tribute before the timer runs out.",
        "text_length": 337,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.01988600753247738,
          0.021867021918296814,
          -0.011376971378922462,
          0.0005300669581629336,
          0.036272451281547546,
          -0.04129636287689209,
          0.017598474398255348,
          -0.025297706946730614,
          -0.03674585372209549,
          0.0887409895658493
        ]
      },
      {
        "chunk_index": 937,
        "relative_chunk_number": 235,
        "text": "Try to be efficient about finding and contributing new knowledge! Maybe you have knowledge about regional dif- ferences in gift giving, so you select the middle question to expand, revealing AI-generated an- swers in blue. Notice that some of these answers have a bright teal bar on the right side. These are answers that the AI is confident in. Others have a dark red bar on the right side.",
        "text_length": 391,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.03331202268600464,
          0.005577535834163427,
          -0.009160801768302917,
          0.019284872338175774,
          -0.023117391392588615,
          -0.004612396005541086,
          0.017305858433246613,
          0.03708121180534363,
          -0.03712980076670647,
          0.03907756134867668
        ]
      },
      {
        "chunk_index": 938,
        "relative_chunk_number": 236,
        "text": "These are answers that the AI was NOT confident in, so they will of- ten be wrong. Since your goal is to contribute new knowledge, it is a good idea to start with the dark red bars. Read these highlighted answers and cor- rect them if they are wrong. For example, this last answer about refusing gifts once or twice isn\u2019t true for everyone \u2013 it applies only for Chinese Indone- sians.",
        "text_length": 384,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.042398158460855484,
          0.041035883128643036,
          -0.021453754976391792,
          0.03450791537761688,
          -0.09288887679576874,
          -0.005298015661537647,
          0.015722814947366714,
          0.00462083425372839,
          -0.04411913827061653,
          -0.009965847246348858
        ]
      },
      {
        "chunk_index": 939,
        "relative_chunk_number": 237,
        "text": "You can add: For Chinese Indonesians, it is polite to refuse a gift once or twice before accept- ing it, to show modesty and humility. and delete which is a common practice across various cultures in Indonesia. At the bottom, you should add any answers you can that aren\u2019t already given by the AI.",
        "text_length": 297,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.007307355757802725,
          0.058242566883563995,
          0.026853537186980247,
          0.06416775286197662,
          -0.05304333567619324,
          -0.007169575430452824,
          0.014588355086743832,
          -0.002850433113053441,
          -0.03571590408682823,
          0.03802502527832985
        ]
      },
      {
        "chunk_index": 940,
        "relative_chunk_number": 238,
        "text": "Here, you might add: For Indian or Hindu Indonesians, wrap- ping should be in bright colors, and leather products and alcohol should not be given. You can also add: For Malays and Muslim Indonesians, gifts that have alcohol or pork in them should not be given. Let\u2019s say you\u2019ve shared all your knowledge about these topics, and doubt you can contribute more here.",
        "text_length": 363,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.003733524587005377,
          0.04731475189328194,
          0.014386780560016632,
          0.049597468227148056,
          -0.012664051726460457,
          -0.0023029616568237543,
          0.015866093337535858,
          0.01827223226428032,
          -0.03449295088648796,
          -0.010769438929855824
        ]
      },
      {
        "chunk_index": 941,
        "relative_chunk_number": 239,
        "text": "Feel free to click \u201cHide its answers\u201d to tidy up your workspace, then move on to a different question. To find an interesting question that AI doesn\u2019t understand, it may help to go deeper into the tree. Even if the AI is confident about the initial question on the branch, we can more challenging questions by going deeper!",
        "text_length": 323,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.022953586652874947,
          0.005460013635456562,
          -0.00985747016966343,
          -0.006775389891117811,
          -0.06259572505950928,
          -0.019234318286180496,
          0.014947365038096905,
          0.00840542558580637,
          -0.03882806375622749,
          -0.0016946358373388648
        ]
      },
      {
        "chunk_index": 942,
        "relative_chunk_number": 240,
        "text": "Each time we generate an an- swer to a question, we can generate more questions about that very answer like this. <expands top question, top answer, top ques- tion> Look how this pink question is highlighted red! The red highlighted answer before meant the AI wasn\u2019t confident about that single answer, but a red highlighted QUESTION means it was a very challenging question overall for the AI.",
        "text_length": 394,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          0.022300265729427338,
          0.008474727161228657,
          -0.03679261356592178,
          0.011224523186683655,
          -0.06523758172988892,
          0.0003379225090611726,
          0.016843952238559723,
          0.024565281346440315,
          -0.03269216790795326,
          0.025366246700286865
        ]
      },
      {
        "chunk_index": 943,
        "relative_chunk_number": 241,
        "text": "Look how all of its answers are red. This is a great place to contribute your knowl- edge, especially because an outsider who isn\u2019t fa- miliar with Indonesian culture would have a very hard time learning about this concept of Dutarikh. I tried Googling some terms and found very little documentation.",
        "text_length": 300,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.004254315048456192,
          0.02261078730225563,
          0.010534475557506084,
          0.04519563168287277,
          -0.0014633858809247613,
          -0.029735902324318886,
          0.013828568160533905,
          -0.00143808638677001,
          -0.0671389177441597,
          -0.006210848689079285
        ]
      },
      {
        "chunk_index": 944,
        "relative_chunk_number": 242,
        "text": "If you have other interesting questions to ask, please contribute them here in the \u201cWhat Else?\u201d section. By answering your own questions, you can contribute even more unique and interesting knowledge. An interesting question should specifically dis- tinguish your culture from others, so you might ask: How does Gayo Dutarikh differ from the embroidery of other regions of Indone- sia?",
        "text_length": 385,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.0410613939166069,
          0.02647397667169571,
          0.012545003555715084,
          0.02705206908285618,
          -0.017813945189118385,
          0.000561959168408066,
          0.016580093652009964,
          0.01483417022973299,
          -0.038258276879787445,
          -0.004030948039144278
        ]
      },
      {
        "chunk_index": 945,
        "relative_chunk_number": 243,
        "text": "Then we can use AI to suggest some answers for this question. Note how these answers are quite vague because this topic is challenging for AI. You should definitely add better answers here, or edit existing answers to make them more specific! Keep building the tree until the timer is up.",
        "text_length": 288,
        "vector_shape": [
          768
        ],
        "vector_norm": 0.9999999403953552,
        "vector_sample_first_10": [
          0.030314220115542412,
          0.031681302934885025,
          -0.0139102041721344,
          0.00391510920599103,
          -0.05009272322058678,
          -0.0011741332709789276,
          0.011852195486426353,
          0.0280306339263916,
          -0.04228459671139717,
          0.025646986439824104
        ]
      },
      {
        "chunk_index": 946,
        "relative_chunk_number": 244,
        "text": "Fi- nally, click Export to download the tree as a file, which we will upload to Upwork, thus completing this round of annotation.",
        "text_length": 129,
        "vector_shape": [
          768
        ],
        "vector_norm": 1.0,
        "vector_sample_first_10": [
          -0.0018528115469962358,
          0.006201089359819889,
          0.021897392347455025,
          0.034129898995161057,
          -0.02329481765627861,
          0.029195183888077736,
          0.018092967569828033,
          0.04387111961841583,
          -0.06527664512395859,
          -0.03806322067975998
        ]
      }
    ]
  }
]