[
  {
    "paper_id": "2510.25701v1",
    "original_length": 41323,
    "modes": {
      "simple": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n1\n0\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nInterpreting LLMs as Credit Risk Classifiers: Do Their Feature\nExplanations Align with Classical ML?\n\nSaeed AlMarri\nKhalifa University\nAbu Dhabi, United Arab Emirates\n100061460@ku.ac.ae",
            "char_count": 257,
            "word_count": 58
          },
          {
            "chunk_id": 1,
            "text": "Kristof Juhasz\nADIA\nAbu Dhabi, United Arab Emirates\nkristof.juhasz@adia.ae\n\nMathieu Ravaut\nADIA\nAbu Dhabi, United Arab Emirates\nmathieu.ravaut@adia.ae\n\nGautier Marti\nADIA\nAbu Dhabi, United Arab Emirates\ngautier.marti@adia.ae\n\nHamdan Al Ahbabi\nKhalifa University\nAbu Dhabi, United Arab Emirates\n100061346@ku.ac.ae",
            "char_count": 312,
            "word_count": 38
          },
          {
            "chunk_id": 2,
            "text": "Ibrahim Elfadel\nKhalifa University\nAbu Dhabi, United Arab Emirates\nibrahim.elfadel@ku.ac.ae\n\nAbstract\nLarge Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification\ntasks through zero-shot prompting. However, their suitability for\nstructured tabular data remains underexplored, especially in high-\nstakes financial applications such as financial risk assessment.",
            "char_count": 435,
            "word_count": 52
          },
          {
            "chunk_id": 3,
            "text": "This\nstudy conducts a systematic comparison between zero-shot LLM-\nbased classifiers and LightGBM, a state-of-the-art gradient-boosting\nmodel, on a real-world loan default prediction task. We evaluate\ntheir predictive performance, analyze feature attributions using\nSHAP, and assess the reliability of LLM-generated self-explanations.",
            "char_count": 334,
            "word_count": 41
          },
          {
            "chunk_id": 4,
            "text": "While LLMs are able to identify key financial risk indicators, their\nfeature importance rankings diverge notably from LightGBM, and\ntheir self-explanations often fail to align with empirical SHAP at-\ntributions. These findings highlight the limitations of LLMs as\nstandalone models for structured financial risk prediction and raise\nconcerns about the trustworthiness of their self-generated expla-\nnations.",
            "char_count": 407,
            "word_count": 56
          }
        ],
        "stats": {
          "mode": "Simple",
          "num_chunks": 113,
          "avg_chars": 363.929203539823,
          "min_chars": 95,
          "max_chars": 621,
          "avg_words": 51.27433628318584,
          "status": "success"
        },
        "time_sec": 0.1855311393737793
      },
      "semantic": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n1\n0\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nInterpreting LLMs as Credit Risk Classifiers: Do Their Feature\nExplanations Align with Classical ML?\n\nSaeed AlMarri\nKhalifa University\nAbu Dhabi, United Arab Emirates\n100061460@ku.ac.ae\n\nKristof Juhasz\nADIA\nAbu Dhabi, United Arab Emirates\nkristof.juhasz@adia.ae\n\nMathieu Ravaut\nADIA\nAbu Dhabi, United Arab Emirates\nmathieu.ravaut@adia.ae\n\nGautier Marti\nADIA\nAbu Dhabi, United Arab Emirates\ngautier.marti@adia.ae\n\nHamdan Al Ahbabi\nKhalifa University\nAbu Dhabi, United Arab Emirates\n100061346@ku.ac.ae\n\nIbrahim Elfadel\nKhalifa University\nAbu Dhabi, United Arab Emirates\nibrahim.elfadel@ku.ac.ae\n\nAbstract\nLarge Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification\ntasks through zero-shot prompting. However, their suitability for\nstructured tabular data remains underexplored, especially in high-\nstakes financial applications such as financial risk assessment. This\nstudy conducts a systematic comparison between zero-shot LLM-\nbased classifiers and LightGBM, a state-of-the-art gradient-boosting\nmodel, on a real-world loan default prediction task. We evaluate\ntheir predictive performance, analyze feature attributions using\nSHAP, and assess the reliability of LLM-generated self-explanations.\nWhile LLMs are able to identify key financial risk indicators, their\nfeature importance rankings diverge notably from LightGBM, and\ntheir self-explanations often fail to align with empirical SHAP at-\ntributions. These findings highlight the limitations of LLMs as\nstandalone models for structured financial risk prediction and raise\nconcerns about the trustworthiness of their self-generated expla-\nnations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-\nthe-loop oversight when deploying LLMs in risk-sensitive financial\nenvironments.\n\nCCS Concepts\n\u2022 Computing methodologies \u2192 Supervised learning by clas-\nsification.\n\nKeywords\nLarge Language Models, Explainable AI, SHAP, Credit Risk Pre-\ndiction, Responsible AI, Financial Machine Learning, Model Inter-\npretability\n\n1 Introduction\nLarge Language Models (LLMs), such as GPT-4, have demonstrated\nstrong performance across a range of natural language processing\n(NLP) tasks, including classification and reasoning [2, 4, 32]. Their\nability to function as classifiers without explicit training pipelines,\nrelying solely on a few-shot or zero-shot prompting, has gained\nsignificant attention. This raises fundamental questions about the\nreliability and validity of LLM-based classification, particularly in\n\ncomparison to classical machine learning models such as gradient-\nboosting decision trees methods like XGBoost [9] or LightGBM\n[19].\n\nTraditional classification tasks require structured pipelines in-\nvolving feature engineering, model training, validation, and hyper-\nparameter tuning. Fine-tuning models on tabular data, in particular,\ndemands expertise in preprocessing, GPU management, and bal-\nancing class distributions to prevent trivial solutions. In contrast,\nLLMs bypass fine-tuning entirely, requiring only natural language\nprompting. This reduces technical barriers, making them accessible\nto non-experts, but raises an important question: How do LLM-based\nclassifiers arrive at their predictions, and do they rely on decision pat-\nterns similar to classical machine learning models?\n\nThis question is especially critical in high-stakes financial do-\nmains, where algorithmic risk assessments directly affect credit\naccess, interest rates, and regulatory compliance [11]. Financial\ninstitutions operate under strict governance frameworks such as\nBasel III [24] and GDPR [1], where opaque models can lead to regu-\nlatory breaches, reputational damage, and unfair or discriminatory\ndecisions. Unlike decision trees or gradient boosting models, LLMs\nare complex black-box models with billions of parameters, making\ninterpretability a key challenge. This has led to growing interest in\nExplainable AI (XAI) techniques to analyze LLMs\u2019 internal logic and\nassess their alignment with human-interpretable decision patterns.\nIn this study, we conduct a systematic evaluation of zero-shot\nLLM classifiers for structured credit risk prediction, directly compar-\ning them with a well-established interpretable baseline (LightGBM).\nBeyond performance comparison, our primary goal is to audit their\nexplainability to determine whether their feature attributions and\nself-generated rationales align with dataset-driven reasoning or\nrely on external priors. We employ Shapley Additive Explanations\n(SHAP) [20] to analyze the faithfulness of their decision patterns.\nLeveraging a public loan default prediction dataset, we address the\nfollowing research questions:\n\n\u2022 Explainability: Do LLM-based classifiers prioritize the same\n\nfeatures as classical models?\n\n\u2022 Self-Explainability: Can LLMs provide self-generated ratio-\n\nnales that align with SHAP-derived feature attributions?\n\n\u2022 Performance: How do LLMs compare to classical machine learn-\ning classifiers (ROC-AUC, PR-AUC)? Does ensembling both types\nof models improve performance?\n\n \n \n \n \n \n \n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\nOur work directly addresses the workshop call on AI safety,\nfairness, and explainability in high-stakes financial environments,\nand responsible deployment in fintech and banking, by auditing the\nfaithfulness of LLM explanations against SHAP on a real credit-risk\ntask. The remainder of this paper is structured as follows: Section 2\nreviews related work, Section 3 details the methodology, Section 4\npresents the experimental setup, Section 5 analyzes results, feature\nattribution and reasoning patterns, and Section 6 concludes with\nkey findings and future directions.\n\n2 Related Work\n2.1 LLMs for Zero-Shot Classification\nLLMs have demonstrated strong classification capabilities, often\nachieving competitive performance without supervision. Brown\net al. [4] introduced GPT-3, highlighting its few-shot and zero-\nshot classification potential. This paradigm removes the need for\nclassical training pipelines, enabling non-experts to perform clas-\nsification via direct natural-language prompting and in-context\nlearning. Subsequent work has explored structured prompting to\nenhance classification accuracy. Hao et al. [16] introduced Chain-\nof-Thought (CoT) prompting, showing that structured reasoning\ncan improve LLM performance\u2014relevant when adapting them to\nstructured data.\n\nA recent line of work examines whether LLMs can function as\nregressors for numerical data. Vacareanu et al. [31] found that LLMs\napproximate regression functions with in-context examples, while\nBuckmann and Hill [5] proposed combining LLMs with logistic\nregression for low-data robustness. Song et al. [28] and Song and\nBahri [27] further extend this research direction, demonstrating\nuniversal regression capabilities using decoding-based inference.\n\nOur Contribution. Unlike prior studies that evaluate LLM clas-\nsification in isolation, our work positions this comparison as a step\ntoward responsible AI deployment in high-stakes financial contexts.\nBy systematically comparing zero-shot LLM and LightGBM on the\nsame dataset and auditing their decision patterns using SHAP-based\nexplainability, we assess not only accuracy but also faithfulness\nand reliability of model reasoning critical elements for trustworthy,\nfair and transparent AI use in credit risk assessment.\n\n2.2 LLM Explainability\nFeature-attribution methods such as SHAP [20] are widely used\nto assess feature importance in machine-learning models. We use\nSHAP to compare LLM-based probabilistic classifiers with Light-\nGBM. A key question is whether LLMs\u2019 self-explanations align\nwith actual feature importance. Huang et al. [18] report that LLM\nrationales are often plausible but do not necessarily reflect inter-\nnal reasoning. Dehghanighobadi et al. [10] analyze counterfactual\nexplanations and show that LLMs can struggle with causal de-\npendencies. Sarkar [25] argues that LLMs lack self-explanatory\ncapabilities due to opaque training dynamics, and Turpin et al. [30]\nshow that CoT-generated explanations can be misleading.\n\nOur Contribution. Prior work predominantly studies LLM self-\nexplanations in text tasks. To our knowledge, our study is the first\nto compute SHAP-based feature importance for LLMs prompted\nto output probabilistic predictions on structured financial data,\n\nenabling a direct faithfulness audit of self-explanations against\nempirical attributions.\n\n2.3 LLMs for Tabular Data\nRecent work explores whether LLMs can replace gradient-boosted\nmodels such as XGBoost, LightGBM, and AdaBoost for tabular clas-\nsification. Fang et al. [13] survey LLMs on tabular data and highlight\nadaptation challenges. Ghaffarzadeh-Esfahani et al. [15] benchmark\nLLMs against classical ML for COVID-19 mortality prediction, con-\ncluding that classical ML models outperform LLMs on structured\ndata. Chen et al. [8] introduce ClinicalBench and similarly find XG-\nBoost superior for clinical prediction tasks. Hegselmann et al. [17]\npropose TabLLM, which reformulates tables as natural language\nfor few-shot classification, while Shi et al. [26] introduce ZET-LLM,\ntreating autoregressive LLMs as feature-embedding models for tab-\nular prediction. While these studies highlight LLM potential, they\ngenerally do not evaluate explainability or faithfulness of rationales\non tabular tasks.\n\nOur Contribution. We conduct a head-to-head comparison of\nLLMs and LightGBM on the same structured dataset and integrate\nSHAP-based explainability, offering a dual analysis of predictive\nperformance and feature attribution to illuminate decision mecha-\nnisms.\n\n2.4 Classical ML, XAI, and LLMs for Financial\n\nAI\n\nExplainability is crucial in financial applications for risk assessment.\nMartins et al. [22] review XAI in finance, while \u010cernevi\u010dien\u02d9e and\nKaba\u0161inskas [7], Misheva et al. [23], and Bussmann et al. [6] analyze\nexplainability in credit-risk modeling. Several studies benchmark\nML models for loan-default prediction: Madaan et al. [21] compare\ndecision trees and random forests without an explainability analysis;\nSrinivasa Rao et al. [29] assess ML techniques for loan risk but do\nnot explore LLMs; and Boughaci and Alkhawaldeh [3] study credit-\nscoring models without evaluating LLM-based predictions.\n\nOur Contribution. While prior work focuses on classical ML\nfor loan prediction, we present the first comparative analysis of LLMs\nand LightGBM on structured loan data, integrating SHAP-based ex-\nplainability. Our findings extend beyond credit risk to broader finan-\ncial applications, including fraud detection, regulatory compliance,\nand algorithmic trading decision-making, where explainability is\nkey to adoption.\n\n3 Methodology\n3.1 Inference Setup\nWe systematically evaluate the predictive performance and explain-\nability of LightGBM and zero-shot LLMs. We design the classifi-\ncation problem such that both LightGBM and LLMs receive the\nsame set of input features and output probability estimates in-\nstead of discrete classes. Probability outputs offer three advantages:\n(i) fine-grained evaluation via discrimination metrics (ROC-AUC,\nPR-AUC); (ii) enhanced explainability, as SHAP feature attribution\nis generally more informative when applied to probability scores\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\nWe use SHAP for post hoc explanations [20], specifically the\nmodel-agnostic PermutationExplainer. We adopt this efficient\nSHAP estimator because our prediction function is an LLM infer-\nence, which is costly. To balance accuracy and runtime, we sample\n250 instances from each dataset for explanation. We construct the\nbackground (masker) via \ud835\udc58-means clustering with \ud835\udc36 = 5 centroids\n(using shap.kmeans) and set the max_evals budget so that the\nexplainer executes exactly \ud835\udc47 = 4 permutations in our experiments.\n\nApproximate cost (model calls). Let \ud835\udc3e be the number of in-\nstances explained, \ud835\udc40 the number of features, \ud835\udc35 the number of\nbackground draws per masked evaluation (here \ud835\udc35 = \ud835\udc36 = 5), and \ud835\udc47\nthe number of random permutations. The PermutationExplainer\nrequires approximately:\n\n#calls \u2248 \ud835\udc3e \u00d7 \ud835\udc47 \u00d7 (\ud835\udc40 + 1) \u00d7 \ud835\udc35 = O (\ud835\udc3e \ud835\udc47 \ud835\udc40 \ud835\udc35)\n(1)\nmodel evaluations. In SHAP\u2019s implementation, \ud835\udc47 is governed by\n\nmax_evals via the practical rule:\n\nFigure 1: Comparative Explainable AI Framework: Classical ML vs.\nLLMs. The dataset is processed through two paradigms: (i) a struc-\ntured LightGBM model and (ii) a zero-shot LLM using natural lan-\nguage prompts. Both generate probability predictions, analyzed in-\ndividually and in an ensemble. Explainability is assessed via SHAP\n(for both) and LLM self-explanations, evaluating their alignment.\n\nrather than hard labels; and (iii) a direct test of LLMs\u2019 capability as\nprobability regressors.\n\n3.2 Explainability\nWe treat explainability as a model-auditing task focused on the\nfaithfulness of the factors a model claims or appears to use. Our\naudit has two complementary components:\n\u2022 (A) SHAP as a model-agnostic audit baseline. We use SHAP\n[20] to assign contribution values to each feature for both Light-\nGBM and the LLM-based probabilistic classifier. SHAP values\noperationalize which features, and in what direction, are driving\neach model\u2019s predictions.\n\n\u2022 (B) LLM self-explanations as unverified rationales. In ad-\ndition to SHAP, we prompt the LLM to provide instance-level\nrationales and feature-level directional judgments (positive/neg-\native/neutral). These self-explanations are treated as claims that\nmust be checked against the SHAP audit checks.\n\nWe compare (i) global feature importance patterns (via SHAP)\nacross models; (ii) directional dependence for key features (e.g.,\nwhether higher values increase or decrease repayment probabil-\nity); and (iii) instance-level coherence between an LLM\u2019s rationale\nand the corresponding SHAP attributions. Misalignment across\nthese checks is interpreted as a faithfulness risk and a caution for\ndeployment.\n\nScaling Shapley-Value Inference for LLMs. To operationalize\n3.2.1\nexplainability as an audit of model decision logic, we estimate\nfeature attributions for both LightGBM and the zero-shot LLM\nclassifiers using SHAP. Faithful auditing requires identifying which\nfeatures actually drive predictions, not just producing plausible\nexplanations.\n\n(cid:107)\n\n\ud835\udc47 \u2248\n\n(cid:106) max_evals\n2\ud835\udc40\ni.e., roughly 2\ud835\udc40 masked evaluations per permutation path. With\nmax_evals = 200 and \ud835\udc40 = 21, this yields \ud835\udc47 = \u230a200/(2 \u00d7 21)\u230b = 4.\nUsing \ud835\udc35 = 5, the per-instance cost is therefore \u2248 4\u00d7(21+1)\u00d75 = 440\nmodel calls.\n\n(2)\n\nWhy is it more efficient than KernelExplainer? With a sum-\nmarized background of \ud835\udc36 centroids, the dominant model-call com-\nplexity of KernelExplainer scales as:\n\n#calls \u2248 \ud835\udc3e \u00d7 \ud835\udc36 \u00d7 \ud835\udc40 2 = O (\ud835\udc3e \ud835\udc36 \ud835\udc40 2)\n\n(3)\n\ndue to sampling coalitions and fitting a kernel-weighted regres-\nsion. In our setting (\ud835\udc36 = 5, \ud835\udc40 = 21) this is \u2248 5 \u00d7 212 = 2205 eval-\nuations per instance. By contrast, PermutationExplainer scales\nlinearly in \ud835\udc40 and avoids the regression solve, yielding an expected\nper-instance reduction of\n\nspeedup \u2248\n\n\ud835\udc36 \ud835\udc40 2\n\ud835\udc47 (\ud835\udc40 + 1) \ud835\udc35\n\n\u2248\n\n2205\n440\n\n\u2248 5\u00d7\n\n(4)\n\nThe fivefold decrease in model calls translates into substantially\nlower LLM inference time while maintaining faithful attributions,\nwhich is why we use PermutationExplainer with \ud835\udc47 = 4.\n\n3.2.2 LLM Self-Explanations. Motivated by the emerging reason-\ning capabilities of large language models (LLMs) [33], we use the\nLLM as an explainability tool alongside SHAP. Specifically, for each\ninput feature, we provide its description to the LLM and prompt it\nto predict whether the feature is likely to have a positive, negative,\nor no effect on the predicted outcome. The LLM also generates a\nbrief textual justification for each directional prediction, which we\nrefer to as its self-explanation.\n\nThese LLM-generated self-explanations are treated as unveri-\nfied rationales and are not assumed to reflect the model\u2019s actual\ninternal reasoning. We compare them against SHAP-based feature\nattributions, which serve as a model-agnostic baseline. When LLM\nexplanations diverge from SHAP attributions, we interpret this\nmisalignment as a potential risk of explainability, an indication\n\n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\nthat LLM may produce externally plausible but internally incon-\nsistent reasoning. While we do not perform formal risk scoring,\nhighlighting such discrepancies can inform governance decisions\nin high-stakes financial applications, where model transparency is\nessential.\n\nFigure 1 illustrates the overall methodological framework. The\ndataset is processed through two parallel pipelines: (i) a structured\nLightGBM model and (ii) a zero-shot LLM using natural language\nprompts. Both produce probability estimates, which are then ana-\nlyzed for predictive performance and explainability through SHAP\nand LLM self-explanations, enabling a cross-comparison of their\ndecision logic.\n\n4 Experiments\n4.1 Data\n4.1.1 Task Description. Loan default prediction is a critical chal-\nlenge in credit risk assessment, where lenders estimate the likeli-\nhood of borrowers failing to repay their loans. Accurate predictions\nenable financial institutions to make informed lending decisions,\nset appropriate interest rates, and manage credit risk effectively.\nBecause loan defaults directly affect credit access, financial stability,\nand regulatory compliance, this task is widely regarded as a high-\nstakes benchmark for testing the safety and reliability of predictive\nmodels in finance.\n\n4.1.2 Dataset Description. We use LendingClub\u2019s publicly avail-\nable loan records hosted by Kaggle 1, which were disclosed as\npart of the company\u2019s regulatory filings with the U.S. Securities\nand Exchange Commission (SEC). As a major peer-to-peer lend-\ning platform, LendingClub was required to provide detailed loan\nissuance and performance data to comply with SEC regulations,\nmaking this dataset a widely used benchmark in credit risk mod-\neling. The dataset includes loan applications issued between 2007\nand 2016, with approximately 396,000 loan records. Each loan is\nlabeled with its repayment status, distinguishing between Fully\nPaid and Charged Off (defaulted) loans. It contains both numerical\nand categorical attributes describing borrower creditworthiness\nand loan characteristics, for a total of 26 financial and credit-related\nfeatures.\n\n4.1.3 Preprocessing. We removed five features due to high cardi-\nnality, redundancy, or potential data leakage:\n\n\u2022 issue_d: Loan issue date (introduces temporal bias)\n\u2022 earliest_cr_line: Borrower\u2019s earliest credit line (high cardi-\n\nnality)\n\n\u2022 address: High-cardinality feature (introduces geographic bias)\n\u2022 emp_title: High-cardinality categorical variable\n\u2022 title: High-cardinality; redundant with the purpose variable\n\nWe also excluded categorical features with more than 40 unique\ncategories to mitigate overfitting in LightGBM. This ensures a fair\ncomparison between models, preventing disadvantages for Light-\nGBM (which lacks natural text processing) and restricting LLMs\nfrom exploiting external knowledge, such as macroeconomic trends\nfrom loan dates or geographic signals from addresses. These steps\n\n1https://www.kaggle.com/datasets/sndpred/loan-data\n\nTable 1: Final Dataset Features.\n\nFeature Description\n\nRange\n\nLoan amount\nTerm\nInterest rate\nInstallment\nGrade\nSub-grade\nEmployment length\n\nHome ownership\n\nAnnual income\nVerification status\n\nPurpose\nDebt-to-income (DTI)\nratio\nOpen credit accounts\nPublic records\nRevolving balance\nRevolving utilization\nrate\nTotal accounts\nInitial listing\nstatus\nApplication type\n\nMortgage accounts\nPublic record\nbankruptcies\n\n[1600.0, 35000.0]\ncategorical: {36 months, 60 months}\n[6.03, 25.29]\n[55.32, 1204.57]\ncategorical: {A, B, C, D, E, F, G}\ncategorical: {A1, A2, . . . , G4, G5}\ncategorical: {<1 year, 1 year, . . . , 10+\nyears}\ncategorical: {MORTGAGE, NONE,\nOTHER, OWN, RENT}\n[19000.0, 250000.0]\ncategorical: {Not Verified, Source\nVerified, Verified}\ncategorical: 14 values (e.g., wedding)\n[1.6, 36.41]\n\n[3.0, 27.0]\n[0.0, 2.0]\n[169.05, 83505.9]\n[1.2, 98.0]\n\n[6.0, 60.0]\ncategorical: {f, w}\n\ncategorical: {DIRECT PAY,\nINDIVIDUAL, JOINT}\n[0.0, 9.0]\n[0.0, 1.0]\n\nhelp control for potential bias and improve the fairness and au-\nditability of the evaluation.\n\nFinal Dataset. After preprocessing, the final dataset consists\n4.1.4\nof 396,000 rows and 21 predictors (12 numerical and 9 categorical).\nThe data was randomly split into training (80%) and testing (20%),\nwith 79,200 instances used for LLM inference. This controlled setup\nensures that both models operate on identical structured inputs\nwithout access to external priors, supporting a transparent and\nauditable comparison.\n\nIn Table 1, we list all features. For numerical features, we report\nthe interval bounded by the 1st and 99th percentiles. For categorical\nfeatures, we report the values space (if it is not too large).\n\n4.2 Models\nLightGBM Training. We trained a LightGBM classifier using\n4.2.1\nthe gradient boosting decision tree (GBDT) algorithm with a binary\nobjective and AUC as the primary evaluation metric. The model\nwas trained with a learning rate of 0.01 and up to 10,000 estima-\ntors, applying early stopping after 100 rounds based on validation\nperformance. To reduce overfitting, we applied a feature fraction\nof 0.8, bagging fraction of 0.8, and L1/L2 regularization (0.1 each).\nThe num_leaves parameter was set to 63 and min_data_in_leaf\nto 50.\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\nInstance-Level Prompt Template\n\nPredict whether a loan application will be \"Fully Paid\" or \"Charged\nOff\" based on the borrower\u2019s information. Use the features provided\nbelow to assess the likelihood of the loan being fully repaid.\nLoan Application Details:\n\n<feature_1 name>: <feature_1 value>\n...\n<feature_N name>: <feature_N value>\n\nProvide your estimated probability of the loan being \"Fully Paid\". Also\nprovide a brief explanation of this estimate based on the features. Your\nanswer should only contain the probability estimate and the explanation\nin the following JSON format.\n\n{\n\n}\n\n\"Estimated Fully Paid Probability\": <float value\n\nbetween 0 and 1>,\n\n\"Explanation\": <string value>\n\nFeature-Level Prompt Template\n\nYou are working on predicting whether a loan application will be \"Fully\nPaid\" or \"Charged Off\" based on the borrower\u2019s information. One of the\nfeatures is the following:\n\n<feature name>\n\nDo you think that this feature will impact the application positively,\nnegatively, or have no impact? Provide your answer as one of the three\nstrings: positive | negative | neutral. Use the following JSON\nformat:\n\n{\n\n}\n\n\"Feature impact\": <positive | negative | neutral>\n\nWe use LightGBM as a transparent and interpretable baseline,\nproviding a benchmark for feature importance and prediction sta-\nbility against which the behavior of large language models (LLMs)\ncan be audited.\n\n4.2.2 LLM Inference. We evaluated three recent open-source in-\nstruction tuned LLMs of comparable size: LLaMA-3.1-8B-Instruct\n[12], Gemma-2-9B-Instruct [14], and Qwen-2.5-7B-Instruct\n[35]. These models were pre-trained on 15.6T, 8T, and 18T tokens,\nrespectively. All experiments used their instruction-tuned versions.\nModel weights were downloaded from the Hugging Face Hub [34],\nand inference was run locally using vLLM2 on four Nvidia A10G\n24GB GPUs.\n\nTo reduce computation time, we randomly sampled 250 test\ninstances for SHAP value estimation. We did not perform any fine-\ntuning and used strict zero-shot inference without any in-context\nlearning. This design ensures the LLMs rely solely on the provided\n\n2https://github.com/vllm-project/vllm\n\nFigure 2: ROC and Precision-Recall curves comparing the\nperformance of zero-shot LLMs and LightGBM on the loan\nclassification task. LightGBM consistently outperforms indi-\nvidual LLMs with Gemma-2-9b showing the most promising\nresult out of the LLMs.\n\nstructured features, preventing data contamination or leakage from\npre-training.\n\nWe used the same prompt templates for all LLMs. The instance-\nlevel prompt asked the model to jointly predict the probability that\na loan would be fully repaid (a float between 0 and 1) and to provide\na brief explanation. The feature-level prompt asked whether each\nfeature would impact the prediction positively, negatively, or not\nat all.\n\nUnlike free-form textual descriptions, the structured dictionary\nformat used for instance-level prompts is unlikely to have appeared\nin the LLMs\u2019 pretraining corpus, which reduces the risk of data\ncontamination.\n\n5 Analysis\n5.1 Performance Results\nTable 2 and Figure 2 compare the performance of zero-shot LLMs\nand LightGBM on the loan repayment prediction task. LightGBM\nachieved the highest ROC\u2013AUC (0.73), outperforming all LLMs in\nthe zero-shot setting. Among LLMs, Gemma-2-9B performed best\n(0.67), followed by LLaMA-3.1-8B (0.65) and Qwen-2.5-7B (0.61).\nThese findings are consistent with prior evidence that gradient\nboosting often surpasses deep learning methods on structured tab-\nular data. The LightGBM\u2013Gemma-2-9B ensemble achieved 0.70\nROC\u2013AUC, indicating no diversification benefit over LightGBM\nalone.\n\nPR\u2013AUC results follow a similar pattern. LightGBM obtained\nthe highest PR\u2013AUC (0.91), exceeding all LLMs. Gemma-2-9B again\nranked highest among the LLMs (0.88), followed by LLaMA-3.1-8B\n(0.86) and Qwen-2.5-7B (0.85). The ensemble model reached 0.90,\nclosely trailing LightGBM. All models outperformed the base-rate\nPR\u2013AUC (0.80), confirming meaningful predictive signal.\n\nOverall, these results show that while zero-shot LLMs achieve\nreasonable predictive performance, they remain inferior to a well-\ntuned LightGBM model on structured financial data, underscoring\nthe need for careful governance if deployed in high-stakes settings.\n\n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\nTable 2: Performance Metrics of Models for Loan Default\nClassification\n\nModel\n\nROC-AUC PR-AUC\n\nLightGBM\nGemma-2-9B\nLlama-3.1-8B\nQwen-2.5-7B\nEnsemble (LightGBM + Gemma-2-9B)\n\nRandom Classifier (Baseline)\nBase Rate for Fully Paid Label (Baseline)\n\n0.73\n0.67\n0.65\n0.61\n0.70\n\n0.50\n\u2013\n\n0.91\n0.88\n0.86\n0.85\n0.90\n\n\u2013\n0.80\n\nTable 3: Feature Importance Comparison\n\nFeature\n\nLGBM Gemma-\n\nSub-grade\nAnnual income\nTerm\nInterest rate\nDTI\nOpen account\nRevolving util\nLoan amount\nHome ownership\nGrade\n\n0.062\n0.026\n0.023\n0.022\n0.019\n0.018\n0.018\n0.018\n0.015\n0.013\n\n2-9B\n\n0.046\n0.031\n0.002\n0.019\n0.019\n0.004\n0.044\n0.006\n0.012\n0.079\n\nLlama-\n3.1-8B\n\nQwen-\n2.5-7B\n\n0.038\n0.037\n0.008\n0.048\n0.027\n0.007\n0.021\n0.026\n0.031\n0.065\n\n0.004\n0.014\n0.000\n0.005\n0.006\n0.001\n0.005\n0.004\n0.003\n0.025\n\n5.2 SHAP Feature Importance Comparison\nFigure 3 presents the SHAP feature importance rankings for Light-\nGBM and the three LLMs, providing insight into the key factors\ninfluencing loan classification decisions across models. The corre-\nsponding numerical values are reported in Table 3.\n\nA primary observation is the strong overlap in the top-ranked\nfeatures across all models, despite the LLMs operating in a zero-shot\nsetting. Features such as Sub-grade, Grade, Annual income, and\nInterest rate consistently emerge as dominant predictors. This\nsuggests that LLMs, even without task-specific fine-tuning, are able\nto extract and prioritize meaningful financial attributes in a manner\nbroadly consistent with classical ML models like LightGBM.\n\nHowever, notable differences emerge in feature weighting and\nrank order. LightGBM assigns greater relative importance to struc-\ntured numerical features such as Sub-grade, Annual income, and\nLoan amount, reflecting its reliance on directly interpretable numer-\nical signals. In contrast, the LLMs particularly LLaMA-3.1-8B and\nGemma-2-9B distribute their importance more evenly across cate-\ngorical and behavioral attributes such as Verification status,\nPurpose, and Home ownership. This indicates that LLMs may be\nleveraging latent semantic relationships within categorical features\nthat are not explicitly modeled by LightGBM.\n\n5.3 SHAP Comparative Analysis\nTo further investigate the decision mechanisms of the models, we\ncompare the SHAP summary plots of LightGBM and the three LLMs.\n\nFigure 3: SHAP feature importance comparison between\nLightGBM and LLMs. Despite being in a zero-shot setting,\nLLMs identify a remarkably similar set of key financial fea-\ntures as LightGBM, though with differences in feature weight-\ning and distribution.\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\nFigure 4: SHAP summary plots comparing feature importance dis-\ntributions for LLMs and LightGBM. LightGBM shows a more struc-\ntured reliance on key financial indicators, while LLMs exhibit more\ndispersed and lower-magnitude SHAP values, indicating weaker fea-\nture dependencies.\n\nFigure 5: SHAP Feature dependence plots and LLM self-\nexplanations for the feature DTI.\n\nIn Figure 4, these plots illustrate how variations in feature values\ninfluence predicted loan repayment probabilities.\n\nDespite operating in a zero-shot setting, the LLMs successfully\nextract meaningful relationships from structured financial features.\nCore predictors such as Sub-grade, Interest rate, and Loan\namount consistently emerge as important across all models, suggest-\ning that the LLMs are able to capture many of the same risk-relevant\nfactors identified by LightGBM. Notably, Gemma-2-9B assigns com-\nparatively higher SHAP values to these features, aligning with its\nsuperior classification performance among the evaluated LLMs.\n\nWhile the models converge on key features, their attribution\npatterns also reveal important differences. LightGBM places sub-\nstantial weight on well-established numerical predictors such as\nSub-grade and Annual income, whereas the LLMs distribute im-\nportance more broadly across behavioral and categorical variables.\nThis more diffuse attribution pattern implies that the LLMs may be\nleveraging latent feature interactions rather than relying solely on\ndirect numerical signals. Such behavior could reflect their ability to\nencode semantic relationships across variables that classical models\ndo not capture explicitly.\n\nA further notable observation is the reversal of SHAP relation-\nships for certain features between the models. For example, in\nLightGBM, higher Interest rate values are associated with a\ngreater predicted probability of being fully repaid (positive SHAP\nimpact), whereas all three LLMs display the opposite trend. Simi-\nlarly, DTI exhibits a different effect in Gemma-2-9B compared to the\nother LLMs and LightGBM. These discrepancies suggest that while\nthe LLMs extract informative patterns, they may rely on internally\nlearned representations that diverge from classical feature logic,\nwhich has implications for their reliability in regulated financial\ncontexts.\n\nFigure 6: SHAP Feature dependence plots and LLM self-\nexplanations for the feature Sub-grade.\n\n5.4 SHAP Feature Dependences and LLM\n\nSelf-Explanation\n\nWe compare classical model-centric explainability (SHAP depen-\ndence plots) with LLM self-reported explanations for two key fea-\ntures: DTI (Debt-to-Income ratio) and Sub-grade. LLM self-reported\ndependence is obtained by directly prompting the models on whether\neach feature exerts a positive, negative, or neutral effect on loan\nrepayment likelihood, and is shown at the top of each chart.\n\nDTI (Debt-to-Income ratio). All three LLMs self-report a nega-\ntive relationship between DTI and loan repayment likelihood, align-\ning with their SHAP dependence plots (Figure 5). LightGBM also\nshows a clear negative dependence. However, Gemma-2-9B displays\n\n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\na notable inconsistency: its SHAP values suggest a positive contri-\nbution at higher DTI levels, contradicting its own self-explanation.\n\nSub-grade. Both the LLM SHAP plots and self-reports consis-\ntently show Sub-grade as a feature negatively correlated with\ndefault risk (A1 being lowest risk and G5 highest) (Figure 6). Light-\nGBM exhibits a strong and nearly monotonic negative dependence,\nindicating heavy reliance on Sub-grade for risk discrimination.\nLLMs reproduce this general trend but with shallower slopes and\ngreater local variability.\n\nSummary of alignment. While LLM self-explanations often\nalign with their SHAP dependence patterns, there are notable di-\nvergences such as DTI, which was self-reported as negative but\nshowed positive SHAP impact at higher values. Such mismatches\nsuggest that LLM self-explanations, while often plausible, do not\nalways reflect their internal decision-making mechanisms. Unlike\nLightGBM, which captures purely statistical relationships from the\nstructured dataset, LLMs may incorporate latent financial priors\nbeyond the data. These inconsistencies reinforce the need for inde-\npendent audits before trusting LLM self-explanations in high-stakes\nfinancial workflows.\n\n6 Conclusion\nThe growing adoption of large language models (LLMs) for struc-\ntured classification raises critical questions about their validity and\nsafety in high-stakes financial decision-making. This study system-\natically compared zero-shot LLM classifiers with LightGBM on a\nstructured loan default prediction task, evaluating both predictive\nperformance and explainability through SHAP-based audits.\n\nOur findings reveal that while LLMs can capture several key\nstatistical patterns similar to LightGBM, they remain inferior in pre-\ndictive accuracy, with LightGBM achieving the highest ROC\u2013AUC\nand PR\u2013AUC scores. LLM-generated self-explanations occasionally\nalign with SHAP feature attributions, but observed discrepancies in-\ndicate that these rationales may rely on external priors rather than\npurely dataset-driven reasoning. Moreover, ensembling LLMs with\nLightGBM did not yield meaningful performance gains, suggesting\nlimited complementarity between the two paradigms.\n\nThese results highlight that while LightGBM remains the more\nreliable choice when skilled data scientists are available, LLMs\ncould serve as a practical fallback in small-data settings where\nfine-tuning is infeasible, provided their outputs are independently\naudited. Future research should explore fine-tuned LLMs for tabular\nmodeling, hybrid approaches that better integrate structured and\nunstructured reasoning, and establish robust reliability and fairness\nassessments to ensure their responsible deployment in financial\napplications.\n\nReferences\n[1] 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council of\n27 April 2016 on the protection of natural persons with regard to the processing\nof personal data and on the free movement of such data, and repealing Directive\n95/46/EC (General Data Protection Regulation). 88 pages. https://eur-lex.europa.\neu/eli/reg/2016/679/oj/eng\n\n[2] Josh Achiam et al. 2023. GPT-4 Technical Report. arXiv:2303.08774 (2023).\n[3] Dalila Boughaci and Abdullah A. K. Alkhawaldeh. ",
            "char_count": 35083,
            "word_count": 4955
          },
          {
            "chunk_id": 1,
            "text": "2020. Appropriate Machine\nLearning Techniques for Credit Scoring and Bankruptcy Prediction in Banking\nand Finance: A Comparative Study. Risk and Decision Analysis 8, 1-2 (2020),\n15\u201324.\n\n[4] T. B. Brown et al. ",
            "char_count": 209,
            "word_count": 32
          },
          {
            "chunk_id": 2,
            "text": "2020. Language Models are Few-Shot Learners. In Advances in\n\nNeural Information Processing Systems (NeurIPS), Vol. 33. 1877\u20131901.\n\n[5] Marcus Buckmann and Edward Hill. ",
            "char_count": 168,
            "word_count": 23
          },
          {
            "chunk_id": 3,
            "text": "2024. Logistic Regression makes small LLMs\nstrong and explainable \"tens-of-shot\" classifiers. arXiv preprint arXiv:2408.03414\n(2024).\n\n[6] Niklas Bussmann, Paolo Giudici, Dimitri Marinelli, and Jochen Papenbrock. ",
            "char_count": 213,
            "word_count": 25
          },
          {
            "chunk_id": 4,
            "text": "2021.\nExplainable Machine Learning in Credit Risk Management. Computational Eco-\nnomics 57, 1 (2021), 203\u2013216.\n\n[7] Jurgita \u010cernevi\u010dien\u02d9e and Audrius Kaba\u0161inskas. 2024. Explainable Artificial Intel-\nligence (XAI) in Finance: A Systematic Literature Review. Artificial Intelligence\nReview 57, 8 (2024), 216.\n\n[8] Canyu Chen et al. 2024. ClinicalBench: Can LLMs Beat Traditional ML Models\n\nin Clinical Prediction? arXiv preprint arXiv:2411.06469 (2024).\n\n[9] Tianqi Chen and Carlos Guestrin. ",
            "char_count": 490,
            "word_count": 66
          }
        ],
        "stats": {
          "mode": "Semantic",
          "num_chunks": 22,
          "avg_chars": 1878.3181818181818,
          "min_chars": 34,
          "max_chars": 35083,
          "avg_words": 263.3636363636364,
          "status": "success"
        },
        "time_sec": 3.4071712493896484
      },
      "proposition": {
        "chunks": [],
        "stats": {
          "mode": "Proposition",
          "num_chunks": 0,
          "avg_chars": 0,
          "min_chars": 0,
          "max_chars": 0,
          "avg_words": 0,
          "status": "not_available"
        },
        "time_sec": 0
      },
      "hybrid": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n1\n0\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nInterpreting LLMs as Credit Risk Classifiers: Do Their Feature\nExplanations Align with Classical ML?\n\nSaeed AlMarri\nKhalifa University\nAbu Dhabi, United Arab Emirates\n100061460@ku.ac.ae\n\nKristof Juhasz\nADIA\nAbu Dhabi, United Arab Emirates\nkristof.juhasz@adia.ae\n\nMathieu Ravaut\nADIA\nAbu Dhabi, United Arab Emirates\nmathieu.ravaut@adia.ae\n\nGautier Marti\nADIA\nAbu Dhabi, United Arab Emirates\ngautier.marti@adia.ae\n\nHamdan Al Ahbabi\nKhalifa University\nAbu Dhabi, United Arab Emirates\n100061346@ku.ac.ae\n\nIbrahim Elfadel\nKhalifa University\nAbu Dhabi, United Arab Emirates\nibrahim.elfadel@ku.ac.ae\n\nAbstract\nLarge Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification\ntasks through zero-shot prompting. However, their suitability for\nstructured tabular data remains underexplored, especially in high-\nstakes financial applications such as financial risk assessment. This\nstudy conducts a systematic comparison between zero-shot LLM-\nbased classifiers and LightGBM, a state-of-the-art gradient-boosting\nmodel, on a real-world loan default prediction task. We evaluate\ntheir predictive performance, analyze feature attributions using\nSHAP, and assess the reliability of LLM-generated self-explanations.\nWhile LLMs are able to identify key financial risk indicators, their\nfeature importance rankings diverge notably from LightGBM, and\ntheir self-explanations often fail to align with empirical SHAP at-\ntributions. These findings highlight the limitations of LLMs as\nstandalone models for structured financial risk prediction and raise\nconcerns about the trustworthiness of their self-generated expla-\nnations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-\nthe-loop oversight when deploying LLMs in risk-sensitive financial\nenvironments.\n\nCCS Concepts\n\u2022 Computing methodologies \u2192 Supervised learning by clas-\nsification.\n\nKeywords\nLarge Language Models, Explainable AI, SHAP, Credit Risk Pre-\ndiction, Responsible AI, Financial Machine Learning, Model Inter-\npretability\n\n1 Introduction\nLarge Language Models (LLMs), such as GPT-4, have demonstrated\nstrong performance across a range of natural language processing\n(NLP) tasks, including classification and reasoning [2, 4, 32]. Their\nability to function as classifiers without explicit training pipelines,\nrelying solely on a few-shot or zero-shot prompting, has gained\nsignificant attention. This raises fundamental questions about the\nreliability and validity of LLM-based classification, particularly in\n\ncomparison to classical machine learning models such as gradient-\nboosting decision trees methods like XGBoost [9] or LightGBM\n[19].\n\nTraditional classification tasks require structured pipelines in-\nvolving feature engineering, model training, validation, and hyper-\nparameter tuning. Fine-tuning models on tabular data, in particular,\ndemands expertise in preprocessing, GPU management, and bal-\nancing class distributions to prevent trivial solutions. In contrast,\nLLMs bypass fine-tuning entirely, requiring only natural language\nprompting. This reduces technical barriers, making them accessible\nto non-experts, but raises an important question: How do LLM-based\nclassifiers arrive at their predictions, and do they rely on decision pat-\nterns similar to classical machine learning models?\n\nThis question is especially critical in high-stakes financial do-\nmains, where algorithmic risk assessments directly affect credit\naccess, interest rates, and regulatory compliance [11]. Financial\ninstitutions operate under strict governance frameworks such as\nBasel III [24] and GDPR [1], where opaque models can lead to regu-\nlatory breaches, reputational damage, and unfair or discriminatory\ndecisions. Unlike decision trees or gradient boosting models, LLMs\nare complex black-box models with billions of parameters, making\ninterpretability a key challenge. This has led to growing interest in\nExplainable AI (XAI) techniques to analyze LLMs\u2019 internal logic and\nassess their alignment with human-interpretable decision patterns.\nIn this study, we conduct a systematic evaluation of zero-shot\nLLM classifiers for structured credit risk prediction, directly compar-\ning them with a well-established interpretable baseline (LightGBM).\nBeyond performance comparison, our primary goal is to audit their\nexplainability to determine whether their feature attributions and\nself-generated rationales align with dataset-driven reasoning or\nrely on external priors. We employ Shapley Additive Explanations\n(SHAP) [20] to analyze the faithfulness of their decision patterns.\nLeveraging a public loan default prediction dataset, we address the\nfollowing research questions:\n\n\u2022 Explainability: Do LLM-based classifiers prioritize the same\n\nfeatures as classical models?\n\n\u2022 Self-Explainability: Can LLMs provide self-generated ratio-\n\nnales that align with SHAP-derived feature attributions?\n\n\u2022 Performance: How do LLMs compare to classical machine learn-\ning classifiers (ROC-AUC, PR-AUC)? Does ensembling both types\nof models improve performance?\n\n \n \n \n \n \n \n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\nOur work directly addresses the workshop call on AI safety,\nfairness, and explainability in high-stakes financial environments,\nand responsible deployment in fintech and banking, by auditing the\nfaithfulness of LLM explanations against SHAP on a real credit-risk\ntask. The remainder of this paper is structured as follows: Section 2\nreviews related work, Section 3 details the methodology, Section 4\npresents the experimental setup, Section 5 analyzes results, feature\nattribution and reasoning patterns, and Section 6 concludes with\nkey findings and future directions.\n\n2 Related Work\n2.1 LLMs for Zero-Shot Classification\nLLMs have demonstrated strong classification capabilities, often\nachieving competitive performance without supervision. Brown\net al. [4] introduced GPT-3, highlighting its few-shot and zero-\nshot classification potential. This paradigm removes the need for\nclassical training pipelines, enabling non-experts to perform clas-\nsification via direct natural-language prompting and in-context\nlearning. Subsequent work has explored structured prompting to\nenhance classification accuracy. Hao et al. [16] introduced Chain-\nof-Thought (CoT) prompting, showing that structured reasoning\ncan improve LLM performance\u2014relevant when adapting them to\nstructured data.\n\nA recent line of work examines whether LLMs can function as\nregressors for numerical data. Vacareanu et al. [31] found that LLMs\napproximate regression functions with in-context examples, while\nBuckmann and Hill [5] proposed combining LLMs with logistic\nregression for low-data robustness. Song et al. [28] and Song and\nBahri [27] further extend this research direction, demonstrating\nuniversal regression capabilities using decoding-based inference.\n\nOur Contribution. Unlike prior studies that evaluate LLM clas-\nsification in isolation, our work positions this comparison as a step\ntoward responsible AI deployment in high-stakes financial contexts.\nBy systematically comparing zero-shot LLM and LightGBM on the\nsame dataset and auditing their decision patterns using SHAP-based\nexplainability, we assess not only accuracy but also faithfulness\nand reliability of model reasoning critical elements for trustworthy,\nfair and transparent AI use in credit risk assessment.\n\n2.2 LLM Explainability\nFeature-attribution methods such as SHAP [20] are widely used\nto assess feature importance in machine-learning models. We use\nSHAP to compare LLM-based probabilistic classifiers with Light-\nGBM. A key question is whether LLMs\u2019 self-explanations align\nwith actual feature importance. Huang et al. [18] report that LLM\nrationales are often plausible but do not necessarily reflect inter-\nnal reasoning. Dehghanighobadi et al. [10] analyze counterfactual\nexplanations and show that LLMs can struggle with causal de-\npendencies. Sarkar [25] argues that LLMs lack self-explanatory\ncapabilities due to opaque training dynamics, and Turpin et al. [30]\nshow that CoT-generated explanations can be misleading.\n\nOur Contribution. Prior work predominantly studies LLM self-\nexplanations in text tasks. To our knowledge, our study is the first\nto compute SHAP-based feature importance for LLMs prompted\nto output probabilistic predictions on structured financial data,\n\nenabling a direct faithfulness audit of self-explanations against\nempirical attributions.\n\n2.3 LLMs for Tabular Data\nRecent work explores whether LLMs can replace gradient-boosted\nmodels such as XGBoost, LightGBM, and AdaBoost for tabular clas-\nsification. Fang et al. [13] survey LLMs on tabular data and highlight\nadaptation challenges. Ghaffarzadeh-Esfahani et al. [15] benchmark\nLLMs against classical ML for COVID-19 mortality prediction, con-\ncluding that classical ML models outperform LLMs on structured\ndata. Chen et al. [8] introduce ClinicalBench and similarly find XG-\nBoost superior for clinical prediction tasks. Hegselmann et al. [17]\npropose TabLLM, which reformulates tables as natural language\nfor few-shot classification, while Shi et al. [26] introduce ZET-LLM,\ntreating autoregressive LLMs as feature-embedding models for tab-\nular prediction. While these studies highlight LLM potential, they\ngenerally do not evaluate explainability or faithfulness of rationales\non tabular tasks.\n\nOur Contribution. We conduct a head-to-head comparison of\nLLMs and LightGBM on the same structured dataset and integrate\nSHAP-based explainability, offering a dual analysis of predictive\nperformance and feature attribution to illuminate decision mecha-\nnisms.\n\n2.4 Classical ML, XAI, and LLMs for Financial\n\nAI\n\nExplainability is crucial in financial applications for risk assessment.\nMartins et al. [22] review XAI in finance, while \u010cernevi\u010dien\u02d9e and\nKaba\u0161inskas [7], Misheva et al. [23], and Bussmann et al. [6] analyze\nexplainability in credit-risk modeling. Several studies benchmark\nML models for loan-default prediction: Madaan et al. [21] compare\ndecision trees and random forests without an explainability analysis;\nSrinivasa Rao et al. [29] assess ML techniques for loan risk but do\nnot explore LLMs; and Boughaci and Alkhawaldeh [3] study credit-\nscoring models without evaluating LLM-based predictions.\n\nOur Contribution. While prior work focuses on classical ML\nfor loan prediction, we present the first comparative analysis of LLMs\nand LightGBM on structured loan data, integrating SHAP-based ex-\nplainability. Our findings extend beyond credit risk to broader finan-\ncial applications, including fraud detection, regulatory compliance,\nand algorithmic trading decision-making, where explainability is\nkey to adoption.\n\n3 Methodology\n3.1 Inference Setup\nWe systematically evaluate the predictive performance and explain-\nability of LightGBM and zero-shot LLMs. We design the classifi-\ncation problem such that both LightGBM and LLMs receive the\nsame set of input features and output probability estimates in-\nstead of discrete classes. Probability outputs offer three advantages:\n(i) fine-grained evaluation via discrimination metrics (ROC-AUC,\nPR-AUC); (ii) enhanced explainability, as SHAP feature attribution\nis generally more informative when applied to probability scores\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\nWe use SHAP for post hoc explanations [20], specifically the\nmodel-agnostic PermutationExplainer. We adopt this efficient\nSHAP estimator because our prediction function is an LLM infer-\nence, which is costly. To balance accuracy and runtime, we sample\n250 instances from each dataset for explanation. We construct the\nbackground (masker) via \ud835\udc58-means clustering with \ud835\udc36 = 5 centroids\n(using shap.kmeans) and set the max_evals budget so that the\nexplainer executes exactly \ud835\udc47 = 4 permutations in our experiments.\n\nApproximate cost (model calls). Let \ud835\udc3e be the number of in-\nstances explained, \ud835\udc40 the number of features, \ud835\udc35 the number of\nbackground draws per masked evaluation (here \ud835\udc35 = \ud835\udc36 = 5), and \ud835\udc47\nthe number of random permutations. The PermutationExplainer\nrequires approximately:\n\n#calls \u2248 \ud835\udc3e \u00d7 \ud835\udc47 \u00d7 (\ud835\udc40 + 1) \u00d7 \ud835\udc35 = O (\ud835\udc3e \ud835\udc47 \ud835\udc40 \ud835\udc35)\n(1)\nmodel evaluations. In SHAP\u2019s implementation, \ud835\udc47 is governed by\n\nmax_evals via the practical rule:\n\nFigure 1: Comparative Explainable AI Framework: Classical ML vs.\nLLMs. The dataset is processed through two paradigms: (i) a struc-\ntured LightGBM model and (ii) a zero-shot LLM using natural lan-\nguage prompts. Both generate probability predictions, analyzed in-\ndividually and in an ensemble. Explainability is assessed via SHAP\n(for both) and LLM self-explanations, evaluating their alignment.\n\nrather than hard labels; and (iii) a direct test of LLMs\u2019 capability as\nprobability regressors.\n\n3.2 Explainability\nWe treat explainability as a model-auditing task focused on the\nfaithfulness of the factors a model claims or appears to use. Our\naudit has two complementary components:\n\u2022 (A) SHAP as a model-agnostic audit baseline. We use SHAP\n[20] to assign contribution values to each feature for both Light-\nGBM and the LLM-based probabilistic classifier. SHAP values\noperationalize which features, and in what direction, are driving\neach model\u2019s predictions.\n\n\u2022 (B) LLM self-explanations as unverified rationales. In ad-\ndition to SHAP, we prompt the LLM to provide instance-level\nrationales and feature-level directional judgments (positive/neg-\native/neutral). These self-explanations are treated as claims that\nmust be checked against the SHAP audit checks.\n\nWe compare (i) global feature importance patterns (via SHAP)\nacross models; (ii) directional dependence for key features (e.g.,\nwhether higher values increase or decrease repayment probabil-\nity); and (iii) instance-level coherence between an LLM\u2019s rationale\nand the corresponding SHAP attributions. Misalignment across\nthese checks is interpreted as a faithfulness risk and a caution for\ndeployment.\n\nScaling Shapley-Value Inference for LLMs. To operationalize\n3.2.1\nexplainability as an audit of model decision logic, we estimate\nfeature attributions for both LightGBM and the zero-shot LLM\nclassifiers using SHAP. Faithful auditing requires identifying which\nfeatures actually drive predictions, not just producing plausible\nexplanations.\n\n(cid:107)\n\n\ud835\udc47 \u2248\n\n(cid:106) max_evals\n2\ud835\udc40\ni.e., roughly 2\ud835\udc40 masked evaluations per permutation path. With\nmax_evals = 200 and \ud835\udc40 = 21, this yields \ud835\udc47 = \u230a200/(2 \u00d7 21)\u230b = 4.\nUsing \ud835\udc35 = 5, the per-instance cost is therefore \u2248 4\u00d7(21+1)\u00d75 = 440\nmodel calls.\n\n(2)\n\nWhy is it more efficient than KernelExplainer? With a sum-\nmarized background of \ud835\udc36 centroids, the dominant model-call com-\nplexity of KernelExplainer scales as:\n\n#calls \u2248 \ud835\udc3e \u00d7 \ud835\udc36 \u00d7 \ud835\udc40 2 = O (\ud835\udc3e \ud835\udc36 \ud835\udc40 2)\n\n(3)\n\ndue to sampling coalitions and fitting a kernel-weighted regres-\nsion. In our setting (\ud835\udc36 = 5, \ud835\udc40 = 21) this is \u2248 5 \u00d7 212 = 2205 eval-\nuations per instance. By contrast, PermutationExplainer scales\nlinearly in \ud835\udc40 and avoids the regression solve, yielding an expected\nper-instance reduction of\n\nspeedup \u2248\n\n\ud835\udc36 \ud835\udc40 2\n\ud835\udc47 (\ud835\udc40 + 1) \ud835\udc35\n\n\u2248\n\n2205\n440\n\n\u2248 5\u00d7\n\n(4)\n\nThe fivefold decrease in model calls translates into substantially\nlower LLM inference time while maintaining faithful attributions,\nwhich is why we use PermutationExplainer with \ud835\udc47 = 4.\n\n3.2.2 LLM Self-Explanations. Motivated by the emerging reason-\ning capabilities of large language models (LLMs) [33], we use the\nLLM as an explainability tool alongside SHAP. Specifically, for each\ninput feature, we provide its description to the LLM and prompt it\nto predict whether the feature is likely to have a positive, negative,\nor no effect on the predicted outcome. The LLM also generates a\nbrief textual justification for each directional prediction, which we\nrefer to as its self-explanation.\n\nThese LLM-generated self-explanations are treated as unveri-\nfied rationales and are not assumed to reflect the model\u2019s actual\ninternal reasoning. We compare them against SHAP-based feature\nattributions, which serve as a model-agnostic baseline. When LLM\nexplanations diverge from SHAP attributions, we interpret this\nmisalignment as a potential risk of explainability, an indication\n\n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\nthat LLM may produce externally plausible but internally incon-\nsistent reasoning. While we do not perform formal risk scoring,\nhighlighting such discrepancies can inform governance decisions\nin high-stakes financial applications, where model transparency is\nessential.\n\nFigure 1 illustrates the overall methodological framework. The\ndataset is processed through two parallel pipelines: (i) a structured\nLightGBM model and (ii) a zero-shot LLM using natural language\nprompts. Both produce probability estimates, which are then ana-\nlyzed for predictive performance and explainability through SHAP\nand LLM self-explanations, enabling a cross-comparison of their\ndecision logic.\n\n4 Experiments\n4.1 Data\n4.1.1 Task Description. Loan default prediction is a critical chal-\nlenge in credit risk assessment, where lenders estimate the likeli-\nhood of borrowers failing to repay their loans. Accurate predictions\nenable financial institutions to make informed lending decisions,\nset appropriate interest rates, and manage credit risk effectively.\nBecause loan defaults directly affect credit access, financial stability,\nand regulatory compliance, this task is widely regarded as a high-\nstakes benchmark for testing the safety and reliability of predictive\nmodels in finance.\n\n4.1.2 Dataset Description. We use LendingClub\u2019s publicly avail-\nable loan records hosted by Kaggle 1, which were disclosed as\npart of the company\u2019s regulatory filings with the U.S. Securities\nand Exchange Commission (SEC). As a major peer-to-peer lend-\ning platform, LendingClub was required to provide detailed loan\nissuance and performance data to comply with SEC regulations,\nmaking this dataset a widely used benchmark in credit risk mod-\neling. The dataset includes loan applications issued between 2007\nand 2016, with approximately 396,000 loan records. Each loan is\nlabeled with its repayment status, distinguishing between Fully\nPaid and Charged Off (defaulted) loans. It contains both numerical\nand categorical attributes describing borrower creditworthiness\nand loan characteristics, for a total of 26 financial and credit-related\nfeatures.\n\n4.1.3 Preprocessing. We removed five features due to high cardi-\nnality, redundancy, or potential data leakage:\n\n\u2022 issue_d: Loan issue date (introduces temporal bias)\n\u2022 earliest_cr_line: Borrower\u2019s earliest credit line (high cardi-\n\nnality)\n\n\u2022 address: High-cardinality feature (introduces geographic bias)\n\u2022 emp_title: High-cardinality categorical variable\n\u2022 title: High-cardinality; redundant with the purpose variable\n\nWe also excluded categorical features with more than 40 unique\ncategories to mitigate overfitting in LightGBM. This ensures a fair\ncomparison between models, preventing disadvantages for Light-\nGBM (which lacks natural text processing) and restricting LLMs\nfrom exploiting external knowledge, such as macroeconomic trends\nfrom loan dates or geographic signals from addresses. These steps\n\n1https://www.kaggle.com/datasets/sndpred/loan-data\n\nTable 1: Final Dataset Features.\n\nFeature Description\n\nRange\n\nLoan amount\nTerm\nInterest rate\nInstallment\nGrade\nSub-grade\nEmployment length\n\nHome ownership\n\nAnnual income\nVerification status\n\nPurpose\nDebt-to-income (DTI)\nratio\nOpen credit accounts\nPublic records\nRevolving balance\nRevolving utilization\nrate\nTotal accounts\nInitial listing\nstatus\nApplication type\n\nMortgage accounts\nPublic record\nbankruptcies\n\n[1600.0, 35000.0]\ncategorical: {36 months, 60 months}\n[6.03, 25.29]\n[55.32, 1204.57]\ncategorical: {A, B, C, D, E, F, G}\ncategorical: {A1, A2, . . . , G4, G5}\ncategorical: {<1 year, 1 year, . . . , 10+\nyears}\ncategorical: {MORTGAGE, NONE,\nOTHER, OWN, RENT}\n[19000.0, 250000.0]\ncategorical: {Not Verified, Source\nVerified, Verified}\ncategorical: 14 values (e.g., wedding)\n[1.6, 36.41]\n\n[3.0, 27.0]\n[0.0, 2.0]\n[169.05, 83505.9]\n[1.2, 98.0]\n\n[6.0, 60.0]\ncategorical: {f, w}\n\ncategorical: {DIRECT PAY,\nINDIVIDUAL, JOINT}\n[0.0, 9.0]\n[0.0, 1.0]\n\nhelp control for potential bias and improve the fairness and au-\nditability of the evaluation.\n\nFinal Dataset. After preprocessing, the final dataset consists\n4.1.4\nof 396,000 rows and 21 predictors (12 numerical and 9 categorical).\nThe data was randomly split into training (80%) and testing (20%),\nwith 79,200 instances used for LLM inference. This controlled setup\nensures that both models operate on identical structured inputs\nwithout access to external priors, supporting a transparent and\nauditable comparison.\n\nIn Table 1, we list all features. For numerical features, we report\nthe interval bounded by the 1st and 99th percentiles. For categorical\nfeatures, we report the values space (if it is not too large).\n\n4.2 Models\nLightGBM Training. We trained a LightGBM classifier using\n4.2.1\nthe gradient boosting decision tree (GBDT) algorithm with a binary\nobjective and AUC as the primary evaluation metric. The model\nwas trained with a learning rate of 0.01 and up to 10,000 estima-\ntors, applying early stopping after 100 rounds based on validation\nperformance. To reduce overfitting, we applied a feature fraction\nof 0.8, bagging fraction of 0.8, and L1/L2 regularization (0.1 each).\nThe num_leaves parameter was set to 63 and min_data_in_leaf\nto 50.\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\nInstance-Level Prompt Template\n\nPredict whether a loan application will be \"Fully Paid\" or \"Charged\nOff\" based on the borrower\u2019s information. Use the features provided\nbelow to assess the likelihood of the loan being fully repaid.\nLoan Application Details:\n\n<feature_1 name>: <feature_1 value>\n...\n<feature_N name>: <feature_N value>\n\nProvide your estimated probability of the loan being \"Fully Paid\". Also\nprovide a brief explanation of this estimate based on the features. Your\nanswer should only contain the probability estimate and the explanation\nin the following JSON format.\n\n{\n\n}\n\n\"Estimated Fully Paid Probability\": <float value\n\nbetween 0 and 1>,\n\n\"Explanation\": <string value>\n\nFeature-Level Prompt Template\n\nYou are working on predicting whether a loan application will be \"Fully\nPaid\" or \"Charged Off\" based on the borrower\u2019s information. One of the\nfeatures is the following:\n\n<feature name>\n\nDo you think that this feature will impact the application positively,\nnegatively, or have no impact? Provide your answer as one of the three\nstrings: positive | negative | neutral. Use the following JSON\nformat:\n\n{\n\n}\n\n\"Feature impact\": <positive | negative | neutral>\n\nWe use LightGBM as a transparent and interpretable baseline,\nproviding a benchmark for feature importance and prediction sta-\nbility against which the behavior of large language models (LLMs)\ncan be audited.\n\n4.2.2 LLM Inference. We evaluated three recent open-source in-\nstruction tuned LLMs of comparable size: LLaMA-3.1-8B-Instruct\n[12], Gemma-2-9B-Instruct [14], and Qwen-2.5-7B-Instruct\n[35]. These models were pre-trained on 15.6T, 8T, and 18T tokens,\nrespectively. All experiments used their instruction-tuned versions.\nModel weights were downloaded from the Hugging Face Hub [34],\nand inference was run locally using vLLM2 on four Nvidia A10G\n24GB GPUs.\n\nTo reduce computation time, we randomly sampled 250 test\ninstances for SHAP value estimation. We did not perform any fine-\ntuning and used strict zero-shot inference without any in-context\nlearning. This design ensures the LLMs rely solely on the provided\n\n2https://github.com/vllm-project/vllm\n\nFigure 2: ROC and Precision-Recall curves comparing the\nperformance of zero-shot LLMs and LightGBM on the loan\nclassification task. LightGBM consistently outperforms indi-\nvidual LLMs with Gemma-2-9b showing the most promising\nresult out of the LLMs.\n\nstructured features, preventing data contamination or leakage from\npre-training.\n\nWe used the same prompt templates for all LLMs. The instance-\nlevel prompt asked the model to jointly predict the probability that\na loan would be fully repaid (a float between 0 and 1) and to provide\na brief explanation. The feature-level prompt asked whether each\nfeature would impact the prediction positively, negatively, or not\nat all.\n\nUnlike free-form textual descriptions, the structured dictionary\nformat used for instance-level prompts is unlikely to have appeared\nin the LLMs\u2019 pretraining corpus, which reduces the risk of data\ncontamination.\n\n5 Analysis\n5.1 Performance Results\nTable 2 and Figure 2 compare the performance of zero-shot LLMs\nand LightGBM on the loan repayment prediction task. LightGBM\nachieved the highest ROC\u2013AUC (0.73), outperforming all LLMs in\nthe zero-shot setting. Among LLMs, Gemma-2-9B performed best\n(0.67), followed by LLaMA-3.1-8B (0.65) and Qwen-2.5-7B (0.61).\nThese findings are consistent with prior evidence that gradient\nboosting often surpasses deep learning methods on structured tab-\nular data. The LightGBM\u2013Gemma-2-9B ensemble achieved 0.70\nROC\u2013AUC, indicating no diversification benefit over LightGBM\nalone.\n\nPR\u2013AUC results follow a similar pattern. LightGBM obtained\nthe highest PR\u2013AUC (0.91), exceeding all LLMs. Gemma-2-9B again\nranked highest among the LLMs (0.88), followed by LLaMA-3.1-8B\n(0.86) and Qwen-2.5-7B (0.85). The ensemble model reached 0.90,\nclosely trailing LightGBM. All models outperformed the base-rate\nPR\u2013AUC (0.80), confirming meaningful predictive signal.\n\nOverall, these results show that while zero-shot LLMs achieve\nreasonable predictive performance, they remain inferior to a well-\ntuned LightGBM model on structured financial data, underscoring\nthe need for careful governance if deployed in high-stakes settings.\n\n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\nTable 2: Performance Metrics of Models for Loan Default\nClassification\n\nModel\n\nROC-AUC PR-AUC\n\nLightGBM\nGemma-2-9B\nLlama-3.1-8B\nQwen-2.5-7B\nEnsemble (LightGBM + Gemma-2-9B)\n\nRandom Classifier (Baseline)\nBase Rate for Fully Paid Label (Baseline)\n\n0.73\n0.67\n0.65\n0.61\n0.70\n\n0.50\n\u2013\n\n0.91\n0.88\n0.86\n0.85\n0.90\n\n\u2013\n0.80\n\nTable 3: Feature Importance Comparison\n\nFeature\n\nLGBM Gemma-\n\nSub-grade\nAnnual income\nTerm\nInterest rate\nDTI\nOpen account\nRevolving util\nLoan amount\nHome ownership\nGrade\n\n0.062\n0.026\n0.023\n0.022\n0.019\n0.018\n0.018\n0.018\n0.015\n0.013\n\n2-9B\n\n0.046\n0.031\n0.002\n0.019\n0.019\n0.004\n0.044\n0.006\n0.012\n0.079\n\nLlama-\n3.1-8B\n\nQwen-\n2.5-7B\n\n0.038\n0.037\n0.008\n0.048\n0.027\n0.007\n0.021\n0.026\n0.031\n0.065\n\n0.004\n0.014\n0.000\n0.005\n0.006\n0.001\n0.005\n0.004\n0.003\n0.025\n\n5.2 SHAP Feature Importance Comparison\nFigure 3 presents the SHAP feature importance rankings for Light-\nGBM and the three LLMs, providing insight into the key factors\ninfluencing loan classification decisions across models. The corre-\nsponding numerical values are reported in Table 3.\n\nA primary observation is the strong overlap in the top-ranked\nfeatures across all models, despite the LLMs operating in a zero-shot\nsetting. Features such as Sub-grade, Grade, Annual income, and\nInterest rate consistently emerge as dominant predictors. This\nsuggests that LLMs, even without task-specific fine-tuning, are able\nto extract and prioritize meaningful financial attributes in a manner\nbroadly consistent with classical ML models like LightGBM.\n\nHowever, notable differences emerge in feature weighting and\nrank order. LightGBM assigns greater relative importance to struc-\ntured numerical features such as Sub-grade, Annual income, and\nLoan amount, reflecting its reliance on directly interpretable numer-\nical signals. In contrast, the LLMs particularly LLaMA-3.1-8B and\nGemma-2-9B distribute their importance more evenly across cate-\ngorical and behavioral attributes such as Verification status,\nPurpose, and Home ownership. This indicates that LLMs may be\nleveraging latent semantic relationships within categorical features\nthat are not explicitly modeled by LightGBM.\n\n5.3 SHAP Comparative Analysis\nTo further investigate the decision mechanisms of the models, we\ncompare the SHAP summary plots of LightGBM and the three LLMs.\n\nFigure 3: SHAP feature importance comparison between\nLightGBM and LLMs. Despite being in a zero-shot setting,\nLLMs identify a remarkably similar set of key financial fea-\ntures as LightGBM, though with differences in feature weight-\ning and distribution.\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\nFigure 4: SHAP summary plots comparing feature importance dis-\ntributions for LLMs and LightGBM. LightGBM shows a more struc-\ntured reliance on key financial indicators, while LLMs exhibit more\ndispersed and lower-magnitude SHAP values, indicating weaker fea-\nture dependencies.\n\nFigure 5: SHAP Feature dependence plots and LLM self-\nexplanations for the feature DTI.\n\nIn Figure 4, these plots illustrate how variations in feature values\ninfluence predicted loan repayment probabilities.\n\nDespite operating in a zero-shot setting, the LLMs successfully\nextract meaningful relationships from structured financial features.\nCore predictors such as Sub-grade, Interest rate, and Loan\namount consistently emerge as important across all models, suggest-\ning that the LLMs are able to capture many of the same risk-relevant\nfactors identified by LightGBM. Notably, Gemma-2-9B assigns com-\nparatively higher SHAP values to these features, aligning with its\nsuperior classification performance among the evaluated LLMs.\n\nWhile the models converge on key features, their attribution\npatterns also reveal important differences. LightGBM places sub-\nstantial weight on well-established numerical predictors such as\nSub-grade and Annual income, whereas the LLMs distribute im-\nportance more broadly across behavioral and categorical variables.\nThis more diffuse attribution pattern implies that the LLMs may be\nleveraging latent feature interactions rather than relying solely on\ndirect numerical signals. Such behavior could reflect their ability to\nencode semantic relationships across variables that classical models\ndo not capture explicitly.\n\nA further notable observation is the reversal of SHAP relation-\nships for certain features between the models. For example, in\nLightGBM, higher Interest rate values are associated with a\ngreater predicted probability of being fully repaid (positive SHAP\nimpact), whereas all three LLMs display the opposite trend. Simi-\nlarly, DTI exhibits a different effect in Gemma-2-9B compared to the\nother LLMs and LightGBM. These discrepancies suggest that while\nthe LLMs extract informative patterns, they may rely on internally\nlearned representations that diverge from classical feature logic,\nwhich has implications for their reliability in regulated financial\ncontexts.\n\nFigure 6: SHAP Feature dependence plots and LLM self-\nexplanations for the feature Sub-grade.\n\n5.4 SHAP Feature Dependences and LLM\n\nSelf-Explanation\n\nWe compare classical model-centric explainability (SHAP depen-\ndence plots) with LLM self-reported explanations for two key fea-\ntures: DTI (Debt-to-Income ratio) and Sub-grade. LLM self-reported\ndependence is obtained by directly prompting the models on whether\neach feature exerts a positive, negative, or neutral effect on loan\nrepayment likelihood, and is shown at the top of each chart.\n\nDTI (Debt-to-Income ratio). All three LLMs self-report a nega-\ntive relationship between DTI and loan repayment likelihood, align-\ning with their SHAP dependence plots (Figure 5). LightGBM also\nshows a clear negative dependence. However, Gemma-2-9B displays\n\n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\na notable inconsistency: its SHAP values suggest a positive contri-\nbution at higher DTI levels, contradicting its own self-explanation.\n\nSub-grade. Both the LLM SHAP plots and self-reports consis-\ntently show Sub-grade as a feature negatively correlated with\ndefault risk (A1 being lowest risk and G5 highest) (Figure 6). Light-\nGBM exhibits a strong and nearly monotonic negative dependence,\nindicating heavy reliance on Sub-grade for risk discrimination.\nLLMs reproduce this general trend but with shallower slopes and\ngreater local variability.\n\nSummary of alignment. While LLM self-explanations often\nalign with their SHAP dependence patterns, there are notable di-\nvergences such as DTI, which was self-reported as negative but\nshowed positive SHAP impact at higher values. Such mismatches\nsuggest that LLM self-explanations, while often plausible, do not\nalways reflect their internal decision-making mechanisms. Unlike\nLightGBM, which captures purely statistical relationships from the\nstructured dataset, LLMs may incorporate latent financial priors\nbeyond the data. These inconsistencies reinforce the need for inde-\npendent audits before trusting LLM self-explanations in high-stakes\nfinancial workflows.\n\n6 Conclusion\nThe growing adoption of large language models (LLMs) for struc-\ntured classification raises critical questions about their validity and\nsafety in high-stakes financial decision-making. This study system-\natically compared zero-shot LLM classifiers with LightGBM on a\nstructured loan default prediction task, evaluating both predictive\nperformance and explainability through SHAP-based audits.\n\nOur findings reveal that while LLMs can capture several key\nstatistical patterns similar to LightGBM, they remain inferior in pre-\ndictive accuracy, with LightGBM achieving the highest ROC\u2013AUC\nand PR\u2013AUC scores. LLM-generated self-explanations occasionally\nalign with SHAP feature attributions, but observed discrepancies in-\ndicate that these rationales may rely on external priors rather than\npurely dataset-driven reasoning. Moreover, ensembling LLMs with\nLightGBM did not yield meaningful performance gains, suggesting\nlimited complementarity between the two paradigms.\n\nThese results highlight that while LightGBM remains the more\nreliable choice when skilled data scientists are available, LLMs\ncould serve as a practical fallback in small-data settings where\nfine-tuning is infeasible, provided their outputs are independently\naudited. Future research should explore fine-tuned LLMs for tabular\nmodeling, hybrid approaches that better integrate structured and\nunstructured reasoning, and establish robust reliability and fairness\nassessments to ensure their responsible deployment in financial\napplications.\n\nReferences\n[1] 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council of\n27 April 2016 on the protection of natural persons with regard to the processing\nof personal data and on the free movement of such data, and repealing Directive\n95/46/EC (General Data Protection Regulation). 88 pages. https://eur-lex.europa.\neu/eli/reg/2016/679/oj/eng\n\n[2] Josh Achiam et al. 2023. GPT-4 Technical Report. arXiv:2303.08774 (2023).\n[3] Dalila Boughaci and Abdullah A. K. Alkhawaldeh. ",
            "char_count": 35083,
            "word_count": 4955
          },
          {
            "chunk_id": 1,
            "text": "2020. Appropriate Machine\nLearning Techniques for Credit Scoring and Bankruptcy Prediction in Banking\nand Finance: A Comparative Study. Risk and Decision Analysis 8, 1-2 (2020),\n15\u201324.\n\n[4] T. B. Brown et al. ",
            "char_count": 209,
            "word_count": 32
          },
          {
            "chunk_id": 2,
            "text": "2020. Language Models are Few-Shot Learners. In Advances in\n\nNeural Information Processing Systems (NeurIPS), Vol. 33. 1877\u20131901.\n\n[5] Marcus Buckmann and Edward Hill. ",
            "char_count": 168,
            "word_count": 23
          },
          {
            "chunk_id": 3,
            "text": "2024. Logistic Regression makes small LLMs\nstrong and explainable \"tens-of-shot\" classifiers. arXiv preprint arXiv:2408.03414\n(2024).\n\n[6] Niklas Bussmann, Paolo Giudici, Dimitri Marinelli, and Jochen Papenbrock. ",
            "char_count": 213,
            "word_count": 25
          },
          {
            "chunk_id": 4,
            "text": "2021.\nExplainable Machine Learning in Credit Risk Management. Computational Eco-\nnomics 57, 1 (2021), 203\u2013216.\n\n[7] Jurgita \u010cernevi\u010dien\u02d9e and Audrius Kaba\u0161inskas. 2024. Explainable Artificial Intel-\nligence (XAI) in Finance: A Systematic Literature Review. Artificial Intelligence\nReview 57, 8 (2024), 216.\n\n[8] Canyu Chen et al. 2024. ClinicalBench: Can LLMs Beat Traditional ML Models\n\nin Clinical Prediction? arXiv preprint arXiv:2411.06469 (2024).\n\n[9] Tianqi Chen and Carlos Guestrin. ",
            "char_count": 490,
            "word_count": 66
          }
        ],
        "stats": {
          "mode": "Hybrid",
          "num_chunks": 22,
          "avg_chars": 1878.3181818181818,
          "min_chars": 34,
          "max_chars": 35083,
          "avg_words": 263.3636363636364,
          "status": "success"
        },
        "time_sec": 0.3839857578277588
      }
    }
  },
  {
    "paper_id": "2510.25704v1",
    "original_length": 107895,
    "modes": {
      "simple": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "Prepared for submission to JHEP\n\nScaling flow-based approaches for topology sampling\nin SU(3) gauge theory\n\n5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nt\na\nl\n-\np\ne\nh\n[\n\n1\nv\n4\n0\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na",
            "char_count": 182,
            "word_count": 51
          },
          {
            "chunk_id": 1,
            "text": "Claudio Bonanno,a Andrea Bulgarelli,b,c Elia Cellini,b Alessandro Nada,b Dario\nPanfalone,b Davide Vadacchino,d Lorenzo Verzichellib\n\naInstituto de F\u00b4\u0131sica Te\u00b4orica UAM-CSIC, c/ Nicol\u00b4as Cabrera 13-15, Universidad Aut\u00b4onoma de\nMadrid, Cantoblanco, E-28049 Madrid, Spain\nbDipartimento di Fisica,",
            "char_count": 293,
            "word_count": 34
          },
          {
            "chunk_id": 2,
            "text": "Universit\u00b4a degli Studi di Torino and INFN, Sezione di Torino, Via Pietro\nGiuria 1, I-10125 Turin, Italy\ncTransdisciplinary Research Area \u201cBuilding Blocks of Matter and Fundamental Interactions\u201d (TRA\nMatter) and Helmholtz Institute for Radiation and Nuclear Physics (HISKP), University of Bonn,\nNussallee 14-16, 53115 Bonn, Germany\ncCentre for Mathematical Sciences, University of Plymouth, Plymouth, PL4 8AA, United Kingdom",
            "char_count": 424,
            "word_count": 58
          },
          {
            "chunk_id": 3,
            "text": "E-mail: claudio.bonanno@csic.es, andrea.bulgarelli@unito.it,\nelia.cellini@unito.it, alessandro.nada@unito.it,\ndario.panfalone@unito.it, davide.vadacchino@plymouth.ac.uk,\nlorenzo.verzichelli@unito.it\n\nAbstract: We develop a methodology based on out-of-equilibrium simulations to miti-\ngate topological freezing when approaching the continuum limit of lattice gauge theories.",
            "char_count": 373,
            "word_count": 31
          },
          {
            "chunk_id": 4,
            "text": "We reduce the autocorrelation of the topological charge employing open boundary condi-\ntions, while removing exactly their unphysical effects using a non-equilibrium Monte Carlo\napproach in which periodic boundary conditions are gradually switched on. We perform\na detailed analysis of the computational costs of this strategy in the case of the four-\ndimensional SU(3) Yang-Mills theory.",
            "char_count": 388,
            "word_count": 56
          }
        ],
        "stats": {
          "mode": "Simple",
          "num_chunks": 332,
          "avg_chars": 323.2710843373494,
          "min_chars": 13,
          "max_chars": 581,
          "avg_words": 52.213855421686745,
          "status": "success"
        },
        "time_sec": 0.031494855880737305
      },
      "semantic": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "Prepared for submission to JHEP\n\nScaling flow-based approaches for topology sampling\nin SU(3) gauge theory\n\n5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nt\na\nl\n-\np\ne\nh\n[\n\n1\nv\n4\n0\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nClaudio Bonanno,a Andrea Bulgarelli,b,c Elia Cellini,b Alessandro Nada,b Dario\nPanfalone,b Davide Vadacchino,d Lorenzo Verzichellib\n\naInstituto de F\u00b4\u0131sica Te\u00b4orica UAM-CSIC, c/ Nicol\u00b4as Cabrera 13-15, Universidad Aut\u00b4onoma de\nMadrid, Cantoblanco, E-28049 Madrid, Spain\nbDipartimento di Fisica, Universit\u00b4a degli Studi di Torino and INFN, Sezione di Torino, Via Pietro\nGiuria 1, I-10125 Turin, Italy\ncTransdisciplinary Research Area \u201cBuilding Blocks of Matter and Fundamental Interactions\u201d (TRA\nMatter) and Helmholtz Institute for Radiation and Nuclear Physics (HISKP), University of Bonn,\nNussallee 14-16, 53115 Bonn, Germany\ncCentre for Mathematical Sciences, University of Plymouth, Plymouth, PL4 8AA, United Kingdom\n\nE-mail: claudio.bonanno@csic.es, andrea.bulgarelli@unito.it,\nelia.cellini@unito.it, alessandro.nada@unito.it,\ndario.panfalone@unito.it, davide.vadacchino@plymouth.ac.uk,\nlorenzo.verzichelli@unito.it\n\nAbstract: We develop a methodology based on out-of-equilibrium simulations to miti-\ngate topological freezing when approaching the continuum limit of lattice gauge theories.\nWe reduce the autocorrelation of the topological charge employing open boundary condi-\ntions, while removing exactly their unphysical effects using a non-equilibrium Monte Carlo\napproach in which periodic boundary conditions are gradually switched on. We perform\na detailed analysis of the computational costs of this strategy in the case of the four-\ndimensional SU(3) Yang-Mills theory. After achieving full control of the scaling, we outline\na clear strategy to sample topology efficiently in the continuum limit, which we check at\nlattice spacings as small as 0.045 fm. We also generalize this approach by designing a cus-\ntomized Stochastic Normalizing Flow for evolutions in the boundary conditions, obtaining\nsuperior performances with respect to the purely stochastic non-equilibrium approach, and\npaving the way for more efficient future flow-based solutions.\n\nKeywords: Algorithms and Theoretical Developments, Lattice QCD, Vacuum Structure\nand Confinement\n\n \n \n \n \n \n \n\fContents\n\n1 Introduction\n\n2 Non-equilibrium Monte Carlo simulations in lattice field theory\n\n2.1 Some insights on NE-MCMC and its metrics\n2.2 Lattice setup and topological observables\n\n3 Scaling of NE-MCMC in the boundary conditions\n\n3.1 Understanding the scaling with the degrees of freedom\n\n4 Accelerating NE-MCMC with Stochastic Normalizing Flows\n\n4.1 Coupling layers for a defect\n\n5 Sampling topology towards the continuum limit\n\n6 Conclusions\n\nA Interpolation strategy for defect coupling layer parameters\n\n1\n\n5\n7\n9\n\n11\n13\n\n15\n17\n\n22\n\n25\n\n28\n\n1 Introduction\n\nNumerical Markov Chain Monte Carlo (MCMC) simulations of lattice field theories are\namongst the most powerful tools for exploring the non-perturbative regime of non-Abelian\ngauge theories. Over the past decades, their use has provided first-principles insights into\nthe theoretical and phenomenological properties of several lattice-regularized models, the\nmost prominent example being lattice Quantum Chromodynamics (QCD). Nonetheless,\nthis approach is accompanied by a number of highly non-trivial computational challenges.\nAlthough advances in the architecture of supercomputing machines have greatly expanded\nthe range of feasible calculations, the development of more efficient and sophisticated al-\ngorithms remains essential to overcome these limitations.\n\nIn lattice gauge theories, and in particular in lattice QCD, one of the most severe\nnumerical issues within the MCMC framework is the so-called critical slowing down, in\nparticular that of topological modes. As the continuum limit is approached, the computa-\ntional cost required to obtain statistically independent configurations grows rapidly with\ndecreasing lattice spacing, ultimately leading to a loss of ergodicity of the Markov chain.\nThis is a critical issue, since ergodicity is a key assumption underlying the validity of en-\nsemble averages as estimators of expectation values. For most observables, critical slowing\ndown manifests as a polynomial growth of the autocorrelation time with the inverse lattice\nspacing with a small exponent. In contrast, for topological quantities such as the topolog-\nical charge Q [1\u20134], the scaling is found to be much more severe and compatible with a\n\n\u2013 1 \u2013\n\n\fpolynomial with a large exponent or even with an exponential. This can be understood in\nterms of the MCMC dynamics of topological modes when standard local updating algo-\nrithms are adopted to generate the Markov chain: while for non-topological quantities this\nis essentially diffusive, for topological ones this is dominated by jumps over the potential\nbarriers among different pseudo-topological sectors. Such barriers eventually diverge in the\ncontinuum limit to restore a proper notion of topological winding number [5]. Since no\nchange of topological sector is allowed via a local deformation of the gauge fields, it be-\ncomes increasingly difficult to change the winding number of a given lattice gauge field as\nthe lattice spacing approaches zero. This severe ergodicity problem affecting the sampling\nof the topological charge typically results, on fine lattices, in few or even no fluctuations of\nQ during feasible MCMC histories: for this reason it is typically called topological freezing.\nTopological freezing poses a serious problem for the determination of topological quan-\ntities from lattice simulations, most notably the topological susceptibility, a quantity of the\nutmost theoretical and phenomenological importance which has been widely addressed in\nthe lattice literature [6\u201319]. However, such a severe loss of ergodicity can in principle bias\nany expectation value estimated from topologically-frozen samples. It is well-known, for\ninstance, that it can affect the calculation of particle spectra [20, 21], as well as observables\ncomputed after the gradient flow like the action density [22, 23], necessary to obtain the\nreference scale t0 or the renormalized strong coupling. For this reason, mitigating topolog-\nical freezing is not only crucial for studies of topological quantities, but also to ensure the\nreliability of a wide range of lattice results. Developing new numerical strategies to address\nthis issue is a major focus within the lattice community, leading to substantial progress in\nthe last decade [24\u201338] (for recent reviews see Refs. [39\u201341]).\n\nThe adoption of Open Boundary Conditions (OBC) in the Euclidean time direc-\ntion [42, 43], instead of the conventional Periodic Boundary Conditions (PBC) is one of\nthe most popular and effective among various strategies proposed to mitigate topological\nfreezing. With OBC, the configuration space of gauge fields becomes simply connected [42]:\nbarriers between topological sectors are removed and the MCMC dynamics of topological\nmodes are now dominated by diffusive phenomena [44], thereby drastically reducing the\nseverity of topological freezing. However, this comes at the price of introducing unwanted\nboundary effects, as now only field fluctuations sufficiently far from the boundaries are\nphysical, leading to enhanced finite-volume effects. Moreover, translation invariance is\nlost, hindering for example the proper definition of a global topological charge. In recent\nyears, a method that has been proven to be very effective in circumventing this issue\u2014while\nat the same time retaining the benefits of OBC simulations\u2014is the Parallel Tempering on\nBoundary Conditions (PTBC) algorithm. After its first introduction in 2d CPN\n1 mod-\nels [45] (see also [46, 47]), it has been widely employed also in 4d gauge theories, both\nin the pure-gauge case [18, 48\u201351] and with dynamical fermions [52]. The idea is to per-\nform a tempering on the boundary conditions within a parallel tempering framework by\nsimultaneously simulating several lattices with different boundary conditions, interpolating\nbetween OBC and PBC. Such lattice replicas are allowed to swap gauge configurations at\nequilibrium (i.e., via a standard Metropolis accept/reject step), so that quickly decorrelated\nfluctuations generated with OBC are transferred to the PBC system, where all observables\n\n\u2212\n\n\u2013 2 \u2013\n\n\fare computed free of boundary effects.\n\nThe present work can be firmly placed within this context, i.e., algorithmic develop-\nment aimed at alleviating topological freezing in lattice gauge theories. Our goal is to\nintroduce a novel numerical strategy to mitigate this computational problem, combining\nideas previously presented in Refs. [53, 54]. Although this new proposal shares its basic un-\nderlying philosophy with the PTBC algorithm\u2014namely, to combine OBC and PBC to ac-\ncelerate the MCMC dynamics of topological modes while neutralizing unwanted boundary\neffects\u2014it is actually rooted on rather different and peculiar ingredients: out-of-equilibrium\nMCMC simulations [55, 56] and flow-based approaches [57, 58].\n\nAt the core of our approach lies a simple and general question: given a field configu-\nration sampled from a starting probability distribution (the prior ), can it be transformed\nin a controlled manner, so that it follows a different probability distribution that closely\napproximates the desired one (the target)? If the prior distribution features only mild\nautocorrelations and the transformation itself (the flow ) is both efficient to find and to\nsample from, these elements can be combined in a robust strategy to mitigate critical slow-\ning down in lattice gauge theories. The development of the so-called trivializing map [59]\nrepresented the first major effort in the construction of such a flow transformation, finding\nhowever limited success [60]. More recently, rapid progress in the field of deep learning has\nprovided the tools to construct much more flexible and complex flow transformations, most\nnotably with the implementation of Normalizing Flows (NFs) [61, 62] for lattice field theory\nsampling [57, 63]. Such architectures possess several desirable features, in particular their\nexpressiveness, allowing them to tackle complicated distributions, and their exactness, as\neffects due to differences between the inferred and the target distributions can be system-\natically removed. In recent years, significant progress has been made by the lattice field\ntheory community in the application of different NF architectures to a variety of models,\nranging from scalar theories [57, 63\u201372] to gauge theories [73\u201379], including formulations\nwith dynamical fermionic variables as well [80\u201384].\n\nThis new generation of flow-based samplers, however, features its own set of challenges.\nIn particular, finding the optimal NF parameters to flow efficiently from one distribution to\nanother requires a potentially very delicate and expensive training procedure. Concretely,\ntraining costs currently suffer from poor scaling, in particular when the number of the\nrelevant degrees of freedom involved in the model under study increases (e.g., with larger\nvolumes in units of the lattice spacing), see Refs. [65, 78, 85, 86].\n\nA different flow-based approach built on non-equilibrium MCMC (NE-MCMC) sim-\nulations addresses this scaling issue directly. This framework is based on two fundamen-\ntal results in non-equilibrium statistical mechanics, i.e., Jarzynski\u2019s equality [87\u201389] and\nCrooks\u2019 theorem [90, 91] and in the last decade it has been successfully applied in lattice\nfield theory. Specifically, its primary application has been the high-precision determina-\ntion of free energy differences [55], in particular for the equation of state [92], the running\ncoupling [93], the entanglement entropy [94, 95] and the Casimir effect [96]. More recently,\nthe same idea has been naturally repurposed as a flow-based approach for the mitigation of\ncritical slowing down [53, 97]: the current work represents the next step in this direction.\nA key advantage of the non-equilibrium approach is the well-understood scaling behaviour:\n\n\u2013 3 \u2013\n\n\fin particular, tests in a variety of models show that sampling costs grow linearly with the\nnumber of degrees of freedom varied during the flow transformation.\n\nIn their basic implementation, out-of-equilibrium simulations require no training and\ncan achieve efficient sampling with no extra costs. Yet, despite their favorable scaling,\nthey can still require significant amount of computational resources.\nInterestingly, this\npurely stochastic approach can be naturally combined with the deterministic transforma-\ntions underlying NFs: the resulting architecture, denoted as Stochastic Normalizing Flows\n(SNFs) [56, 98], has found natural applications in scalar field theories [56, 99, 100] and,\nmost relevant for this work, in the SU(3) Yang-Mills theory in 4 spacetime dimensions [54].\nSNFs still retain the same desirable scaling properties of NE-MCMC, while at the same\ntime markedly improving its computational efficiency: even more importantly, this is ob-\ntained with very limited training costs, a direct consequence of the stochastic nature of\nthese flows.\n\nIt is worth noting that related ideas have appeared in different contexts. NE-MCMC\nis equivalent to Annealed Importance Sampling [101], which has been reworked recently in\nthe so-called Sequential Monte Carlo [102] and also combined with normalizing flows [103,\n104]. Recent developments in sampling with Langevin dynamics [105] can be seen as\na continuous-time realization of SNFs. Likewise, applications to lattice field theories of\ndiffusion models [106\u2013110] also bear several similarities with the ones described in this work:\na fundamental difference is that in diffusion models the path between two distributions\nis defined implicitly, whereas the protocol underlying NE-MCMC and SNFs is defined\nexplicitly.\n\nIn this work, we apply both non-equilibrium Monte Carlo and Stochastic Normaliz-\ning Flows as flow-based approaches for efficient topology sampling in the four-dimensional\nSU(3) Yang-Mills theory. This model exhibits particularly severe topological freezing near\nthe continuum limit, thus offering an ideal test-bed for flow-based approaches before mov-\ning to full QCD simulations. In Section 2 we introduce the general features of the non-\nequilibrium Monte Carlo approach and describe our lattice gauge theory setup, including\nthe definition of OBC and of the topological observables of interest. Section 3 presents a\ncareful analysis of the scaling of the sampling costs of non-equilibrium simulations, both\nfrom a general perspective and from the point of view of flows connecting OBC with PBC.\nSection 4 focuses on the definition of the customized Stochastic Normalizing Flow used in\nthis work, which has been designed to act specifically on the open boundaries, and on its\nsuperior performance with respect to the purely stochastic counterpart. Finally, in Sec-\ntion 5 we discuss the application of this family of flows to simulations at fine lattice spacing,\nwhere topological freezing is most severe: here we outline a strategy to sample topolog-\nical observables towards the continuum limit and present results to further corroborate\nthe effectiveness of this approach. Section 6 concludes with a broader discussion of future\ndevelopments, both in terms of advancements in the flow architectures and of applications\nto more challenging theoretical setups.\n\n\u2013 4 \u2013\n\n\f2 Non-equilibrium Monte Carlo simulations in lattice field theory\n\nOn the lattice, given an appropriately discretized action S[U ], we wish to compute the\nvacuum expectation value of a given observable\n\nas\n\n(cid:90)\n\n\u27e8O\u27e9p =\n\ndU\n\nO\n\n(U ) p(U ) =\n\n(cid:90)\n\n(U ) e\u2212\n\nS[U ],\n\ndU\n\nO\n\n(2.1)\n\nO\n1\nZ\n\nwhere p(U ) = e\u2212\nwith\n\nS[U ]/Z will be referred to as the target Boltzmann probability distribution,\n\n(cid:90)\n\nZp =\n\ndU e\u2212\n\nS[U ]\n\n(2.2)\n\n\u2212\n\nlog Z.\n\nbeing the partition function, from which we can immediately define the dimensionless free\nenergy F =\nIn a standard Monte Carlo simulation, field configurations U are\nsampled directly from p(U ) by updating them sequentially along a Markov Chain. More\nprecisely, the updating algorithm is characterized by a transition probability Pp which\nis constructed such that the chain converges to the distribution p, called the equilibrium\ndistribution. To ensure this, it is standard procedure at the beginning of a simulation to\nundergo a burn-in period called thermalization, after which the Markov chain is assumed\nto be at equilibrium.\n\nRecent advances in non-equilibrium statistical mechanics, however, enable, under cer-\ntain conditions and following precise procedures, to perform simulations out of equilibrium.\nIn particular, Jarzynski\u2019s equality [88, 89] represents a fundamental result in this regard:\nin the case of Markov Chain Monte Carlo (MCMC) simulations, it allows for the calcula-\ntion of \u201cequilibrium\u201d quantities (namely, differences in free energy) from those evaluated\non non-thermalized Markov Chains. In particular it is possible to leverage this identity to\ncompute expectation values (in particular in lattice field theory) with a Non-Equilibrium\nMarkov Chain Monte Carlo (NE-MCMC), which we describe in the following.\n\nIn this approach, we build non-equilibrium \u201cevolutions\u201d that start from a prior distri-\nbution q0 = exp(\nS)/Z, which we aim\nto sample from. More precisely, each evolution is composed of a sequence of nstep field\nconfigurations Un:\n\nS0)/Z0 and reach a target distribution p = exp(\n\n\u2212\n\n\u2212\n\n: U0\n\nU\n\nP\u03bb(1)\n\u2212\u2192\n\nU1\n\nP\u03bb(2)\n\u2212\u2192\n\nU2\n\nP\u03bb(3)\n\u2212\u2192\n\n. . .\n\nP\u03bb(nstep)\n\u2212\u2192\n\nUnstep \u2261\n\nU.\n\n(2.3)\n\n= [U0, U1, . . . ",
            "char_count": 17662,
            "word_count": 2630
          },
          {
            "chunk_id": 1,
            "text": ", U ]. At the beginning we have a config-\nfor which we use the shorthand\nuration U0, sampled directly from q0: the latter can be, for example, a Markov Chain at\nequilibrium or a known analytical distribution one can sample directly from. Then, the evo-\nlution proceeds using a composition of Monte Carlo updates with transition probabilities\nP\u03bb(n) (the arrows in the above sequence) which satisfy detailed balance.\n\nU\n\nNote that the transition probabilities change throughout the evolution according to a\n(set of) parameter(s) \u03bb(n), called the protocol. Each P\u03bb(n) is defined with an equilibrium\ndistribution proportional to exp(\nS\u03bb(n)): the dependence on the protocol \u03bb(n) is explicit\nin the action S\u03bb(n), which interpolates (in general completely arbitrarily) between the prior\nand the target. The only exception is the last transition probability, which is fixed to\n\n\u2212\n\n\u2013 5 \u2013\n\n\fFigure 1. Scheme of a typical non-equilibrium simulation. A thermalized configuration (black\ncircle) is sampled from the prior distribution every nbetween MCMC steps (black squares); and an\nout-of-equilibrium evolution starts from it, following a given protocol \u03bb for nstep MCMC steps (red\ndiamonds) until the desired target distribution is reached. The work W of Eq. (2.5) is computed\nalong each evolution, while the value of the desired observable(s) is calculated in the last configu-\nration (red circle). The estimators of Eqs. (2.4) and (2.6) are obtained by taking the average\nacross different evolutions.\n\n. ",
            "char_count": 1496,
            "word_count": 235
          },
          {
            "chunk_id": 2,
            "text": ". ",
            "char_count": 2,
            "word_count": 1
          },
          {
            "chunk_id": 3,
            "text": ".\n\n\u27e9f\n\n\u27e8\n\nPp or, equivalently,\nhave the target distribution as equilibrium distribution, i.e., P\u03bb(nstep) \u2261\n\u03bb(nstep) must coincide with the value of the target distribution we want to sample from.\nThe fact that \u03bb(n) (and, thus, P\u03bb(n)) changes after each Monte Carlo update defines a\ntrue non-equilibrium evolution: since we do not let it thermalize at each step, the Markov\nChain is never at equilibrium.\n\nIn order to compute the expectation values of Eq. (2.1) with NE-MCMC we use the\n\nestimator\n\n(U )\n\n\u27e9p = \u27e8O\n\n\u27e8O\n\nW (\n\n)\n\nU\n\n(U ) e\u2212\nW (\ne\u2212\n\nU\n\n\u27e8\n\n)\n\n\u27e9f\n\n\u27e9f\n\n.\n\n(2.4)\n\n\u27e9f an average over all evolutions of the type of Eq. (2.3); more-\nHere, we indicate with\nover, W is the dimensionless work done on the system during the non-equilibrium trans-\nformation from the initial to the final state:\n\n. . .\n\n\u27e8\n\nW (\n\nU\n\n) =\n\n1\n\nnstep\n\u2212\n(cid:88)\n\nn=0\n\n(cid:8)S\u03bb(n+1) [Un]\n\nS\u03bb(n) [Un](cid:9) .\n\n\u2212\n\nFinally, we can write down Jarzynski\u2019s equality [88, 89]:\n\ne\u2212\n\nW (\n\n)\n\nU\n\n\u27e9f = e\u2212\n\n\u2206F =\n\n\u27e8\n\nZp\nZq0\n\n,\n\n(2.5)\n\n(2.6)\n\nwhich connects the average of the exponential of the work on non-equilibrium evolutions\nwith the difference in free energy between the system described by the target and the prior\nprobability distributions. We show in Fig. 1 a scheme of a typical NE-MCMC simulation.\n\n\u2013 6 \u2013\n\nnstepnbetweennstepnbetweennstepnbetweennstepnbetweeneqMCnon-eqMC\f2.1 Some insights on NE-MCMC and its metrics\n\nLet us first formally define the\n\u27e9f average of Eq. (2.4). Once the protocol (i.e., \u03bb(n)\nand nstep) and the Monte Carlo update (i.e., the details of P\u03bb(n)) have been chosen, we can\ndefine the forward\n\nPr transition probabilities for a given evolution\n\nPf and reverse\n\n. . .\n\nas\n\nU\n\n\u27e8\n\nnstep\n(cid:89)\n\nn=1\n\nnstep\n(cid:89)\n\nn=1\n\nand\n\n] =\n\nPf [\n\nU\n\n] =\n\nPr[\n\nU\n\nP\u03bb(n)(Un\n\nUn),\n\n1 \u2192\n\n\u2212\n\nP\u03bb(n)(Un \u2192\n\nUn\n\n1).\n\n\u2212\n\n(2.7)\n\n(2.8)\n\nSince the Monte Carlo updates must satisfy detailed balance, it is possible to state Crooks\u2019\nfluctuation theorem [90, 91], which for Markov Chains relates the forward and reverse\n:\nprobability densities of a given NE-MCMC evolution to the dissipation of the sequence\n\nU\n\n]\n\nq0(U0)\np(U )\n\nPf [\nU\n]\nPr[\nU\n\n= exp(W (\n\n)\n\nU\n\n\u2212\n\n\u2206F );\n\n(2.9)\n\nthis result can be proved rather easily using the properties of Markov Chain transition\nprobabilities that satisfy detailed balance.\nIt is useful to introduce the pseudo-heat Q\nexchanged during each transformation:\n\n) =\n\nQ(\n\nU\n\nnstep\n(cid:88)\n\nn=1\n\n(cid:8)S\u03bb(n) [Un]\n\nS\u03bb(n) [Un\n\n\u2212\n\n1](cid:9) ,\n\n\u2212\n\n(2.10)\n\n\u2212\n). We have now all the elements to properly define the expectation values over forward\n\nwhich takes this form following the First Law of Thermodynamics, S[U ]\nQ(\nevolutions as:\n\nS0[U0] = W (\n\n\u2212\n\nU\n\nU\n\n)\n\n. . ",
            "char_count": 2665,
            "word_count": 474
          },
          {
            "chunk_id": 4,
            "text": ".\n\n\u27e8\n\nq0\n\n\u27e9U\u223c\n\nf \u2261 \u27e8\nP\n\n. . .\n\n\u27e9f =\n\n(cid:90)\n\n(cid:90)\n\ndU0 . . . dU q0(U0)\n\nPf [U0, . . . , U ] . . ",
            "char_count": 102,
            "word_count": 31
          }
        ],
        "stats": {
          "mode": "Semantic",
          "num_chunks": 50,
          "avg_chars": 2157.9,
          "min_chars": 2,
          "max_chars": 17662,
          "avg_words": 346.56,
          "status": "success"
        },
        "time_sec": 3.1676459312438965
      },
      "proposition": {
        "chunks": [],
        "stats": {
          "mode": "Proposition",
          "num_chunks": 0,
          "avg_chars": 0,
          "min_chars": 0,
          "max_chars": 0,
          "avg_words": 0,
          "status": "not_available"
        },
        "time_sec": 0
      },
      "hybrid": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "Prepared for submission to JHEP\n\nScaling flow-based approaches for topology sampling\nin SU(3) gauge theory\n\n5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nt\na\nl\n-\np\ne\nh\n[\n\n1\nv\n4\n0\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nClaudio Bonanno,a Andrea Bulgarelli,b,c Elia Cellini,b Alessandro Nada,b Dario\nPanfalone,b Davide Vadacchino,d Lorenzo Verzichellib\n\naInstituto de F\u00b4\u0131sica Te\u00b4orica UAM-CSIC, c/ Nicol\u00b4as Cabrera 13-15, Universidad Aut\u00b4onoma de\nMadrid, Cantoblanco, E-28049 Madrid, Spain\nbDipartimento di Fisica, Universit\u00b4a degli Studi di Torino and INFN, Sezione di Torino, Via Pietro\nGiuria 1, I-10125 Turin, Italy\ncTransdisciplinary Research Area \u201cBuilding Blocks of Matter and Fundamental Interactions\u201d (TRA\nMatter) and Helmholtz Institute for Radiation and Nuclear Physics (HISKP), University of Bonn,\nNussallee 14-16, 53115 Bonn, Germany\ncCentre for Mathematical Sciences, University of Plymouth, Plymouth, PL4 8AA, United Kingdom\n\nE-mail: claudio.bonanno@csic.es, andrea.bulgarelli@unito.it,\nelia.cellini@unito.it, alessandro.nada@unito.it,\ndario.panfalone@unito.it, davide.vadacchino@plymouth.ac.uk,\nlorenzo.verzichelli@unito.it\n\nAbstract: We develop a methodology based on out-of-equilibrium simulations to miti-\ngate topological freezing when approaching the continuum limit of lattice gauge theories.\nWe reduce the autocorrelation of the topological charge employing open boundary condi-\ntions, while removing exactly their unphysical effects using a non-equilibrium Monte Carlo\napproach in which periodic boundary conditions are gradually switched on. We perform\na detailed analysis of the computational costs of this strategy in the case of the four-\ndimensional SU(3) Yang-Mills theory. After achieving full control of the scaling, we outline\na clear strategy to sample topology efficiently in the continuum limit, which we check at\nlattice spacings as small as 0.045 fm. We also generalize this approach by designing a cus-\ntomized Stochastic Normalizing Flow for evolutions in the boundary conditions, obtaining\nsuperior performances with respect to the purely stochastic non-equilibrium approach, and\npaving the way for more efficient future flow-based solutions.\n\nKeywords: Algorithms and Theoretical Developments, Lattice QCD, Vacuum Structure\nand Confinement\n\n \n \n \n \n \n \n\fContents\n\n1 Introduction\n\n2 Non-equilibrium Monte Carlo simulations in lattice field theory\n\n2.1 Some insights on NE-MCMC and its metrics\n2.2 Lattice setup and topological observables\n\n3 Scaling of NE-MCMC in the boundary conditions\n\n3.1 Understanding the scaling with the degrees of freedom\n\n4 Accelerating NE-MCMC with Stochastic Normalizing Flows\n\n4.1 Coupling layers for a defect\n\n5 Sampling topology towards the continuum limit\n\n6 Conclusions\n\nA Interpolation strategy for defect coupling layer parameters\n\n1\n\n5\n7\n9\n\n11\n13\n\n15\n17\n\n22\n\n25\n\n28\n\n1 Introduction\n\nNumerical Markov Chain Monte Carlo (MCMC) simulations of lattice field theories are\namongst the most powerful tools for exploring the non-perturbative regime of non-Abelian\ngauge theories. Over the past decades, their use has provided first-principles insights into\nthe theoretical and phenomenological properties of several lattice-regularized models, the\nmost prominent example being lattice Quantum Chromodynamics (QCD). Nonetheless,\nthis approach is accompanied by a number of highly non-trivial computational challenges.\nAlthough advances in the architecture of supercomputing machines have greatly expanded\nthe range of feasible calculations, the development of more efficient and sophisticated al-\ngorithms remains essential to overcome these limitations.\n\nIn lattice gauge theories, and in particular in lattice QCD, one of the most severe\nnumerical issues within the MCMC framework is the so-called critical slowing down, in\nparticular that of topological modes. As the continuum limit is approached, the computa-\ntional cost required to obtain statistically independent configurations grows rapidly with\ndecreasing lattice spacing, ultimately leading to a loss of ergodicity of the Markov chain.\nThis is a critical issue, since ergodicity is a key assumption underlying the validity of en-\nsemble averages as estimators of expectation values. For most observables, critical slowing\ndown manifests as a polynomial growth of the autocorrelation time with the inverse lattice\nspacing with a small exponent. In contrast, for topological quantities such as the topolog-\nical charge Q [1\u20134], the scaling is found to be much more severe and compatible with a\n\n\u2013 1 \u2013\n\n\fpolynomial with a large exponent or even with an exponential. This can be understood in\nterms of the MCMC dynamics of topological modes when standard local updating algo-\nrithms are adopted to generate the Markov chain: while for non-topological quantities this\nis essentially diffusive, for topological ones this is dominated by jumps over the potential\nbarriers among different pseudo-topological sectors. Such barriers eventually diverge in the\ncontinuum limit to restore a proper notion of topological winding number [5]. Since no\nchange of topological sector is allowed via a local deformation of the gauge fields, it be-\ncomes increasingly difficult to change the winding number of a given lattice gauge field as\nthe lattice spacing approaches zero. This severe ergodicity problem affecting the sampling\nof the topological charge typically results, on fine lattices, in few or even no fluctuations of\nQ during feasible MCMC histories: for this reason it is typically called topological freezing.\nTopological freezing poses a serious problem for the determination of topological quan-\ntities from lattice simulations, most notably the topological susceptibility, a quantity of the\nutmost theoretical and phenomenological importance which has been widely addressed in\nthe lattice literature [6\u201319]. However, such a severe loss of ergodicity can in principle bias\nany expectation value estimated from topologically-frozen samples. It is well-known, for\ninstance, that it can affect the calculation of particle spectra [20, 21], as well as observables\ncomputed after the gradient flow like the action density [22, 23], necessary to obtain the\nreference scale t0 or the renormalized strong coupling. For this reason, mitigating topolog-\nical freezing is not only crucial for studies of topological quantities, but also to ensure the\nreliability of a wide range of lattice results. Developing new numerical strategies to address\nthis issue is a major focus within the lattice community, leading to substantial progress in\nthe last decade [24\u201338] (for recent reviews see Refs. [39\u201341]).\n\nThe adoption of Open Boundary Conditions (OBC) in the Euclidean time direc-\ntion [42, 43], instead of the conventional Periodic Boundary Conditions (PBC) is one of\nthe most popular and effective among various strategies proposed to mitigate topological\nfreezing. With OBC, the configuration space of gauge fields becomes simply connected [42]:\nbarriers between topological sectors are removed and the MCMC dynamics of topological\nmodes are now dominated by diffusive phenomena [44], thereby drastically reducing the\nseverity of topological freezing. However, this comes at the price of introducing unwanted\nboundary effects, as now only field fluctuations sufficiently far from the boundaries are\nphysical, leading to enhanced finite-volume effects. Moreover, translation invariance is\nlost, hindering for example the proper definition of a global topological charge. In recent\nyears, a method that has been proven to be very effective in circumventing this issue\u2014while\nat the same time retaining the benefits of OBC simulations\u2014is the Parallel Tempering on\nBoundary Conditions (PTBC) algorithm. After its first introduction in 2d CPN\n1 mod-\nels [45] (see also [46, 47]), it has been widely employed also in 4d gauge theories, both\nin the pure-gauge case [18, 48\u201351] and with dynamical fermions [52]. The idea is to per-\nform a tempering on the boundary conditions within a parallel tempering framework by\nsimultaneously simulating several lattices with different boundary conditions, interpolating\nbetween OBC and PBC. Such lattice replicas are allowed to swap gauge configurations at\nequilibrium (i.e., via a standard Metropolis accept/reject step), so that quickly decorrelated\nfluctuations generated with OBC are transferred to the PBC system, where all observables\n\n\u2212\n\n\u2013 2 \u2013\n\n\fare computed free of boundary effects.\n\nThe present work can be firmly placed within this context, i.e., algorithmic develop-\nment aimed at alleviating topological freezing in lattice gauge theories. Our goal is to\nintroduce a novel numerical strategy to mitigate this computational problem, combining\nideas previously presented in Refs. [53, 54]. Although this new proposal shares its basic un-\nderlying philosophy with the PTBC algorithm\u2014namely, to combine OBC and PBC to ac-\ncelerate the MCMC dynamics of topological modes while neutralizing unwanted boundary\neffects\u2014it is actually rooted on rather different and peculiar ingredients: out-of-equilibrium\nMCMC simulations [55, 56] and flow-based approaches [57, 58].\n\nAt the core of our approach lies a simple and general question: given a field configu-\nration sampled from a starting probability distribution (the prior ), can it be transformed\nin a controlled manner, so that it follows a different probability distribution that closely\napproximates the desired one (the target)? If the prior distribution features only mild\nautocorrelations and the transformation itself (the flow ) is both efficient to find and to\nsample from, these elements can be combined in a robust strategy to mitigate critical slow-\ning down in lattice gauge theories. The development of the so-called trivializing map [59]\nrepresented the first major effort in the construction of such a flow transformation, finding\nhowever limited success [60]. More recently, rapid progress in the field of deep learning has\nprovided the tools to construct much more flexible and complex flow transformations, most\nnotably with the implementation of Normalizing Flows (NFs) [61, 62] for lattice field theory\nsampling [57, 63]. Such architectures possess several desirable features, in particular their\nexpressiveness, allowing them to tackle complicated distributions, and their exactness, as\neffects due to differences between the inferred and the target distributions can be system-\natically removed. In recent years, significant progress has been made by the lattice field\ntheory community in the application of different NF architectures to a variety of models,\nranging from scalar theories [57, 63\u201372] to gauge theories [73\u201379], including formulations\nwith dynamical fermionic variables as well [80\u201384].\n\nThis new generation of flow-based samplers, however, features its own set of challenges.\nIn particular, finding the optimal NF parameters to flow efficiently from one distribution to\nanother requires a potentially very delicate and expensive training procedure. Concretely,\ntraining costs currently suffer from poor scaling, in particular when the number of the\nrelevant degrees of freedom involved in the model under study increases (e.g., with larger\nvolumes in units of the lattice spacing), see Refs. [65, 78, 85, 86].\n\nA different flow-based approach built on non-equilibrium MCMC (NE-MCMC) sim-\nulations addresses this scaling issue directly. This framework is based on two fundamen-\ntal results in non-equilibrium statistical mechanics, i.e., Jarzynski\u2019s equality [87\u201389] and\nCrooks\u2019 theorem [90, 91] and in the last decade it has been successfully applied in lattice\nfield theory. Specifically, its primary application has been the high-precision determina-\ntion of free energy differences [55], in particular for the equation of state [92], the running\ncoupling [93], the entanglement entropy [94, 95] and the Casimir effect [96]. More recently,\nthe same idea has been naturally repurposed as a flow-based approach for the mitigation of\ncritical slowing down [53, 97]: the current work represents the next step in this direction.\nA key advantage of the non-equilibrium approach is the well-understood scaling behaviour:\n\n\u2013 3 \u2013\n\n\fin particular, tests in a variety of models show that sampling costs grow linearly with the\nnumber of degrees of freedom varied during the flow transformation.\n\nIn their basic implementation, out-of-equilibrium simulations require no training and\ncan achieve efficient sampling with no extra costs. Yet, despite their favorable scaling,\nthey can still require significant amount of computational resources.\nInterestingly, this\npurely stochastic approach can be naturally combined with the deterministic transforma-\ntions underlying NFs: the resulting architecture, denoted as Stochastic Normalizing Flows\n(SNFs) [56, 98], has found natural applications in scalar field theories [56, 99, 100] and,\nmost relevant for this work, in the SU(3) Yang-Mills theory in 4 spacetime dimensions [54].\nSNFs still retain the same desirable scaling properties of NE-MCMC, while at the same\ntime markedly improving its computational efficiency: even more importantly, this is ob-\ntained with very limited training costs, a direct consequence of the stochastic nature of\nthese flows.\n\nIt is worth noting that related ideas have appeared in different contexts. NE-MCMC\nis equivalent to Annealed Importance Sampling [101], which has been reworked recently in\nthe so-called Sequential Monte Carlo [102] and also combined with normalizing flows [103,\n104]. Recent developments in sampling with Langevin dynamics [105] can be seen as\na continuous-time realization of SNFs. Likewise, applications to lattice field theories of\ndiffusion models [106\u2013110] also bear several similarities with the ones described in this work:\na fundamental difference is that in diffusion models the path between two distributions\nis defined implicitly, whereas the protocol underlying NE-MCMC and SNFs is defined\nexplicitly.\n\nIn this work, we apply both non-equilibrium Monte Carlo and Stochastic Normaliz-\ning Flows as flow-based approaches for efficient topology sampling in the four-dimensional\nSU(3) Yang-Mills theory. This model exhibits particularly severe topological freezing near\nthe continuum limit, thus offering an ideal test-bed for flow-based approaches before mov-\ning to full QCD simulations. In Section 2 we introduce the general features of the non-\nequilibrium Monte Carlo approach and describe our lattice gauge theory setup, including\nthe definition of OBC and of the topological observables of interest. Section 3 presents a\ncareful analysis of the scaling of the sampling costs of non-equilibrium simulations, both\nfrom a general perspective and from the point of view of flows connecting OBC with PBC.\nSection 4 focuses on the definition of the customized Stochastic Normalizing Flow used in\nthis work, which has been designed to act specifically on the open boundaries, and on its\nsuperior performance with respect to the purely stochastic counterpart. Finally, in Sec-\ntion 5 we discuss the application of this family of flows to simulations at fine lattice spacing,\nwhere topological freezing is most severe: here we outline a strategy to sample topolog-\nical observables towards the continuum limit and present results to further corroborate\nthe effectiveness of this approach. Section 6 concludes with a broader discussion of future\ndevelopments, both in terms of advancements in the flow architectures and of applications\nto more challenging theoretical setups.\n\n\u2013 4 \u2013\n\n\f2 Non-equilibrium Monte Carlo simulations in lattice field theory\n\nOn the lattice, given an appropriately discretized action S[U ], we wish to compute the\nvacuum expectation value of a given observable\n\nas\n\n(cid:90)\n\n\u27e8O\u27e9p =\n\ndU\n\nO\n\n(U ) p(U ) =\n\n(cid:90)\n\n(U ) e\u2212\n\nS[U ],\n\ndU\n\nO\n\n(2.1)\n\nO\n1\nZ\n\nwhere p(U ) = e\u2212\nwith\n\nS[U ]/Z will be referred to as the target Boltzmann probability distribution,\n\n(cid:90)\n\nZp =\n\ndU e\u2212\n\nS[U ]\n\n(2.2)\n\n\u2212\n\nlog Z.\n\nbeing the partition function, from which we can immediately define the dimensionless free\nenergy F =\nIn a standard Monte Carlo simulation, field configurations U are\nsampled directly from p(U ) by updating them sequentially along a Markov Chain. More\nprecisely, the updating algorithm is characterized by a transition probability Pp which\nis constructed such that the chain converges to the distribution p, called the equilibrium\ndistribution. To ensure this, it is standard procedure at the beginning of a simulation to\nundergo a burn-in period called thermalization, after which the Markov chain is assumed\nto be at equilibrium.\n\nRecent advances in non-equilibrium statistical mechanics, however, enable, under cer-\ntain conditions and following precise procedures, to perform simulations out of equilibrium.\nIn particular, Jarzynski\u2019s equality [88, 89] represents a fundamental result in this regard:\nin the case of Markov Chain Monte Carlo (MCMC) simulations, it allows for the calcula-\ntion of \u201cequilibrium\u201d quantities (namely, differences in free energy) from those evaluated\non non-thermalized Markov Chains. In particular it is possible to leverage this identity to\ncompute expectation values (in particular in lattice field theory) with a Non-Equilibrium\nMarkov Chain Monte Carlo (NE-MCMC), which we describe in the following.\n\nIn this approach, we build non-equilibrium \u201cevolutions\u201d that start from a prior distri-\nbution q0 = exp(\nS)/Z, which we aim\nto sample from. More precisely, each evolution is composed of a sequence of nstep field\nconfigurations Un:\n\nS0)/Z0 and reach a target distribution p = exp(\n\n\u2212\n\n\u2212\n\n: U0\n\nU\n\nP\u03bb(1)\n\u2212\u2192\n\nU1\n\nP\u03bb(2)\n\u2212\u2192\n\nU2\n\nP\u03bb(3)\n\u2212\u2192\n\n. . .\n\nP\u03bb(nstep)\n\u2212\u2192\n\nUnstep \u2261\n\nU.\n\n(2.3)\n\n= [U0, U1, . . . ",
            "char_count": 17662,
            "word_count": 2630
          },
          {
            "chunk_id": 1,
            "text": ", U ]. At the beginning we have a config-\nfor which we use the shorthand\nuration U0, sampled directly from q0: the latter can be, for example, a Markov Chain at\nequilibrium or a known analytical distribution one can sample directly from. Then, the evo-\nlution proceeds using a composition of Monte Carlo updates with transition probabilities\nP\u03bb(n) (the arrows in the above sequence) which satisfy detailed balance.\n\nU\n\nNote that the transition probabilities change throughout the evolution according to a\n(set of) parameter(s) \u03bb(n), called the protocol. Each P\u03bb(n) is defined with an equilibrium\ndistribution proportional to exp(\nS\u03bb(n)): the dependence on the protocol \u03bb(n) is explicit\nin the action S\u03bb(n), which interpolates (in general completely arbitrarily) between the prior\nand the target. The only exception is the last transition probability, which is fixed to\n\n\u2212\n\n\u2013 5 \u2013\n\n\fFigure 1. Scheme of a typical non-equilibrium simulation. A thermalized configuration (black\ncircle) is sampled from the prior distribution every nbetween MCMC steps (black squares); and an\nout-of-equilibrium evolution starts from it, following a given protocol \u03bb for nstep MCMC steps (red\ndiamonds) until the desired target distribution is reached. The work W of Eq. (2.5) is computed\nalong each evolution, while the value of the desired observable(s) is calculated in the last configu-\nration (red circle). The estimators of Eqs. (2.4) and (2.6) are obtained by taking the average\nacross different evolutions.\n\n. ",
            "char_count": 1496,
            "word_count": 235
          },
          {
            "chunk_id": 2,
            "text": ". ",
            "char_count": 2,
            "word_count": 1
          },
          {
            "chunk_id": 3,
            "text": ".\n\n\u27e9f\n\n\u27e8\n\nPp or, equivalently,\nhave the target distribution as equilibrium distribution, i.e., P\u03bb(nstep) \u2261\n\u03bb(nstep) must coincide with the value of the target distribution we want to sample from.\nThe fact that \u03bb(n) (and, thus, P\u03bb(n)) changes after each Monte Carlo update defines a\ntrue non-equilibrium evolution: since we do not let it thermalize at each step, the Markov\nChain is never at equilibrium.\n\nIn order to compute the expectation values of Eq. (2.1) with NE-MCMC we use the\n\nestimator\n\n(U )\n\n\u27e9p = \u27e8O\n\n\u27e8O\n\nW (\n\n)\n\nU\n\n(U ) e\u2212\nW (\ne\u2212\n\nU\n\n\u27e8\n\n)\n\n\u27e9f\n\n\u27e9f\n\n.\n\n(2.4)\n\n\u27e9f an average over all evolutions of the type of Eq. (2.3); more-\nHere, we indicate with\nover, W is the dimensionless work done on the system during the non-equilibrium trans-\nformation from the initial to the final state:\n\n. . .\n\n\u27e8\n\nW (\n\nU\n\n) =\n\n1\n\nnstep\n\u2212\n(cid:88)\n\nn=0\n\n(cid:8)S\u03bb(n+1) [Un]\n\nS\u03bb(n) [Un](cid:9) .\n\n\u2212\n\nFinally, we can write down Jarzynski\u2019s equality [88, 89]:\n\ne\u2212\n\nW (\n\n)\n\nU\n\n\u27e9f = e\u2212\n\n\u2206F =\n\n\u27e8\n\nZp\nZq0\n\n,\n\n(2.5)\n\n(2.6)\n\nwhich connects the average of the exponential of the work on non-equilibrium evolutions\nwith the difference in free energy between the system described by the target and the prior\nprobability distributions. We show in Fig. 1 a scheme of a typical NE-MCMC simulation.\n\n\u2013 6 \u2013\n\nnstepnbetweennstepnbetweennstepnbetweennstepnbetweeneqMCnon-eqMC\f2.1 Some insights on NE-MCMC and its metrics\n\nLet us first formally define the\n\u27e9f average of Eq. (2.4). Once the protocol (i.e., \u03bb(n)\nand nstep) and the Monte Carlo update (i.e., the details of P\u03bb(n)) have been chosen, we can\ndefine the forward\n\nPr transition probabilities for a given evolution\n\nPf and reverse\n\n. . .\n\nas\n\nU\n\n\u27e8\n\nnstep\n(cid:89)\n\nn=1\n\nnstep\n(cid:89)\n\nn=1\n\nand\n\n] =\n\nPf [\n\nU\n\n] =\n\nPr[\n\nU\n\nP\u03bb(n)(Un\n\nUn),\n\n1 \u2192\n\n\u2212\n\nP\u03bb(n)(Un \u2192\n\nUn\n\n1).\n\n\u2212\n\n(2.7)\n\n(2.8)\n\nSince the Monte Carlo updates must satisfy detailed balance, it is possible to state Crooks\u2019\nfluctuation theorem [90, 91], which for Markov Chains relates the forward and reverse\n:\nprobability densities of a given NE-MCMC evolution to the dissipation of the sequence\n\nU\n\n]\n\nq0(U0)\np(U )\n\nPf [\nU\n]\nPr[\nU\n\n= exp(W (\n\n)\n\nU\n\n\u2212\n\n\u2206F );\n\n(2.9)\n\nthis result can be proved rather easily using the properties of Markov Chain transition\nprobabilities that satisfy detailed balance.\nIt is useful to introduce the pseudo-heat Q\nexchanged during each transformation:\n\n) =\n\nQ(\n\nU\n\nnstep\n(cid:88)\n\nn=1\n\n(cid:8)S\u03bb(n) [Un]\n\nS\u03bb(n) [Un\n\n\u2212\n\n1](cid:9) ,\n\n\u2212\n\n(2.10)\n\n\u2212\n). We have now all the elements to properly define the expectation values over forward\n\nwhich takes this form following the First Law of Thermodynamics, S[U ]\nQ(\nevolutions as:\n\nS0[U0] = W (\n\n\u2212\n\nU\n\nU\n\n)\n\n. . ",
            "char_count": 2665,
            "word_count": 474
          },
          {
            "chunk_id": 4,
            "text": ".\n\n\u27e8\n\nq0\n\n\u27e9U\u223c\n\nf \u2261 \u27e8\nP\n\n. . .\n\n\u27e9f =\n\n(cid:90)\n\n(cid:90)\n\ndU0 . . . dU q0(U0)\n\nPf [U0, . . . , U ] . . ",
            "char_count": 102,
            "word_count": 31
          }
        ],
        "stats": {
          "mode": "Hybrid",
          "num_chunks": 50,
          "avg_chars": 2157.9,
          "min_chars": 2,
          "max_chars": 17662,
          "avg_words": 346.56,
          "status": "success"
        },
        "time_sec": 1.0024187564849854
      }
    }
  },
  {
    "paper_id": "2510.25726v1",
    "original_length": 90281,
    "modes": {
      "simple": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n6\n2\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nTHE TOOL DECATHLON: BENCHMARKING LANGUAGE\nAGENTS FOR DIVERSE, REALISTIC, AND LONG-\nHORIZON TASK EXECUTION\n\nWenshuo Zhao1*",
            "char_count": 193,
            "word_count": 50
          },
          {
            "chunk_id": 1,
            "text": "Junlong Li1\u2217\nXiaochen Wang1 Rui Ge1 Yuxuan Cao1 Yuzhen Huang1 Wei Liu1\nZhaochen Su1 Yiyang Guo1\nXingyao Wang2 Xiang Yue3\n\nFan Zhou1 Lueyang Zhang1\nShuyan Zhou4 Graham Neubig2,3\n\nJian Zhao1* Weihao Zeng1* Haoze Wu1*\n\nJuan Michelini2\n\nJunxian He1\u2020\n\nJunteng Liu1",
            "char_count": 259,
            "word_count": 40
          },
          {
            "chunk_id": 2,
            "text": "1The Hong Kong University of Science and Technology\n2All Hands AI\nWebsite: toolathlon.xyz\n\n3Carnegie Mellon University\n\n4Duke University\n\n\u00a7 github.com/hkust-nlp/toolathlon\n\nABSTRACT\n\nReal-world language agents must handle complex, multi-step workflows across\ndiverse applications. For instance, an agent may manage emails by coordinating\nwith calendars and file systems, or monitor a production database like BigQuery\nto detect anomalies and generate reports following a standard operating manual.",
            "char_count": 497,
            "word_count": 64
          },
          {
            "chunk_id": 3,
            "text": "However, existing language agent benchmarks often focus on narrow domains\nor simplified tasks that lack the diversity, realism, and long-horizon complexity\nrequired to evaluate agents\u2019 real-world performance. To address this gap, we\nintroduce the Tool Decathlon (dubbed as TOOLATHLON), a benchmark for language\nagents offering diverse applications and tools, realistic environment setup, and\nreliable execution-based evaluation.",
            "char_count": 428,
            "word_count": 56
          },
          {
            "chunk_id": 4,
            "text": "TOOLATHLON spans 32 software applications\nand 604 tools, ranging from everyday platforms such as Google Calendar and\nNotion to professional applications like WooCommerce, Kubernetes, and BigQuery.\nMost of the tools are based on a high-quality set of Model Context Protocol (MCP)\nservers that we may have revised or implemented ourselves. Unlike prior works,\nwhich primarily ensure functional realism but offer limited environment state\ndiversity, we provide realistic initial environment states from real software, such as\nCanvas courses with dozens of students or real-world financial spreadsheets.",
            "char_count": 599,
            "word_count": 85
          }
        ],
        "stats": {
          "mode": "Simple",
          "num_chunks": 239,
          "avg_chars": 376.1882845188284,
          "min_chars": 19,
          "max_chars": 636,
          "avg_words": 53.10878661087866,
          "status": "success"
        },
        "time_sec": 0.02452874183654785
      },
      "semantic": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nL\nC\n.\n",
            "char_count": 28,
            "word_count": 13
          },
          {
            "chunk_id": 1,
            "text": "s\nc\n[\n\n1\nv\n6\n2\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nTHE TOOL DECATHLON: BENCHMARKING LANGUAGE\nAGENTS FOR DIVERSE, REALISTIC, AND LONG-\nHORIZON TASK EXECUTION\n\nWenshuo Zhao1*\n\nJunlong Li1\u2217\nXiaochen Wang1 Rui Ge1 Yuxuan Cao1 Yuzhen Huang1 Wei Liu1\nZhaochen Su1 Yiyang Guo1\nXingyao Wang2 Xiang Yue3\n\nFan Zhou1 Lueyang Zhang1\nShuyan Zhou4 Graham Neubig2,3\n\nJian Zhao1* Weihao Zeng1* Haoze Wu1*\n\nJuan Michelini2\n\nJunxian He1\u2020\n\nJunteng Liu1\n\n1The Hong Kong University of Science and Technology\n2All Hands AI\nWebsite: toolathlon.xyz\n\n3Carnegie Mellon University\n\n4Duke University\n\n\u00a7 github.com/hkust-nlp/toolathlon\n\nABSTRACT\n\nReal-world language agents must handle complex, multi-step workflows across\ndiverse applications. For instance, an agent may manage emails by coordinating\nwith calendars and file systems, or monitor a production database like BigQuery\nto detect anomalies and generate reports following a standard operating manual.\nHowever, existing language agent benchmarks often focus on narrow domains\nor simplified tasks that lack the diversity, realism, and long-horizon complexity\nrequired to evaluate agents\u2019 real-world performance. To address this gap, we\nintroduce the Tool Decathlon (dubbed as TOOLATHLON), a benchmark for language\nagents offering diverse applications and tools, realistic environment setup, and\nreliable execution-based evaluation. TOOLATHLON spans 32 software applications\nand 604 tools, ranging from everyday platforms such as Google Calendar and\nNotion to professional applications like WooCommerce, Kubernetes, and BigQuery.\nMost of the tools are based on a high-quality set of Model Context Protocol (MCP)\nservers that we may have revised or implemented ourselves. Unlike prior works,\nwhich primarily ensure functional realism but offer limited environment state\ndiversity, we provide realistic initial environment states from real software, such as\nCanvas courses with dozens of students or real-world financial spreadsheets. The\nTOOLATHLON benchmark includes 108 manually sourced or crafted tasks in total,\nrequiring interacting with multiple applications over around 20 turns on average\nto complete. Each task is strictly verifiable through dedicated evaluation scripts.\nComprehensive evaluation of state-of-the-art models highlights their significant\nshortcomings in performing real-world, long-horizon tasks: the best-performing\nmodel, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool\ncalling turns on average, while the top open-weights model DeepSeek-V3.2-Exp\nreaches 20.1%. We expect TOOLATHLON to drive the development of more capable\nlanguage agents for real-world, long-horizon task execution.\n\n1\n\nINTRODUCTION\n\nTool-based language agents have already demonstrated their impact in real-world domains such as\nsoftware engineering (Jimenez et al., 2024; The Terminal-Bench Team, 2025), deep research (OpenAI,\n2024), and web browsing (Zhou et al., 2024). To further expand the reach of language agents across\ndiverse domains and applications, the Model Context Protocol (MCP) has been proposed to establish\na standard for connecting language agents to tens of thousands of applications (Anthropic, 2024).\n\nExisting benchmarks for language agents, however, are restricted to limited domains and tools (Mialon\net al., 2023; Liu et al., 2024; Ma et al., 2024; Jimenez et al., 2024; Zhou et al., 2024; Yao et al., 2025;\n\n\u2217 Equal Contribution. \u2020Corresponding author.\n\n1\n\n \n \n \n \n \n \n\fToolathlon\n\nFigure 1: Two examples and the initial environment states in TOOLATHLON. We showcase real-world environ-\nment interaction (\u00a72.2) and realistc state initialization (\u00a72.3) here.\n\nWei et al., 2025; Xu et al., 2025). By contrast, real-world tasks often require switching across various\napplications. For example, as demonstrated in Figure 1 (Example #2), a company\u2019s administrative\nagent may need to monitor a real Snowflake database for customer tickets, locate the appropriate PDF\noperation manual containing instructions on how to identify and handle overdue tickets, and then\nsend the required emails to managers and customers in accordance with the manual. Importantly,\nthis diversity gap extends far beyond differences in tool names or descriptions. The diversity and\ncomplexity of environment states across applications, compounded by interaction with them in long\ntrajectories, present substantial challenges for generalization.\n\nTo address these challenges, we introduce the Tool Decathlon (TOOLATHLON), a benchmark for\nevaluating language agents on diverse, realistic, and long-horizon tasks. TOOLATHLON spans 32\nreal-world applications and 604 tools across 108 tasks, covering a wide spectrum of domains ranging\nfrom daily affair and education to technology and finance. Tasks are grounded in realistic scenarios\nand mostly require coordinating multiple applications. Each task is fully verifiable with a dedicated,\ndeterministic evaluation script, comparing outcomes against either static or dynamically generated\nground-truth states (e.g., tasks involving the latest NVIDIA shareholder information or real-time train\nschedules). All tools in TOOLATHLON are sourced from the real world, with the majority obtained\nfrom MCP servers.\n\nTo faithfully capture the realism and complexity of practical environment states, we tried to adopt\nthe most representative applications such as Google Sheet, Gmail, and Snowflake. However, some\nremote environment states are difficult to set up to mimic real scenarios. For example, simulating a\nCanvas course with tens of students would require registering a real account for each student and\nresetting the states at each evaluation run. Therefore, while we adopt the commonly used applications\nmost of the time, we also incorporate several open-source software deployed locally via containers for\nconvenient and complex environment simulation, such as poste.io for email management to replace\nGmail and WooCommerce for online ecommerce platform to replace Shopify. These services provide\ncomplex observations while allowing us to set up the states in a scalable way. This stands in stark\ncontrast with simplified or artificial environment states as in prior benchmarks (Patil et al., 2025).\nIn addition, task prompts in TOOLATHLON are crafted to mirror authentic user queries, which are\noften concise and fuzzy. Models must therefore infer user intent and autonomously devise plans to\naccomplish tasks, an example is shown in Figure 3.\n\nConcurrent with this work, several MCP-based tool-use benchmarks have emerged (Liu et al., 2025;\nMo et al., 2025; Yan et al., 2025; Yin et al., 2025; The MCPMark Team, 2025), but they do not\nmatch TOOLATHLON in its reflection of real-world complexity. Some rely on LLM judges without\n\n2\n\nTask Prompt #1:  Your task is to check your email for homework2 submissions and grade on Canvas. Please download Python \ufb01les from email attachments to local workspace and execute each Python \ufb01le in terminal to check for errors. If the Python \ufb01le is correct, give it a score of 10 in Canvas; otherwise, give it a score of 0. You could check the requirements of homework2 in `assignments/homework2.md`and students' ID in `student_canvas_ids.csv`. For students who submitted multiple times, use the latest submission. FilesystemFilesystemPDFTerminalEmail (Poste.io)Canvas-LMSTask Prompt #2: Identify the tickets in the database that have exceeded the initial response time according to the relevant documentation, and send reminder emails, based on the templates mentioned in the manual, to the respective responsible managers, as well as apology emails to all involved users.SnowflakeEmail (Poste.io)\fToolathlon\n\nTable 1: Comparison of Tool-Based Language Agent Benchmarks. \u201c# Apps\u201d denotes the number of MCP servers,\nwhich we do not annotate for benchmarks without clear application definition. \u201cAvg # Turns\u201d denotes the number\nof tool calling turns made by Claude-4-Sonnet, which we use as a proxy for task complexity. \u201cReal States & Init\u201d\n(\u00a72.2, \u00a72.3) means the environment states and observations are from real-world software rather than artificial\ndatabases, and evaluation begins with a realistic state initialization. \u201cVerifiable Execution\u201d (\u00a72.4) denotes that\nmodels need to execute the tools and final results are evaluated based on states. \u201cFuzzy Prompt\u201d represents that\nthe task instructions are often fuzzy and ambiguous to mimic real user input (\u00a73.1). \u2217For MCPUniverse, only\n10% of the tasks involve multiple applications. In-depth discussion of these related works is in Appendix 6.\n\nBenchmark\n\n# Tasks\n\n# Apps\n\nAvg #\nTurns\n\nReal\nStates & Init\n\n\u03c4 -Bench\nBFCLv3-MT\nACEBench\nAppWorld\nMCPWorld\nMCP-RADAR\nMCPEval\nLiveMCPBench\nMCP-AgentBench\nLiveMCP-101\nMCPAtlas\nMCPUniverse\nMCPMark\nGAIA2\n\nTOOLATHLON\n\n165\n800\n2000\n750\n201\n300\n676\n95\n600\n101\n1000\n231\n127\n800\n\n108\n\n2\n\u2013\n\u2013\n9\n10\n9\n19\n70\n33\n41\n40+\n11\n5\n12\n\n32\n\n\u2013\n3.8\n1.7\n\u2013\n\u2013\n\u2013\n\u2013\n5.6\n\u2013\n5.4\n3-6\n7.5\n18.5\n22.5\n\n26.8\n\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u2713\n\nVerifiable\nExecution\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\n\u2713\n\nCross-App\nTask\n\nFuzzy\nPrompt\n\n\u00d7\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nPartial\u2217\n\u00d7\n\u2713\n\n\u2713\n\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\n\u2713\n\nverifiable tasks (Mo et al., 2025; Yin et al., 2025), while others cover few domains or mostly single-\napplication tasks. For instance, MCPUniverse (Luo et al., 2025) spans only six domains, with 90%\nof tasks involving one app and synthetic initial states, yielding simplified, short interactions (<8\nturns). Similarly, MCPMark (The MCPMark Team, 2025) includes only five apps and overly detailed\nprompts (Figure 3). GAIA2 (Andrews et al., 2025) covers merely the mobile domain on mostly daily\ntasks with simplified synthetic environments. A full comparison is shown in Table 1.\n\nTOOLATHLON includes a lightweight framework for automated, safe, and scalable evaluation. Each\ntask comes with initial states setup if needed as well as an evaluation script (Figure 2). Executing\nand evaluating each task is isolated in separate containers to prevent interference. This enables fast\nparallel evaluation\u2014for example, running Claude-4.5-Sonnet on all 108 tasks takes only 70 minutes\nusing 10 parallel processes. With extensive experiments on TOOLATHLON, the best-performing\nmodels, Claude-4.5-Sonnet, achieve only 38.6% accuracy, highlighting the unique challenges posed\nby TOOLATHLON. DeepSeek-V3.2-Exp (DeepSeek-AI, 2025) achieves 20.1% success rate as the\nbest performer among open-source models. Further analysis reveals that weaknesses in long-context\nmodeling and robust tool calling error tracking are major challenges for all evaluated models. We have\nfully open-sourced the benchmark and the TOOLATHLON environment, aiming for TOOLATHLON to\naccelerate the development of practical language agents.\n\n2 THE TOOLATHLON ENVIRONMENT AND EVALUATION FRAMEWORK\n\n2.1 TASK DEFINITION\n\nEach task in TOOLATHLON can be formulated as a partially observable Markov decision process\n(POMDP) (S, A, O, T , R, U) with state space S, action space A, observation space O, transition\nfunction T : S \u00d7 A \u2192 S \u00d7 O, reward function R : S \u2192 [0, 1], and instruction space U. The\nenvironment states (\u00a72.2, \u00a72.3) can be the status in the current email inbox and the observations are\nthe sequential input to the model. The action space A is the available tools for the respective task\nand the tool implementation directly defines the transition function. The reward function R (\u00a72.4)\nrepresents our execution-based evaluation which directly evaluates the environment state. Intuitively,\nreal-world tools and environments will yield significantly more complex and diverse environment\nstates and observations than the synthetic ones, and in the following sections, we will detail our\ndesigns of these variables in TOOLATHLON.\n\n3\n\n\fToolathlon\n\n2.2 TOOLS, ENVIRONMENTS, AND FRAMEWORK\n\nMCP Servers:\nIn TOOLATHLON, we source our tools through a variety of MCP servers. Specifi-\ncally, we first decide a list of valuable and common real applications that we aim to benchmark on,\nthen we see if we can find the corresponding open-source MCP servers for them. If not, we implement\nthe MCP servers by ourselves. Notably, many open-source MCP server implementations contain\nbugs or exhibit certain limitations, for example, without the tools needed to complete our tasks. We\nfurther refine and improve these implementations ourselves. This way, we obtain a high-quality set\nof 32 MCP servers in total, where we include a complete list and their sources in Appendix A. The\napplications span diverse domains, extending well beyond common daily-use applications such as\nGoogle Maps, Notion, and Google Calendar, and we also incorporate a number of professional and\ndomain-specific applications to evaluate language agents in high-value productivity scenarios, such\nas Snowflake for enterprise data management and Kubernetes for cluster management. Although\nthe majority of tools are sourced from MCP servers, the benchmark usage itself is not tied to MCP\nemployment from the model developer side. For examples, these tasks can also been solved via pure\nGUI or CLI workflow, as long as certain account information like usernames, passwords, tokens or\ncredentials are explicitly given to the agents.\n\nRemote and Locally Containerized Environments: While tools provide an interface for inter-\nacting with environments, they do not directly constitute the environments. Many real-world tools\ninteract directly with existing, remote environments, such as Google Sheets, Google Calendar, No-\ntion, and Gmail. Although remote environments require no implementation effort, they introduce\nsignificant challenges when benchmarking tasks that involve modifying environment states. For\ninstance, simulating a realistic Gmail inbox with hundreds of emails from diverse senders would\nrequire registering hundreds of Google accounts for every benchmark user, and this inbox would need\nto be reset prior to each evaluation run. Previous works have attempted to bypass this issue by only\nsupporting read operation to the states (Mialon et al., 2023), or implementing simplified synthetic\ndata structures to mimic environment states (Patil et al., 2025; Yao et al., 2025), but such approaches\ndrastically reduce realism and fail to reflect the complexity of real software environments. In contrast,\nin TOOLATHLON we leverage both remote environments and locally containerized, open-source\napplications. Specifically, we deploy the open-source Poste.io for email management, Canvas for\ncourse administration, Kubernetes for cluster orchestration, and WooCommerce for e-commerce\nmanagement. By hosting these realistic applications locally within containers, we can efficiently set\nup dozens of accounts and initialize complex environment states during evaluation. Compared with\nexisting dedicated agent sandboxes such as SWE-Bench (Jimenez et al., 2024), our environments are\nmore diverse and encompass a wider range of software.\n\nAgent Framework: We implement a simple agent framework based on the OpenAI Agents SDK\n(v0.0.15) 1 to conduct the agent action loop \u2013 at each turn, the model is expected to (optionally)\nreason explicitly and make tool calls. We make several enhancements to improve its basic setup for a\nmore robust workaround to evaluate language agents, including tool error handling, overlong tool\nresponse handling and context history management. We also equip this framework with some basic\nyet common local tools like python execution, web search, claim done and sleep. The details can be\nfound in Appendix B.\n\n2.3\n\nINITIAL STATE SETUP\n\nIn real world, tasks are rarely executed from an empty environment state (e.g., an empty inbox).\nInstead, agents are typically required to operate based on pre-existing environment states. In agentic\nscenarios, task difficulty is determined not only by the task instructions but also by the underlying\nenvironment states. For example, operating on a folder with only one file to be used is easier than\nworking with 10 mixed useful and unrelated files (Figure 1, example #2), even if the task descriptions\nare nearly identical. To capture this, for tasks in TOOLATHLON that starts with an initial state,2,\neach of these tasks is equipped with a state initialization script to set up the states at running time,\nor (and) an initial workspace directory containing pre-set files. Figure 1 and Figure 3 showcase\nsuch initial states. When constructing these initial environment states, we design them to closely\n\n1https://github.com/openai/openai-agents-python.\n2As shown in Table 2, 67% of the tasks fall into this category.\n\n4\n\n\fToolathlon\n\nFigure 2: Overview of the TOOLATHLON evaluation framework.\n\nreflect realistic scenarios. Notably, only very few previous benchmarks have incorporated realistic\ninitial state construction before entering the agent loop, as summarized in Table 1. By contrast, most\nexisting benchmarks start from empty state or overly simplified environment states, thus failing to\ncapture the full complexity of real-world task execution.\n\n2.4 RELIABLE EXECUTION-BASED EVALUATION\n\nFirst, unlike some traditional tool-calling benchmarks that measure single-step tool call accuracy\ngiven a fixed context without actual execution (Patil et al., 2024), we think that execution-based\nevaluation is essential for reliably assessing language agents in realistic scenarios. Second, while\nmany existing benchmarks rely on LLMs as judges to score agent trajectories (Gao et al., 2025; Yin\net al., 2025), we contend that verifying the final environment states using deterministic rules offers a\nfar more reliable and reproducible evaluation framework, as demonstrated in several widely adopted\nagent benchmarks (Zhou et al., 2024; Xie et al., 2024; Jimenez et al., 2024). To achieve this, each task\nin TOOLATHLON is equipped with a unique, manually crafted evaluation script that ensures precise\nand consistent measurement of task success. The script may perform robust matching against a static\nsnapshot of the ground-truth environment or follow a reference execution workflow to dynamically\nretrieve and match real-time information (e.g., NVIDIA shareholders). During evaluation, each task\nis associated with a configuration file that specifies the MCP servers and tools available for use.\nIntuitively, providing the model with a larger set of unrelated tools increases task difficulty, as the\nagent must identify the relevant tools while ignoring distracting ones.\n\nSafe and Efficient Parallel Evaluation in Containers: Our TOOLATHLON evaluation framework\nsupports parallel execution to enable efficient model evaluation. Our framework launches each task\ninside a separate container in parallel, providing strict workspace isolation. On a standard Ubuntu\n24.04 Linux cluster with 16 CPUs and 64 GB of memory, we are able to evaluate Claude-4.5-Sonnet\non 108 tasks in just about 70 minutes of wall time using only 10 parallel processes. This demonstrates\nthat TOOLATHLON is both convenient and efficient for practical use by model developers to get\ninstant feedback on how their models perform in realistic scenarios and requirements.\n\n3 THE TOOLATHLON TASKS\n\n3.1 TASK SOURCING AND FUZZY TASK INSTRUCTION\n\nThe authors of this work, who are researchers and senior undergraduate students in computer science,\nsource and implement the tasks. We carefully design and adhere to several principles when collecting\ntasks: (1) Real User Demands: All tasks are either directly sourced from real-world websites or\ncrafted to reflect genuine user demands. (2) Multi-App Orchestration: We intentionally source\ntasks that require interaction with multiple MCP servers, as this reflects authentic human workflows\nand increases task complexity. (3) Diversity: To ensure broad task diversity, we adopt a two-stage\nsourcing process. In the first stage, we start with an initial MCP server list covering more than 50\napplications and freely source tasks without restricting to specific servers. In the second stage, we\nanalyze the distribution of the sourced tasks and identify Apps that are important but underrepresented.\nWe then conduct an additional round of targeted task sourcing specifically for them.\n\n5\n\nLanguage AgentsTask InstructionRealistic State InitializationReal-World Software EnvironmentsMCP & Local ToolsTool CallTool OutputsExecuteState-based Evaluation\fToolathlon\n\nFigure 3: Example task instructions from our benchmark (Left) and MCPMark (The MCPMark Team, 2025)\n(Right). Ours contain more fuzzy intent that the model need to infer from the environment states.\n\nRealistic Fuzzy Task Instruction: We de-\nsign task instructions to resemble authentic\nuser input, which is often fuzzy or ambigu-\nous, but whose actual intent can be deter-\nministically inferred from the environment\u2019s\nstates (e.g., existing data examples or docu-\nment templates). This requires the agent to\ninfer the user\u2019s intent from the environment\nstate, formulate plans, execute them, and in-\ntellectually handle unexpected events such as\ntool call errors. For example, as shown in Fig-\nure 3 Left, a real user may simply say \u201cPlease\nupdate the candidate information on HR\nRecord subpage according to all the\nresumes...... if the position applied\nfor by the applicant is currently not\nopen, ......\u201d This is a fuzzy instruction\nwithout specifying in which format the agent\nshould fill in the information, but the Notion\ndatabase has provided some examples that the\nagent needs to know to check itself. Also, the\ninstruction does not mention where to find the status of the posted job and the agent needs to check\nNotion to find that by itself. In contrast, task instructions in some existing benchmarks (Figure 3\nRight) explicitly include detailed step-by-step plans, which reduce the role agents for planning. More\nexamples of this kind are shown in Figure 10 and 11.\n\nFigure 4: Task topic distribution of TOOLATHLON.\n\nAll the sourced tasks experience multiple rounds of rigorous quality check, filtering and refinement\nwhich last for several weeks before we implement them into our benchmark, and finally we obtain\n108 tasks in total. The topic distribution of all tasks is shown in Figure 4 and Table 2 show some key\nstatistics of the complete benchmark.\n\n3.2 TASK IMPLEMENTATION\n\nAs described in \u00a72.4, each task in our bench-\nmark is fully implemented with a correspond-\ning evaluation script and potential initial states\nsetup. This process involves collecting ground-\ntruth states statically or dynamically, and design\nscripts to automatically clear and re-fill new ini-\ntial states. To ensure realistic setups and reli-\nable evaluation, implementing a single task in\n\n6\n\nTable 2: Key statistics of TOOLATHLON.\n\nStatistics\n\nValue\n\n# MCP servers (# tools)\n# Local toolkits (# tools)\nAvg/Min/Max tools per task\nTasks with state initialization\n\n32 (604)\n7 (16)\n69.9/28/128\n72/108 (67%)\n\n\u2026 23 more positions & other distractors blocksCreate a comprehensive weekend adventure planner that analyzes the Toronto Guide databases and generates a structured itinerary page. I need you to create a new page called 'Perfect Weekend Adventure' as a child of the main Toronto Guide page.Task Requirements:  1. Create a new page titled 'Perfect Weekend Adventure' as a child page of the main Toronto Guide page  2. Query the Activities database to identify all activities that have the \"Beaches\" tag  3. Query the Food database to find all restaurants with \"Turkish\" or \"Hakka\" tags  4. Query the Cafes database to retrieve all cafes entries  5. Structure the page with the following specific format:    - [SOME SPECIFIC REQUIREMENTS]  6. After the summary paragraph, add a divider block  7. Finally, add a callout block with the \ud83d\udca1 emoji containing the text: \"Pro tip: Check the Seasons database for the best time to enjoy outdoor activities!\"  8. Ensure all headings use the exact emoji and text format specified above  9. The lists must be in the exact format specified (bulleted for beaches, numbered for restaurants, to-do for cafes)Please update the candidate information on HR Record subpage of Notion according to all the resumes in my workspace. All information must be filled out strictly according to the content in the resumes, without making any unauthorized modifications or adding/removing any words. Also, please delete the existing sample entries in the record table. At the same time, if the position applied for by the applicant is currently not open for recruitment, please send an email to the corresponding applicant using the following template information (including line breaks). Do not send the email by mistake: [A TEMPLATE HERE]Clear Step by Step Guides  Infer from  # Head CountInfer from  existing examplesFind needed info by agentInfer from   resumesLiterature ReviewManuscript EditingScholarly ActivitiesProfileManagementCourseSchool ApplicationTeachingAcademic AffairsQuantitativeStrategyStock TradingMarketingAnalyticsDeploymentDevOps SweepData CurationOSS EngagementProject SyncExperimentAnalysisData WranglingTalentManagementData AuditingFinancial AdminHealth GuidanceTravel HelperSocial MediaTransportationPersonal AffairSports AnalysisOnline PurchaseProductAssortmentInventory ControlDTC ManagementResearch(13.9%)Campus(16.7%)Finance(9.3%)Tech(17.6%)Business(16.7%)Daily(15.7%)E-commerce(10.1%)\fToolathlon\n\nTable 3: Main results for all the models. P@1, P@3, P\u02c63 and # Turns represents Pass@1, Pass@3, Pass\u02c63 and\naverage numbers of turns, respectively. We make bold the highest score.\n\nModel\n\nResearch Campus Finance Tech Business Daily E-com P@1 P@3 P\u02c63 # Turns\n\nClaude-4.5-Sonnet\nGPT-5\nClaude-4-Sonnet\nGPT-5-high\nGrok-4\nClaude-4.5-Haiku\nGrok-code-Fast-1\nGrok-4-Fast\no3\no4-mini\nGPT-5-mini\nGemini-2.5-Pro\nGemini-2.5-Flash\n\nDeepSeek-v3.2-Exp\nGLM-4.6\nQwen-3-Coder\nKimi-k2-0905\n\n31.1\n20.0\n33.3\n15.6\n24.4\n11.1\n20.0\n15.6\n15.6\n13.3\n11.1\n4.4\n4.4\n\n11.1\n22.2\n4.4\n8.9\n\n42.6\n33.3\n33.3\n31.5\n22.2\n22.2\n14.8\n22.2\n14.8\n11.1\n16.7\n3.7\n3.7\n\n16.7\n18.5\n16.7\n22.2\n\nProprietary Models\n\n33.3\n13.3\n30.0\n23.3\n13.3\n26.7\n16.7\n16.7\n10.0\n20.0\n20.0\n3.3\n6.7\n\n42.1\n40.4\n26.3\n29.8\n43.9\n29.8\n19.3\n24.6\n22.8\n17.5\n15.8\n15.8\n3.5\n\n42.6\n38.9\n25.9\n44.4\n24.1\n27.8\n14.8\n14.8\n13.0\n11.1\n9.3\n5.6\n1.9\n\nOpen-Source Models\n\n23.3\n20.0\n10.0\n16.7\n\n19.3\n17.5\n19.3\n14.0\n\n14.8\n16.7\n14.8\n5.6\n\n35.3\n39.2\n33.3\n33.3\n27.5\n37.3\n21.6\n13.7\n29.4\n21.6\n21.6\n27.5\n3.9\n\n29.4\n11.8\n17.6\n9.8\n\n38.6\u00b12.7 51.9 20.4\n39.4\n12.1 30.6\u00b11.5 43.5 16.7\n27.3 29.9\u00b11.6 41.7 17.6\n15.2 29.0\u00b13.1 42.6 16.7\n30.3 27.5\u00b11.7 38.9 16.7\n27.3 26.2\u00b11.9 39.8 13.0\n24.2 18.5\u00b12.0 30.6 9.3\n21.2 18.5\u00b12.0 32.4 5.6\n17.0\u00b10.9 25.0 9.3\n6.1\n14.8\u00b10.8 26.9 3.7\n9.1\n14.5\u00b11.2 23.1 5.6\n6.1\n10.5\u00b11.9 21.3 2.8\n9.1\n3.7\u00b11.5 8.3\n0.0\n3.0\n\n30.3 20.1\u00b11.2 27.8 12.0\n30.3 18.8\u00b12.2 29.6 9.3\n15.2 14.5\u00b11.9 21.3 6.5\n15.2 13.0\u00b12.0 22.2 5.6\n\n20.2\n18.7\n27.3\n19.0\n20.3\n21.9\n20.2\n15.9\n19.4\n16.6\n19.7\n26.5\n8.3\n\n26.0\n27.9\n28.5\n26.6\n\nTOOLATHLON requires, on average, 4\u20136 hours of work by a research graduate student majoring in\ncomputer science.\n\nFinalizing Tasks and Quality Check: After crowd-sourcing task implementations from multiple\ncontributors, we perform intensive quality checks conducted by 5\u20136 experienced authors. ",
            "char_count": 26968,
            "word_count": 4029
          },
          {
            "chunk_id": 2,
            "text": "In this\nstage, each task is carefully reviewed and revised to unify standards across all tasks and ensure\ncorrectness, solvability, and unambiguity, which requires approximately 5 hours of labor per task per\nround of checking. Once all tasks are finalized, we perform an additional round of comprehensive\ncross-checking and bug fixing of the entire benchmark before running the final experiments.\n\n4 EXPERIMENT\n\nIn this section, we present the configuration details and experimental settings for several leading\ncommercial and open models on TOOLATHLON, as well as their performance.\n\n4.1 SETUP\n\nModels and Configuration: Our evaluation includes the leading commercial model series in\nterms of agentic abilities, such as GPT-5(-mini) (OpenAI, 2025b), o3&o4-mini (OpenAI, 2025a),\nClaude-4-Sonnet (Anthropic, 2025a), Claude-4.5(-Sonnet,-Haiku) (Anthropic, 2025c;b), Gemini 2.5(-\nPro,-Flash) (Comanici et al., 2025), Grok-4(-Fast) (xAI, 2025a;b), Grok-Code-Fast-1 (xAI, 2025c).\nWe also benchmark the best-performing open-weight models including Qwen-3-Coder (Qwen Team,\n2025), DeepSeek-V3.2-Exp (DeepSeek-AI, 2025), Kimi-K2-0905 (Kimi Team et al., 2025) and\nGLM-4.6 (Zhipu AI, 2025). As described in \u00a72.4, each task is preconfigured with a list of MCP\nservers and common tools to access. During evaluation, we set the maximum allowable number of\nturns as 100 for all models. In our main evaluation setup, we only provide the models with the MCP\nservers and common tools that are useful for executing the task. We note that as each MCP server\nis equipped with multiple related tools, so the models will still see many unnecessary tools during\nevaluation.\n\nMetrics: We evaluate each model three times and report the average pass@1 success rate as well as\nthe standard deviation. We also include the pass@3 \u2013 the fraction of tasks with at least one correct\ntrajectory, and pass\u02c63 (Yao et al., 2025) \u2013 the fraction of tasks where all three trajectories are correct,\nto measure the model\u2019s potential capability coverage and its ability to complete tasks reliably. We\nalso report the average number of turns used.\n\n7\n\n\fToolathlon\n\nFigure 5: Two kinds of tool calling error presence ratios in calling tools for different models.\n\n4.2 MAIN RESULTS\n\nResults in Table 3 show that Claude-4.5-Sonnet ranks first, but still achieves a success rate of less\nthan 40%. GPT-5, Claude-4-Sonnet, Grok-4, and Claude-4.5-Haiku, each with Pass@1 scores above\n26% but below 30%, clearly fall into the second tier. All other models remain at 20% or below.\nThis indicates that our benchmark remains challenging for state-of-the-art models and effectively\ndistinguishes their capabilities. For open-source models, scores are less than or equal to 20%, with the\nbest, DeepSeek-V3.2-Exp, achieving 20.1%, revealing a clear gap compared with proprietary models.\nInterestingly, increased reasoning effort for thinking-oriented models (e.g., GPT-5 vs. GPT-5-high)\nshows no benefit, suggesting that exploring new observations matters more than extended internal\nreasoning in agentic tasks. We also find that Gemini-2.5\u2019s ability to understand requirements and\nproactively explore is insufficient\u2014it may neglect certain requirements or give up prematurely during\nexecution, resulting in poor performance on complex tasks.\n\nLooking at performance across task categories, Claude-4.5-Sonnet excels in almost all domains,\nespecially in Campus and E-Commerce tasks, demonstrating strong general capabilities across diverse\ntools. GPT-5 performs exceptionally well in Daily tasks, showcasing its effectiveness in everyday\nscenarios, while Grok-4 stands out in Tech, indicating a specialized strength in technology-related\ndevelopment operations. We also observe significant differences between Pass@3 and Pass\u02c63 success\nrates. This indicates that while many models have certain capability coverage, they lack consistency\nin producing reliable results. For real-world tasks, building agents with both high success rates and\nrobust consistency remains a critical challenge.\n\n5 ANALYSIS\n\nIn this section, we conduct analysis in depth to better understand model performance in TOOLATHLON,\nfocusing on tool-call errors, as well as long-context and overlong-output challenges. More analysis,\nincluding how tool error and the involvement of unrelated MCP servers impact model performance,\nand qualitative analysis & case studies, can be found in Appendix C and E.\n\n5.1 THE FAILURE OF CALLING TOOLS\n\nWe mainly focus on two major tool-calling er-\nrors: hallucinating non-existing tools (e.g., in-\ncorrect tool names) and errors raised during tool\nexecution. Statistics for different models on\nthese two types of errors are shown in Figure\n5. It can be seen that all models produce tool\nexecution errors to varying degrees, possibly\ndue to incorrect parameter passing or attempts\nto access non-existent resources. However, we\nfound no significant correlation between overall\nsuccess rate and the frequency of such errors. In\nfact, error messages from tools may help models\nunderstand the tool implementation or structure,\nallowing adjustments in subsequent turns. The\nother type of error\u2013incorrect tool names\u2013more\nlikely affects final scores. Leading models produce few tool name errors. In Appendix C, Figure\n\nFigure 6: Model performance on three groups of tasks\ndivided by average turns. The x-axis represents different\ntask difficulty groups determined by different avg turns\nrange [Min Turns, Max Turns]\n\n8\n\nGPT-5-highKimi-K2-0905GLM-4.6Claude-4.5-HaikuDeepSeek-V3.2-ExpGPT-5Grok-4Claude-4-SonnetGemini-2.5-ProGPT-5-miniGrok-Code-Fast-1Qwen-3-CoderGrok-4-FastClaude-4.5-Sonneto3Gemini-2.5-Flasho4-mini01020Error Presence (%)Wrong Tool NameTool Call ErrorPass@1010203040Pass@1 (%)Easy[4.2, 15.6]Medium[15.7, 23.8]Hard[24.1, 52.7]01020304050Success Rate (%)45%32%37%9Turns19Turns31Turns41%23%26%9Turns15Turns30Turns21%20%13%9Turns13Turns24Turns26%23%10%12Turns26Turns39Turns21%11%11%17Turns23Turns44TurnsClaude-4.5-SonnetGPT-5Grok-4-FastDeepSeek-V3.2-ExpQwen-3-Coder\fToolathlon\n\n9, we further analyze the success rate difference between trajectories containing tool-calling errors\nversus error-free trajectories, showing that most models do suffer from tool call errors.\n\n5.2 THE LONG-CONTEXT CHALLENGES FOR LANGUAGE AGENTS\n\nSince our benchmark is built on real tools and environments,\nit naturally generates many long-horizon trajectories. ",
            "char_count": 6391,
            "word_count": 860
          },
          {
            "chunk_id": 3,
            "text": "To quan-\ntitatively describe the differences between tasks, we calculate\nthe average number of execution turns for each task across all\nmodels, and use this as a proxy to divide all tasks into three\nequally sized groups: Easy, Medium, and Hard, with execu-\ntion turns increasing with difficulty. In Figure 6, we show the\nperformance of five representative models on different groups,\nalong with their average turns in each group. The results indi-\ncate that groups with higher average turns generally have lower\nsuccess rates across models, and leading models like Claude-\n4.5-Sonnet maintain clear advantages in all groups. We also\nfind that there is no significant difficulty difference between the\nMedium and Hard groups, and that even Claude-4.5-Sonnet and\nGPT-5 achieve higher scores on the Hard subset then in Medium\nones. This suggests that our benchmark\u2019s difficulty does not\nentirely stem from standard multi-step long-horizon execution,\nbut possibly from models ending tasks prematurely without\nsufficiently exploring available observations, leading to failure.\n\nFigure 7: Avg. Success Rate on Trajec-\ntories w/wo overlong tool outputs.\n\nAnother concern is whether models can successfully complete tasks when encountering overlong\ntool outputs, like fetching lengthy HTML source code or directly listing all data from a database\n(we refer the readers to Appendix B for handling overlong outputs in our framework). We calculate\nthe proportion of trajectories containing overlong tool outputs encountered by all models during\nevaluation, as well as each model\u2019s success rates with and without overlong tool outputs. Results\nshow that the proportion of overlong tool outputs varies from approximately 15% to 35% across\ndifferent models. Additionally, Figure 7 shows that most models experience a decline in success rate\nwhen encountering overlong tool outputs, with only a few models maintaining nearly unchanged\nperformance. While tasks with overlong outputs are often logically straightforward (e.g., price\ncomparison, data extraction), most models get trapped trying to process these lengthy outputs.\n\n5.3 THE RELATIONSHIP BETWEEN PERFORMANCE AND EXPENSES\n\nSince we evaluate the models in realistic settings, both cost and token usage are important factors, as\nthey determine how a model should be selected for different budget constraints. Therefore, during the\nevaluation period, we measure the actual number of output tokens and the associated cost with prompt\ncaching enabled. For costs, Figure 8 Left shows that Claude-4-Sonnet and Grok-4 incur relatively\nhigh expenses, whereas most other models remain under $1 per task. Claude-4.5-Sonnet achieves the\nhighest performance but ranks third in cost. Several models, such as Grok-4-Fast, Grok-Code-Fast-1,\nand DeepSeek-V3.2-Exp, incur only a small cost, suggesting that they can serve as strong alternatives\nunder limited budgets without an extreme pursuit of maximum performance.\n\nWe also plot the output token count distribution in Figure 8 Right, which illustrates how the success\nrate varies with different average output token counts. Most models cluster between 5K and 10K\noutput tokens. Some reasoning-focused models, such as o4-mini and GPT-5(-high), generate more\ntokens, whereas the Claude series and Grok-4 achieve strong results with fewer tokens, suggesting\nthey rely more on environment observation rather than extensive internal reasoning. Models like\nGemini-2.5-Flash have the lowest output token counts and correspondingly lower accuracy, while\nothers exhibit a similarly concentrated distribution.\n\n6 RELATED WORK\n\nBenchmarks for tool-based language agents differ substantially in the realism of their tools, environ-\nments, and task configurations, and can be viewed along a spectrum from fully simulated settings\n\n9\n\n010203040Success Rate - No Overlong Tool Output (%)010203040Success Rate - With Overlong Tool Output (%)GPT-5GPT-5-highGPT-5-minio3o4-miniGrok-4Grok-4-FastGrok-Code-Fast-1Gemini-2.5-ProGemini-2.5-FlashClaude-4-SonnetClaude-4.5-SonnetClaude-4.5-HaikuDeepSeek-V3.2-ExpKimi-K2-0905GLM-4.6Qwen-3-Coder\fToolathlon\n\nFigure 8: The relationship between average task success rate and average cost (Left) and output tokens (Right).\n\nto those grounded in real-world applications. At one end of this spectrum, several works evaluate\ntool use purely through simulation, without executing real APIs or interacting with actual application\nbackends. Representative examples include \u03c4 -Bench (Yao et al., 2025), BFCL (Patil et al., 2025), and\nACEBench (Chen et al., 2025), which assess function calling accuracy or multi-turn tool selection in\ncontrolled scenarios, but rely on mock implementations or language-model-based emulation. While\nsuch designs enable efficiency and reproducibility, they omit many of the challenges that arise from\nexecuting real tools in unpredictable environments.\n\nMoving beyond simulated tools, other benchmarks connect agents to real APIs yet operate in\nsynthetic or constrained environments where initial states are artificially constructed. For exam-\nple, AppWorld (Trivedi et al., 2024) offers a high-fidelity simulation of multiple apps, and MCP-\nWorld (Yan et al., 2025), MCP-RADAR (Gao et al., 2025), MCPEval (Liu et al., 2025), and MCP-\nAgentBench (Guo et al., 2025) grant access to real Apps via Model Context Protocol (MCP) (An-\nthropic, 2024) but often begin from zero or artificially designed states or center on single-application\ntasks. These setups capture tool execution more faithfully than pure simulation, yet still fall short of\nrepresenting the complexity of authentic, multi-application workflows.\n\nCloser to realistic settings, a number of recent benchmarks combine real tools with more authentic\nenvironment conditions. LiveMCPBench (Mo et al., 2025), LiveMCP-101 (Yin et al., 2025), MCPAt-\nlas (Scale AI, 2025), MCPUniverse (Luo et al., 2025), and MCPMark (The MCPMark Team, 2025)\nintroduce production-grade MCP servers, multi-step workflows, and realistic tool outputs. Neverthe-\nless, they remain limited in diversity of domains, the realism of environment state initialization, or\nthe naturalness of task instructions\u2014many lack genuinely ambiguous or underspecified prompts that\nmimic real user requests.\n\nOur work, TOOLATHLON, advances this trajectory by combining real tools with genuinely realistic\nenvironments across 32 applications and 604 tools, spanning a broad range of domains. Initial states\nare grounded in authentic usage scenarios rather than synthetic constructs, and tasks often require\nlong-horizon, cross-application orchestration. Moreover, prompts are intentionally concise and fuzzy,\ncompelling agents to infer intent and autonomously plan, while deterministic, script-based evaluation\nensures correctness in evaluation.\n\n7 CONCLUSION\n\nWe introduce TOOLATHLON, a comprehensive benchmark for evaluating language agents on real-\nworld, long-horizon tasks spanning 32 applications and 604 tools. Our evaluation reveals significant\nlimitations in current models, with the best-performing Claude-4.5-Sonnet achieving only 38.6% suc-\ncess rate, highlighting substantial room for improvement in handling complex multi-step workflows.\nThrough detailed analyses, we identified key challenges including long context handling, tool-calling\nerrors, and the need for greater robustness in execution. We believe TOOLATHLON will drive the\ndevelopment of more capable and robust language agents for practical real-world deployment.\n\nACKNOWLEDGMENT\n\nWe thank Pengcheng Yin for helpful discussion on this project.\n\n10\n\n101100average cost per task ($, log scaled)010203040pass@1 (%)GPT-5GPT-5-HighClaude-4-SonnetClaude-4.5-SonnetClaude-4.5-HaikuDeepSeek-V3.2-ExpGemini-2.5-ProKimi-K2-0905GLM-4.6Gemini-2.5-FlashGPT-5-minio3o4-miniGrok-4Grok-Code--Fast-1Grok--4-Fast05K10K15K20K25K30Kaverage output tokens per task010203040pass@1 (%)GPT-5GPT-5-highClaude-4-SonnetClaude-4.5-SonnetClaude-4.5-HaikuDeepSeek-V3.2-ExpGemini-2.5-ProKimi-K2-0905GLM--4.6Qwen-3--CoderGemini-2.5-FlashGPT-5-minio3o4-miniGrok-4Grok-Code-Fast-1Grok-4--Fast\fToolathlon\n\nREFERENCES\n\nPierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja,\nRicardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo\nLaurenc\u00b8on, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre M\u00b4enard, Gr\u00b4egoire Mialon,\nUlyana Piterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav\nVorotilov, Mengjue Wang, and Ian Yu. ARE: Scaling up agent environments and evaluations, 2025.\n",
            "char_count": 8566,
            "word_count": 1123
          },
          {
            "chunk_id": 4,
            "text": "URL https://arxiv.org/abs/2509.17158.\n\nAnthropic.\n\nIntroducing the model context protocol.\n\nhttps://www.anthropic.com/news/\n\nmodel-context-protocol, 2024.\n\nAnthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, 2025a.\n\nAnthropic. https://www.anthropic.com/news/claude-haiku-4-5. https://www.anthropic.com/\n\nnews/claude-haiku-4-5, 2025b.\n\nAnthropic.\n\nIntroducing claude\n\nsonnet 4.5.\n\nhttps://www.anthropic.com/news/\n\nclaude-sonnet-4-5, 2025c.\n\nChen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai\nWang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng\nWang, and Wu Liu. ACEBench: Who wins the match point in tool usage?, 2025. URL https:\n//arxiv.org/abs/2501.12851.\n\n",
            "char_count": 753,
            "word_count": 76
          }
        ],
        "stats": {
          "mode": "Semantic",
          "num_chunks": 28,
          "avg_chars": 3224.3214285714284,
          "min_chars": 8,
          "max_chars": 26968,
          "avg_words": 452.64285714285717,
          "status": "success"
        },
        "time_sec": 1.2711739540100098
      },
      "proposition": {
        "chunks": [],
        "stats": {
          "mode": "Proposition",
          "num_chunks": 0,
          "avg_chars": 0,
          "min_chars": 0,
          "max_chars": 0,
          "avg_words": 0,
          "status": "not_available"
        },
        "time_sec": 0
      },
      "hybrid": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nL\nC\n.\n",
            "char_count": 28,
            "word_count": 13
          },
          {
            "chunk_id": 1,
            "text": "s\nc\n[\n\n1\nv\n6\n2\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nTHE TOOL DECATHLON: BENCHMARKING LANGUAGE\nAGENTS FOR DIVERSE, REALISTIC, AND LONG-\nHORIZON TASK EXECUTION\n\nWenshuo Zhao1*\n\nJunlong Li1\u2217\nXiaochen Wang1 Rui Ge1 Yuxuan Cao1 Yuzhen Huang1 Wei Liu1\nZhaochen Su1 Yiyang Guo1\nXingyao Wang2 Xiang Yue3\n\nFan Zhou1 Lueyang Zhang1\nShuyan Zhou4 Graham Neubig2,3\n\nJian Zhao1* Weihao Zeng1* Haoze Wu1*\n\nJuan Michelini2\n\nJunxian He1\u2020\n\nJunteng Liu1\n\n1The Hong Kong University of Science and Technology\n2All Hands AI\nWebsite: toolathlon.xyz\n\n3Carnegie Mellon University\n\n4Duke University\n\n\u00a7 github.com/hkust-nlp/toolathlon\n\nABSTRACT\n\nReal-world language agents must handle complex, multi-step workflows across\ndiverse applications. For instance, an agent may manage emails by coordinating\nwith calendars and file systems, or monitor a production database like BigQuery\nto detect anomalies and generate reports following a standard operating manual.\nHowever, existing language agent benchmarks often focus on narrow domains\nor simplified tasks that lack the diversity, realism, and long-horizon complexity\nrequired to evaluate agents\u2019 real-world performance. To address this gap, we\nintroduce the Tool Decathlon (dubbed as TOOLATHLON), a benchmark for language\nagents offering diverse applications and tools, realistic environment setup, and\nreliable execution-based evaluation. TOOLATHLON spans 32 software applications\nand 604 tools, ranging from everyday platforms such as Google Calendar and\nNotion to professional applications like WooCommerce, Kubernetes, and BigQuery.\nMost of the tools are based on a high-quality set of Model Context Protocol (MCP)\nservers that we may have revised or implemented ourselves. Unlike prior works,\nwhich primarily ensure functional realism but offer limited environment state\ndiversity, we provide realistic initial environment states from real software, such as\nCanvas courses with dozens of students or real-world financial spreadsheets. The\nTOOLATHLON benchmark includes 108 manually sourced or crafted tasks in total,\nrequiring interacting with multiple applications over around 20 turns on average\nto complete. Each task is strictly verifiable through dedicated evaluation scripts.\nComprehensive evaluation of state-of-the-art models highlights their significant\nshortcomings in performing real-world, long-horizon tasks: the best-performing\nmodel, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool\ncalling turns on average, while the top open-weights model DeepSeek-V3.2-Exp\nreaches 20.1%. We expect TOOLATHLON to drive the development of more capable\nlanguage agents for real-world, long-horizon task execution.\n\n1\n\nINTRODUCTION\n\nTool-based language agents have already demonstrated their impact in real-world domains such as\nsoftware engineering (Jimenez et al., 2024; The Terminal-Bench Team, 2025), deep research (OpenAI,\n2024), and web browsing (Zhou et al., 2024). To further expand the reach of language agents across\ndiverse domains and applications, the Model Context Protocol (MCP) has been proposed to establish\na standard for connecting language agents to tens of thousands of applications (Anthropic, 2024).\n\nExisting benchmarks for language agents, however, are restricted to limited domains and tools (Mialon\net al., 2023; Liu et al., 2024; Ma et al., 2024; Jimenez et al., 2024; Zhou et al., 2024; Yao et al., 2025;\n\n\u2217 Equal Contribution. \u2020Corresponding author.\n\n1\n\n \n \n \n \n \n \n\fToolathlon\n\nFigure 1: Two examples and the initial environment states in TOOLATHLON. We showcase real-world environ-\nment interaction (\u00a72.2) and realistc state initialization (\u00a72.3) here.\n\nWei et al., 2025; Xu et al., 2025). By contrast, real-world tasks often require switching across various\napplications. For example, as demonstrated in Figure 1 (Example #2), a company\u2019s administrative\nagent may need to monitor a real Snowflake database for customer tickets, locate the appropriate PDF\noperation manual containing instructions on how to identify and handle overdue tickets, and then\nsend the required emails to managers and customers in accordance with the manual. Importantly,\nthis diversity gap extends far beyond differences in tool names or descriptions. The diversity and\ncomplexity of environment states across applications, compounded by interaction with them in long\ntrajectories, present substantial challenges for generalization.\n\nTo address these challenges, we introduce the Tool Decathlon (TOOLATHLON), a benchmark for\nevaluating language agents on diverse, realistic, and long-horizon tasks. TOOLATHLON spans 32\nreal-world applications and 604 tools across 108 tasks, covering a wide spectrum of domains ranging\nfrom daily affair and education to technology and finance. Tasks are grounded in realistic scenarios\nand mostly require coordinating multiple applications. Each task is fully verifiable with a dedicated,\ndeterministic evaluation script, comparing outcomes against either static or dynamically generated\nground-truth states (e.g., tasks involving the latest NVIDIA shareholder information or real-time train\nschedules). All tools in TOOLATHLON are sourced from the real world, with the majority obtained\nfrom MCP servers.\n\nTo faithfully capture the realism and complexity of practical environment states, we tried to adopt\nthe most representative applications such as Google Sheet, Gmail, and Snowflake. However, some\nremote environment states are difficult to set up to mimic real scenarios. For example, simulating a\nCanvas course with tens of students would require registering a real account for each student and\nresetting the states at each evaluation run. Therefore, while we adopt the commonly used applications\nmost of the time, we also incorporate several open-source software deployed locally via containers for\nconvenient and complex environment simulation, such as poste.io for email management to replace\nGmail and WooCommerce for online ecommerce platform to replace Shopify. These services provide\ncomplex observations while allowing us to set up the states in a scalable way. This stands in stark\ncontrast with simplified or artificial environment states as in prior benchmarks (Patil et al., 2025).\nIn addition, task prompts in TOOLATHLON are crafted to mirror authentic user queries, which are\noften concise and fuzzy. Models must therefore infer user intent and autonomously devise plans to\naccomplish tasks, an example is shown in Figure 3.\n\nConcurrent with this work, several MCP-based tool-use benchmarks have emerged (Liu et al., 2025;\nMo et al., 2025; Yan et al., 2025; Yin et al., 2025; The MCPMark Team, 2025), but they do not\nmatch TOOLATHLON in its reflection of real-world complexity. Some rely on LLM judges without\n\n2\n\nTask Prompt #1:  Your task is to check your email for homework2 submissions and grade on Canvas. Please download Python \ufb01les from email attachments to local workspace and execute each Python \ufb01le in terminal to check for errors. If the Python \ufb01le is correct, give it a score of 10 in Canvas; otherwise, give it a score of 0. You could check the requirements of homework2 in `assignments/homework2.md`and students' ID in `student_canvas_ids.csv`. For students who submitted multiple times, use the latest submission. FilesystemFilesystemPDFTerminalEmail (Poste.io)Canvas-LMSTask Prompt #2: Identify the tickets in the database that have exceeded the initial response time according to the relevant documentation, and send reminder emails, based on the templates mentioned in the manual, to the respective responsible managers, as well as apology emails to all involved users.SnowflakeEmail (Poste.io)\fToolathlon\n\nTable 1: Comparison of Tool-Based Language Agent Benchmarks. \u201c# Apps\u201d denotes the number of MCP servers,\nwhich we do not annotate for benchmarks without clear application definition. \u201cAvg # Turns\u201d denotes the number\nof tool calling turns made by Claude-4-Sonnet, which we use as a proxy for task complexity. \u201cReal States & Init\u201d\n(\u00a72.2, \u00a72.3) means the environment states and observations are from real-world software rather than artificial\ndatabases, and evaluation begins with a realistic state initialization. \u201cVerifiable Execution\u201d (\u00a72.4) denotes that\nmodels need to execute the tools and final results are evaluated based on states. \u201cFuzzy Prompt\u201d represents that\nthe task instructions are often fuzzy and ambiguous to mimic real user input (\u00a73.1). \u2217For MCPUniverse, only\n10% of the tasks involve multiple applications. In-depth discussion of these related works is in Appendix 6.\n\nBenchmark\n\n# Tasks\n\n# Apps\n\nAvg #\nTurns\n\nReal\nStates & Init\n\n\u03c4 -Bench\nBFCLv3-MT\nACEBench\nAppWorld\nMCPWorld\nMCP-RADAR\nMCPEval\nLiveMCPBench\nMCP-AgentBench\nLiveMCP-101\nMCPAtlas\nMCPUniverse\nMCPMark\nGAIA2\n\nTOOLATHLON\n\n165\n800\n2000\n750\n201\n300\n676\n95\n600\n101\n1000\n231\n127\n800\n\n108\n\n2\n\u2013\n\u2013\n9\n10\n9\n19\n70\n33\n41\n40+\n11\n5\n12\n\n32\n\n\u2013\n3.8\n1.7\n\u2013\n\u2013\n\u2013\n\u2013\n5.6\n\u2013\n5.4\n3-6\n7.5\n18.5\n22.5\n\n26.8\n\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u2713\n\nVerifiable\nExecution\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\n\u2713\n\nCross-App\nTask\n\nFuzzy\nPrompt\n\n\u00d7\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nPartial\u2217\n\u00d7\n\u2713\n\n\u2713\n\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\n\u2713\n\nverifiable tasks (Mo et al., 2025; Yin et al., 2025), while others cover few domains or mostly single-\napplication tasks. For instance, MCPUniverse (Luo et al., 2025) spans only six domains, with 90%\nof tasks involving one app and synthetic initial states, yielding simplified, short interactions (<8\nturns). Similarly, MCPMark (The MCPMark Team, 2025) includes only five apps and overly detailed\nprompts (Figure 3). GAIA2 (Andrews et al., 2025) covers merely the mobile domain on mostly daily\ntasks with simplified synthetic environments. A full comparison is shown in Table 1.\n\nTOOLATHLON includes a lightweight framework for automated, safe, and scalable evaluation. Each\ntask comes with initial states setup if needed as well as an evaluation script (Figure 2). Executing\nand evaluating each task is isolated in separate containers to prevent interference. This enables fast\nparallel evaluation\u2014for example, running Claude-4.5-Sonnet on all 108 tasks takes only 70 minutes\nusing 10 parallel processes. With extensive experiments on TOOLATHLON, the best-performing\nmodels, Claude-4.5-Sonnet, achieve only 38.6% accuracy, highlighting the unique challenges posed\nby TOOLATHLON. DeepSeek-V3.2-Exp (DeepSeek-AI, 2025) achieves 20.1% success rate as the\nbest performer among open-source models. Further analysis reveals that weaknesses in long-context\nmodeling and robust tool calling error tracking are major challenges for all evaluated models. We have\nfully open-sourced the benchmark and the TOOLATHLON environment, aiming for TOOLATHLON to\naccelerate the development of practical language agents.\n\n2 THE TOOLATHLON ENVIRONMENT AND EVALUATION FRAMEWORK\n\n2.1 TASK DEFINITION\n\nEach task in TOOLATHLON can be formulated as a partially observable Markov decision process\n(POMDP) (S, A, O, T , R, U) with state space S, action space A, observation space O, transition\nfunction T : S \u00d7 A \u2192 S \u00d7 O, reward function R : S \u2192 [0, 1], and instruction space U. The\nenvironment states (\u00a72.2, \u00a72.3) can be the status in the current email inbox and the observations are\nthe sequential input to the model. The action space A is the available tools for the respective task\nand the tool implementation directly defines the transition function. The reward function R (\u00a72.4)\nrepresents our execution-based evaluation which directly evaluates the environment state. Intuitively,\nreal-world tools and environments will yield significantly more complex and diverse environment\nstates and observations than the synthetic ones, and in the following sections, we will detail our\ndesigns of these variables in TOOLATHLON.\n\n3\n\n\fToolathlon\n\n2.2 TOOLS, ENVIRONMENTS, AND FRAMEWORK\n\nMCP Servers:\nIn TOOLATHLON, we source our tools through a variety of MCP servers. Specifi-\ncally, we first decide a list of valuable and common real applications that we aim to benchmark on,\nthen we see if we can find the corresponding open-source MCP servers for them. If not, we implement\nthe MCP servers by ourselves. Notably, many open-source MCP server implementations contain\nbugs or exhibit certain limitations, for example, without the tools needed to complete our tasks. We\nfurther refine and improve these implementations ourselves. This way, we obtain a high-quality set\nof 32 MCP servers in total, where we include a complete list and their sources in Appendix A. The\napplications span diverse domains, extending well beyond common daily-use applications such as\nGoogle Maps, Notion, and Google Calendar, and we also incorporate a number of professional and\ndomain-specific applications to evaluate language agents in high-value productivity scenarios, such\nas Snowflake for enterprise data management and Kubernetes for cluster management. Although\nthe majority of tools are sourced from MCP servers, the benchmark usage itself is not tied to MCP\nemployment from the model developer side. For examples, these tasks can also been solved via pure\nGUI or CLI workflow, as long as certain account information like usernames, passwords, tokens or\ncredentials are explicitly given to the agents.\n\nRemote and Locally Containerized Environments: While tools provide an interface for inter-\nacting with environments, they do not directly constitute the environments. Many real-world tools\ninteract directly with existing, remote environments, such as Google Sheets, Google Calendar, No-\ntion, and Gmail. Although remote environments require no implementation effort, they introduce\nsignificant challenges when benchmarking tasks that involve modifying environment states. For\ninstance, simulating a realistic Gmail inbox with hundreds of emails from diverse senders would\nrequire registering hundreds of Google accounts for every benchmark user, and this inbox would need\nto be reset prior to each evaluation run. Previous works have attempted to bypass this issue by only\nsupporting read operation to the states (Mialon et al., 2023), or implementing simplified synthetic\ndata structures to mimic environment states (Patil et al., 2025; Yao et al., 2025), but such approaches\ndrastically reduce realism and fail to reflect the complexity of real software environments. In contrast,\nin TOOLATHLON we leverage both remote environments and locally containerized, open-source\napplications. Specifically, we deploy the open-source Poste.io for email management, Canvas for\ncourse administration, Kubernetes for cluster orchestration, and WooCommerce for e-commerce\nmanagement. By hosting these realistic applications locally within containers, we can efficiently set\nup dozens of accounts and initialize complex environment states during evaluation. Compared with\nexisting dedicated agent sandboxes such as SWE-Bench (Jimenez et al., 2024), our environments are\nmore diverse and encompass a wider range of software.\n\nAgent Framework: We implement a simple agent framework based on the OpenAI Agents SDK\n(v0.0.15) 1 to conduct the agent action loop \u2013 at each turn, the model is expected to (optionally)\nreason explicitly and make tool calls. We make several enhancements to improve its basic setup for a\nmore robust workaround to evaluate language agents, including tool error handling, overlong tool\nresponse handling and context history management. We also equip this framework with some basic\nyet common local tools like python execution, web search, claim done and sleep. The details can be\nfound in Appendix B.\n\n2.3\n\nINITIAL STATE SETUP\n\nIn real world, tasks are rarely executed from an empty environment state (e.g., an empty inbox).\nInstead, agents are typically required to operate based on pre-existing environment states. In agentic\nscenarios, task difficulty is determined not only by the task instructions but also by the underlying\nenvironment states. For example, operating on a folder with only one file to be used is easier than\nworking with 10 mixed useful and unrelated files (Figure 1, example #2), even if the task descriptions\nare nearly identical. To capture this, for tasks in TOOLATHLON that starts with an initial state,2,\neach of these tasks is equipped with a state initialization script to set up the states at running time,\nor (and) an initial workspace directory containing pre-set files. Figure 1 and Figure 3 showcase\nsuch initial states. When constructing these initial environment states, we design them to closely\n\n1https://github.com/openai/openai-agents-python.\n2As shown in Table 2, 67% of the tasks fall into this category.\n\n4\n\n\fToolathlon\n\nFigure 2: Overview of the TOOLATHLON evaluation framework.\n\nreflect realistic scenarios. Notably, only very few previous benchmarks have incorporated realistic\ninitial state construction before entering the agent loop, as summarized in Table 1. By contrast, most\nexisting benchmarks start from empty state or overly simplified environment states, thus failing to\ncapture the full complexity of real-world task execution.\n\n2.4 RELIABLE EXECUTION-BASED EVALUATION\n\nFirst, unlike some traditional tool-calling benchmarks that measure single-step tool call accuracy\ngiven a fixed context without actual execution (Patil et al., 2024), we think that execution-based\nevaluation is essential for reliably assessing language agents in realistic scenarios. Second, while\nmany existing benchmarks rely on LLMs as judges to score agent trajectories (Gao et al., 2025; Yin\net al., 2025), we contend that verifying the final environment states using deterministic rules offers a\nfar more reliable and reproducible evaluation framework, as demonstrated in several widely adopted\nagent benchmarks (Zhou et al., 2024; Xie et al., 2024; Jimenez et al., 2024). To achieve this, each task\nin TOOLATHLON is equipped with a unique, manually crafted evaluation script that ensures precise\nand consistent measurement of task success. The script may perform robust matching against a static\nsnapshot of the ground-truth environment or follow a reference execution workflow to dynamically\nretrieve and match real-time information (e.g., NVIDIA shareholders). During evaluation, each task\nis associated with a configuration file that specifies the MCP servers and tools available for use.\nIntuitively, providing the model with a larger set of unrelated tools increases task difficulty, as the\nagent must identify the relevant tools while ignoring distracting ones.\n\nSafe and Efficient Parallel Evaluation in Containers: Our TOOLATHLON evaluation framework\nsupports parallel execution to enable efficient model evaluation. Our framework launches each task\ninside a separate container in parallel, providing strict workspace isolation. On a standard Ubuntu\n24.04 Linux cluster with 16 CPUs and 64 GB of memory, we are able to evaluate Claude-4.5-Sonnet\non 108 tasks in just about 70 minutes of wall time using only 10 parallel processes. This demonstrates\nthat TOOLATHLON is both convenient and efficient for practical use by model developers to get\ninstant feedback on how their models perform in realistic scenarios and requirements.\n\n3 THE TOOLATHLON TASKS\n\n3.1 TASK SOURCING AND FUZZY TASK INSTRUCTION\n\nThe authors of this work, who are researchers and senior undergraduate students in computer science,\nsource and implement the tasks. We carefully design and adhere to several principles when collecting\ntasks: (1) Real User Demands: All tasks are either directly sourced from real-world websites or\ncrafted to reflect genuine user demands. (2) Multi-App Orchestration: We intentionally source\ntasks that require interaction with multiple MCP servers, as this reflects authentic human workflows\nand increases task complexity. (3) Diversity: To ensure broad task diversity, we adopt a two-stage\nsourcing process. In the first stage, we start with an initial MCP server list covering more than 50\napplications and freely source tasks without restricting to specific servers. In the second stage, we\nanalyze the distribution of the sourced tasks and identify Apps that are important but underrepresented.\nWe then conduct an additional round of targeted task sourcing specifically for them.\n\n5\n\nLanguage AgentsTask InstructionRealistic State InitializationReal-World Software EnvironmentsMCP & Local ToolsTool CallTool OutputsExecuteState-based Evaluation\fToolathlon\n\nFigure 3: Example task instructions from our benchmark (Left) and MCPMark (The MCPMark Team, 2025)\n(Right). Ours contain more fuzzy intent that the model need to infer from the environment states.\n\nRealistic Fuzzy Task Instruction: We de-\nsign task instructions to resemble authentic\nuser input, which is often fuzzy or ambigu-\nous, but whose actual intent can be deter-\nministically inferred from the environment\u2019s\nstates (e.g., existing data examples or docu-\nment templates). This requires the agent to\ninfer the user\u2019s intent from the environment\nstate, formulate plans, execute them, and in-\ntellectually handle unexpected events such as\ntool call errors. For example, as shown in Fig-\nure 3 Left, a real user may simply say \u201cPlease\nupdate the candidate information on HR\nRecord subpage according to all the\nresumes...... if the position applied\nfor by the applicant is currently not\nopen, ......\u201d This is a fuzzy instruction\nwithout specifying in which format the agent\nshould fill in the information, but the Notion\ndatabase has provided some examples that the\nagent needs to know to check itself. Also, the\ninstruction does not mention where to find the status of the posted job and the agent needs to check\nNotion to find that by itself. In contrast, task instructions in some existing benchmarks (Figure 3\nRight) explicitly include detailed step-by-step plans, which reduce the role agents for planning. More\nexamples of this kind are shown in Figure 10 and 11.\n\nFigure 4: Task topic distribution of TOOLATHLON.\n\nAll the sourced tasks experience multiple rounds of rigorous quality check, filtering and refinement\nwhich last for several weeks before we implement them into our benchmark, and finally we obtain\n108 tasks in total. The topic distribution of all tasks is shown in Figure 4 and Table 2 show some key\nstatistics of the complete benchmark.\n\n3.2 TASK IMPLEMENTATION\n\nAs described in \u00a72.4, each task in our bench-\nmark is fully implemented with a correspond-\ning evaluation script and potential initial states\nsetup. This process involves collecting ground-\ntruth states statically or dynamically, and design\nscripts to automatically clear and re-fill new ini-\ntial states. To ensure realistic setups and reli-\nable evaluation, implementing a single task in\n\n6\n\nTable 2: Key statistics of TOOLATHLON.\n\nStatistics\n\nValue\n\n# MCP servers (# tools)\n# Local toolkits (# tools)\nAvg/Min/Max tools per task\nTasks with state initialization\n\n32 (604)\n7 (16)\n69.9/28/128\n72/108 (67%)\n\n\u2026 23 more positions & other distractors blocksCreate a comprehensive weekend adventure planner that analyzes the Toronto Guide databases and generates a structured itinerary page. I need you to create a new page called 'Perfect Weekend Adventure' as a child of the main Toronto Guide page.Task Requirements:  1. Create a new page titled 'Perfect Weekend Adventure' as a child page of the main Toronto Guide page  2. Query the Activities database to identify all activities that have the \"Beaches\" tag  3. Query the Food database to find all restaurants with \"Turkish\" or \"Hakka\" tags  4. Query the Cafes database to retrieve all cafes entries  5. Structure the page with the following specific format:    - [SOME SPECIFIC REQUIREMENTS]  6. After the summary paragraph, add a divider block  7. Finally, add a callout block with the \ud83d\udca1 emoji containing the text: \"Pro tip: Check the Seasons database for the best time to enjoy outdoor activities!\"  8. Ensure all headings use the exact emoji and text format specified above  9. The lists must be in the exact format specified (bulleted for beaches, numbered for restaurants, to-do for cafes)Please update the candidate information on HR Record subpage of Notion according to all the resumes in my workspace. All information must be filled out strictly according to the content in the resumes, without making any unauthorized modifications or adding/removing any words. Also, please delete the existing sample entries in the record table. At the same time, if the position applied for by the applicant is currently not open for recruitment, please send an email to the corresponding applicant using the following template information (including line breaks). Do not send the email by mistake: [A TEMPLATE HERE]Clear Step by Step Guides  Infer from  # Head CountInfer from  existing examplesFind needed info by agentInfer from   resumesLiterature ReviewManuscript EditingScholarly ActivitiesProfileManagementCourseSchool ApplicationTeachingAcademic AffairsQuantitativeStrategyStock TradingMarketingAnalyticsDeploymentDevOps SweepData CurationOSS EngagementProject SyncExperimentAnalysisData WranglingTalentManagementData AuditingFinancial AdminHealth GuidanceTravel HelperSocial MediaTransportationPersonal AffairSports AnalysisOnline PurchaseProductAssortmentInventory ControlDTC ManagementResearch(13.9%)Campus(16.7%)Finance(9.3%)Tech(17.6%)Business(16.7%)Daily(15.7%)E-commerce(10.1%)\fToolathlon\n\nTable 3: Main results for all the models. P@1, P@3, P\u02c63 and # Turns represents Pass@1, Pass@3, Pass\u02c63 and\naverage numbers of turns, respectively. We make bold the highest score.\n\nModel\n\nResearch Campus Finance Tech Business Daily E-com P@1 P@3 P\u02c63 # Turns\n\nClaude-4.5-Sonnet\nGPT-5\nClaude-4-Sonnet\nGPT-5-high\nGrok-4\nClaude-4.5-Haiku\nGrok-code-Fast-1\nGrok-4-Fast\no3\no4-mini\nGPT-5-mini\nGemini-2.5-Pro\nGemini-2.5-Flash\n\nDeepSeek-v3.2-Exp\nGLM-4.6\nQwen-3-Coder\nKimi-k2-0905\n\n31.1\n20.0\n33.3\n15.6\n24.4\n11.1\n20.0\n15.6\n15.6\n13.3\n11.1\n4.4\n4.4\n\n11.1\n22.2\n4.4\n8.9\n\n42.6\n33.3\n33.3\n31.5\n22.2\n22.2\n14.8\n22.2\n14.8\n11.1\n16.7\n3.7\n3.7\n\n16.7\n18.5\n16.7\n22.2\n\nProprietary Models\n\n33.3\n13.3\n30.0\n23.3\n13.3\n26.7\n16.7\n16.7\n10.0\n20.0\n20.0\n3.3\n6.7\n\n42.1\n40.4\n26.3\n29.8\n43.9\n29.8\n19.3\n24.6\n22.8\n17.5\n15.8\n15.8\n3.5\n\n42.6\n38.9\n25.9\n44.4\n24.1\n27.8\n14.8\n14.8\n13.0\n11.1\n9.3\n5.6\n1.9\n\nOpen-Source Models\n\n23.3\n20.0\n10.0\n16.7\n\n19.3\n17.5\n19.3\n14.0\n\n14.8\n16.7\n14.8\n5.6\n\n35.3\n39.2\n33.3\n33.3\n27.5\n37.3\n21.6\n13.7\n29.4\n21.6\n21.6\n27.5\n3.9\n\n29.4\n11.8\n17.6\n9.8\n\n38.6\u00b12.7 51.9 20.4\n39.4\n12.1 30.6\u00b11.5 43.5 16.7\n27.3 29.9\u00b11.6 41.7 17.6\n15.2 29.0\u00b13.1 42.6 16.7\n30.3 27.5\u00b11.7 38.9 16.7\n27.3 26.2\u00b11.9 39.8 13.0\n24.2 18.5\u00b12.0 30.6 9.3\n21.2 18.5\u00b12.0 32.4 5.6\n17.0\u00b10.9 25.0 9.3\n6.1\n14.8\u00b10.8 26.9 3.7\n9.1\n14.5\u00b11.2 23.1 5.6\n6.1\n10.5\u00b11.9 21.3 2.8\n9.1\n3.7\u00b11.5 8.3\n0.0\n3.0\n\n30.3 20.1\u00b11.2 27.8 12.0\n30.3 18.8\u00b12.2 29.6 9.3\n15.2 14.5\u00b11.9 21.3 6.5\n15.2 13.0\u00b12.0 22.2 5.6\n\n20.2\n18.7\n27.3\n19.0\n20.3\n21.9\n20.2\n15.9\n19.4\n16.6\n19.7\n26.5\n8.3\n\n26.0\n27.9\n28.5\n26.6\n\nTOOLATHLON requires, on average, 4\u20136 hours of work by a research graduate student majoring in\ncomputer science.\n\nFinalizing Tasks and Quality Check: After crowd-sourcing task implementations from multiple\ncontributors, we perform intensive quality checks conducted by 5\u20136 experienced authors. ",
            "char_count": 26968,
            "word_count": 4029
          },
          {
            "chunk_id": 2,
            "text": "In this\nstage, each task is carefully reviewed and revised to unify standards across all tasks and ensure\ncorrectness, solvability, and unambiguity, which requires approximately 5 hours of labor per task per\nround of checking. Once all tasks are finalized, we perform an additional round of comprehensive\ncross-checking and bug fixing of the entire benchmark before running the final experiments.\n\n4 EXPERIMENT\n\nIn this section, we present the configuration details and experimental settings for several leading\ncommercial and open models on TOOLATHLON, as well as their performance.\n\n4.1 SETUP\n\nModels and Configuration: Our evaluation includes the leading commercial model series in\nterms of agentic abilities, such as GPT-5(-mini) (OpenAI, 2025b), o3&o4-mini (OpenAI, 2025a),\nClaude-4-Sonnet (Anthropic, 2025a), Claude-4.5(-Sonnet,-Haiku) (Anthropic, 2025c;b), Gemini 2.5(-\nPro,-Flash) (Comanici et al., 2025), Grok-4(-Fast) (xAI, 2025a;b), Grok-Code-Fast-1 (xAI, 2025c).\nWe also benchmark the best-performing open-weight models including Qwen-3-Coder (Qwen Team,\n2025), DeepSeek-V3.2-Exp (DeepSeek-AI, 2025), Kimi-K2-0905 (Kimi Team et al., 2025) and\nGLM-4.6 (Zhipu AI, 2025). As described in \u00a72.4, each task is preconfigured with a list of MCP\nservers and common tools to access. During evaluation, we set the maximum allowable number of\nturns as 100 for all models. In our main evaluation setup, we only provide the models with the MCP\nservers and common tools that are useful for executing the task. We note that as each MCP server\nis equipped with multiple related tools, so the models will still see many unnecessary tools during\nevaluation.\n\nMetrics: We evaluate each model three times and report the average pass@1 success rate as well as\nthe standard deviation. We also include the pass@3 \u2013 the fraction of tasks with at least one correct\ntrajectory, and pass\u02c63 (Yao et al., 2025) \u2013 the fraction of tasks where all three trajectories are correct,\nto measure the model\u2019s potential capability coverage and its ability to complete tasks reliably. We\nalso report the average number of turns used.\n\n7\n\n\fToolathlon\n\nFigure 5: Two kinds of tool calling error presence ratios in calling tools for different models.\n\n4.2 MAIN RESULTS\n\nResults in Table 3 show that Claude-4.5-Sonnet ranks first, but still achieves a success rate of less\nthan 40%. GPT-5, Claude-4-Sonnet, Grok-4, and Claude-4.5-Haiku, each with Pass@1 scores above\n26% but below 30%, clearly fall into the second tier. All other models remain at 20% or below.\nThis indicates that our benchmark remains challenging for state-of-the-art models and effectively\ndistinguishes their capabilities. For open-source models, scores are less than or equal to 20%, with the\nbest, DeepSeek-V3.2-Exp, achieving 20.1%, revealing a clear gap compared with proprietary models.\nInterestingly, increased reasoning effort for thinking-oriented models (e.g., GPT-5 vs. GPT-5-high)\nshows no benefit, suggesting that exploring new observations matters more than extended internal\nreasoning in agentic tasks. We also find that Gemini-2.5\u2019s ability to understand requirements and\nproactively explore is insufficient\u2014it may neglect certain requirements or give up prematurely during\nexecution, resulting in poor performance on complex tasks.\n\nLooking at performance across task categories, Claude-4.5-Sonnet excels in almost all domains,\nespecially in Campus and E-Commerce tasks, demonstrating strong general capabilities across diverse\ntools. GPT-5 performs exceptionally well in Daily tasks, showcasing its effectiveness in everyday\nscenarios, while Grok-4 stands out in Tech, indicating a specialized strength in technology-related\ndevelopment operations. We also observe significant differences between Pass@3 and Pass\u02c63 success\nrates. This indicates that while many models have certain capability coverage, they lack consistency\nin producing reliable results. For real-world tasks, building agents with both high success rates and\nrobust consistency remains a critical challenge.\n\n5 ANALYSIS\n\nIn this section, we conduct analysis in depth to better understand model performance in TOOLATHLON,\nfocusing on tool-call errors, as well as long-context and overlong-output challenges. More analysis,\nincluding how tool error and the involvement of unrelated MCP servers impact model performance,\nand qualitative analysis & case studies, can be found in Appendix C and E.\n\n5.1 THE FAILURE OF CALLING TOOLS\n\nWe mainly focus on two major tool-calling er-\nrors: hallucinating non-existing tools (e.g., in-\ncorrect tool names) and errors raised during tool\nexecution. Statistics for different models on\nthese two types of errors are shown in Figure\n5. It can be seen that all models produce tool\nexecution errors to varying degrees, possibly\ndue to incorrect parameter passing or attempts\nto access non-existent resources. However, we\nfound no significant correlation between overall\nsuccess rate and the frequency of such errors. In\nfact, error messages from tools may help models\nunderstand the tool implementation or structure,\nallowing adjustments in subsequent turns. The\nother type of error\u2013incorrect tool names\u2013more\nlikely affects final scores. Leading models produce few tool name errors. In Appendix C, Figure\n\nFigure 6: Model performance on three groups of tasks\ndivided by average turns. The x-axis represents different\ntask difficulty groups determined by different avg turns\nrange [Min Turns, Max Turns]\n\n8\n\nGPT-5-highKimi-K2-0905GLM-4.6Claude-4.5-HaikuDeepSeek-V3.2-ExpGPT-5Grok-4Claude-4-SonnetGemini-2.5-ProGPT-5-miniGrok-Code-Fast-1Qwen-3-CoderGrok-4-FastClaude-4.5-Sonneto3Gemini-2.5-Flasho4-mini01020Error Presence (%)Wrong Tool NameTool Call ErrorPass@1010203040Pass@1 (%)Easy[4.2, 15.6]Medium[15.7, 23.8]Hard[24.1, 52.7]01020304050Success Rate (%)45%32%37%9Turns19Turns31Turns41%23%26%9Turns15Turns30Turns21%20%13%9Turns13Turns24Turns26%23%10%12Turns26Turns39Turns21%11%11%17Turns23Turns44TurnsClaude-4.5-SonnetGPT-5Grok-4-FastDeepSeek-V3.2-ExpQwen-3-Coder\fToolathlon\n\n9, we further analyze the success rate difference between trajectories containing tool-calling errors\nversus error-free trajectories, showing that most models do suffer from tool call errors.\n\n5.2 THE LONG-CONTEXT CHALLENGES FOR LANGUAGE AGENTS\n\nSince our benchmark is built on real tools and environments,\nit naturally generates many long-horizon trajectories. ",
            "char_count": 6391,
            "word_count": 860
          },
          {
            "chunk_id": 3,
            "text": "To quan-\ntitatively describe the differences between tasks, we calculate\nthe average number of execution turns for each task across all\nmodels, and use this as a proxy to divide all tasks into three\nequally sized groups: Easy, Medium, and Hard, with execu-\ntion turns increasing with difficulty. In Figure 6, we show the\nperformance of five representative models on different groups,\nalong with their average turns in each group. The results indi-\ncate that groups with higher average turns generally have lower\nsuccess rates across models, and leading models like Claude-\n4.5-Sonnet maintain clear advantages in all groups. We also\nfind that there is no significant difficulty difference between the\nMedium and Hard groups, and that even Claude-4.5-Sonnet and\nGPT-5 achieve higher scores on the Hard subset then in Medium\nones. This suggests that our benchmark\u2019s difficulty does not\nentirely stem from standard multi-step long-horizon execution,\nbut possibly from models ending tasks prematurely without\nsufficiently exploring available observations, leading to failure.\n\nFigure 7: Avg. Success Rate on Trajec-\ntories w/wo overlong tool outputs.\n\nAnother concern is whether models can successfully complete tasks when encountering overlong\ntool outputs, like fetching lengthy HTML source code or directly listing all data from a database\n(we refer the readers to Appendix B for handling overlong outputs in our framework). We calculate\nthe proportion of trajectories containing overlong tool outputs encountered by all models during\nevaluation, as well as each model\u2019s success rates with and without overlong tool outputs. Results\nshow that the proportion of overlong tool outputs varies from approximately 15% to 35% across\ndifferent models. Additionally, Figure 7 shows that most models experience a decline in success rate\nwhen encountering overlong tool outputs, with only a few models maintaining nearly unchanged\nperformance. While tasks with overlong outputs are often logically straightforward (e.g., price\ncomparison, data extraction), most models get trapped trying to process these lengthy outputs.\n\n5.3 THE RELATIONSHIP BETWEEN PERFORMANCE AND EXPENSES\n\nSince we evaluate the models in realistic settings, both cost and token usage are important factors, as\nthey determine how a model should be selected for different budget constraints. Therefore, during the\nevaluation period, we measure the actual number of output tokens and the associated cost with prompt\ncaching enabled. For costs, Figure 8 Left shows that Claude-4-Sonnet and Grok-4 incur relatively\nhigh expenses, whereas most other models remain under $1 per task. Claude-4.5-Sonnet achieves the\nhighest performance but ranks third in cost. Several models, such as Grok-4-Fast, Grok-Code-Fast-1,\nand DeepSeek-V3.2-Exp, incur only a small cost, suggesting that they can serve as strong alternatives\nunder limited budgets without an extreme pursuit of maximum performance.\n\nWe also plot the output token count distribution in Figure 8 Right, which illustrates how the success\nrate varies with different average output token counts. Most models cluster between 5K and 10K\noutput tokens. Some reasoning-focused models, such as o4-mini and GPT-5(-high), generate more\ntokens, whereas the Claude series and Grok-4 achieve strong results with fewer tokens, suggesting\nthey rely more on environment observation rather than extensive internal reasoning. Models like\nGemini-2.5-Flash have the lowest output token counts and correspondingly lower accuracy, while\nothers exhibit a similarly concentrated distribution.\n\n6 RELATED WORK\n\nBenchmarks for tool-based language agents differ substantially in the realism of their tools, environ-\nments, and task configurations, and can be viewed along a spectrum from fully simulated settings\n\n9\n\n010203040Success Rate - No Overlong Tool Output (%)010203040Success Rate - With Overlong Tool Output (%)GPT-5GPT-5-highGPT-5-minio3o4-miniGrok-4Grok-4-FastGrok-Code-Fast-1Gemini-2.5-ProGemini-2.5-FlashClaude-4-SonnetClaude-4.5-SonnetClaude-4.5-HaikuDeepSeek-V3.2-ExpKimi-K2-0905GLM-4.6Qwen-3-Coder\fToolathlon\n\nFigure 8: The relationship between average task success rate and average cost (Left) and output tokens (Right).\n\nto those grounded in real-world applications. At one end of this spectrum, several works evaluate\ntool use purely through simulation, without executing real APIs or interacting with actual application\nbackends. Representative examples include \u03c4 -Bench (Yao et al., 2025), BFCL (Patil et al., 2025), and\nACEBench (Chen et al., 2025), which assess function calling accuracy or multi-turn tool selection in\ncontrolled scenarios, but rely on mock implementations or language-model-based emulation. While\nsuch designs enable efficiency and reproducibility, they omit many of the challenges that arise from\nexecuting real tools in unpredictable environments.\n\nMoving beyond simulated tools, other benchmarks connect agents to real APIs yet operate in\nsynthetic or constrained environments where initial states are artificially constructed. For exam-\nple, AppWorld (Trivedi et al., 2024) offers a high-fidelity simulation of multiple apps, and MCP-\nWorld (Yan et al., 2025), MCP-RADAR (Gao et al., 2025), MCPEval (Liu et al., 2025), and MCP-\nAgentBench (Guo et al., 2025) grant access to real Apps via Model Context Protocol (MCP) (An-\nthropic, 2024) but often begin from zero or artificially designed states or center on single-application\ntasks. These setups capture tool execution more faithfully than pure simulation, yet still fall short of\nrepresenting the complexity of authentic, multi-application workflows.\n\nCloser to realistic settings, a number of recent benchmarks combine real tools with more authentic\nenvironment conditions. LiveMCPBench (Mo et al., 2025), LiveMCP-101 (Yin et al., 2025), MCPAt-\nlas (Scale AI, 2025), MCPUniverse (Luo et al., 2025), and MCPMark (The MCPMark Team, 2025)\nintroduce production-grade MCP servers, multi-step workflows, and realistic tool outputs. Neverthe-\nless, they remain limited in diversity of domains, the realism of environment state initialization, or\nthe naturalness of task instructions\u2014many lack genuinely ambiguous or underspecified prompts that\nmimic real user requests.\n\nOur work, TOOLATHLON, advances this trajectory by combining real tools with genuinely realistic\nenvironments across 32 applications and 604 tools, spanning a broad range of domains. Initial states\nare grounded in authentic usage scenarios rather than synthetic constructs, and tasks often require\nlong-horizon, cross-application orchestration. Moreover, prompts are intentionally concise and fuzzy,\ncompelling agents to infer intent and autonomously plan, while deterministic, script-based evaluation\nensures correctness in evaluation.\n\n7 CONCLUSION\n\nWe introduce TOOLATHLON, a comprehensive benchmark for evaluating language agents on real-\nworld, long-horizon tasks spanning 32 applications and 604 tools. Our evaluation reveals significant\nlimitations in current models, with the best-performing Claude-4.5-Sonnet achieving only 38.6% suc-\ncess rate, highlighting substantial room for improvement in handling complex multi-step workflows.\nThrough detailed analyses, we identified key challenges including long context handling, tool-calling\nerrors, and the need for greater robustness in execution. We believe TOOLATHLON will drive the\ndevelopment of more capable and robust language agents for practical real-world deployment.\n\nACKNOWLEDGMENT\n\nWe thank Pengcheng Yin for helpful discussion on this project.\n\n10\n\n101100average cost per task ($, log scaled)010203040pass@1 (%)GPT-5GPT-5-HighClaude-4-SonnetClaude-4.5-SonnetClaude-4.5-HaikuDeepSeek-V3.2-ExpGemini-2.5-ProKimi-K2-0905GLM-4.6Gemini-2.5-FlashGPT-5-minio3o4-miniGrok-4Grok-Code--Fast-1Grok--4-Fast05K10K15K20K25K30Kaverage output tokens per task010203040pass@1 (%)GPT-5GPT-5-highClaude-4-SonnetClaude-4.5-SonnetClaude-4.5-HaikuDeepSeek-V3.2-ExpGemini-2.5-ProKimi-K2-0905GLM--4.6Qwen-3--CoderGemini-2.5-FlashGPT-5-minio3o4-miniGrok-4Grok-Code-Fast-1Grok-4--Fast\fToolathlon\n\nREFERENCES\n\nPierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja,\nRicardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo\nLaurenc\u00b8on, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre M\u00b4enard, Gr\u00b4egoire Mialon,\nUlyana Piterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav\nVorotilov, Mengjue Wang, and Ian Yu. ARE: Scaling up agent environments and evaluations, 2025.\n",
            "char_count": 8566,
            "word_count": 1123
          },
          {
            "chunk_id": 4,
            "text": "URL https://arxiv.org/abs/2509.17158.\n\nAnthropic.\n\nIntroducing the model context protocol.\n\nhttps://www.anthropic.com/news/\n\nmodel-context-protocol, 2024.\n\nAnthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, 2025a.\n\nAnthropic. https://www.anthropic.com/news/claude-haiku-4-5. https://www.anthropic.com/\n\nnews/claude-haiku-4-5, 2025b.\n\nAnthropic.\n\nIntroducing claude\n\nsonnet 4.5.\n\nhttps://www.anthropic.com/news/\n\nclaude-sonnet-4-5, 2025c.\n\nChen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai\nWang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng\nWang, and Wu Liu. ACEBench: Who wins the match point in tool usage?, 2025. URL https:\n//arxiv.org/abs/2501.12851.\n\n",
            "char_count": 753,
            "word_count": 76
          }
        ],
        "stats": {
          "mode": "Hybrid",
          "num_chunks": 28,
          "avg_chars": 3224.3214285714284,
          "min_chars": 8,
          "max_chars": 26968,
          "avg_words": 452.64285714285717,
          "status": "success"
        },
        "time_sec": 0.5897941589355469
      }
    }
  },
  {
    "paper_id": "2510.25729v1",
    "original_length": 57533,
    "modes": {
      "simple": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "Physics-Guided Conditional Diffusion Networks for\nMicrowave Image Reconstruction\nShirin Chehelgami, Joe LoVetri, and Vahab Khoshdel\u2217\n\nDepartment of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Manitoba, Canada\n\u2217Corresponding author: vahab.khoshdel@umanitoba.ca\n\n5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\n\nV\n\nI\n.\ns\ns\ne\ne\n[",
            "char_count": 327,
            "word_count": 48
          },
          {
            "chunk_id": 1,
            "text": "1\nv\n9\n2\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nAbstract\u2014A conditional latent-diffusion based framework for\nsolving the electromagnetic inverse scattering problem associ-\nated with microwave imaging is introduced. This generative\nmachine-learning model explicitly mirrors the non-uniqueness\nof the ill-posed inverse problem.",
            "char_count": 313,
            "word_count": 50
          },
          {
            "chunk_id": 2,
            "text": "Unlike existing inverse solvers\nutilizing deterministic machine learning techniques that produce\na single reconstruction,\nthe proposed latent-diffusion model\ngenerates multiple plausible permittivity maps conditioned on\nmeasured scattered-field data, thereby generating several po-\ntential instances in the range-space of the non-unique inverse\nmapping. A forward electromagnetic solver is integrated into the\nreconstruction pipeline as a physics-based evaluation mechanism.",
            "char_count": 474,
            "word_count": 57
          },
          {
            "chunk_id": 3,
            "text": "The space of candidate reconstructions form a distribution\nof possibilities consistent with the conditioning data and the\nmember of this space yielding the lowest scattered-field data\ndiscrepancy between the predicted and measured scattered fields\nis reported as the final solution. Synthetic and experimental\nlabeled datasets are used for training and evaluation of the\nmodel. An innovative labeled synthetic dataset is created that\nexemplifies a varied set of scattering features.",
            "char_count": 482,
            "word_count": 69
          },
          {
            "chunk_id": 4,
            "text": "Training of the\nmodel using this new dataset produces high-quality permittivity\nreconstructions achieving improved generalization with excellent\nfidelity to shape recognition. The results highlight the potential of\nhybrid generative\u2013physics frameworks as a promising direction\nfor robust, data-driven microwave imaging.\n\nIndex Terms\u2014Conditional Diffusion Model, Physics-Guided\nGenerative Model, Microwave Imaging, Electromagnetic Inverse\nScattering, Inverse Problems.\n\nI. INTRODUCTION\n\nQ UANTITATIVE Microwave Imaging (MWI) is a non-",
            "char_count": 533,
            "word_count": 63
          }
        ],
        "stats": {
          "mode": "Simple",
          "num_chunks": 154,
          "avg_chars": 372.0974025974026,
          "min_chars": 99,
          "max_chars": 655,
          "avg_words": 48.77922077922078,
          "status": "success"
        },
        "time_sec": 0.015518903732299805
      },
      "semantic": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "Physics-Guided Conditional Diffusion Networks for\nMicrowave Image Reconstruction\nShirin Chehelgami, Joe LoVetri, and Vahab Khoshdel\u2217\n\nDepartment of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Manitoba, Canada\n\u2217Corresponding author: vahab.khoshdel@umanitoba.ca\n\n5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\n\nV\n\nI\n.\ns\ns\ne\ne\n[\n\n1\nv\n9\n2\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nAbstract\u2014A conditional latent-diffusion based framework for\nsolving the electromagnetic inverse scattering problem associ-\nated with microwave imaging is introduced. This generative\nmachine-learning model explicitly mirrors the non-uniqueness\nof the ill-posed inverse problem. Unlike existing inverse solvers\nutilizing deterministic machine learning techniques that produce\na single reconstruction,\nthe proposed latent-diffusion model\ngenerates multiple plausible permittivity maps conditioned on\nmeasured scattered-field data, thereby generating several po-\ntential instances in the range-space of the non-unique inverse\nmapping. A forward electromagnetic solver is integrated into the\nreconstruction pipeline as a physics-based evaluation mechanism.\nThe space of candidate reconstructions form a distribution\nof possibilities consistent with the conditioning data and the\nmember of this space yielding the lowest scattered-field data\ndiscrepancy between the predicted and measured scattered fields\nis reported as the final solution. Synthetic and experimental\nlabeled datasets are used for training and evaluation of the\nmodel. An innovative labeled synthetic dataset is created that\nexemplifies a varied set of scattering features. Training of the\nmodel using this new dataset produces high-quality permittivity\nreconstructions achieving improved generalization with excellent\nfidelity to shape recognition. The results highlight the potential of\nhybrid generative\u2013physics frameworks as a promising direction\nfor robust, data-driven microwave imaging.\n\nIndex Terms\u2014Conditional Diffusion Model, Physics-Guided\nGenerative Model, Microwave Imaging, Electromagnetic Inverse\nScattering, Inverse Problems.\n\nI. INTRODUCTION\n\nQ UANTITATIVE Microwave Imaging (MWI) is a non-\n\ninvasive technique with applications in a wide range\nof areas such as medical diagnostics, and non-destructive\ntesting in agricultural and industrial settings [1]\u2013[3]. At its\ncore, MWI is a wavefield modality that has associated with\nit an electromagnetic inverse scattering problem (ISCP). In\nan electromagnetic ISCP the objective is to quantitatively\nreconstruct a map of the dielectric properties, and potentially\nthe dielectric loss, of the inaccessible interior of an object or\nregion of interest (OI/ROI). The data used for this reconstruc-\ntion are the measured scattered fields when the target OI/ROI is\ninterrogated by a known, and usually controllable, impinging\nincident field.\n\nThe electromagnetic ISCP has associated with it an ill-\nposed wavefield inverse source problem (ISP), that in addition\nto being highly sensitive to noise in the measurement data,\nmeans that more than one permittivity map will produce the\nthe solution to the ISP is non-\nsame scattered field,\nunique (see, e.g., [4], [5]). This makes accurate reconstructions\n\ni.e.,\n\nparticularly challenging and usually requires the augmentation\nof the acquired scattered-field data with some sort of prior\ninformation, e.g., the use of regularization methods [4], [6].\nA question arises as to whether the non-uniqueness of the\nill-posed wavefield inversion problem can be mirrored in an\nalgorithmic procedure to some benefit, as it has, for example,\nin the formulation of design problems as an inverse problem\n[7]\u2013[11].\n\nTraditional algorithms for the ISCP, such as Contrast Source\nInversion (CSI) [12] and Gauss-Newton Inversion [13], [14],\nrely on iterative optimization. These approaches typically\nincorporate a physically rigorous model of the data-acquisition\nsystem and discretization of the permittivity map of the target\nbeing imaged [15] makes these techniques computationally\nexpensive. Even with the use of calibration techniques that\nlessen demands on accurate models of the data-acquisition\nsystem, these optimization-based inversion methods require\nmany iterations of a forward solver before converging. In\naddition, the non-uniqueness of the underlying ISCP means\nthat they don\u2019t always converge to the true permittivity map\nfor the OI/ROI.\n\nIn recent years, machine learning\u2014particularly deep learn-\ning\u2014has gained significant\ntraction for MWI [16]. Deep\nlearning methods have been applied across nearly all stages\nof the imaging pipeline, including calibration, post-processing,\nimage enhancement, and inverse problem solving [17]. Among\nthese tasks, addressing the associated ISCP is clearly the\nmost challenging but can have the greatest impact. Supervised\nlearning approaches attempt to bypass iterative optimization\nby directly mapping scattered-field measurements to target\nreconstructions using large datasets of measurement/target\npairs [18]\u2013[20]. Studies have shown that deep networks can\nachieve high-quality reconstructions and even enable real-\ntime imaging [20], particularly when combined with physics-\ninformed pre-processing such as the Born approximation or\nelectromagnetic backpropagation [21]\u2013[23].\n\nA critical limitation of both traditional inversion techniques\nand deterministic Machine-Learning (ML) frameworks is that,\nby definition, they output a single reconstruction for a given in-\nput, neglecting the fundamental non-uniqueness of the electro-\nmagnetic inverse problem. In reality, multiple plausible target\npermittivity maps may correspond to the same measurement\ndata [4], [5]. In deterministic machine-learning frameworks\nthis one-to-one map is ultimately created by the training data\nset and thereby limits the trained ML model\u2019s generalizability\nto making predictions on scattered-field data corresponding\n\n \n \n \n \n \n \n\fto unseen targets that are too far away from the training data.\nThat is, it is widely recognized that such ML models are highly\nsensitive to \u201cdomain gaps\u201d [24].\n\nIn addition to the deterministic nature of many supervised\nML models, there is the related issue that these approaches\nrequire large datasets for training. To address this, researchers\nhave employed generative models such as Generative Adver-\nsarial Networks (GANs) and diffusion models to create large\nlabeled datasets for subsequent use in supervised learning [25].\nFor instance, Shao et al. [26] combined a GAN with a\ndeep neural network to approximate electromagnetic scattering\nin microwave breast imaging, thereby generating additional\ntraining data.\n\nAlthough synthetic datasets are convenient, they often fail\nto capture practical complexities such as antenna coupling,\ncalibration errors, and hardware imperfections [27]. Therefore,\nalthough large synthetically generated datasets have indeed\nbeen used for training, resulting ML models typically require\neither augmenting the training set with experimental data or\n\u201ccalibrating out\u201d the experimental setup to make the data more\nrepresentative of synthetic conditions. Otherwise, networks\ntrained exclusively on synthetic data often struggle to general-\nize to experimental measurements acquired using unique lab-\nspecific systems. It should be commented that currently there\ndoes not exist a standardized MWI data-acquisition setup\u2013\nmost researchers have developed there own unique systems.\n\nThus, the gap between synthetic training data and exper-\nimental measurements remains a major barrier to deploying\ngeneralizable deep learning\u2013based MWI systems. On the other\nhand, generative approaches are inherently less sensitive to\ndomain discrepancies, as they learn the underlying data man-\nifold rather than a deterministic mapping, thereby mitigating\nthe impact of distributional shifts between training and testing\ndatasets.\n\nMost recently, researchers have leveraged the inherent non-\nuniqueness of generative models to tackle the non-unique\nnature of inverse problems. Early efforts predominantly em-\nployed GANs and VAEs. For example, Bhadra et al. intro-\nduced an image-adaptive GAN framework that allows high-\nfidelity reconstruction of under-sampled MRI data and im-\nproved data consistency in ill-posed inverse imaging prob-\nlems [28]. Similarly, Yao et al. [29] introduced a conditional\nGAN framework to map scattered electromagnetic fields di-\nrectly to dielectric contrasts, enabling real-time image recon-\nstruction. Despite these advancements, GAN-based approaches\noften suffer from training instabilities, mode collapse, and\nlimited control over output diversity.\n\nTo overcome these limitations, diffusion and score-based\ngenerative models have emerged as more robust, theoretically\ngrounded alternatives. These models have demonstrated state-\nof-the-art performance across a broad range of imaging tasks,\nincluding synthesis, reconstruction, and enhancement [30]\u2013\n[32]. Compared with GANs and VAEs, diffusion-based ap-\nproaches exhibit stable optimization dynamics, provide well-\ndefined likelihood formulations, and yield superior generative\nfidelity. Accumulating evidence indicates that they consistently\nsurpass earlier generative paradigms in various medical imag-\ning applications [30], [31], [33], [34].\n\nIn the domain of medical image reconstruction, Song et\nal. [34] proposed a score-based generative framework capa-\nble of reconstructing medical images from partial CT and\nMRI measurements in an unsupervised manner, achieving\nstrong generalization across diverse measurement processes.\nNonetheless, applications of such models as inverse solvers for\nMWI remain limited. Recently, Bi et al. [35] introduced Diffu-\nsionEMIS, a diffusion-based method that iteratively refines 3-\nD point clouds to reconstruct scatterer geometries conditioned\non measured scattered fields.\n\nBuilding on these advances, we propose a conditional\ndiffusion-based generative model that explicitly incorporates\nthe non-unique nature of the microwave inverse scattering\nproblem. Unlike deterministic supervised networks that pro-\nduce a single estimate for a given set of measurements,\nour framework generates multiple plausible reconstructions\nconsistent with the same data, thereby reflecting the inherent\nnon-uniqueness of the ill-posed problem and leveraging the\nstrengths of diffusion models to produce physically mean-\ningful solutions. The core innovation of our approach lies\nin the integration of a physics-based selection mechanism,\ntransforming the framework into a physics-informed gener-\native system. After the diffusion model produces multiple\ncandidate reconstructions, a forward electromagnetic solver\nis applied to each candidate to predict\nthe corresponding\nscattered fields. The reconstruction yielding the lowest data\ndiscrepancy with respect to the measured fields is reported as\nthe final solution. This physics-guided validation ensures that\nthe chosen reconstruction is not only statistically plausible,\nbased on the conditioning data, but also physically consistent\nwith the underlying electromagnetic principles.\n\nThis integration of a forward solver marks a departure from\npurely data-driven approaches, forming a hybrid architecture\nthat combines the generative flexibility of diffusion models\nwith the physical accuracy of electromagnetic modeling. By\nembedding physics-based validation directly within the infer-\nence process, our method effectively bridges the gap between\nmachine learning efficiency and the physics of electromagnetic\ninteractions with the target, addressing one of the major\nlimitations of deep learning-based inverse solvers.\n\nThe proposed methodology is particularly well-suited for\ndata collected using actual MWI systems, where measurement\nnoise, calibration errors, and model mismatches often degrade\nperformance. By generating multiple candidate solutions and\nselecting the most physically consistent one, our approach\nachieves enhanced robustness and generalization. We validate\nthe method using both synthetic and experimental datasets,\ndemonstrating improved reconstruction accuracy and stability\ncompared to deterministic baselines.\n\nTo address the challenges outlined above, the remainder\nof this paper is organized as follows. Section II provides\nan overview of the methodology and fundamental concepts\nbehind diffusion models, including the forward and reverse\nprocesses and the use of diffusion priors for inverse problems.\nSection III introduces the proposed latent diffusion framework,\nwhich integrates a latent autoencoder representation, a physics-\naware conditioning mechanism, and an end-to-end inversion\narchitecture augmented by a physics-based selection strategy.\n\n\fSection IV describes the dataset used for training and evalua-\ntion, followed by Section V, which presents the experimental\nresults, including reconstruction consistency on synthetic data,\ngeneralization to experimental measurements, multi-frequency\ninversion, and performance evaluation of model error. Finally,\nSection VI concludes the paper with key findings and future\ndirections, and Section VII acknowledges supporting contri-\nbutions.\n\nII. METHODOLOGY\n\nA. Diffusion Models\n\nForward and reverse processes.: Diffusion models aim\nto approximate a target data distribution by constructing a\nforward process that incrementally adds Gaussian noise and\na reverse process that learns to remove this noise. Given a\nclean sample x0 \u223c pdata, the forward Markov chain {xt}T\nevolves as\nxt = (cid:112)1 \u2212 \u03b2t xt\u22121+(cid:112)\u03b2t wt, wt\nwhere the noise schedule {\u03b2t} governs how quickly the\nsignal is corrupted. As t increases, xt approaches an isotropic\nGaussian.\n\ni.i.d.\u223c N (0, I),\n\n1 \u2264 t \u2264 T,\n\nt=1\n\nGeneration proceeds by starting from a Gaussian xrev\n\nT \u223c\n\nN (0, I) and iteratively denoising back to xrev\n0 :\nT \u22121 \u2212\u2192 \u00b7 \u00b7 \u00b7 \u2212\u2192 xrev\n0 .\n\nT \u2212\u2192 xrev\nxrev\n\nSampling algorithms for this reverse chain\u2014deterministic or\nstochastic\u2014can be interpreted as discretizations of underlying\nODEs or SDEs. They rely on the score function s\u22c6\nt (x) =\n\u2207x log pxt(x) of the intermediate distributions. Tweedie\u2019s for-\nmula provides an explicit expression,\n\ns\u22c6\nt (x) = \u2212\n\n(cid:90)\n\n1\n1 \u2212 \u03b1t\n\npX0|Xt(x0 | x) (cid:0)x \u2212\n\n\u221a\n\n\u03b1t x0\n\n(cid:1) dx0,\n\n:= 1 \u2212 \u03b2t and \u03b1t\n\nwith \u03b1t\nk=1 \u03b1k. In practice, one\ntrains a neural network to approximate these score functions\nvia score matching, enabling the learned model to generate\nsamples from pdata.\n\n:= (cid:81)t\n\nDiffusion priors for inverse problems.: The flexibility of\ndiffusion models makes them attractive as priors for ill-posed\ninverse problems. Suppose we observe data y generated from\nan unknown signal x\u22c6 through a known forward operator\nand additive noise. A Bayesian formulation samples from the\nposterior\n\np(x | y) \u221d pprior(x) p(cid:0)y | x(cid:1),\n\nwhere pprior(x) is the diffusion-model prior and p(y | x) is the\nlikelihood. Sampling from this posterior combines the learned\nscore functions with the physics of the measurement process.\n\nB. Proposed Latent Diffusion Model\n\nWe consider the electromagnetic inverse problem of re-\nconstructing two-dimensional permittivity distributions from\nscattered field measurements. The scattered fields are repre-\nsented as two channels, corresponding to the real and imagi-\nnary components of the electric field. To overcome the non-\nuniqueness of the inverse problem, we design a generative\n\ninversion framework based on a conditional diffusion model\noperating in a compact latent space of permittivity maps. This\nframework produces multiple plausible reconstructions that are\ndriven by, i.e., consistent with, the measured fields, after which\na post-processing step is applied to identify the most physically\nmeaningful solution.\n\n1) Latent Representation with Autoencoder: Directly ap-\nplying a diffusion model to full-resolution permittivity maps\nis computationally expensive, particularly when extending this\nwork to 3D medical imaging applications, which represent the\nultimate goal of our research. To address this, we first train\nan autoencoder (AE) to learn a compact latent representation,\nfollowing a design similar to the one used in [36].\n\n\u2022 Encoder (E): compresses the permittivity grid x into a\nlatent representation z = E(x) using convolution and\ndownsampling layers.\n\n\u2022 Decoder (D): reconstructs the grid from the latent code\nas \u02dcx = D(z) = D(E(x)) using upsampling operations.\n\nThe AE is trained with a composite loss function:\n\u2022 Reconstruction loss (pixelwise error) ensures quantitative\n\nfidelity to the input.\n\n\u2022 Perceptual loss [37], computed from intermediate features\nof a pretrained VGG16 network [38], encourages preser-\nvation of edges and structural features.\n\n\u2022 Adversarial loss [39] penalizes overly smooth reconstruc-\n\ntions and promotes realistic textural detail.\n\nThe relative weights of these terms are tuned to balance\nnumerical accuracy with perceptual quality. By compressing a\n100\u00d7100 grid into a 16\u00d724\u00d724 latent vector, the AE reduces\nthe computational cost of the subsequent diffusion process.\n\n2) Physics-Aware Conditioning Mechanism: The learned\nlatent space serves as the domain for our diffusion model. The\nmodel is conditioned on measured scattered fields collected in\nthe scenario depicted in Fig. xxxx where a dielectric target is\nlocated at the center of a ring of transmitter and receivers. The\ntransmitters and receivers are located at equidistant points on\na circle of radius XXX m surrounding the target. A complete\ndescriprion can be found in [40]. This condition ensures that\nthe generated permittivity maps are physically consistent with\nthe observations. The data is composed of a 24\u00d724 scattered-\nfield matrix of complex-valued data representing the real and\nimaginary parts of the received frequency-domain phasor. Data\nconsists of up to five frequencies collected at 3.0, 3.5, 4.0, 4.5,\nand 5.0 GHz.\n\n\u2022 Forward diffusion: Gaussian noise is added to latent\n\nvectors over a predefined schedule.\n\n\u2022 Reverse denoising: A U-Net\u2013based denoiser is trained\nto iteratively predict and remove this noise. The denoiser\nis conditioned on the scattered fields via cross-attention\nlayers.\n\n\u2022 Conditioning mechanism: In the proposed framework,\nthe scattered electromagnetic fields are initially processed\nthrough convolutional layers to project them into a feature\nspace. Because electromagnetic inverse problems depend\non the spatial arrangement of transmitters, receivers,\ninforma-\nand objects,\ntion\u2014something that standard convolution and pooling\n\nto retain positional\n\nis vital\n\nit\n\n\f(a) Training the model\n\nFig. 1: Overview of the proposed framework: (a) training phase of the denoising diffusion model; (b) inference phase for\nreconstructing the output from measured fields.\n\n(b) Inference\n\nlayers tend to lose. To address this, we augment the 24\u00d724\nscattered-field inputs with sinusoidal positional encodings\nthat explicitly encode transmitters coordinates. Injecting\nthese encodings into the network provides an absolute\nreference frame, enabling the model to distinguish sensor\nlocations and object orientations. This spatial awareness\nis especially important for microwave imaging, where the\ngeometry of the scene strongly influences the observed\nscattering. These features are then injected back into\nthe diffusion model\u2019s denoiser at every step: they are\nconcatenated with intermediate feature maps and linked\nvia cross-attention blocks so that the denoising operation\nremains conditioned on the measured electromagnetic\nresponse. By coupling the generative model to the ob-\nserved physics in this way, we encourage it to produce\nreconstructions that are physically consistent with the\nactual scattered fields.\n\n\u2022 Architecture: The U-Net consists of downsampling and\nupsampling blocks with intermediate attention [41]. Time\nembeddings indicate the current noise level, while pro-\njected scattered field features provide physical guidance.\n\nThe denoiser is trained using a Mean-Squared-Error (MSE)\nbetween the true and predicted noise. This objective allows\nthe network to implicitly learn the conditional distribution of\nlatent permittivity representations.\n\n3) End-to-End Inversion Framework: The full reconstruc-\n\ntion pipeline proceeds as follows:\n\n\u2022 Encoding: The AE encoder maps the permittivity grid to\n\na latent representation.\n\n\u2022 Diffusion training: The conditional diffusion model\nlearns to denoise noisy latent codes, conditioned on\nscattered fields.\n\n\u2022 Sampling: For unseen measurements,\n\nthe diffusion\nmodel generates latent permittivity representations driven\nby the observed fields.\n\n\u2022 Decoding: The AE decoder\nresolution permittivity grid.\n\nreconstructs\n\nthe full-\n\nThis framework, illustrated in Fig. 1, combines the represen-\ntational efficiency of autoencoders with the generative power\nof diffusion models, resulting in a distribution of generated\nhigh-quality reconstructions of permittivity maps from elec-\ntromagnetic data.\n\n4) Physic Based Selection: The conditional diffusion\nmodel produces a set of reconstructions for one scattered field\ndata that are broadly consistent with the measured scattered\nfields, mirroring the non-uniqueness of the ill-posed inverse\nproblem. That is, after training, the diffusion model is used\nto make several inferences for each measurement, generating\na set of candidate permittivity maps. To choose between\nthe reconstructed candidates, we introduce a post-processing\nstage designed to select the most physically meaningful re-\n\n\fconstruction. Specifically, each candidate is passed through\na forward solver to compute the scattered fields it would\nproduce, illustrated in part b of Fig. 1. We compute the MSE\nbetween the simulated and measured scattered fields, choosing\nthe candidate permittivity map that produces the minimum\nscattered-field MSE, providing a global measure of accuracy.\nBy relying exclusively on scattered-field MSE we do not use\nany prior information regarding the actual permittivity map.\n\nIn summary, the conditional diffusion model ensures that\nthe candidate permittivity maps appear physically reasonable,\nthe\nand the forward-solver selection process ensures that\ncorresponding scattered fields closely match the experimen-\ntal measurements. This paradigm opens the door for post-\nprocessing selection procedures based on different criteria\nand/or other available prior information, e.g, smoothness of\nthe permitivity map.\n\nC. Dataset\n\nTwo datasets are used in this study. The first dataset,\nDataSet1, is the publicly available benchmark presented by\nCathers et al. [42], which includes both synthetic and exper-\nimental measurements. It contains scattering data for Nylon-\n66 cylinders with diameters of 3.8 cm and 10.2 cm (reported\npermittivity: \u03f5r = 3.03 \u2212 j0.03) and a complex-shaped object\nreferred to as the E-phantom, machined from an ultra-high\nmolecular weight polyethylene (UHMWP) block (reported\npermittivity: \u03f5r = 2.3 [43]). The cylinders were translated\nspatially to produce multiple configurations within the imaging\ndomain, while the E-phantom was both translated and rotated\nto generate a diverse set of measurement data. For each target,\nthe dataset provides both synthetic scattered fields, computed\nusing a 2D scalar Method-of-Moments (MoM) forward solver\nbased on Richmond\u2019s method [44] on a 100 \u00d7 100 permittivity\ngrid, and calibrated experimental measurements acquired un-\nder comparable conditions [40], [45]. Representative examples\nof the synthetic and experimental data for each target are\nillustrated in first column of Figure 2.\n\nAlthough DataSet1, obtained from [42], [45], provides valu-\nable benchmarking data, it includes a limited number targets\ndisplaying a restricted class of scattering features. This limits\none\u2019s ability to properly investigate and evaluate an machine-\nlearning model\u2019s robustness and generalizability as an inverse\nsolver. To address this limitation and assess the robustness\nof the proposed inverse solver, we created a new dataset\ncomprising 2000 samples of dielectric targets (DataSet2). The\nscattered fields were generated under ideal 2D point-source\nillumination (3D line sources) using the same 2D scalar MoM\nforward solver on a 100 \u00d7 100 permittivity grid. This dataset\nintroduces composite dielectric targets made up of random\nconfigurations of basic canonical dielectric shapes. The canon-\nical dielectric shapes are circles of varying sizes, hollow\ncircles, and U-shapes of different dimensions. Although the\npermittivity of each is uniform across the shape, one of two\npossible values of permittivity are chosen, either \u03b5r = 2.3 or\n\u03b5r = 3.03, for any particular shape. Each canonical shape\nintroduces challenging scattering features of its own, e.g.,\nthe hollow circles introduces the feature of penetrability of\n\nFig. 2: Evaluation of the proposed model using single fre-\nquency synthetic measurements for three representative sam-\nples from each category. The best reconstruction is selected\nfrom 100 generated candidates using the proposed physics-\nguided selection mechanism.\n\nenergy, and multiple scattering from within the target. The\nU-shaped targets introduce direction-based, or anisotropic,\nscattering. Both have stronger frequency-dependent scattering\nfeatures than the simple solid circles of DataSet1. Although\nthe E-phantom of DataSet1 is approximately a combination of\noverlapping canonical U-shapes, in the E-phantom these are\nfixed with respect to their relative positions.\n\nThe positions, sizes, and orientations of the canonical shapes\nwere randomly varied to create diverse spatial configurations.\nIn addition, each composite target was created using up to\nfour canonical shapes positioned at random within the grid.\nThis produces random overlapping intersections of the shapes,\nfurther increasing the overall geometric complexity of the final\ncomposite scatterer. Some representative composite targets in\nDataSet2 are depicted in Figs. 6, 7 and 8. These two datasets\nenable a comprehensive variation of scattered fields allowing\nus to evaluate the performance of the proposed ML model.\n\nIII. RESULTS\n\nA. Reconstruction Consistency on Synthetic Data\n\nThe latent diffusion model was first\n\ntrained using only\nsynthetic data from DataSet1 and subsequently evaluated also\nusing only unseen synthetic samples that differed in position\nand orientation from those in the training set. Figure 3 presents\nfour independent reconstructions conditioned on scattered-\nfield data at 5 GHz from a single instance of the E-phantom.\nEach of the four candidate solutions shown was generated\nfrom a distinct random noise initialization of the diffusion\nmodel. As illustrated, the model consistently reproduces the\ntarget permittivity distribution with minimal reconstruction\n\n020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 9.8191e-02Perm MSE: 2.7776e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 5.4958e-02Perm MSE: 2.4480e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 5.0247e-02Perm MSE: 4.2363e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012\ferror, demonstrating its stability and robustness to stochastic\nvariation in the sampling process.\n\nTo identify the best reconstruction,\n\nthe diffusion model\nwas used to generated 100 candidate permittivity maps, each\nevaluated using a forward electromagnetic solver to compute\nthe corresponding scattered fields. It should be noted that the\nchoice of 100 reconstructions is arbitrary and can be adjusted\naccording to the time or computational constraints of each\napplication. The reconstruction that was selected as the final\nresult was the one yielding the lowest mean squared error\n(MSE) between the predicted and ground-truth scattered fields.\nAs illustrated in Figure 2 , the selected reconstruction not only\nachieves a low MSE in the scattered field domain but also\nexhibits a low MSE in the permittivity distribution, indicating\nthat the reconstructed permittivity grid closely matches the\nground truth in both material properties and resulting electro-\nmagnetic behavior.The corresponding MSE values for this part\ncan be found in Table I, row 3.\n\nB. Generalization to Experimental Data\n\nTo assess the proposed model\u2019s ability to generalize beyond\nsynthetic training data, the trained model was subsequently\nevaluated on experimental scattered-field measurements from\nDataSet1. As shown in Figure 4, the model achieved promising\nreconstruction performance, successfully recovering key struc-\ntural features from previously unseen experimental samples.\nAlthough the reconstructed permittivity maps were less accu-\nrate than those obtained when testing synthetic data, the results\ndemonstrate the model\u2019s capacity to adapt to experimental\nmeasurements, even though it was trained solely on synthetic\ndata.\n\nThis generalization result is notable given the significant\ndifferences between synthetic and experimental measurements,\nparticularly for the near-field electromagnetic imaging system\nfrom which the experimental data was obtained [45]. Near-\nfield systems, especially those utilizing co-resident antennas,\nexhibit field behaviors that can differ significantly from the\nfields generated using, necessarily approximate, computational\nmodels. For example, the synthetic training data was generated\n\nusing the incident fields of idealized point sources (line-soures)\nand the received fields were simply taken to be scattered\nfields at precise point-locations. In contrast, the experimental\ndata was collected as microwave scattering parameters (S-\nparameters) at the antenna ports. An antenna effectively inte-\ngrates the spatially varying fields across its aperture. In addi-\ntion, the mutual-coupling with nonactive antennas is ignored in\nthe synthetically generated data. Despite these discrepancies,\nour ML model was able to deliver strong reconstruction\nperformance. Of course, this is partly due to the scattered-\nfield calibration procedure that was implemented [45], [46].\nIncorporating the calibration procedure into a machine learn-\ning model of its own has been attempted in the past and is a\nresearch subject of the greatest importance.\n\nTABLE I: Quantitative comparison of average MSE values\nbetween reconstructed and ground-truth permittivity maps\n(MSEimage) and average MSE between true scattered-field data\nand corresponding scattered-field data (MSEdata) for various\ntraining and testing conditions.\n\nTest Case\n\nDataset 1\n\nMSEimage MSEdata\n\nCNN based Model Train on Synthetic and\nTest on Synthetic [42]\n\nCNN based Model Train on both Synthetic\nand Experimental / Test Experimental [42]\n\n0.163\n\n0.089\n\nNA\n\nNA\n\nProposed model Train on Synthetic / Test\non Synthetic (Single Frequency)\n\n0.0590\n\n0.0848\n\nProposed model Train on Synthetic / Test\nExperimental (Single Frequency)\n\n0.0905\n\n0.0869\n\nProposed model Train on Synthetic / Test\nSynthetic (Multi-Frequency)\n\n0.0334\n\n0.0669\n\nDataset 2\n\nTrain Synthetic / Test Synthetic (Single-Fre)\n\n0.0988\n\nTrain Synthetic / Test Synthetic (Multi-Fre)\n\n0.0399\n\n0.0735\n\n0.0679\n\nFig. 3: Four candidate reconstructions of the same scattered-field data at 5 Ghz generated from different random noise\ninitializations, illustrating the generative and stochastic nature of the diffusion-based model.\n\nGround Truth0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.5\fC. Multi-Frequency Inversion and Dataset Enhancement\n\nResults shown so far were obtained by conditioning the\nML model on single frequency scattered-field data. It is well-\nknown that using data from multiple frequencies, either simul-\ntaneously or via frequency-hopping, can enhance reconstruc-\ntion performance especially for non-dispersive targets [47],\n[48]. To investigate how incorporating multiple frequencies\nimproves the ML model\u2019s ability to learn the relationship\nbetween the scattered-field data and the corresponding per-\nmittivity distribution, we conditioned the diffusion model on\nscattered-field measurements acquired at five distinct frequen-\ncies\u20143.0, 3.5, 4.0, 4.5, and 5.0 GHz\u2014instead of a single\nfrequency.\n\nIn addition, as previously mentioned, DataSet1 includes a\nlimited range of object types\u2014specifically, solid circularly-\ncylindrical\ntargets and the E-phantom. Each sample in\nDataSet1 consists of a single object, albeit represented across\nmultiple positions and orientations within the imaging grid.\nTo ensure that the ML model was not over-fitted to a narrow\nset of object geometries\u2014a limitation commonly observed\nin prior studies\u2014we developed the more diverse synthetic\ndataset, Dataset2. It provides a more challenging evaluation\nof the model\u2019s generalization ability.\n\nThe model was re-trained using this newly generated dataset\nand tested on previously unseen samples. Representative re-\nconstructions and their corresponding ground-truth targets are\nshown in Fig. 7. As expected, the inverse problem associ-\nated with this more complex, multi-object dataset presents\nincreased difficulty compared to Dataset 1. Nevertheless, the\nmodel successfully reconstructed the underlying permittivity\ndistributions, effectively capturing both the global structure\nand spatial organization of the objects.\n\nTo further enhance the model\u2019s capacity to learn the map-\nping between scattered-field measurements and correspond-\ning permittivity distributions, we investigated the integration\nof multi-frequency data. Specifically, scattered-field measure-\nments were acquired at five frequencies: 3.0, 3.5, 4.0, 4.5, and\n5.0 GHz. Figure 8 illustrates the model\u2019s performance when\nconditioned on multi-frequency data, demonstrating improved\nreconstruction consistency and sharper structural recovery\nacross test samples.\n\nThe results indicate that while the single-frequency model\noccasionally struggles to accurately reconstruct the shapes of\nclosely spaced or sharp-edged objects\u2014sometimes confusing\nU-shaped targets with hollow circles (see rows 2 and 4 of\nFig. 7)\u2014the multi-frequency model performs substantially bet-\nter. By leveraging additional frequency-domain information, it\naccurately resolves fine structural details and preserves object\nboundaries even in complex spatial configurations.\n\nThis improvement is quantitatively reflected in the last two\nrows of Table I, where the average MSEimage values for the\nmulti-frequency case decrease to approximately one-third of\nthe single-frequency error. The averages were computed over\n10 test cases, with each case reconstructed 100 times to select\nthe best reconstruction based on the MSEdata.\nthe potential of\n\nincorporating\nricher physical information\u2014beyond frequency diversity\u2014to\nimprove data representation and model robustness. Future\n\nThese findings highlight\n\nFig. 4: Evaluation of the proposed model on single-frequency\nexperimental measurements, where the model was trained ex-\nclusively on synthetic data. The best reconstruction is selected\nfrom 100 generated candidates using the proposed physics-\nguided selection mechanism.\n\nFig. 5: Evaluation of the proposed model on experimental\nmeasurements, where the model was trained exclusively on\nusing multi-frequency synthetic data generated using data from\nfive distinct frequencies\u20143.0, 3.5, 4.0, 4.5, and 5.0 GHz. The\nbest reconstruction is selected from 100 generated candidates\nusing the proposed physics-guided selection mechanism.\n\n020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 1.0479e-01Perm MSE: 8.6991e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 5.2647e-02Perm MSE: 1.2157e-01020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 4.9410e-02Perm MSE: 2.3016e-01020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 8.3886e-02Perm MSE: 2.2262e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 2.9979e-02Perm MSE: 2.1566e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 9.9803e-02Perm MSE: 1.9255e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.52.0\fresearch will explore additional labeled datasets encompassing\na more comprehensive set of composite scattering targets to\nfurther enhance the model\u2019s capacity for accurate and stable\ninversion in challenging electromagnetic imaging scenarios.\n\nWhile the qualitative evaluation demonstrates substantial\nimprovements over both traditional inverse solvers such as\nContrast Source Inversion (CSI) and previously reported\nstate-of-the-art models\u2014specifically, the reconstructed images\npresented in Figures 11\u201314 of [42]\u2014a quantitative assess-\nment was also conducted to provide a more comprehensive\nperformance analysis. This evaluation includes comparisons\nwith the state-of-the-art CNN-based supervised inverse solvers\npreviously reported in the literature [42] and the proposed\ngenerative diffusion-based approach. Specifically, the mean\nsquared error (MSE) was evaluated in both the scattered-\nfield domain (MSEdata) and the image domain (MSEimage),\nwith averages computed over 10 test samples. The results,\nsummarized in Table I, clearly indicate that\nthe proposed\nmethod outperforms state-of-the-art inverse solvers, achieves\nnotable performance gains with multi-frequency data, and\nexhibits enhanced generalization capability when evaluated on\nDataset 2.\n\nD. Performance Evaluation of ML Model\u2019s Error\n\nAs the architecture of the designed ML model is quite\ncomplicated it is important to carefully analyze where the\ninference errors ar occuring. This analysis revealed that the\nreconstruction error originates not only from the diffusion\ncomponent but also from the autoencoder (AE) reconstruc-\ntion stage. The AE-induced error was found to be relatively\nminor when the model was trained on the simpler DataSet1\ncontaining a limited number of object\ntypes, but became\nmore pronounced when trained on the more diverse Dataset2.\nTable II presents a quantitative comparison of AE performance\nacross both datasets, reporting the average MSE computed\nover 100 random samples. As shown, the reconstruction error\nincreases for the more complex dataset, reflecting the greater\ndifficulty of compressing and reconstructing diverse geomet-\nrical structures. As the diffusion model works on the latent\nspace of the AE, it is inevitable that errors in compressing\nthe permittivity maps into the latent space would degrade the\noverall performance.\n\nFigure 6 qualitatively illustrates the compression error ob-\ntained when only the AE component of the model is applied\nto both datasets. While improving the AE architecture lies\nbeyond the scope of this study, the results show that the\ncompression-related error (1.39%) accounts for approximately\n25% of the total reconstruction error (3.99%). In an attempt\nto mitigate this effect, the AE performance was optimized\nthrough hyperparameter tuning, particularly by adjusting the\nrelative weighting of loss terms to balance numerical accuracy\nwith perceptual reconstruction quality. The reported results are\nfor performance after this tuning was implemented.\n\nIV. CONCLUSION\n\nThis work presented a physics-informed conditional gen-\nerative framework for solving the electromagnetic inverse\n\nTABLE II: Comparison of AE compression performance av-\neraged over 100 random test samples.\n\nDataset\nDataSet1\nDataSet2\n\nAverage MSE\n0.71%\n1.39%\n\nFig. 6: Comparison of autoencoder\nDataSet1 and DataSet2.\n\nreconstructions\n\nfor\n\nscattering problem in microwave imaging. Leveraging the\ngenerative nature of the diffusion model, the proposed ap-\nproach explicitly captures the inherent uncertainty and non-\nuniqueness of the inverse problem by producing multiple\nplausible reconstructions of the permittivity distribution. A\nforward-solver-based physical evaluation is then employed to\nselect the most consistent reconstruction, ensuring that the\nfinal solution aligns with both the measurement data and the\nunderlying electromagnetic physics.\nThe results demonstrated that\n\nthe model can accurately\nreconstruct complex permittivity distributions from synthetic\ndata, achieving stable and low-error reconstructions. When\napplied to experimental scattered-field measurements\u2014despite\nbeing trained exclusively on synthetic data\u2014the model suc-\ncessfully recovered key structural features, confirming its\nrobustness and generalization capability across measurement\ndomains. The proposed approach outperformed the state-of-\nthe-art deep learning\u2013based inverse solvers reported in [42],\nachieving lower reconstruction error and improved structural\nfidelity in both synthetic and experimental evaluations. In order\nto ensure that our model was not overfitted to a limited number\nof object types\u2014an issue commonly observed in previous\nstudies\u2014we developed a more diverse synthetic dataset. This\nexpanded dataset enabled the model to better generalize to\npreviously unseen geometries.\n\nIncorporating multi-frequency scattered-field data further\nimproved reconstruction accuracy, yielding finer structural\ndetails and enhanced consistency across test samples. These\nfindings are consistent with prior literature, underscoring the\nbenefits of multi-frequency illumination in improving stability\nand resolution in microwave imaging.\n\nThe results also revealed an increase in the reconstruction\nerror attributable to the autoencoder (AE) component. Al-\nthough optimizing the compression AE network lies beyond\nthe scope of this study, preliminary hyperparameter tuning and\n\nGround TruthReconstruction  MSE: 0.009963Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.5Ground TruthReconstruction  MSE: 0.017970Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.5\fFig. 7: Evaluation of the proposed model using single fre-\nquency synthetic measurements for several representative sam-\nples from each category\n\nFig. 8: Evaluation of\nthe proposed model using multi-\nfrequency synthetic measurements for several representative\nsamples from each category\n\n020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 7.3766e-02Perm MSE: 1.2631e-01020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 5.3953e-02Perm MSE: 8.2925e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 7.4881e-02Perm MSE: 9.7638e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 5.3750e-02Perm MSE: 5.0465e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 6.0107e-02Perm MSE: 9.2093e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 8.1709e-02Perm MSE: 1.4224e-01020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 8.1381e-02Perm MSE: 1.3535e-01020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 7.1261e-02Perm MSE: 5.0692e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 3.9557e-02Perm MSE: 3.5397e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 8.8159e-02Perm MSE: 5.6142e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 4.4760e-02Perm MSE: 3.6711e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 4.7440e-02Perm MSE: 4.5211e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 9.5337e-02Perm MSE: 3.8503e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 7.0237e-02Perm MSE: 3.4904e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.00.50.00.51.01.52.0\floss-weight adjustments helped balance numerical accuracy\nand perceptual reconstruction quality.\n\nOverall, the proposed diffusion-based framework demon-\nstrates strong potential for robust, data-driven microwave\nimaging. It achieves superior reconstruction quality relative\nto existing deep learning baselines and exhibits strong cross-\ndomain generalization from synthetic to experimental data. Fu-\nture work will extend this approach to full three-dimensional\nmedical imaging and further refine the underlying AE archi-\ntecture to enhance reconstruction fidelity and computational\nefficiency.\n\nV. ACKNOWLEDGMENT\n\nThe authors acknowledge the help of Mr. Seth Cathers who\n\nsupplied the forward solver.\n\nREFERENCES\n\n[1] N. AlSawaftah, S. El-Abed, S. Dhou, and A. Zakaria, \u201cMicrowave\nimaging for early breast cancer detection: Current state, challenges, and\nfuture directions,\u201d Journal of Imaging, vol. 8, no. 5, 2022. [Online].\nAvailable: https://www.mdpi.com/2313-433X/8/5/123\n\n[2] K. Brinker, M. Dvorsky, M. T. Al Qaseer, and R. Zoughi, \u201cReview of\nadvances in microwave and millimetre-wave NDT&E: Principles and\napplications,\u201d Philosophical Transactions of the Royal Society A, vol.\n378, no. 2182, p. 20190585, 2020.\n\n[3] J. LoVetri, M. Asefi, C. Gilmore, and I. Jeffrey, \u201cInnovations in electro-\nmagnetic imaging technology: The stored-grain-monitoring case,\u201d IEEE\nAntennas and Propagation Magazine, vol. 62, no. ",
            "char_count": 47140,
            "word_count": 6011
          },
          {
            "chunk_id": 1,
            "text": "5, pp. ",
            "char_count": 7,
            "word_count": 2
          },
          {
            "chunk_id": 2,
            "text": "33\u201342, 2020.\n[4] D. L. Colton, R. Kress, and R. Kress, Inverse acoustic and electromag-\n\nnetic scattering theory. Springer, 1998, vol. 93.\n\n[5] A. J. Devaney, Mathematical foundations of imaging, tomography and\n\nwavefield inversion. Cambridge University Press, 2012.\n\n[6] P. Mojabi and J. LoVetri, \u201cOverview and classification of some reg-\nularization techniques for the gauss-newton inversion method applied\nto inverse scattering problems,\u201d IEEE Transactions on Antennas and\nPropagation, vol. 57, no. ",
            "char_count": 502,
            "word_count": 71
          },
          {
            "chunk_id": 3,
            "text": "9, pp. ",
            "char_count": 7,
            "word_count": 2
          },
          {
            "chunk_id": 4,
            "text": "2658\u20132665, 2009.\n\n[7] O. Bucci, I. Catapano, L. Crocco, and T. Isernia, \u201cSynthesis of new\nvariable dielectric profile antennas via inverse scattering techniques:\na feasibility study,\u201d IEEE Transactions on Antennas and Propagation,\nvol. 53, no. ",
            "char_count": 244,
            "word_count": 35
          }
        ],
        "stats": {
          "mode": "Semantic",
          "num_chunks": 23,
          "avg_chars": 2501.4347826086955,
          "min_chars": 6,
          "max_chars": 47140,
          "avg_words": 325.6521739130435,
          "status": "success"
        },
        "time_sec": 1.0886039733886719
      },
      "proposition": {
        "chunks": [],
        "stats": {
          "mode": "Proposition",
          "num_chunks": 0,
          "avg_chars": 0,
          "min_chars": 0,
          "max_chars": 0,
          "avg_words": 0,
          "status": "not_available"
        },
        "time_sec": 0
      },
      "hybrid": {
        "chunks": [
          {
            "chunk_id": 0,
            "text": "Physics-Guided Conditional Diffusion Networks for\nMicrowave Image Reconstruction\nShirin Chehelgami, Joe LoVetri, and Vahab Khoshdel\u2217\n\nDepartment of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Manitoba, Canada\n\u2217Corresponding author: vahab.khoshdel@umanitoba.ca\n\n5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\n\nV\n\nI\n.\ns\ns\ne\ne\n[\n\n1\nv\n9\n2\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nAbstract\u2014A conditional latent-diffusion based framework for\nsolving the electromagnetic inverse scattering problem associ-\nated with microwave imaging is introduced. This generative\nmachine-learning model explicitly mirrors the non-uniqueness\nof the ill-posed inverse problem. Unlike existing inverse solvers\nutilizing deterministic machine learning techniques that produce\na single reconstruction,\nthe proposed latent-diffusion model\ngenerates multiple plausible permittivity maps conditioned on\nmeasured scattered-field data, thereby generating several po-\ntential instances in the range-space of the non-unique inverse\nmapping. A forward electromagnetic solver is integrated into the\nreconstruction pipeline as a physics-based evaluation mechanism.\nThe space of candidate reconstructions form a distribution\nof possibilities consistent with the conditioning data and the\nmember of this space yielding the lowest scattered-field data\ndiscrepancy between the predicted and measured scattered fields\nis reported as the final solution. Synthetic and experimental\nlabeled datasets are used for training and evaluation of the\nmodel. An innovative labeled synthetic dataset is created that\nexemplifies a varied set of scattering features. Training of the\nmodel using this new dataset produces high-quality permittivity\nreconstructions achieving improved generalization with excellent\nfidelity to shape recognition. The results highlight the potential of\nhybrid generative\u2013physics frameworks as a promising direction\nfor robust, data-driven microwave imaging.\n\nIndex Terms\u2014Conditional Diffusion Model, Physics-Guided\nGenerative Model, Microwave Imaging, Electromagnetic Inverse\nScattering, Inverse Problems.\n\nI. INTRODUCTION\n\nQ UANTITATIVE Microwave Imaging (MWI) is a non-\n\ninvasive technique with applications in a wide range\nof areas such as medical diagnostics, and non-destructive\ntesting in agricultural and industrial settings [1]\u2013[3]. At its\ncore, MWI is a wavefield modality that has associated with\nit an electromagnetic inverse scattering problem (ISCP). In\nan electromagnetic ISCP the objective is to quantitatively\nreconstruct a map of the dielectric properties, and potentially\nthe dielectric loss, of the inaccessible interior of an object or\nregion of interest (OI/ROI). The data used for this reconstruc-\ntion are the measured scattered fields when the target OI/ROI is\ninterrogated by a known, and usually controllable, impinging\nincident field.\n\nThe electromagnetic ISCP has associated with it an ill-\nposed wavefield inverse source problem (ISP), that in addition\nto being highly sensitive to noise in the measurement data,\nmeans that more than one permittivity map will produce the\nthe solution to the ISP is non-\nsame scattered field,\nunique (see, e.g., [4], [5]). This makes accurate reconstructions\n\ni.e.,\n\nparticularly challenging and usually requires the augmentation\nof the acquired scattered-field data with some sort of prior\ninformation, e.g., the use of regularization methods [4], [6].\nA question arises as to whether the non-uniqueness of the\nill-posed wavefield inversion problem can be mirrored in an\nalgorithmic procedure to some benefit, as it has, for example,\nin the formulation of design problems as an inverse problem\n[7]\u2013[11].\n\nTraditional algorithms for the ISCP, such as Contrast Source\nInversion (CSI) [12] and Gauss-Newton Inversion [13], [14],\nrely on iterative optimization. These approaches typically\nincorporate a physically rigorous model of the data-acquisition\nsystem and discretization of the permittivity map of the target\nbeing imaged [15] makes these techniques computationally\nexpensive. Even with the use of calibration techniques that\nlessen demands on accurate models of the data-acquisition\nsystem, these optimization-based inversion methods require\nmany iterations of a forward solver before converging. In\naddition, the non-uniqueness of the underlying ISCP means\nthat they don\u2019t always converge to the true permittivity map\nfor the OI/ROI.\n\nIn recent years, machine learning\u2014particularly deep learn-\ning\u2014has gained significant\ntraction for MWI [16]. Deep\nlearning methods have been applied across nearly all stages\nof the imaging pipeline, including calibration, post-processing,\nimage enhancement, and inverse problem solving [17]. Among\nthese tasks, addressing the associated ISCP is clearly the\nmost challenging but can have the greatest impact. Supervised\nlearning approaches attempt to bypass iterative optimization\nby directly mapping scattered-field measurements to target\nreconstructions using large datasets of measurement/target\npairs [18]\u2013[20]. Studies have shown that deep networks can\nachieve high-quality reconstructions and even enable real-\ntime imaging [20], particularly when combined with physics-\ninformed pre-processing such as the Born approximation or\nelectromagnetic backpropagation [21]\u2013[23].\n\nA critical limitation of both traditional inversion techniques\nand deterministic Machine-Learning (ML) frameworks is that,\nby definition, they output a single reconstruction for a given in-\nput, neglecting the fundamental non-uniqueness of the electro-\nmagnetic inverse problem. In reality, multiple plausible target\npermittivity maps may correspond to the same measurement\ndata [4], [5]. In deterministic machine-learning frameworks\nthis one-to-one map is ultimately created by the training data\nset and thereby limits the trained ML model\u2019s generalizability\nto making predictions on scattered-field data corresponding\n\n \n \n \n \n \n \n\fto unseen targets that are too far away from the training data.\nThat is, it is widely recognized that such ML models are highly\nsensitive to \u201cdomain gaps\u201d [24].\n\nIn addition to the deterministic nature of many supervised\nML models, there is the related issue that these approaches\nrequire large datasets for training. To address this, researchers\nhave employed generative models such as Generative Adver-\nsarial Networks (GANs) and diffusion models to create large\nlabeled datasets for subsequent use in supervised learning [25].\nFor instance, Shao et al. [26] combined a GAN with a\ndeep neural network to approximate electromagnetic scattering\nin microwave breast imaging, thereby generating additional\ntraining data.\n\nAlthough synthetic datasets are convenient, they often fail\nto capture practical complexities such as antenna coupling,\ncalibration errors, and hardware imperfections [27]. Therefore,\nalthough large synthetically generated datasets have indeed\nbeen used for training, resulting ML models typically require\neither augmenting the training set with experimental data or\n\u201ccalibrating out\u201d the experimental setup to make the data more\nrepresentative of synthetic conditions. Otherwise, networks\ntrained exclusively on synthetic data often struggle to general-\nize to experimental measurements acquired using unique lab-\nspecific systems. It should be commented that currently there\ndoes not exist a standardized MWI data-acquisition setup\u2013\nmost researchers have developed there own unique systems.\n\nThus, the gap between synthetic training data and exper-\nimental measurements remains a major barrier to deploying\ngeneralizable deep learning\u2013based MWI systems. On the other\nhand, generative approaches are inherently less sensitive to\ndomain discrepancies, as they learn the underlying data man-\nifold rather than a deterministic mapping, thereby mitigating\nthe impact of distributional shifts between training and testing\ndatasets.\n\nMost recently, researchers have leveraged the inherent non-\nuniqueness of generative models to tackle the non-unique\nnature of inverse problems. Early efforts predominantly em-\nployed GANs and VAEs. For example, Bhadra et al. intro-\nduced an image-adaptive GAN framework that allows high-\nfidelity reconstruction of under-sampled MRI data and im-\nproved data consistency in ill-posed inverse imaging prob-\nlems [28]. Similarly, Yao et al. [29] introduced a conditional\nGAN framework to map scattered electromagnetic fields di-\nrectly to dielectric contrasts, enabling real-time image recon-\nstruction. Despite these advancements, GAN-based approaches\noften suffer from training instabilities, mode collapse, and\nlimited control over output diversity.\n\nTo overcome these limitations, diffusion and score-based\ngenerative models have emerged as more robust, theoretically\ngrounded alternatives. These models have demonstrated state-\nof-the-art performance across a broad range of imaging tasks,\nincluding synthesis, reconstruction, and enhancement [30]\u2013\n[32]. Compared with GANs and VAEs, diffusion-based ap-\nproaches exhibit stable optimization dynamics, provide well-\ndefined likelihood formulations, and yield superior generative\nfidelity. Accumulating evidence indicates that they consistently\nsurpass earlier generative paradigms in various medical imag-\ning applications [30], [31], [33], [34].\n\nIn the domain of medical image reconstruction, Song et\nal. [34] proposed a score-based generative framework capa-\nble of reconstructing medical images from partial CT and\nMRI measurements in an unsupervised manner, achieving\nstrong generalization across diverse measurement processes.\nNonetheless, applications of such models as inverse solvers for\nMWI remain limited. Recently, Bi et al. [35] introduced Diffu-\nsionEMIS, a diffusion-based method that iteratively refines 3-\nD point clouds to reconstruct scatterer geometries conditioned\non measured scattered fields.\n\nBuilding on these advances, we propose a conditional\ndiffusion-based generative model that explicitly incorporates\nthe non-unique nature of the microwave inverse scattering\nproblem. Unlike deterministic supervised networks that pro-\nduce a single estimate for a given set of measurements,\nour framework generates multiple plausible reconstructions\nconsistent with the same data, thereby reflecting the inherent\nnon-uniqueness of the ill-posed problem and leveraging the\nstrengths of diffusion models to produce physically mean-\ningful solutions. The core innovation of our approach lies\nin the integration of a physics-based selection mechanism,\ntransforming the framework into a physics-informed gener-\native system. After the diffusion model produces multiple\ncandidate reconstructions, a forward electromagnetic solver\nis applied to each candidate to predict\nthe corresponding\nscattered fields. The reconstruction yielding the lowest data\ndiscrepancy with respect to the measured fields is reported as\nthe final solution. This physics-guided validation ensures that\nthe chosen reconstruction is not only statistically plausible,\nbased on the conditioning data, but also physically consistent\nwith the underlying electromagnetic principles.\n\nThis integration of a forward solver marks a departure from\npurely data-driven approaches, forming a hybrid architecture\nthat combines the generative flexibility of diffusion models\nwith the physical accuracy of electromagnetic modeling. By\nembedding physics-based validation directly within the infer-\nence process, our method effectively bridges the gap between\nmachine learning efficiency and the physics of electromagnetic\ninteractions with the target, addressing one of the major\nlimitations of deep learning-based inverse solvers.\n\nThe proposed methodology is particularly well-suited for\ndata collected using actual MWI systems, where measurement\nnoise, calibration errors, and model mismatches often degrade\nperformance. By generating multiple candidate solutions and\nselecting the most physically consistent one, our approach\nachieves enhanced robustness and generalization. We validate\nthe method using both synthetic and experimental datasets,\ndemonstrating improved reconstruction accuracy and stability\ncompared to deterministic baselines.\n\nTo address the challenges outlined above, the remainder\nof this paper is organized as follows. Section II provides\nan overview of the methodology and fundamental concepts\nbehind diffusion models, including the forward and reverse\nprocesses and the use of diffusion priors for inverse problems.\nSection III introduces the proposed latent diffusion framework,\nwhich integrates a latent autoencoder representation, a physics-\naware conditioning mechanism, and an end-to-end inversion\narchitecture augmented by a physics-based selection strategy.\n\n\fSection IV describes the dataset used for training and evalua-\ntion, followed by Section V, which presents the experimental\nresults, including reconstruction consistency on synthetic data,\ngeneralization to experimental measurements, multi-frequency\ninversion, and performance evaluation of model error. Finally,\nSection VI concludes the paper with key findings and future\ndirections, and Section VII acknowledges supporting contri-\nbutions.\n\nII. METHODOLOGY\n\nA. Diffusion Models\n\nForward and reverse processes.: Diffusion models aim\nto approximate a target data distribution by constructing a\nforward process that incrementally adds Gaussian noise and\na reverse process that learns to remove this noise. Given a\nclean sample x0 \u223c pdata, the forward Markov chain {xt}T\nevolves as\nxt = (cid:112)1 \u2212 \u03b2t xt\u22121+(cid:112)\u03b2t wt, wt\nwhere the noise schedule {\u03b2t} governs how quickly the\nsignal is corrupted. As t increases, xt approaches an isotropic\nGaussian.\n\ni.i.d.\u223c N (0, I),\n\n1 \u2264 t \u2264 T,\n\nt=1\n\nGeneration proceeds by starting from a Gaussian xrev\n\nT \u223c\n\nN (0, I) and iteratively denoising back to xrev\n0 :\nT \u22121 \u2212\u2192 \u00b7 \u00b7 \u00b7 \u2212\u2192 xrev\n0 .\n\nT \u2212\u2192 xrev\nxrev\n\nSampling algorithms for this reverse chain\u2014deterministic or\nstochastic\u2014can be interpreted as discretizations of underlying\nODEs or SDEs. They rely on the score function s\u22c6\nt (x) =\n\u2207x log pxt(x) of the intermediate distributions. Tweedie\u2019s for-\nmula provides an explicit expression,\n\ns\u22c6\nt (x) = \u2212\n\n(cid:90)\n\n1\n1 \u2212 \u03b1t\n\npX0|Xt(x0 | x) (cid:0)x \u2212\n\n\u221a\n\n\u03b1t x0\n\n(cid:1) dx0,\n\n:= 1 \u2212 \u03b2t and \u03b1t\n\nwith \u03b1t\nk=1 \u03b1k. In practice, one\ntrains a neural network to approximate these score functions\nvia score matching, enabling the learned model to generate\nsamples from pdata.\n\n:= (cid:81)t\n\nDiffusion priors for inverse problems.: The flexibility of\ndiffusion models makes them attractive as priors for ill-posed\ninverse problems. Suppose we observe data y generated from\nan unknown signal x\u22c6 through a known forward operator\nand additive noise. A Bayesian formulation samples from the\nposterior\n\np(x | y) \u221d pprior(x) p(cid:0)y | x(cid:1),\n\nwhere pprior(x) is the diffusion-model prior and p(y | x) is the\nlikelihood. Sampling from this posterior combines the learned\nscore functions with the physics of the measurement process.\n\nB. Proposed Latent Diffusion Model\n\nWe consider the electromagnetic inverse problem of re-\nconstructing two-dimensional permittivity distributions from\nscattered field measurements. The scattered fields are repre-\nsented as two channels, corresponding to the real and imagi-\nnary components of the electric field. To overcome the non-\nuniqueness of the inverse problem, we design a generative\n\ninversion framework based on a conditional diffusion model\noperating in a compact latent space of permittivity maps. This\nframework produces multiple plausible reconstructions that are\ndriven by, i.e., consistent with, the measured fields, after which\na post-processing step is applied to identify the most physically\nmeaningful solution.\n\n1) Latent Representation with Autoencoder: Directly ap-\nplying a diffusion model to full-resolution permittivity maps\nis computationally expensive, particularly when extending this\nwork to 3D medical imaging applications, which represent the\nultimate goal of our research. To address this, we first train\nan autoencoder (AE) to learn a compact latent representation,\nfollowing a design similar to the one used in [36].\n\n\u2022 Encoder (E): compresses the permittivity grid x into a\nlatent representation z = E(x) using convolution and\ndownsampling layers.\n\n\u2022 Decoder (D): reconstructs the grid from the latent code\nas \u02dcx = D(z) = D(E(x)) using upsampling operations.\n\nThe AE is trained with a composite loss function:\n\u2022 Reconstruction loss (pixelwise error) ensures quantitative\n\nfidelity to the input.\n\n\u2022 Perceptual loss [37], computed from intermediate features\nof a pretrained VGG16 network [38], encourages preser-\nvation of edges and structural features.\n\n\u2022 Adversarial loss [39] penalizes overly smooth reconstruc-\n\ntions and promotes realistic textural detail.\n\nThe relative weights of these terms are tuned to balance\nnumerical accuracy with perceptual quality. By compressing a\n100\u00d7100 grid into a 16\u00d724\u00d724 latent vector, the AE reduces\nthe computational cost of the subsequent diffusion process.\n\n2) Physics-Aware Conditioning Mechanism: The learned\nlatent space serves as the domain for our diffusion model. The\nmodel is conditioned on measured scattered fields collected in\nthe scenario depicted in Fig. xxxx where a dielectric target is\nlocated at the center of a ring of transmitter and receivers. The\ntransmitters and receivers are located at equidistant points on\na circle of radius XXX m surrounding the target. A complete\ndescriprion can be found in [40]. This condition ensures that\nthe generated permittivity maps are physically consistent with\nthe observations. The data is composed of a 24\u00d724 scattered-\nfield matrix of complex-valued data representing the real and\nimaginary parts of the received frequency-domain phasor. Data\nconsists of up to five frequencies collected at 3.0, 3.5, 4.0, 4.5,\nand 5.0 GHz.\n\n\u2022 Forward diffusion: Gaussian noise is added to latent\n\nvectors over a predefined schedule.\n\n\u2022 Reverse denoising: A U-Net\u2013based denoiser is trained\nto iteratively predict and remove this noise. The denoiser\nis conditioned on the scattered fields via cross-attention\nlayers.\n\n\u2022 Conditioning mechanism: In the proposed framework,\nthe scattered electromagnetic fields are initially processed\nthrough convolutional layers to project them into a feature\nspace. Because electromagnetic inverse problems depend\non the spatial arrangement of transmitters, receivers,\ninforma-\nand objects,\ntion\u2014something that standard convolution and pooling\n\nto retain positional\n\nis vital\n\nit\n\n\f(a) Training the model\n\nFig. 1: Overview of the proposed framework: (a) training phase of the denoising diffusion model; (b) inference phase for\nreconstructing the output from measured fields.\n\n(b) Inference\n\nlayers tend to lose. To address this, we augment the 24\u00d724\nscattered-field inputs with sinusoidal positional encodings\nthat explicitly encode transmitters coordinates. Injecting\nthese encodings into the network provides an absolute\nreference frame, enabling the model to distinguish sensor\nlocations and object orientations. This spatial awareness\nis especially important for microwave imaging, where the\ngeometry of the scene strongly influences the observed\nscattering. These features are then injected back into\nthe diffusion model\u2019s denoiser at every step: they are\nconcatenated with intermediate feature maps and linked\nvia cross-attention blocks so that the denoising operation\nremains conditioned on the measured electromagnetic\nresponse. By coupling the generative model to the ob-\nserved physics in this way, we encourage it to produce\nreconstructions that are physically consistent with the\nactual scattered fields.\n\n\u2022 Architecture: The U-Net consists of downsampling and\nupsampling blocks with intermediate attention [41]. Time\nembeddings indicate the current noise level, while pro-\njected scattered field features provide physical guidance.\n\nThe denoiser is trained using a Mean-Squared-Error (MSE)\nbetween the true and predicted noise. This objective allows\nthe network to implicitly learn the conditional distribution of\nlatent permittivity representations.\n\n3) End-to-End Inversion Framework: The full reconstruc-\n\ntion pipeline proceeds as follows:\n\n\u2022 Encoding: The AE encoder maps the permittivity grid to\n\na latent representation.\n\n\u2022 Diffusion training: The conditional diffusion model\nlearns to denoise noisy latent codes, conditioned on\nscattered fields.\n\n\u2022 Sampling: For unseen measurements,\n\nthe diffusion\nmodel generates latent permittivity representations driven\nby the observed fields.\n\n\u2022 Decoding: The AE decoder\nresolution permittivity grid.\n\nreconstructs\n\nthe full-\n\nThis framework, illustrated in Fig. 1, combines the represen-\ntational efficiency of autoencoders with the generative power\nof diffusion models, resulting in a distribution of generated\nhigh-quality reconstructions of permittivity maps from elec-\ntromagnetic data.\n\n4) Physic Based Selection: The conditional diffusion\nmodel produces a set of reconstructions for one scattered field\ndata that are broadly consistent with the measured scattered\nfields, mirroring the non-uniqueness of the ill-posed inverse\nproblem. That is, after training, the diffusion model is used\nto make several inferences for each measurement, generating\na set of candidate permittivity maps. To choose between\nthe reconstructed candidates, we introduce a post-processing\nstage designed to select the most physically meaningful re-\n\n\fconstruction. Specifically, each candidate is passed through\na forward solver to compute the scattered fields it would\nproduce, illustrated in part b of Fig. 1. We compute the MSE\nbetween the simulated and measured scattered fields, choosing\nthe candidate permittivity map that produces the minimum\nscattered-field MSE, providing a global measure of accuracy.\nBy relying exclusively on scattered-field MSE we do not use\nany prior information regarding the actual permittivity map.\n\nIn summary, the conditional diffusion model ensures that\nthe candidate permittivity maps appear physically reasonable,\nthe\nand the forward-solver selection process ensures that\ncorresponding scattered fields closely match the experimen-\ntal measurements. This paradigm opens the door for post-\nprocessing selection procedures based on different criteria\nand/or other available prior information, e.g, smoothness of\nthe permitivity map.\n\nC. Dataset\n\nTwo datasets are used in this study. The first dataset,\nDataSet1, is the publicly available benchmark presented by\nCathers et al. [42], which includes both synthetic and exper-\nimental measurements. It contains scattering data for Nylon-\n66 cylinders with diameters of 3.8 cm and 10.2 cm (reported\npermittivity: \u03f5r = 3.03 \u2212 j0.03) and a complex-shaped object\nreferred to as the E-phantom, machined from an ultra-high\nmolecular weight polyethylene (UHMWP) block (reported\npermittivity: \u03f5r = 2.3 [43]). The cylinders were translated\nspatially to produce multiple configurations within the imaging\ndomain, while the E-phantom was both translated and rotated\nto generate a diverse set of measurement data. For each target,\nthe dataset provides both synthetic scattered fields, computed\nusing a 2D scalar Method-of-Moments (MoM) forward solver\nbased on Richmond\u2019s method [44] on a 100 \u00d7 100 permittivity\ngrid, and calibrated experimental measurements acquired un-\nder comparable conditions [40], [45]. Representative examples\nof the synthetic and experimental data for each target are\nillustrated in first column of Figure 2.\n\nAlthough DataSet1, obtained from [42], [45], provides valu-\nable benchmarking data, it includes a limited number targets\ndisplaying a restricted class of scattering features. This limits\none\u2019s ability to properly investigate and evaluate an machine-\nlearning model\u2019s robustness and generalizability as an inverse\nsolver. To address this limitation and assess the robustness\nof the proposed inverse solver, we created a new dataset\ncomprising 2000 samples of dielectric targets (DataSet2). The\nscattered fields were generated under ideal 2D point-source\nillumination (3D line sources) using the same 2D scalar MoM\nforward solver on a 100 \u00d7 100 permittivity grid. This dataset\nintroduces composite dielectric targets made up of random\nconfigurations of basic canonical dielectric shapes. The canon-\nical dielectric shapes are circles of varying sizes, hollow\ncircles, and U-shapes of different dimensions. Although the\npermittivity of each is uniform across the shape, one of two\npossible values of permittivity are chosen, either \u03b5r = 2.3 or\n\u03b5r = 3.03, for any particular shape. Each canonical shape\nintroduces challenging scattering features of its own, e.g.,\nthe hollow circles introduces the feature of penetrability of\n\nFig. 2: Evaluation of the proposed model using single fre-\nquency synthetic measurements for three representative sam-\nples from each category. The best reconstruction is selected\nfrom 100 generated candidates using the proposed physics-\nguided selection mechanism.\n\nenergy, and multiple scattering from within the target. The\nU-shaped targets introduce direction-based, or anisotropic,\nscattering. Both have stronger frequency-dependent scattering\nfeatures than the simple solid circles of DataSet1. Although\nthe E-phantom of DataSet1 is approximately a combination of\noverlapping canonical U-shapes, in the E-phantom these are\nfixed with respect to their relative positions.\n\nThe positions, sizes, and orientations of the canonical shapes\nwere randomly varied to create diverse spatial configurations.\nIn addition, each composite target was created using up to\nfour canonical shapes positioned at random within the grid.\nThis produces random overlapping intersections of the shapes,\nfurther increasing the overall geometric complexity of the final\ncomposite scatterer. Some representative composite targets in\nDataSet2 are depicted in Figs. 6, 7 and 8. These two datasets\nenable a comprehensive variation of scattered fields allowing\nus to evaluate the performance of the proposed ML model.\n\nIII. RESULTS\n\nA. Reconstruction Consistency on Synthetic Data\n\nThe latent diffusion model was first\n\ntrained using only\nsynthetic data from DataSet1 and subsequently evaluated also\nusing only unseen synthetic samples that differed in position\nand orientation from those in the training set. Figure 3 presents\nfour independent reconstructions conditioned on scattered-\nfield data at 5 GHz from a single instance of the E-phantom.\nEach of the four candidate solutions shown was generated\nfrom a distinct random noise initialization of the diffusion\nmodel. As illustrated, the model consistently reproduces the\ntarget permittivity distribution with minimal reconstruction\n\n020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 9.8191e-02Perm MSE: 2.7776e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 5.4958e-02Perm MSE: 2.4480e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 5.0247e-02Perm MSE: 4.2363e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012\ferror, demonstrating its stability and robustness to stochastic\nvariation in the sampling process.\n\nTo identify the best reconstruction,\n\nthe diffusion model\nwas used to generated 100 candidate permittivity maps, each\nevaluated using a forward electromagnetic solver to compute\nthe corresponding scattered fields. It should be noted that the\nchoice of 100 reconstructions is arbitrary and can be adjusted\naccording to the time or computational constraints of each\napplication. The reconstruction that was selected as the final\nresult was the one yielding the lowest mean squared error\n(MSE) between the predicted and ground-truth scattered fields.\nAs illustrated in Figure 2 , the selected reconstruction not only\nachieves a low MSE in the scattered field domain but also\nexhibits a low MSE in the permittivity distribution, indicating\nthat the reconstructed permittivity grid closely matches the\nground truth in both material properties and resulting electro-\nmagnetic behavior.The corresponding MSE values for this part\ncan be found in Table I, row 3.\n\nB. Generalization to Experimental Data\n\nTo assess the proposed model\u2019s ability to generalize beyond\nsynthetic training data, the trained model was subsequently\nevaluated on experimental scattered-field measurements from\nDataSet1. As shown in Figure 4, the model achieved promising\nreconstruction performance, successfully recovering key struc-\ntural features from previously unseen experimental samples.\nAlthough the reconstructed permittivity maps were less accu-\nrate than those obtained when testing synthetic data, the results\ndemonstrate the model\u2019s capacity to adapt to experimental\nmeasurements, even though it was trained solely on synthetic\ndata.\n\nThis generalization result is notable given the significant\ndifferences between synthetic and experimental measurements,\nparticularly for the near-field electromagnetic imaging system\nfrom which the experimental data was obtained [45]. Near-\nfield systems, especially those utilizing co-resident antennas,\nexhibit field behaviors that can differ significantly from the\nfields generated using, necessarily approximate, computational\nmodels. For example, the synthetic training data was generated\n\nusing the incident fields of idealized point sources (line-soures)\nand the received fields were simply taken to be scattered\nfields at precise point-locations. In contrast, the experimental\ndata was collected as microwave scattering parameters (S-\nparameters) at the antenna ports. An antenna effectively inte-\ngrates the spatially varying fields across its aperture. In addi-\ntion, the mutual-coupling with nonactive antennas is ignored in\nthe synthetically generated data. Despite these discrepancies,\nour ML model was able to deliver strong reconstruction\nperformance. Of course, this is partly due to the scattered-\nfield calibration procedure that was implemented [45], [46].\nIncorporating the calibration procedure into a machine learn-\ning model of its own has been attempted in the past and is a\nresearch subject of the greatest importance.\n\nTABLE I: Quantitative comparison of average MSE values\nbetween reconstructed and ground-truth permittivity maps\n(MSEimage) and average MSE between true scattered-field data\nand corresponding scattered-field data (MSEdata) for various\ntraining and testing conditions.\n\nTest Case\n\nDataset 1\n\nMSEimage MSEdata\n\nCNN based Model Train on Synthetic and\nTest on Synthetic [42]\n\nCNN based Model Train on both Synthetic\nand Experimental / Test Experimental [42]\n\n0.163\n\n0.089\n\nNA\n\nNA\n\nProposed model Train on Synthetic / Test\non Synthetic (Single Frequency)\n\n0.0590\n\n0.0848\n\nProposed model Train on Synthetic / Test\nExperimental (Single Frequency)\n\n0.0905\n\n0.0869\n\nProposed model Train on Synthetic / Test\nSynthetic (Multi-Frequency)\n\n0.0334\n\n0.0669\n\nDataset 2\n\nTrain Synthetic / Test Synthetic (Single-Fre)\n\n0.0988\n\nTrain Synthetic / Test Synthetic (Multi-Fre)\n\n0.0399\n\n0.0735\n\n0.0679\n\nFig. 3: Four candidate reconstructions of the same scattered-field data at 5 Ghz generated from different random noise\ninitializations, illustrating the generative and stochastic nature of the diffusion-based model.\n\nGround Truth0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.5\fC. Multi-Frequency Inversion and Dataset Enhancement\n\nResults shown so far were obtained by conditioning the\nML model on single frequency scattered-field data. It is well-\nknown that using data from multiple frequencies, either simul-\ntaneously or via frequency-hopping, can enhance reconstruc-\ntion performance especially for non-dispersive targets [47],\n[48]. To investigate how incorporating multiple frequencies\nimproves the ML model\u2019s ability to learn the relationship\nbetween the scattered-field data and the corresponding per-\nmittivity distribution, we conditioned the diffusion model on\nscattered-field measurements acquired at five distinct frequen-\ncies\u20143.0, 3.5, 4.0, 4.5, and 5.0 GHz\u2014instead of a single\nfrequency.\n\nIn addition, as previously mentioned, DataSet1 includes a\nlimited range of object types\u2014specifically, solid circularly-\ncylindrical\ntargets and the E-phantom. Each sample in\nDataSet1 consists of a single object, albeit represented across\nmultiple positions and orientations within the imaging grid.\nTo ensure that the ML model was not over-fitted to a narrow\nset of object geometries\u2014a limitation commonly observed\nin prior studies\u2014we developed the more diverse synthetic\ndataset, Dataset2. It provides a more challenging evaluation\nof the model\u2019s generalization ability.\n\nThe model was re-trained using this newly generated dataset\nand tested on previously unseen samples. Representative re-\nconstructions and their corresponding ground-truth targets are\nshown in Fig. 7. As expected, the inverse problem associ-\nated with this more complex, multi-object dataset presents\nincreased difficulty compared to Dataset 1. Nevertheless, the\nmodel successfully reconstructed the underlying permittivity\ndistributions, effectively capturing both the global structure\nand spatial organization of the objects.\n\nTo further enhance the model\u2019s capacity to learn the map-\nping between scattered-field measurements and correspond-\ning permittivity distributions, we investigated the integration\nof multi-frequency data. Specifically, scattered-field measure-\nments were acquired at five frequencies: 3.0, 3.5, 4.0, 4.5, and\n5.0 GHz. Figure 8 illustrates the model\u2019s performance when\nconditioned on multi-frequency data, demonstrating improved\nreconstruction consistency and sharper structural recovery\nacross test samples.\n\nThe results indicate that while the single-frequency model\noccasionally struggles to accurately reconstruct the shapes of\nclosely spaced or sharp-edged objects\u2014sometimes confusing\nU-shaped targets with hollow circles (see rows 2 and 4 of\nFig. 7)\u2014the multi-frequency model performs substantially bet-\nter. By leveraging additional frequency-domain information, it\naccurately resolves fine structural details and preserves object\nboundaries even in complex spatial configurations.\n\nThis improvement is quantitatively reflected in the last two\nrows of Table I, where the average MSEimage values for the\nmulti-frequency case decrease to approximately one-third of\nthe single-frequency error. The averages were computed over\n10 test cases, with each case reconstructed 100 times to select\nthe best reconstruction based on the MSEdata.\nthe potential of\n\nincorporating\nricher physical information\u2014beyond frequency diversity\u2014to\nimprove data representation and model robustness. Future\n\nThese findings highlight\n\nFig. 4: Evaluation of the proposed model on single-frequency\nexperimental measurements, where the model was trained ex-\nclusively on synthetic data. The best reconstruction is selected\nfrom 100 generated candidates using the proposed physics-\nguided selection mechanism.\n\nFig. 5: Evaluation of the proposed model on experimental\nmeasurements, where the model was trained exclusively on\nusing multi-frequency synthetic data generated using data from\nfive distinct frequencies\u20143.0, 3.5, 4.0, 4.5, and 5.0 GHz. The\nbest reconstruction is selected from 100 generated candidates\nusing the proposed physics-guided selection mechanism.\n\n020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 1.0479e-01Perm MSE: 8.6991e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 5.2647e-02Perm MSE: 1.2157e-01020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 4.9410e-02Perm MSE: 2.3016e-01020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 8.3886e-02Perm MSE: 2.2262e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 2.9979e-02Perm MSE: 2.1566e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 9.9803e-02Perm MSE: 1.9255e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.52.0\fresearch will explore additional labeled datasets encompassing\na more comprehensive set of composite scattering targets to\nfurther enhance the model\u2019s capacity for accurate and stable\ninversion in challenging electromagnetic imaging scenarios.\n\nWhile the qualitative evaluation demonstrates substantial\nimprovements over both traditional inverse solvers such as\nContrast Source Inversion (CSI) and previously reported\nstate-of-the-art models\u2014specifically, the reconstructed images\npresented in Figures 11\u201314 of [42]\u2014a quantitative assess-\nment was also conducted to provide a more comprehensive\nperformance analysis. This evaluation includes comparisons\nwith the state-of-the-art CNN-based supervised inverse solvers\npreviously reported in the literature [42] and the proposed\ngenerative diffusion-based approach. Specifically, the mean\nsquared error (MSE) was evaluated in both the scattered-\nfield domain (MSEdata) and the image domain (MSEimage),\nwith averages computed over 10 test samples. The results,\nsummarized in Table I, clearly indicate that\nthe proposed\nmethod outperforms state-of-the-art inverse solvers, achieves\nnotable performance gains with multi-frequency data, and\nexhibits enhanced generalization capability when evaluated on\nDataset 2.\n\nD. Performance Evaluation of ML Model\u2019s Error\n\nAs the architecture of the designed ML model is quite\ncomplicated it is important to carefully analyze where the\ninference errors ar occuring. This analysis revealed that the\nreconstruction error originates not only from the diffusion\ncomponent but also from the autoencoder (AE) reconstruc-\ntion stage. The AE-induced error was found to be relatively\nminor when the model was trained on the simpler DataSet1\ncontaining a limited number of object\ntypes, but became\nmore pronounced when trained on the more diverse Dataset2.\nTable II presents a quantitative comparison of AE performance\nacross both datasets, reporting the average MSE computed\nover 100 random samples. As shown, the reconstruction error\nincreases for the more complex dataset, reflecting the greater\ndifficulty of compressing and reconstructing diverse geomet-\nrical structures. As the diffusion model works on the latent\nspace of the AE, it is inevitable that errors in compressing\nthe permittivity maps into the latent space would degrade the\noverall performance.\n\nFigure 6 qualitatively illustrates the compression error ob-\ntained when only the AE component of the model is applied\nto both datasets. While improving the AE architecture lies\nbeyond the scope of this study, the results show that the\ncompression-related error (1.39%) accounts for approximately\n25% of the total reconstruction error (3.99%). In an attempt\nto mitigate this effect, the AE performance was optimized\nthrough hyperparameter tuning, particularly by adjusting the\nrelative weighting of loss terms to balance numerical accuracy\nwith perceptual reconstruction quality. The reported results are\nfor performance after this tuning was implemented.\n\nIV. CONCLUSION\n\nThis work presented a physics-informed conditional gen-\nerative framework for solving the electromagnetic inverse\n\nTABLE II: Comparison of AE compression performance av-\neraged over 100 random test samples.\n\nDataset\nDataSet1\nDataSet2\n\nAverage MSE\n0.71%\n1.39%\n\nFig. 6: Comparison of autoencoder\nDataSet1 and DataSet2.\n\nreconstructions\n\nfor\n\nscattering problem in microwave imaging. Leveraging the\ngenerative nature of the diffusion model, the proposed ap-\nproach explicitly captures the inherent uncertainty and non-\nuniqueness of the inverse problem by producing multiple\nplausible reconstructions of the permittivity distribution. A\nforward-solver-based physical evaluation is then employed to\nselect the most consistent reconstruction, ensuring that the\nfinal solution aligns with both the measurement data and the\nunderlying electromagnetic physics.\nThe results demonstrated that\n\nthe model can accurately\nreconstruct complex permittivity distributions from synthetic\ndata, achieving stable and low-error reconstructions. When\napplied to experimental scattered-field measurements\u2014despite\nbeing trained exclusively on synthetic data\u2014the model suc-\ncessfully recovered key structural features, confirming its\nrobustness and generalization capability across measurement\ndomains. The proposed approach outperformed the state-of-\nthe-art deep learning\u2013based inverse solvers reported in [42],\nachieving lower reconstruction error and improved structural\nfidelity in both synthetic and experimental evaluations. In order\nto ensure that our model was not overfitted to a limited number\nof object types\u2014an issue commonly observed in previous\nstudies\u2014we developed a more diverse synthetic dataset. This\nexpanded dataset enabled the model to better generalize to\npreviously unseen geometries.\n\nIncorporating multi-frequency scattered-field data further\nimproved reconstruction accuracy, yielding finer structural\ndetails and enhanced consistency across test samples. These\nfindings are consistent with prior literature, underscoring the\nbenefits of multi-frequency illumination in improving stability\nand resolution in microwave imaging.\n\nThe results also revealed an increase in the reconstruction\nerror attributable to the autoencoder (AE) component. Al-\nthough optimizing the compression AE network lies beyond\nthe scope of this study, preliminary hyperparameter tuning and\n\nGround TruthReconstruction  MSE: 0.009963Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.5Ground TruthReconstruction  MSE: 0.017970Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.5\fFig. 7: Evaluation of the proposed model using single fre-\nquency synthetic measurements for several representative sam-\nples from each category\n\nFig. 8: Evaluation of\nthe proposed model using multi-\nfrequency synthetic measurements for several representative\nsamples from each category\n\n020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 7.3766e-02Perm MSE: 1.2631e-01020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 5.3953e-02Perm MSE: 8.2925e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 7.4881e-02Perm MSE: 9.7638e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 5.3750e-02Perm MSE: 5.0465e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 6.0107e-02Perm MSE: 9.2093e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 8.1709e-02Perm MSE: 1.4224e-01020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 8.1381e-02Perm MSE: 1.3535e-01020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 7.1261e-02Perm MSE: 5.0692e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 3.9557e-02Perm MSE: 3.5397e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 8.8159e-02Perm MSE: 5.6142e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 4.4760e-02Perm MSE: 3.6711e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 4.7440e-02Perm MSE: 4.5211e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 9.5337e-02Perm MSE: 3.8503e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 7.0237e-02Perm MSE: 3.4904e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.00.50.00.51.01.52.0\floss-weight adjustments helped balance numerical accuracy\nand perceptual reconstruction quality.\n\nOverall, the proposed diffusion-based framework demon-\nstrates strong potential for robust, data-driven microwave\nimaging. It achieves superior reconstruction quality relative\nto existing deep learning baselines and exhibits strong cross-\ndomain generalization from synthetic to experimental data. Fu-\nture work will extend this approach to full three-dimensional\nmedical imaging and further refine the underlying AE archi-\ntecture to enhance reconstruction fidelity and computational\nefficiency.\n\nV. ACKNOWLEDGMENT\n\nThe authors acknowledge the help of Mr. Seth Cathers who\n\nsupplied the forward solver.\n\nREFERENCES\n\n[1] N. AlSawaftah, S. El-Abed, S. Dhou, and A. Zakaria, \u201cMicrowave\nimaging for early breast cancer detection: Current state, challenges, and\nfuture directions,\u201d Journal of Imaging, vol. 8, no. 5, 2022. [Online].\nAvailable: https://www.mdpi.com/2313-433X/8/5/123\n\n[2] K. Brinker, M. Dvorsky, M. T. Al Qaseer, and R. Zoughi, \u201cReview of\nadvances in microwave and millimetre-wave NDT&E: Principles and\napplications,\u201d Philosophical Transactions of the Royal Society A, vol.\n378, no. 2182, p. 20190585, 2020.\n\n[3] J. LoVetri, M. Asefi, C. Gilmore, and I. Jeffrey, \u201cInnovations in electro-\nmagnetic imaging technology: The stored-grain-monitoring case,\u201d IEEE\nAntennas and Propagation Magazine, vol. 62, no. ",
            "char_count": 47140,
            "word_count": 6011
          },
          {
            "chunk_id": 1,
            "text": "5, pp. ",
            "char_count": 7,
            "word_count": 2
          },
          {
            "chunk_id": 2,
            "text": "33\u201342, 2020.\n[4] D. L. Colton, R. Kress, and R. Kress, Inverse acoustic and electromag-\n\nnetic scattering theory. Springer, 1998, vol. 93.\n\n[5] A. J. Devaney, Mathematical foundations of imaging, tomography and\n\nwavefield inversion. Cambridge University Press, 2012.\n\n[6] P. Mojabi and J. LoVetri, \u201cOverview and classification of some reg-\nularization techniques for the gauss-newton inversion method applied\nto inverse scattering problems,\u201d IEEE Transactions on Antennas and\nPropagation, vol. 57, no. ",
            "char_count": 502,
            "word_count": 71
          },
          {
            "chunk_id": 3,
            "text": "9, pp. ",
            "char_count": 7,
            "word_count": 2
          },
          {
            "chunk_id": 4,
            "text": "2658\u20132665, 2009.\n\n[7] O. Bucci, I. Catapano, L. Crocco, and T. Isernia, \u201cSynthesis of new\nvariable dielectric profile antennas via inverse scattering techniques:\na feasibility study,\u201d IEEE Transactions on Antennas and Propagation,\nvol. 53, no. ",
            "char_count": 244,
            "word_count": 35
          }
        ],
        "stats": {
          "mode": "Hybrid",
          "num_chunks": 23,
          "avg_chars": 2501.4347826086955,
          "min_chars": 6,
          "max_chars": 47140,
          "avg_words": 325.6521739130435,
          "status": "success"
        },
        "time_sec": 0.44902610778808594
      }
    }
  }
]