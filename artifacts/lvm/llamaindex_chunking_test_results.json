[
  {
    "paper_id": "2510.25701v1",
    "original_length": 41323,
    "num_chunks": 25,
    "chunks": [
      {
        "chunk_id": 0,
        "paper_id": "2510.25701v1",
        "text": "5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n1\n0\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nInterpreting LLMs as Credit Risk Classifiers: Do Their Feature\nExplanations Align with Classical ML?\n\nSaeed AlMarri\nKhalifa University\nAbu Dhabi, United Arab Emirates\n100061460@ku.ac.ae\n\nKristof Juhasz\nADIA\nAbu Dhabi, United Arab Emirates\nkristof.juhasz@adia.ae\n\nMathieu Ravaut\nADIA\nAbu Dhabi, United Arab Emirates\nmathieu.ravaut@adia.ae\n\nGautier Marti\nADIA\nAbu Dhabi, United Arab Emirates\ngautier.marti@adia.ae\n\nHamdan Al Ahbabi\nKhalifa University\nAbu Dhabi, United Arab Emirates\n100061346@ku.ac.ae\n\nIbrahim Elfadel\nKhalifa University\nAbu Dhabi, United Arab Emirates\nibrahim.elfadel@ku.ac.ae",
        "char_count": 664,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 1,
        "paper_id": "2510.25701v1",
        "text": "Ibrahim Elfadel\nKhalifa University\nAbu Dhabi, United Arab Emirates\nibrahim.elfadel@ku.ac.ae\n\nAbstract\nLarge Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification\ntasks through zero-shot prompting. However, their suitability for\nstructured tabular data remains underexplored, especially in high-\nstakes financial applications such as financial risk assessment. This\nstudy conducts a systematic comparison between zero-shot LLM-\nbased classifiers and LightGBM, a state-of-the-art gradient-boosting\nmodel, on a real-world loan default prediction task. We evaluate\ntheir predictive performance, analyze feature attributions using\nSHAP, and assess the reliability of LLM-generated self-explanations.\nWhile LLMs are able to identify key financial risk indicators, their\nfeature importance rankings diverge notably from LightGBM, and\ntheir self-explanations often fail to align with empirical SHAP at-\ntributions. These findings highlight the limitations of LLMs as\nstandalone models for structured financial risk prediction and raise\nconcerns about the trustworthiness of their self-generated expla-\nnations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-\nthe-loop oversight when deploying LLMs in risk-sensitive financial\nenvironments.\n\nCCS Concepts\n\u2022 Computing methodologies \u2192 Supervised learning by clas-\nsification.\n\nKeywords\nLarge Language Models, Explainable AI, SHAP, Credit Risk Pre-\ndiction, Responsible AI, Financial Machine Learning, Model Inter-\npretability\n\n1 Introduction\nLarge Language Models (LLMs), such as GPT-4, have demonstrated\nstrong performance across a range of natural language processing\n(NLP) tasks, including classification and reasoning [2, 4, 32]. Their\nability to function as classifiers without explicit training pipelines,\nrelying solely on a few-shot or zero-shot prompting, has gained\nsignificant attention. This raises fundamental questions about the\nreliability and validity of LLM-based classification, particularly in\n\ncomparison to classical machine learning models such as gradient-\nboosting decision trees methods like XGBoost [9] or LightGBM\n[19].",
        "char_count": 2234,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 2,
        "paper_id": "2510.25701v1",
        "text": "comparison to classical machine learning models such as gradient-\nboosting decision trees methods like XGBoost [9] or LightGBM\n[19].\n\nTraditional classification tasks require structured pipelines in-\nvolving feature engineering, model training, validation, and hyper-\nparameter tuning. Fine-tuning models on tabular data, in particular,\ndemands expertise in preprocessing, GPU management, and bal-\nancing class distributions to prevent trivial solutions. In contrast,\nLLMs bypass fine-tuning entirely, requiring only natural language\nprompting. This reduces technical barriers, making them accessible\nto non-experts, but raises an important question: How do LLM-based\nclassifiers arrive at their predictions, and do they rely on decision pat-\nterns similar to classical machine learning models?\n\nThis question is especially critical in high-stakes financial do-\nmains, where algorithmic risk assessments directly affect credit\naccess, interest rates, and regulatory compliance [11]. Financial\ninstitutions operate under strict governance frameworks such as\nBasel III [24] and GDPR [1], where opaque models can lead to regu-\nlatory breaches, reputational damage, and unfair or discriminatory\ndecisions. Unlike decision trees or gradient boosting models, LLMs\nare complex black-box models with billions of parameters, making\ninterpretability a key challenge. This has led to growing interest in\nExplainable AI (XAI) techniques to analyze LLMs\u2019 internal logic and\nassess their alignment with human-interpretable decision patterns.\nIn this study, we conduct a systematic evaluation of zero-shot\nLLM classifiers for structured credit risk prediction, directly compar-\ning them with a well-established interpretable baseline (LightGBM).\nBeyond performance comparison, our primary goal is to audit their\nexplainability to determine whether their feature attributions and\nself-generated rationales align with dataset-driven reasoning or\nrely on external priors. We employ Shapley Additive Explanations\n(SHAP) [20] to analyze the faithfulness of their decision patterns.\nLeveraging a public loan default prediction dataset, we address the\nfollowing research questions:\n\n\u2022 Explainability: Do LLM-based classifiers prioritize the same\n\nfeatures as classical models?\n\n\u2022 Self-Explainability: Can LLMs provide self-generated ratio-\n\nnales that align with SHAP-derived feature attributions?",
        "char_count": 2375,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 3,
        "paper_id": "2510.25701v1",
        "text": "\u2022 Explainability: Do LLM-based classifiers prioritize the same\n\nfeatures as classical models?\n\n\u2022 Self-Explainability: Can LLMs provide self-generated ratio-\n\nnales that align with SHAP-derived feature attributions?\n\n\u2022 Performance: How do LLMs compare to classical machine learn-\ning classifiers (ROC-AUC, PR-AUC)? Does ensembling both types\nof models improve performance?\n\n \n \n \n \n \n \n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\nOur work directly addresses the workshop call on AI safety,\nfairness, and explainability in high-stakes financial environments,\nand responsible deployment in fintech and banking, by auditing the\nfaithfulness of LLM explanations against SHAP on a real credit-risk\ntask. The remainder of this paper is structured as follows: Section 2\nreviews related work, Section 3 details the methodology, Section 4\npresents the experimental setup, Section 5 analyzes results, feature\nattribution and reasoning patterns, and Section 6 concludes with\nkey findings and future directions.\n\n2 Related Work\n2.1 LLMs for Zero-Shot Classification\nLLMs have demonstrated strong classification capabilities, often\nachieving competitive performance without supervision. Brown\net al. [4] introduced GPT-3, highlighting its few-shot and zero-\nshot classification potential. This paradigm removes the need for\nclassical training pipelines, enabling non-experts to perform clas-\nsification via direct natural-language prompting and in-context\nlearning. Subsequent work has explored structured prompting to\nenhance classification accuracy. Hao et al. [16] introduced Chain-\nof-Thought (CoT) prompting, showing that structured reasoning\ncan improve LLM performance\u2014relevant when adapting them to\nstructured data.\n\nA recent line of work examines whether LLMs can function as\nregressors for numerical data. Vacareanu et al. [31] found that LLMs\napproximate regression functions with in-context examples, while\nBuckmann and Hill [5] proposed combining LLMs with logistic\nregression for low-data robustness. Song et al. [28] and Song and\nBahri [27] further extend this research direction, demonstrating\nuniversal regression capabilities using decoding-based inference.",
        "char_count": 2161,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 4,
        "paper_id": "2510.25701v1",
        "text": "Our Contribution. Unlike prior studies that evaluate LLM clas-\nsification in isolation, our work positions this comparison as a step\ntoward responsible AI deployment in high-stakes financial contexts.\nBy systematically comparing zero-shot LLM and LightGBM on the\nsame dataset and auditing their decision patterns using SHAP-based\nexplainability, we assess not only accuracy but also faithfulness\nand reliability of model reasoning critical elements for trustworthy,\nfair and transparent AI use in credit risk assessment.\n\n2.2 LLM Explainability\nFeature-attribution methods such as SHAP [20] are widely used\nto assess feature importance in machine-learning models. We use\nSHAP to compare LLM-based probabilistic classifiers with Light-\nGBM. A key question is whether LLMs\u2019 self-explanations align\nwith actual feature importance. Huang et al. [18] report that LLM\nrationales are often plausible but do not necessarily reflect inter-\nnal reasoning. Dehghanighobadi et al. [10] analyze counterfactual\nexplanations and show that LLMs can struggle with causal de-\npendencies. Sarkar [25] argues that LLMs lack self-explanatory\ncapabilities due to opaque training dynamics, and Turpin et al. [30]\nshow that CoT-generated explanations can be misleading.\n\nOur Contribution. Prior work predominantly studies LLM self-\nexplanations in text tasks. To our knowledge, our study is the first\nto compute SHAP-based feature importance for LLMs prompted\nto output probabilistic predictions on structured financial data,\n\nenabling a direct faithfulness audit of self-explanations against\nempirical attributions.",
        "char_count": 1592,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 5,
        "paper_id": "2510.25701v1",
        "text": "enabling a direct faithfulness audit of self-explanations against\nempirical attributions.\n\n2.3 LLMs for Tabular Data\nRecent work explores whether LLMs can replace gradient-boosted\nmodels such as XGBoost, LightGBM, and AdaBoost for tabular clas-\nsification. Fang et al. [13] survey LLMs on tabular data and highlight\nadaptation challenges. Ghaffarzadeh-Esfahani et al. [15] benchmark\nLLMs against classical ML for COVID-19 mortality prediction, con-\ncluding that classical ML models outperform LLMs on structured\ndata. Chen et al. [8] introduce ClinicalBench and similarly find XG-\nBoost superior for clinical prediction tasks. Hegselmann et al. [17]\npropose TabLLM, which reformulates tables as natural language\nfor few-shot classification, while Shi et al. [26] introduce ZET-LLM,\ntreating autoregressive LLMs as feature-embedding models for tab-\nular prediction. While these studies highlight LLM potential, they\ngenerally do not evaluate explainability or faithfulness of rationales\non tabular tasks.\n\nOur Contribution. We conduct a head-to-head comparison of\nLLMs and LightGBM on the same structured dataset and integrate\nSHAP-based explainability, offering a dual analysis of predictive\nperformance and feature attribution to illuminate decision mecha-\nnisms.\n\n2.4 Classical ML, XAI, and LLMs for Financial\n\nAI\n\nExplainability is crucial in financial applications for risk assessment.\nMartins et al. [22] review XAI in finance, while \u010cernevi\u010dien\u02d9e and\nKaba\u0161inskas [7], Misheva et al. [23], and Bussmann et al. [6] analyze\nexplainability in credit-risk modeling. Several studies benchmark\nML models for loan-default prediction: Madaan et al. [21] compare\ndecision trees and random forests without an explainability analysis;\nSrinivasa Rao et al. [29] assess ML techniques for loan risk but do\nnot explore LLMs; and Boughaci and Alkhawaldeh [3] study credit-\nscoring models without evaluating LLM-based predictions.",
        "char_count": 1918,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 6,
        "paper_id": "2510.25701v1",
        "text": "Our Contribution. While prior work focuses on classical ML\nfor loan prediction, we present the first comparative analysis of LLMs\nand LightGBM on structured loan data, integrating SHAP-based ex-\nplainability. Our findings extend beyond credit risk to broader finan-\ncial applications, including fraud detection, regulatory compliance,\nand algorithmic trading decision-making, where explainability is\nkey to adoption.\n\n3 Methodology\n3.1 Inference Setup\nWe systematically evaluate the predictive performance and explain-\nability of LightGBM and zero-shot LLMs. We design the classifi-\ncation problem such that both LightGBM and LLMs receive the\nsame set of input features and output probability estimates in-\nstead of discrete classes. Probability outputs offer three advantages:\n(i) fine-grained evaluation via discrimination metrics (ROC-AUC,\nPR-AUC); (ii) enhanced explainability, as SHAP feature attribution\nis generally more informative when applied to probability scores\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\nWe use SHAP for post hoc explanations [20], specifically the\nmodel-agnostic PermutationExplainer. We adopt this efficient\nSHAP estimator because our prediction function is an LLM infer-\nence, which is costly. To balance accuracy and runtime, we sample\n250 instances from each dataset for explanation. We construct the\nbackground (masker) via \ud835\udc58-means clustering with \ud835\udc36 = 5 centroids\n(using shap.kmeans) and set the max_evals budget so that the\nexplainer executes exactly \ud835\udc47 = 4 permutations in our experiments.\n\nApproximate cost (model calls). Let \ud835\udc3e be the number of in-\nstances explained, \ud835\udc40 the number of features, \ud835\udc35 the number of\nbackground draws per masked evaluation (here \ud835\udc35 = \ud835\udc36 = 5), and \ud835\udc47\nthe number of random permutations. The PermutationExplainer\nrequires approximately:\n\n#calls \u2248 \ud835\udc3e \u00d7 \ud835\udc47 \u00d7 (\ud835\udc40 + 1) \u00d7 \ud835\udc35 = O (\ud835\udc3e \ud835\udc47 \ud835\udc40 \ud835\udc35)\n(1)\nmodel evaluations. In SHAP\u2019s implementation, \ud835\udc47 is governed by\n\nmax_evals via the practical rule:",
        "char_count": 2027,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 7,
        "paper_id": "2510.25701v1",
        "text": "max_evals via the practical rule:\n\nFigure 1: Comparative Explainable AI Framework: Classical ML vs.\nLLMs. The dataset is processed through two paradigms: (i) a struc-\ntured LightGBM model and (ii) a zero-shot LLM using natural lan-\nguage prompts. Both generate probability predictions, analyzed in-\ndividually and in an ensemble. Explainability is assessed via SHAP\n(for both) and LLM self-explanations, evaluating their alignment.\n\nrather than hard labels; and (iii) a direct test of LLMs\u2019 capability as\nprobability regressors.\n\n3.2 Explainability\nWe treat explainability as a model-auditing task focused on the\nfaithfulness of the factors a model claims or appears to use. Our\naudit has two complementary components:\n\u2022 (A) SHAP as a model-agnostic audit baseline. We use SHAP\n[20] to assign contribution values to each feature for both Light-\nGBM and the LLM-based probabilistic classifier. SHAP values\noperationalize which features, and in what direction, are driving\neach model\u2019s predictions.\n\n\u2022 (B) LLM self-explanations as unverified rationales. In ad-\ndition to SHAP, we prompt the LLM to provide instance-level\nrationales and feature-level directional judgments (positive/neg-\native/neutral). These self-explanations are treated as claims that\nmust be checked against the SHAP audit checks.\n\nWe compare (i) global feature importance patterns (via SHAP)\nacross models; (ii) directional dependence for key features (e.g.,\nwhether higher values increase or decrease repayment probabil-\nity); and (iii) instance-level coherence between an LLM\u2019s rationale\nand the corresponding SHAP attributions. Misalignment across\nthese checks is interpreted as a faithfulness risk and a caution for\ndeployment.\n\nScaling Shapley-Value Inference for LLMs. To operationalize\n3.2.1\nexplainability as an audit of model decision logic, we estimate\nfeature attributions for both LightGBM and the zero-shot LLM\nclassifiers using SHAP. Faithful auditing requires identifying which\nfeatures actually drive predictions, not just producing plausible\nexplanations.\n\n(cid:107)\n\n\ud835\udc47 \u2248",
        "char_count": 2057,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 8,
        "paper_id": "2510.25701v1",
        "text": "(cid:107)\n\n\ud835\udc47 \u2248\n\n(cid:106) max_evals\n2\ud835\udc40\ni.e., roughly 2\ud835\udc40 masked evaluations per permutation path. With\nmax_evals = 200 and \ud835\udc40 = 21, this yields \ud835\udc47 = \u230a200/(2 \u00d7 21)\u230b = 4.\nUsing \ud835\udc35 = 5, the per-instance cost is therefore \u2248 4\u00d7(21+1)\u00d75 = 440\nmodel calls.\n\n(2)\n\nWhy is it more efficient than KernelExplainer? With a sum-\nmarized background of \ud835\udc36 centroids, the dominant model-call com-\nplexity of KernelExplainer scales as:\n\n#calls \u2248 \ud835\udc3e \u00d7 \ud835\udc36 \u00d7 \ud835\udc40 2 = O (\ud835\udc3e \ud835\udc36 \ud835\udc40 2)\n\n(3)\n\ndue to sampling coalitions and fitting a kernel-weighted regres-\nsion. In our setting (\ud835\udc36 = 5, \ud835\udc40 = 21) this is \u2248 5 \u00d7 212 = 2205 eval-\nuations per instance. By contrast, PermutationExplainer scales\nlinearly in \ud835\udc40 and avoids the regression solve, yielding an expected\nper-instance reduction of\n\nspeedup \u2248\n\n\ud835\udc36 \ud835\udc40 2\n\ud835\udc47 (\ud835\udc40 + 1) \ud835\udc35\n\n\u2248\n\n2205\n440\n\n\u2248 5\u00d7\n\n(4)\n\nThe fivefold decrease in model calls translates into substantially\nlower LLM inference time while maintaining faithful attributions,\nwhich is why we use PermutationExplainer with \ud835\udc47 = 4.\n\n3.2.2 LLM Self-Explanations. Motivated by the emerging reason-\ning capabilities of large language models (LLMs) [33], we use the\nLLM as an explainability tool alongside SHAP. Specifically, for each\ninput feature, we provide its description to the LLM and prompt it\nto predict whether the feature is likely to have a positive, negative,\nor no effect on the predicted outcome. The LLM also generates a\nbrief textual justification for each directional prediction, which we\nrefer to as its self-explanation.",
        "char_count": 1490,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 9,
        "paper_id": "2510.25701v1",
        "text": "These LLM-generated self-explanations are treated as unveri-\nfied rationales and are not assumed to reflect the model\u2019s actual\ninternal reasoning. We compare them against SHAP-based feature\nattributions, which serve as a model-agnostic baseline. When LLM\nexplanations diverge from SHAP attributions, we interpret this\nmisalignment as a potential risk of explainability, an indication\n\n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\nthat LLM may produce externally plausible but internally incon-\nsistent reasoning. While we do not perform formal risk scoring,\nhighlighting such discrepancies can inform governance decisions\nin high-stakes financial applications, where model transparency is\nessential.\n\nFigure 1 illustrates the overall methodological framework. The\ndataset is processed through two parallel pipelines: (i) a structured\nLightGBM model and (ii) a zero-shot LLM using natural language\nprompts. Both produce probability estimates, which are then ana-\nlyzed for predictive performance and explainability through SHAP\nand LLM self-explanations, enabling a cross-comparison of their\ndecision logic.\n\n4 Experiments\n4.1 Data\n4.1.1 Task Description. Loan default prediction is a critical chal-\nlenge in credit risk assessment, where lenders estimate the likeli-\nhood of borrowers failing to repay their loans. Accurate predictions\nenable financial institutions to make informed lending decisions,\nset appropriate interest rates, and manage credit risk effectively.\nBecause loan defaults directly affect credit access, financial stability,\nand regulatory compliance, this task is widely regarded as a high-\nstakes benchmark for testing the safety and reliability of predictive\nmodels in finance.",
        "char_count": 1698,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 10,
        "paper_id": "2510.25701v1",
        "text": "4.1.2 Dataset Description. We use LendingClub\u2019s publicly avail-\nable loan records hosted by Kaggle 1, which were disclosed as\npart of the company\u2019s regulatory filings with the U.S. Securities\nand Exchange Commission (SEC). As a major peer-to-peer lend-\ning platform, LendingClub was required to provide detailed loan\nissuance and performance data to comply with SEC regulations,\nmaking this dataset a widely used benchmark in credit risk mod-\neling. The dataset includes loan applications issued between 2007\nand 2016, with approximately 396,000 loan records. Each loan is\nlabeled with its repayment status, distinguishing between Fully\nPaid and Charged Off (defaulted) loans. It contains both numerical\nand categorical attributes describing borrower creditworthiness\nand loan characteristics, for a total of 26 financial and credit-related\nfeatures.\n\n4.1.3 Preprocessing. We removed five features due to high cardi-\nnality, redundancy, or potential data leakage:\n\n\u2022 issue_d: Loan issue date (introduces temporal bias)\n\u2022 earliest_cr_line: Borrower\u2019s earliest credit line (high cardi-\n\nnality)\n\n\u2022 address: High-cardinality feature (introduces geographic bias)\n\u2022 emp_title: High-cardinality categorical variable\n\u2022 title: High-cardinality; redundant with the purpose variable\n\nWe also excluded categorical features with more than 40 unique\ncategories to mitigate overfitting in LightGBM. This ensures a fair\ncomparison between models, preventing disadvantages for Light-\nGBM (which lacks natural text processing) and restricting LLMs\nfrom exploiting external knowledge, such as macroeconomic trends\nfrom loan dates or geographic signals from addresses. These steps\n\n1https://www.kaggle.com/datasets/sndpred/loan-data\n\nTable 1: Final Dataset Features.\n\nFeature Description\n\nRange\n\nLoan amount\nTerm\nInterest rate\nInstallment\nGrade\nSub-grade\nEmployment length\n\nHome ownership\n\nAnnual income\nVerification status\n\nPurpose\nDebt-to-income (DTI)\nratio\nOpen credit accounts\nPublic records\nRevolving balance\nRevolving utilization\nrate\nTotal accounts\nInitial listing\nstatus\nApplication type\n\nMortgage accounts\nPublic record\nbankruptcies",
        "char_count": 2122,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 11,
        "paper_id": "2510.25701v1",
        "text": "Mortgage accounts\nPublic record\nbankruptcies\n\n[1600.0, 35000.0]\ncategorical: {36 months, 60 months}\n[6.03, 25.29]\n[55.32, 1204.57]\ncategorical: {A, B, C, D, E, F, G}\ncategorical: {A1, A2, . . . , G4, G5}\ncategorical: {<1 year, 1 year, . . . , 10+\nyears}\ncategorical: {MORTGAGE, NONE,\nOTHER, OWN, RENT}\n[19000.0, 250000.0]\ncategorical: {Not Verified, Source\nVerified, Verified}\ncategorical: 14 values (e.g., wedding)\n[1.6, 36.41]\n\n[3.0, 27.0]\n[0.0, 2.0]\n[169.05, 83505.9]\n[1.2, 98.0]\n\n[6.0, 60.0]\ncategorical: {f, w}\n\ncategorical: {DIRECT PAY,\nINDIVIDUAL, JOINT}\n[0.0, 9.0]\n[0.0, 1.0]\n\nhelp control for potential bias and improve the fairness and au-\nditability of the evaluation.\n\nFinal Dataset. After preprocessing, the final dataset consists\n4.1.4\nof 396,000 rows and 21 predictors (12 numerical and 9 categorical).\nThe data was randomly split into training (80%) and testing (20%),\nwith 79,200 instances used for LLM inference. This controlled setup\nensures that both models operate on identical structured inputs\nwithout access to external priors, supporting a transparent and\nauditable comparison.\n\nIn Table 1, we list all features. For numerical features, we report\nthe interval bounded by the 1st and 99th percentiles. For categorical\nfeatures, we report the values space (if it is not too large).",
        "char_count": 1304,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 12,
        "paper_id": "2510.25701v1",
        "text": "4.2 Models\nLightGBM Training. We trained a LightGBM classifier using\n4.2.1\nthe gradient boosting decision tree (GBDT) algorithm with a binary\nobjective and AUC as the primary evaluation metric. The model\nwas trained with a learning rate of 0.01 and up to 10,000 estima-\ntors, applying early stopping after 100 rounds based on validation\nperformance. To reduce overfitting, we applied a feature fraction\nof 0.8, bagging fraction of 0.8, and L1/L2 regularization (0.1 each).\nThe num_leaves parameter was set to 63 and min_data_in_leaf\nto 50.\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\nInstance-Level Prompt Template\n\nPredict whether a loan application will be \"Fully Paid\" or \"Charged\nOff\" based on the borrower\u2019s information. Use the features provided\nbelow to assess the likelihood of the loan being fully repaid.\nLoan Application Details:\n\n<feature_1 name>: <feature_1 value>\n...\n<feature_N name>: <feature_N value>\n\nProvide your estimated probability of the loan being \"Fully Paid\". Also\nprovide a brief explanation of this estimate based on the features. Your\nanswer should only contain the probability estimate and the explanation\nin the following JSON format.\n\n{\n\n}\n\n\"Estimated Fully Paid Probability\": <float value\n\nbetween 0 and 1>,\n\n\"Explanation\": <string value>\n\nFeature-Level Prompt Template\n\nYou are working on predicting whether a loan application will be \"Fully\nPaid\" or \"Charged Off\" based on the borrower\u2019s information. One of the\nfeatures is the following:\n\n<feature name>\n\nDo you think that this feature will impact the application positively,\nnegatively, or have no impact? Provide your answer as one of the three\nstrings: positive | negative | neutral. Use the following JSON\nformat:\n\n{\n\n}\n\n\"Feature impact\": <positive | negative | neutral>\n\nWe use LightGBM as a transparent and interpretable baseline,\nproviding a benchmark for feature importance and prediction sta-\nbility against which the behavior of large language models (LLMs)\ncan be audited.",
        "char_count": 2056,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 13,
        "paper_id": "2510.25701v1",
        "text": "We use LightGBM as a transparent and interpretable baseline,\nproviding a benchmark for feature importance and prediction sta-\nbility against which the behavior of large language models (LLMs)\ncan be audited.\n\n4.2.2 LLM Inference. We evaluated three recent open-source in-\nstruction tuned LLMs of comparable size: LLaMA-3.1-8B-Instruct\n[12], Gemma-2-9B-Instruct [14], and Qwen-2.5-7B-Instruct\n[35]. These models were pre-trained on 15.6T, 8T, and 18T tokens,\nrespectively. All experiments used their instruction-tuned versions.\nModel weights were downloaded from the Hugging Face Hub [34],\nand inference was run locally using vLLM2 on four Nvidia A10G\n24GB GPUs.\n\nTo reduce computation time, we randomly sampled 250 test\ninstances for SHAP value estimation. We did not perform any fine-\ntuning and used strict zero-shot inference without any in-context\nlearning. This design ensures the LLMs rely solely on the provided\n\n2https://github.com/vllm-project/vllm\n\nFigure 2: ROC and Precision-Recall curves comparing the\nperformance of zero-shot LLMs and LightGBM on the loan\nclassification task. LightGBM consistently outperforms indi-\nvidual LLMs with Gemma-2-9b showing the most promising\nresult out of the LLMs.\n\nstructured features, preventing data contamination or leakage from\npre-training.\n\nWe used the same prompt templates for all LLMs. The instance-\nlevel prompt asked the model to jointly predict the probability that\na loan would be fully repaid (a float between 0 and 1) and to provide\na brief explanation. The feature-level prompt asked whether each\nfeature would impact the prediction positively, negatively, or not\nat all.\n\nUnlike free-form textual descriptions, the structured dictionary\nformat used for instance-level prompts is unlikely to have appeared\nin the LLMs\u2019 pretraining corpus, which reduces the risk of data\ncontamination.",
        "char_count": 1846,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 14,
        "paper_id": "2510.25701v1",
        "text": "Unlike free-form textual descriptions, the structured dictionary\nformat used for instance-level prompts is unlikely to have appeared\nin the LLMs\u2019 pretraining corpus, which reduces the risk of data\ncontamination.\n\n5 Analysis\n5.1 Performance Results\nTable 2 and Figure 2 compare the performance of zero-shot LLMs\nand LightGBM on the loan repayment prediction task. LightGBM\nachieved the highest ROC\u2013AUC (0.73), outperforming all LLMs in\nthe zero-shot setting. Among LLMs, Gemma-2-9B performed best\n(0.67), followed by LLaMA-3.1-8B (0.65) and Qwen-2.5-7B (0.61).\nThese findings are consistent with prior evidence that gradient\nboosting often surpasses deep learning methods on structured tab-\nular data. The LightGBM\u2013Gemma-2-9B ensemble achieved 0.70\nROC\u2013AUC, indicating no diversification benefit over LightGBM\nalone.\n\nPR\u2013AUC results follow a similar pattern. LightGBM obtained\nthe highest PR\u2013AUC (0.91), exceeding all LLMs. Gemma-2-9B again\nranked highest among the LLMs (0.88), followed by LLaMA-3.1-8B\n(0.86) and Qwen-2.5-7B (0.85). The ensemble model reached 0.90,\nclosely trailing LightGBM. All models outperformed the base-rate\nPR\u2013AUC (0.80), confirming meaningful predictive signal.\n\nOverall, these results show that while zero-shot LLMs achieve\nreasonable predictive performance, they remain inferior to a well-\ntuned LightGBM model on structured financial data, underscoring\nthe need for careful governance if deployed in high-stakes settings.\n\n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\nTable 2: Performance Metrics of Models for Loan Default\nClassification\n\nModel\n\nROC-AUC PR-AUC\n\nLightGBM\nGemma-2-9B\nLlama-3.1-8B\nQwen-2.5-7B\nEnsemble (LightGBM + Gemma-2-9B)",
        "char_count": 1668,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 15,
        "paper_id": "2510.25701v1",
        "text": "LightGBM\nGemma-2-9B\nLlama-3.1-8B\nQwen-2.5-7B\nEnsemble (LightGBM + Gemma-2-9B)\n\nRandom Classifier (Baseline)\nBase Rate for Fully Paid Label (Baseline)\n\n0.73\n0.67\n0.65\n0.61\n0.70\n\n0.50\n\u2013\n\n0.91\n0.88\n0.86\n0.85\n0.90\n\n\u2013\n0.80\n\nTable 3: Feature Importance Comparison\n\nFeature\n\nLGBM Gemma-\n\nSub-grade\nAnnual income\nTerm\nInterest rate\nDTI\nOpen account\nRevolving util\nLoan amount\nHome ownership\nGrade\n\n0.062\n0.026\n0.023\n0.022\n0.019\n0.018\n0.018\n0.018\n0.015\n0.013\n\n2-9B\n\n0.046\n0.031\n0.002\n0.019\n0.019\n0.004\n0.044\n0.006\n0.012\n0.079\n\nLlama-\n3.1-8B\n\nQwen-\n2.5-7B\n\n0.038\n0.037\n0.008\n0.048\n0.027\n0.007\n0.021\n0.026\n0.031\n0.065\n\n0.004\n0.014\n0.000\n0.005\n0.006\n0.001\n0.005\n0.004\n0.003\n0.025\n\n5.2 SHAP Feature Importance Comparison\nFigure 3 presents the SHAP feature importance rankings for Light-\nGBM and the three LLMs, providing insight into the key factors\ninfluencing loan classification decisions across models. The corre-\nsponding numerical values are reported in Table 3.",
        "char_count": 955,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 16,
        "paper_id": "2510.25701v1",
        "text": "A primary observation is the strong overlap in the top-ranked\nfeatures across all models, despite the LLMs operating in a zero-shot\nsetting. Features such as Sub-grade, Grade, Annual income, and\nInterest rate consistently emerge as dominant predictors. This\nsuggests that LLMs, even without task-specific fine-tuning, are able\nto extract and prioritize meaningful financial attributes in a manner\nbroadly consistent with classical ML models like LightGBM.\n\nHowever, notable differences emerge in feature weighting and\nrank order. LightGBM assigns greater relative importance to struc-\ntured numerical features such as Sub-grade, Annual income, and\nLoan amount, reflecting its reliance on directly interpretable numer-\nical signals. In contrast, the LLMs particularly LLaMA-3.1-8B and\nGemma-2-9B distribute their importance more evenly across cate-\ngorical and behavioral attributes such as Verification status,\nPurpose, and Home ownership. This indicates that LLMs may be\nleveraging latent semantic relationships within categorical features\nthat are not explicitly modeled by LightGBM.\n\n5.3 SHAP Comparative Analysis\nTo further investigate the decision mechanisms of the models, we\ncompare the SHAP summary plots of LightGBM and the three LLMs.\n\nFigure 3: SHAP feature importance comparison between\nLightGBM and LLMs. Despite being in a zero-shot setting,\nLLMs identify a remarkably similar set of key financial fea-\ntures as LightGBM, though with differences in feature weight-\ning and distribution.\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\nFigure 4: SHAP summary plots comparing feature importance dis-\ntributions for LLMs and LightGBM. LightGBM shows a more struc-\ntured reliance on key financial indicators, while LLMs exhibit more\ndispersed and lower-magnitude SHAP values, indicating weaker fea-\nture dependencies.\n\nFigure 5: SHAP Feature dependence plots and LLM self-\nexplanations for the feature DTI.\n\nIn Figure 4, these plots illustrate how variations in feature values\ninfluence predicted loan repayment probabilities.",
        "char_count": 2119,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 17,
        "paper_id": "2510.25701v1",
        "text": "Figure 5: SHAP Feature dependence plots and LLM self-\nexplanations for the feature DTI.\n\nIn Figure 4, these plots illustrate how variations in feature values\ninfluence predicted loan repayment probabilities.\n\nDespite operating in a zero-shot setting, the LLMs successfully\nextract meaningful relationships from structured financial features.\nCore predictors such as Sub-grade, Interest rate, and Loan\namount consistently emerge as important across all models, suggest-\ning that the LLMs are able to capture many of the same risk-relevant\nfactors identified by LightGBM. Notably, Gemma-2-9B assigns com-\nparatively higher SHAP values to these features, aligning with its\nsuperior classification performance among the evaluated LLMs.\n\nWhile the models converge on key features, their attribution\npatterns also reveal important differences. LightGBM places sub-\nstantial weight on well-established numerical predictors such as\nSub-grade and Annual income, whereas the LLMs distribute im-\nportance more broadly across behavioral and categorical variables.\nThis more diffuse attribution pattern implies that the LLMs may be\nleveraging latent feature interactions rather than relying solely on\ndirect numerical signals. Such behavior could reflect their ability to\nencode semantic relationships across variables that classical models\ndo not capture explicitly.\n\nA further notable observation is the reversal of SHAP relation-\nships for certain features between the models. For example, in\nLightGBM, higher Interest rate values are associated with a\ngreater predicted probability of being fully repaid (positive SHAP\nimpact), whereas all three LLMs display the opposite trend. Simi-\nlarly, DTI exhibits a different effect in Gemma-2-9B compared to the\nother LLMs and LightGBM. These discrepancies suggest that while\nthe LLMs extract informative patterns, they may rely on internally\nlearned representations that diverge from classical feature logic,\nwhich has implications for their reliability in regulated financial\ncontexts.\n\nFigure 6: SHAP Feature dependence plots and LLM self-\nexplanations for the feature Sub-grade.\n\n5.4 SHAP Feature Dependences and LLM\n\nSelf-Explanation",
        "char_count": 2171,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 18,
        "paper_id": "2510.25701v1",
        "text": "Figure 6: SHAP Feature dependence plots and LLM self-\nexplanations for the feature Sub-grade.\n\n5.4 SHAP Feature Dependences and LLM\n\nSelf-Explanation\n\nWe compare classical model-centric explainability (SHAP depen-\ndence plots) with LLM self-reported explanations for two key fea-\ntures: DTI (Debt-to-Income ratio) and Sub-grade. LLM self-reported\ndependence is obtained by directly prompting the models on whether\neach feature exerts a positive, negative, or neutral effect on loan\nrepayment likelihood, and is shown at the top of each chart.\n\nDTI (Debt-to-Income ratio). All three LLMs self-report a nega-\ntive relationship between DTI and loan repayment likelihood, align-\ning with their SHAP dependence plots (Figure 5). LightGBM also\nshows a clear negative dependence. However, Gemma-2-9B displays\n\n\fCIKM 2025 FinAI Workshop,\n\nAlMarri et al.\n\na notable inconsistency: its SHAP values suggest a positive contri-\nbution at higher DTI levels, contradicting its own self-explanation.\n\nSub-grade. Both the LLM SHAP plots and self-reports consis-\ntently show Sub-grade as a feature negatively correlated with\ndefault risk (A1 being lowest risk and G5 highest) (Figure 6). Light-\nGBM exhibits a strong and nearly monotonic negative dependence,\nindicating heavy reliance on Sub-grade for risk discrimination.\nLLMs reproduce this general trend but with shallower slopes and\ngreater local variability.\n\nSummary of alignment. While LLM self-explanations often\nalign with their SHAP dependence patterns, there are notable di-\nvergences such as DTI, which was self-reported as negative but\nshowed positive SHAP impact at higher values. Such mismatches\nsuggest that LLM self-explanations, while often plausible, do not\nalways reflect their internal decision-making mechanisms. Unlike\nLightGBM, which captures purely statistical relationships from the\nstructured dataset, LLMs may incorporate latent financial priors\nbeyond the data. These inconsistencies reinforce the need for inde-\npendent audits before trusting LLM self-explanations in high-stakes\nfinancial workflows.",
        "char_count": 2062,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 19,
        "paper_id": "2510.25701v1",
        "text": "6 Conclusion\nThe growing adoption of large language models (LLMs) for struc-\ntured classification raises critical questions about their validity and\nsafety in high-stakes financial decision-making. This study system-\natically compared zero-shot LLM classifiers with LightGBM on a\nstructured loan default prediction task, evaluating both predictive\nperformance and explainability through SHAP-based audits.\n\nOur findings reveal that while LLMs can capture several key\nstatistical patterns similar to LightGBM, they remain inferior in pre-\ndictive accuracy, with LightGBM achieving the highest ROC\u2013AUC\nand PR\u2013AUC scores. LLM-generated self-explanations occasionally\nalign with SHAP feature attributions, but observed discrepancies in-\ndicate that these rationales may rely on external priors rather than\npurely dataset-driven reasoning. Moreover, ensembling LLMs with\nLightGBM did not yield meaningful performance gains, suggesting\nlimited complementarity between the two paradigms.\n\nThese results highlight that while LightGBM remains the more\nreliable choice when skilled data scientists are available, LLMs\ncould serve as a practical fallback in small-data settings where\nfine-tuning is infeasible, provided their outputs are independently\naudited. Future research should explore fine-tuned LLMs for tabular\nmodeling, hybrid approaches that better integrate structured and\nunstructured reasoning, and establish robust reliability and fairness\nassessments to ensure their responsible deployment in financial\napplications.\n\nReferences\n[1] 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council of\n27 April 2016 on the protection of natural persons with regard to the processing\nof personal data and on the free movement of such data, and repealing Directive\n95/46/EC (General Data Protection Regulation). 88 pages. https://eur-lex.europa.\neu/eli/reg/2016/679/oj/eng",
        "char_count": 1886,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 20,
        "paper_id": "2510.25701v1",
        "text": "[2] Josh Achiam et al. 2023. GPT-4 Technical Report. arXiv:2303.08774 (2023).\n[3] Dalila Boughaci and Abdullah A. K. Alkhawaldeh. 2020. Appropriate Machine\nLearning Techniques for Credit Scoring and Bankruptcy Prediction in Banking\nand Finance: A Comparative Study. Risk and Decision Analysis 8, 1-2 (2020),\n15\u201324.\n\n[4] T. B. Brown et al. 2020. Language Models are Few-Shot Learners. In Advances in\n\nNeural Information Processing Systems (NeurIPS), Vol. 33. 1877\u20131901.\n\n[5] Marcus Buckmann and Edward Hill. 2024. Logistic Regression makes small LLMs\nstrong and explainable \"tens-of-shot\" classifiers. arXiv preprint arXiv:2408.03414\n(2024).\n\n[6] Niklas Bussmann, Paolo Giudici, Dimitri Marinelli, and Jochen Papenbrock. 2021.\nExplainable Machine Learning in Credit Risk Management. Computational Eco-\nnomics 57, 1 (2021), 203\u2013216.\n\n[7] Jurgita \u010cernevi\u010dien\u02d9e and Audrius Kaba\u0161inskas. 2024. Explainable Artificial Intel-\nligence (XAI) in Finance: A Systematic Literature Review. Artificial Intelligence\nReview 57, 8 (2024), 216.\n\n[8] Canyu Chen et al. 2024. ClinicalBench: Can LLMs Beat Traditional ML Models\n\nin Clinical Prediction? arXiv preprint arXiv:2411.06469 (2024).\n\n[9] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting\nSystem. In Proceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining. 785\u2013794.\n\n[10] Zahra Dehghanighobadi, Asja Fischer, and Muhammad Bilal Zafar. 2025. Can\nLLMs Explain Themselves Counterfactually? arXiv preprint arXiv:2502.18156\n(2025).",
        "char_count": 1530,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 21,
        "paper_id": "2510.25701v1",
        "text": "[11] F. Doshi-Velez and B. Kim. 2017. Towards a Rigorous Science of Interpretable\n\nMachine Learning. arXiv preprint arXiv:1702.08608 (2017).\n\n[12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, et al. 2024. The LLaMA 3\n\nHerd of Models. arXiv preprint arXiv:2407.21783 (2024).\n\n[13] Xi Fang et al. 2024. Large Language Models (LLMs) on Tabular Data: Predic-\ntion, Generation, and Understanding\u2014A Survey. arXiv preprint arXiv:2402.17944\n(2024).\n\n[14] Gemma Team et al. 2024. Gemma 2: Improving Open Language Models at a\n\nPractical Size. arXiv preprint arXiv:2408.00118 (2024).\n\n[15] Mohammadreza Ghaffarzadeh-Esfahani et al. 2024. Large Language Models ver-\nsus Classical Machine Learning: Performance in COVID-19 Mortality Prediction\nUsing High-Dimensional Tabular Data. arXiv preprint arXiv:2409.02136 (2024).\n[16] Y. Hao, L. Dong, F. Wei, and K. Xu. 2020. Self-Attention Attribution: Interpreting\nInformation Interactions Inside Transformer. (2020). https://paperswithcode.\ncom/paper/self-attention-attribution-interpreting\n\n[17] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi\nJiang, and David Sontag. 2023. TabLLM: Few-Shot Classification of Tabular Data\nwith Large Language Models. In Proceedings of the 40th International Conference\non Machine Learning (ICML).",
        "char_count": 1293,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 22,
        "paper_id": "2510.25701v1",
        "text": "[18] Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and\nLeilani H. Gilpin. 2023. Can Large Language Models Explain Themselves? A Study\nof LLM-Generated Self-Explanations. arXiv preprint arXiv:2310.11207 (2023).\n[19] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,\nQiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient Boosting\nDecision Tree. Advances in Neural Information Processing Systems 30 (2017).\n[20] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model\nPredictions. In Advances in Neural Information Processing Systems (NeurIPS). 4765\u2013\n4774.\n\n[21] Mehul Madaan, Aniket Kumar, Chirag Keshri, Rachna Jain, and Preeti Nagrath.\n2021. Loan Default Prediction Using Decision Trees and Random Forest: A\nComparative Study. In IOP Conference Series: Materials Science and Engineering,\nVol. 1022. IOP Publishing, 012042.\n\n[22] Tiago Martins, Ana Maria De Almeida, Elsa Cardoso, and Lu\u00eds Nunes. 2023.\nExplainable Artificial Intelligence (XAI): A Systematic Literature Review on\nTaxonomies and Applications in Finance. IEEE Access 12 (2023), 618\u2013629.\n[23] Branka Hadji Misheva, Joerg Osterrieder, Ali Hirsa, Onkar Kulkarni, and\nStephen Fung Lin. 2021. Explainable AI in Credit Risk Management. arXiv\npreprint arXiv:2103.00949 (2021).\n\n[24] Basel Committee on Banking Supervision and Bank for International Settlements.\n2017. Basel III: Finalising post-crisis reforms. https://www.bis.org/bcbs/publ/\nd424.htm\n\n[25] Advait Sarkar. 2024. Large Language Models Cannot Explain Themselves. arXiv\n\npreprint arXiv:2405.04382 (2024).",
        "char_count": 1605,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 23,
        "paper_id": "2510.25701v1",
        "text": "[25] Advait Sarkar. 2024. Large Language Models Cannot Explain Themselves. arXiv\n\npreprint arXiv:2405.04382 (2024).\n\n[26] Zhiyi Shi, Junsik Kim, Davin Jeong, and Hanspeter Pfister. 2024. Surprisingly\nSimple: Large Language Models are Zero-Shot Feature Extractors for Tabular\nand Text Data. arXiv preprint arXiv:2409.00079 (2024).\n\n[27] Xingyou Song and Dara Bahri. 2025. Decoding-based Regression. arXiv preprint\n\narXiv:2501.19383 (2025).\n\n[28] Xingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi Perel,\nand Yutian Chen. 2024. Omnipred: Language models as universal regressors.\narXiv preprint arXiv:2402.14547 (2024).\n\n[29] M. Srinivasa Rao, Ch. Sekhar, and Debnath Bhattacharyya. 2021. Compara-\ntive Analysis of Machine Learning Models on Loan Risk Analysis. In Machine\nIntelligence and Soft Computing: Proceedings of ICMISC 2020. Springer, 81\u201390.\n\n[30] Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2023. Language\nModels Don\u2019t Always Say What They Think: Unfaithful Explanations in Chain-\nof-Thought Prompting. Advances in Neural Information Processing Systems 36\n(2023), 74952\u201374965.\n\n\fInterpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?\n\nCIKM 2025 FinAI Workshop,\n\n[31] R. Vacareanu, V.-A. Negru, V. Suciu, and M. Surdeanu. 2024. From Words to\nNumbers: Your Large Language Model Is Secretly A Capable Regressor When\nGiven In-Context Examples. COLM (2024).\n\n[32] A. Vaswani et al. 2017. Attention is All You Need. In Advances in Neural Informa-",
        "char_count": 1529,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      },
      {
        "chunk_id": 24,
        "paper_id": "2510.25701v1",
        "text": "[32] A. Vaswani et al. 2017. Attention is All You Need. In Advances in Neural Informa-\n\ntion Processing Systems (NeurIPS), Vol. 30. 5998\u20136008.\n\n[33] Jason Wei et al. 2022. Chain-of-thought prompting elicits reasoning in large\nlanguage models. Advances in Neural Information Processing Systems 35 (2022),\n\n24824\u201324837.\n\n[34] Thomas Wolf et al. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations. 38\u201345.\n\n[35] An Yang et al. 2024. Qwen2.5 Technical Report. arXiv:2412.15115 (2024).",
        "char_count": 607,
        "metadata": {
          "paper_id": "2510.25701v1"
        }
      }
    ]
  },
  {
    "paper_id": "2510.25704v1",
    "original_length": 107895,
    "num_chunks": 76,
    "chunks": [
      {
        "chunk_id": 0,
        "paper_id": "2510.25704v1",
        "text": "Prepared for submission to JHEP\n\nScaling flow-based approaches for topology sampling\nin SU(3) gauge theory\n\n5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nt\na\nl\n-\np\ne\nh\n[\n\n1\nv\n4\n0\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nClaudio Bonanno,a Andrea Bulgarelli,b,c Elia Cellini,b Alessandro Nada,b Dario\nPanfalone,b Davide Vadacchino,d Lorenzo Verzichellib\n\naInstituto de F\u00b4\u0131sica Te\u00b4orica UAM-CSIC, c/ Nicol\u00b4as Cabrera 13-15, Universidad Aut\u00b4onoma de\nMadrid, Cantoblanco, E-28049 Madrid, Spain\nbDipartimento di Fisica, Universit\u00b4a degli Studi di Torino and INFN, Sezione di Torino, Via Pietro\nGiuria 1, I-10125 Turin, Italy\ncTransdisciplinary Research Area \u201cBuilding Blocks of Matter and Fundamental Interactions\u201d (TRA\nMatter) and Helmholtz Institute for Radiation and Nuclear Physics (HISKP), University of Bonn,\nNussallee 14-16, 53115 Bonn, Germany\ncCentre for Mathematical Sciences, University of Plymouth, Plymouth, PL4 8AA, United Kingdom\n\nE-mail: claudio.bonanno@csic.es, andrea.bulgarelli@unito.it,\nelia.cellini@unito.it, alessandro.nada@unito.it,\ndario.panfalone@unito.it, davide.vadacchino@plymouth.ac.uk,\nlorenzo.verzichelli@unito.it",
        "char_count": 1102,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 1,
        "paper_id": "2510.25704v1",
        "text": "Abstract: We develop a methodology based on out-of-equilibrium simulations to miti-\ngate topological freezing when approaching the continuum limit of lattice gauge theories.\nWe reduce the autocorrelation of the topological charge employing open boundary condi-\ntions, while removing exactly their unphysical effects using a non-equilibrium Monte Carlo\napproach in which periodic boundary conditions are gradually switched on. We perform\na detailed analysis of the computational costs of this strategy in the case of the four-\ndimensional SU(3) Yang-Mills theory. After achieving full control of the scaling, we outline\na clear strategy to sample topology efficiently in the continuum limit, which we check at\nlattice spacings as small as 0.045 fm. We also generalize this approach by designing a cus-\ntomized Stochastic Normalizing Flow for evolutions in the boundary conditions, obtaining\nsuperior performances with respect to the purely stochastic non-equilibrium approach, and\npaving the way for more efficient future flow-based solutions.\n\nKeywords: Algorithms and Theoretical Developments, Lattice QCD, Vacuum Structure\nand Confinement\n\n \n \n \n \n \n \n\fContents\n\n1 Introduction\n\n2 Non-equilibrium Monte Carlo simulations in lattice field theory\n\n2.1 Some insights on NE-MCMC and its metrics\n2.2 Lattice setup and topological observables\n\n3 Scaling of NE-MCMC in the boundary conditions\n\n3.1 Understanding the scaling with the degrees of freedom\n\n4 Accelerating NE-MCMC with Stochastic Normalizing Flows\n\n4.1 Coupling layers for a defect\n\n5 Sampling topology towards the continuum limit\n\n6 Conclusions\n\nA Interpolation strategy for defect coupling layer parameters\n\n1\n\n5\n7\n9\n\n11\n13\n\n15\n17\n\n22\n\n25\n\n28\n\n1 Introduction",
        "char_count": 1717,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 2,
        "paper_id": "2510.25704v1",
        "text": "5 Sampling topology towards the continuum limit\n\n6 Conclusions\n\nA Interpolation strategy for defect coupling layer parameters\n\n1\n\n5\n7\n9\n\n11\n13\n\n15\n17\n\n22\n\n25\n\n28\n\n1 Introduction\n\nNumerical Markov Chain Monte Carlo (MCMC) simulations of lattice field theories are\namongst the most powerful tools for exploring the non-perturbative regime of non-Abelian\ngauge theories. Over the past decades, their use has provided first-principles insights into\nthe theoretical and phenomenological properties of several lattice-regularized models, the\nmost prominent example being lattice Quantum Chromodynamics (QCD). Nonetheless,\nthis approach is accompanied by a number of highly non-trivial computational challenges.\nAlthough advances in the architecture of supercomputing machines have greatly expanded\nthe range of feasible calculations, the development of more efficient and sophisticated al-\ngorithms remains essential to overcome these limitations.\n\nIn lattice gauge theories, and in particular in lattice QCD, one of the most severe\nnumerical issues within the MCMC framework is the so-called critical slowing down, in\nparticular that of topological modes. As the continuum limit is approached, the computa-\ntional cost required to obtain statistically independent configurations grows rapidly with\ndecreasing lattice spacing, ultimately leading to a loss of ergodicity of the Markov chain.\nThis is a critical issue, since ergodicity is a key assumption underlying the validity of en-\nsemble averages as estimators of expectation values. For most observables, critical slowing\ndown manifests as a polynomial growth of the autocorrelation time with the inverse lattice\nspacing with a small exponent. In contrast, for topological quantities such as the topolog-\nical charge Q [1\u20134], the scaling is found to be much more severe and compatible with a\n\n\u2013 1 \u2013",
        "char_count": 1847,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 3,
        "paper_id": "2510.25704v1",
        "text": "\u2013 1 \u2013\n\n\fpolynomial with a large exponent or even with an exponential. This can be understood in\nterms of the MCMC dynamics of topological modes when standard local updating algo-\nrithms are adopted to generate the Markov chain: while for non-topological quantities this\nis essentially diffusive, for topological ones this is dominated by jumps over the potential\nbarriers among different pseudo-topological sectors. Such barriers eventually diverge in the\ncontinuum limit to restore a proper notion of topological winding number [5]. Since no\nchange of topological sector is allowed via a local deformation of the gauge fields, it be-\ncomes increasingly difficult to change the winding number of a given lattice gauge field as\nthe lattice spacing approaches zero. This severe ergodicity problem affecting the sampling\nof the topological charge typically results, on fine lattices, in few or even no fluctuations of\nQ during feasible MCMC histories: for this reason it is typically called topological freezing.\nTopological freezing poses a serious problem for the determination of topological quan-\ntities from lattice simulations, most notably the topological susceptibility, a quantity of the\nutmost theoretical and phenomenological importance which has been widely addressed in\nthe lattice literature [6\u201319]. However, such a severe loss of ergodicity can in principle bias\nany expectation value estimated from topologically-frozen samples. It is well-known, for\ninstance, that it can affect the calculation of particle spectra [20, 21], as well as observables\ncomputed after the gradient flow like the action density [22, 23], necessary to obtain the\nreference scale t0 or the renormalized strong coupling. For this reason, mitigating topolog-\nical freezing is not only crucial for studies of topological quantities, but also to ensure the\nreliability of a wide range of lattice results. Developing new numerical strategies to address\nthis issue is a major focus within the lattice community, leading to substantial progress in\nthe last decade [24\u201338] (for recent reviews see Refs. [39\u201341]).",
        "char_count": 2093,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 4,
        "paper_id": "2510.25704v1",
        "text": "The adoption of Open Boundary Conditions (OBC) in the Euclidean time direc-\ntion [42, 43], instead of the conventional Periodic Boundary Conditions (PBC) is one of\nthe most popular and effective among various strategies proposed to mitigate topological\nfreezing. With OBC, the configuration space of gauge fields becomes simply connected [42]:\nbarriers between topological sectors are removed and the MCMC dynamics of topological\nmodes are now dominated by diffusive phenomena [44], thereby drastically reducing the\nseverity of topological freezing. However, this comes at the price of introducing unwanted\nboundary effects, as now only field fluctuations sufficiently far from the boundaries are\nphysical, leading to enhanced finite-volume effects. Moreover, translation invariance is\nlost, hindering for example the proper definition of a global topological charge. In recent\nyears, a method that has been proven to be very effective in circumventing this issue\u2014while\nat the same time retaining the benefits of OBC simulations\u2014is the Parallel Tempering on\nBoundary Conditions (PTBC) algorithm. After its first introduction in 2d CPN\n1 mod-\nels [45] (see also [46, 47]), it has been widely employed also in 4d gauge theories, both\nin the pure-gauge case [18, 48\u201351] and with dynamical fermions [52]. The idea is to per-\nform a tempering on the boundary conditions within a parallel tempering framework by\nsimultaneously simulating several lattices with different boundary conditions, interpolating\nbetween OBC and PBC. Such lattice replicas are allowed to swap gauge configurations at\nequilibrium (i.e., via a standard Metropolis accept/reject step), so that quickly decorrelated\nfluctuations generated with OBC are transferred to the PBC system, where all observables\n\n\u2212\n\n\u2013 2 \u2013\n\n\fare computed free of boundary effects.",
        "char_count": 1820,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 5,
        "paper_id": "2510.25704v1",
        "text": "\u2212\n\n\u2013 2 \u2013\n\n\fare computed free of boundary effects.\n\nThe present work can be firmly placed within this context, i.e., algorithmic develop-\nment aimed at alleviating topological freezing in lattice gauge theories. Our goal is to\nintroduce a novel numerical strategy to mitigate this computational problem, combining\nideas previously presented in Refs. [53, 54]. Although this new proposal shares its basic un-\nderlying philosophy with the PTBC algorithm\u2014namely, to combine OBC and PBC to ac-\ncelerate the MCMC dynamics of topological modes while neutralizing unwanted boundary\neffects\u2014it is actually rooted on rather different and peculiar ingredients: out-of-equilibrium\nMCMC simulations [55, 56] and flow-based approaches [57, 58].\n\nAt the core of our approach lies a simple and general question: given a field configu-\nration sampled from a starting probability distribution (the prior ), can it be transformed\nin a controlled manner, so that it follows a different probability distribution that closely\napproximates the desired one (the target)? If the prior distribution features only mild\nautocorrelations and the transformation itself (the flow ) is both efficient to find and to\nsample from, these elements can be combined in a robust strategy to mitigate critical slow-\ning down in lattice gauge theories. The development of the so-called trivializing map [59]\nrepresented the first major effort in the construction of such a flow transformation, finding\nhowever limited success [60]. More recently, rapid progress in the field of deep learning has\nprovided the tools to construct much more flexible and complex flow transformations, most\nnotably with the implementation of Normalizing Flows (NFs) [61, 62] for lattice field theory\nsampling [57, 63]. Such architectures possess several desirable features, in particular their\nexpressiveness, allowing them to tackle complicated distributions, and their exactness, as\neffects due to differences between the inferred and the target distributions can be system-\natically removed. In recent years, significant progress has been made by the lattice field\ntheory community in the application of different NF architectures to a variety of models,\nranging from scalar theories [57, 63\u201372] to gauge theories [73\u201379], including formulations\nwith dynamical fermionic variables as well [80\u201384].",
        "char_count": 2338,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 6,
        "paper_id": "2510.25704v1",
        "text": "This new generation of flow-based samplers, however, features its own set of challenges.\nIn particular, finding the optimal NF parameters to flow efficiently from one distribution to\nanother requires a potentially very delicate and expensive training procedure. Concretely,\ntraining costs currently suffer from poor scaling, in particular when the number of the\nrelevant degrees of freedom involved in the model under study increases (e.g., with larger\nvolumes in units of the lattice spacing), see Refs. [65, 78, 85, 86].\n\nA different flow-based approach built on non-equilibrium MCMC (NE-MCMC) sim-\nulations addresses this scaling issue directly. This framework is based on two fundamen-\ntal results in non-equilibrium statistical mechanics, i.e., Jarzynski\u2019s equality [87\u201389] and\nCrooks\u2019 theorem [90, 91] and in the last decade it has been successfully applied in lattice\nfield theory. Specifically, its primary application has been the high-precision determina-\ntion of free energy differences [55], in particular for the equation of state [92], the running\ncoupling [93], the entanglement entropy [94, 95] and the Casimir effect [96]. More recently,\nthe same idea has been naturally repurposed as a flow-based approach for the mitigation of\ncritical slowing down [53, 97]: the current work represents the next step in this direction.\nA key advantage of the non-equilibrium approach is the well-understood scaling behaviour:\n\n\u2013 3 \u2013\n\n\fin particular, tests in a variety of models show that sampling costs grow linearly with the\nnumber of degrees of freedom varied during the flow transformation.",
        "char_count": 1597,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 7,
        "paper_id": "2510.25704v1",
        "text": "\u2013 3 \u2013\n\n\fin particular, tests in a variety of models show that sampling costs grow linearly with the\nnumber of degrees of freedom varied during the flow transformation.\n\nIn their basic implementation, out-of-equilibrium simulations require no training and\ncan achieve efficient sampling with no extra costs. Yet, despite their favorable scaling,\nthey can still require significant amount of computational resources.\nInterestingly, this\npurely stochastic approach can be naturally combined with the deterministic transforma-\ntions underlying NFs: the resulting architecture, denoted as Stochastic Normalizing Flows\n(SNFs) [56, 98], has found natural applications in scalar field theories [56, 99, 100] and,\nmost relevant for this work, in the SU(3) Yang-Mills theory in 4 spacetime dimensions [54].\nSNFs still retain the same desirable scaling properties of NE-MCMC, while at the same\ntime markedly improving its computational efficiency: even more importantly, this is ob-\ntained with very limited training costs, a direct consequence of the stochastic nature of\nthese flows.\n\nIt is worth noting that related ideas have appeared in different contexts. NE-MCMC\nis equivalent to Annealed Importance Sampling [101], which has been reworked recently in\nthe so-called Sequential Monte Carlo [102] and also combined with normalizing flows [103,\n104]. Recent developments in sampling with Langevin dynamics [105] can be seen as\na continuous-time realization of SNFs. Likewise, applications to lattice field theories of\ndiffusion models [106\u2013110] also bear several similarities with the ones described in this work:\na fundamental difference is that in diffusion models the path between two distributions\nis defined implicitly, whereas the protocol underlying NE-MCMC and SNFs is defined\nexplicitly.",
        "char_count": 1789,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 8,
        "paper_id": "2510.25704v1",
        "text": "In this work, we apply both non-equilibrium Monte Carlo and Stochastic Normaliz-\ning Flows as flow-based approaches for efficient topology sampling in the four-dimensional\nSU(3) Yang-Mills theory. This model exhibits particularly severe topological freezing near\nthe continuum limit, thus offering an ideal test-bed for flow-based approaches before mov-\ning to full QCD simulations. In Section 2 we introduce the general features of the non-\nequilibrium Monte Carlo approach and describe our lattice gauge theory setup, including\nthe definition of OBC and of the topological observables of interest. Section 3 presents a\ncareful analysis of the scaling of the sampling costs of non-equilibrium simulations, both\nfrom a general perspective and from the point of view of flows connecting OBC with PBC.\nSection 4 focuses on the definition of the customized Stochastic Normalizing Flow used in\nthis work, which has been designed to act specifically on the open boundaries, and on its\nsuperior performance with respect to the purely stochastic counterpart. Finally, in Sec-\ntion 5 we discuss the application of this family of flows to simulations at fine lattice spacing,\nwhere topological freezing is most severe: here we outline a strategy to sample topolog-\nical observables towards the continuum limit and present results to further corroborate\nthe effectiveness of this approach. Section 6 concludes with a broader discussion of future\ndevelopments, both in terms of advancements in the flow architectures and of applications\nto more challenging theoretical setups.\n\n\u2013 4 \u2013\n\n\f2 Non-equilibrium Monte Carlo simulations in lattice field theory\n\nOn the lattice, given an appropriately discretized action S[U ], we wish to compute the\nvacuum expectation value of a given observable\n\nas\n\n(cid:90)\n\n\u27e8O\u27e9p =\n\ndU\n\nO\n\n(U ) p(U ) =\n\n(cid:90)\n\n(U ) e\u2212\n\nS[U ],\n\ndU\n\nO\n\n(2.1)\n\nO\n1\nZ\n\nwhere p(U ) = e\u2212\nwith\n\nS[U ]/Z will be referred to as the target Boltzmann probability distribution,\n\n(cid:90)\n\nZp =\n\ndU e\u2212\n\nS[U ]\n\n(2.2)\n\n\u2212\n\nlog Z.",
        "char_count": 2017,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 9,
        "paper_id": "2510.25704v1",
        "text": "S[U ]/Z will be referred to as the target Boltzmann probability distribution,\n\n(cid:90)\n\nZp =\n\ndU e\u2212\n\nS[U ]\n\n(2.2)\n\n\u2212\n\nlog Z.\n\nbeing the partition function, from which we can immediately define the dimensionless free\nenergy F =\nIn a standard Monte Carlo simulation, field configurations U are\nsampled directly from p(U ) by updating them sequentially along a Markov Chain. More\nprecisely, the updating algorithm is characterized by a transition probability Pp which\nis constructed such that the chain converges to the distribution p, called the equilibrium\ndistribution. To ensure this, it is standard procedure at the beginning of a simulation to\nundergo a burn-in period called thermalization, after which the Markov chain is assumed\nto be at equilibrium.\n\nRecent advances in non-equilibrium statistical mechanics, however, enable, under cer-\ntain conditions and following precise procedures, to perform simulations out of equilibrium.\nIn particular, Jarzynski\u2019s equality [88, 89] represents a fundamental result in this regard:\nin the case of Markov Chain Monte Carlo (MCMC) simulations, it allows for the calcula-\ntion of \u201cequilibrium\u201d quantities (namely, differences in free energy) from those evaluated\non non-thermalized Markov Chains. In particular it is possible to leverage this identity to\ncompute expectation values (in particular in lattice field theory) with a Non-Equilibrium\nMarkov Chain Monte Carlo (NE-MCMC), which we describe in the following.\n\nIn this approach, we build non-equilibrium \u201cevolutions\u201d that start from a prior distri-\nbution q0 = exp(\nS)/Z, which we aim\nto sample from. More precisely, each evolution is composed of a sequence of nstep field\nconfigurations Un:\n\nS0)/Z0 and reach a target distribution p = exp(\n\n\u2212\n\n\u2212\n\n: U0\n\nU\n\nP\u03bb(1)\n\u2212\u2192\n\nU1\n\nP\u03bb(2)\n\u2212\u2192\n\nU2\n\nP\u03bb(3)\n\u2212\u2192\n\n. . .\n\nP\u03bb(nstep)\n\u2212\u2192\n\nUnstep \u2261\n\nU.\n\n(2.3)",
        "char_count": 1838,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 10,
        "paper_id": "2510.25704v1",
        "text": "U1\n\nP\u03bb(2)\n\u2212\u2192\n\nU2\n\nP\u03bb(3)\n\u2212\u2192\n\n. . .\n\nP\u03bb(nstep)\n\u2212\u2192\n\nUnstep \u2261\n\nU.\n\n(2.3)\n\n= [U0, U1, . . . , U ]. At the beginning we have a config-\nfor which we use the shorthand\nuration U0, sampled directly from q0: the latter can be, for example, a Markov Chain at\nequilibrium or a known analytical distribution one can sample directly from. Then, the evo-\nlution proceeds using a composition of Monte Carlo updates with transition probabilities\nP\u03bb(n) (the arrows in the above sequence) which satisfy detailed balance.\n\nU\n\nNote that the transition probabilities change throughout the evolution according to a\n(set of) parameter(s) \u03bb(n), called the protocol. Each P\u03bb(n) is defined with an equilibrium\ndistribution proportional to exp(\nS\u03bb(n)): the dependence on the protocol \u03bb(n) is explicit\nin the action S\u03bb(n), which interpolates (in general completely arbitrarily) between the prior\nand the target. The only exception is the last transition probability, which is fixed to\n\n\u2212\n\n\u2013 5 \u2013\n\n\fFigure 1. Scheme of a typical non-equilibrium simulation. A thermalized configuration (black\ncircle) is sampled from the prior distribution every nbetween MCMC steps (black squares); and an\nout-of-equilibrium evolution starts from it, following a given protocol \u03bb for nstep MCMC steps (red\ndiamonds) until the desired target distribution is reached. The work W of Eq. (2.5) is computed\nalong each evolution, while the value of the desired observable(s) is calculated in the last configu-\nration (red circle). The estimators of Eqs. (2.4) and (2.6) are obtained by taking the average\nacross different evolutions.\n\n. . .\n\n\u27e9f\n\n\u27e8",
        "char_count": 1593,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 11,
        "paper_id": "2510.25704v1",
        "text": ". . .\n\n\u27e9f\n\n\u27e8\n\nPp or, equivalently,\nhave the target distribution as equilibrium distribution, i.e., P\u03bb(nstep) \u2261\n\u03bb(nstep) must coincide with the value of the target distribution we want to sample from.\nThe fact that \u03bb(n) (and, thus, P\u03bb(n)) changes after each Monte Carlo update defines a\ntrue non-equilibrium evolution: since we do not let it thermalize at each step, the Markov\nChain is never at equilibrium.\n\nIn order to compute the expectation values of Eq. (2.1) with NE-MCMC we use the\n\nestimator\n\n(U )\n\n\u27e9p = \u27e8O\n\n\u27e8O\n\nW (\n\n)\n\nU\n\n(U ) e\u2212\nW (\ne\u2212\n\nU\n\n\u27e8\n\n)\n\n\u27e9f\n\n\u27e9f\n\n.\n\n(2.4)\n\n\u27e9f an average over all evolutions of the type of Eq. (2.3); more-\nHere, we indicate with\nover, W is the dimensionless work done on the system during the non-equilibrium trans-\nformation from the initial to the final state:\n\n. . .\n\n\u27e8\n\nW (\n\nU\n\n) =\n\n1\n\nnstep\n\u2212\n(cid:88)\n\nn=0\n\n(cid:8)S\u03bb(n+1) [Un]\n\nS\u03bb(n) [Un](cid:9) .\n\n\u2212\n\nFinally, we can write down Jarzynski\u2019s equality [88, 89]:\n\ne\u2212\n\nW (\n\n)\n\nU\n\n\u27e9f = e\u2212\n\n\u2206F =\n\n\u27e8\n\nZp\nZq0\n\n,\n\n(2.5)\n\n(2.6)\n\nwhich connects the average of the exponential of the work on non-equilibrium evolutions\nwith the difference in free energy between the system described by the target and the prior\nprobability distributions. We show in Fig. 1 a scheme of a typical NE-MCMC simulation.\n\n\u2013 6 \u2013\n\nnstepnbetweennstepnbetweennstepnbetweennstepnbetweeneqMCnon-eqMC\f2.1 Some insights on NE-MCMC and its metrics",
        "char_count": 1393,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 12,
        "paper_id": "2510.25704v1",
        "text": "\u2013 6 \u2013\n\nnstepnbetweennstepnbetweennstepnbetweennstepnbetweeneqMCnon-eqMC\f2.1 Some insights on NE-MCMC and its metrics\n\nLet us first formally define the\n\u27e9f average of Eq. (2.4). Once the protocol (i.e., \u03bb(n)\nand nstep) and the Monte Carlo update (i.e., the details of P\u03bb(n)) have been chosen, we can\ndefine the forward\n\nPr transition probabilities for a given evolution\n\nPf and reverse\n\n. . .\n\nas\n\nU\n\n\u27e8\n\nnstep\n(cid:89)\n\nn=1\n\nnstep\n(cid:89)\n\nn=1\n\nand\n\n] =\n\nPf [\n\nU\n\n] =\n\nPr[\n\nU\n\nP\u03bb(n)(Un\n\nUn),\n\n1 \u2192\n\n\u2212\n\nP\u03bb(n)(Un \u2192\n\nUn\n\n1).\n\n\u2212\n\n(2.7)\n\n(2.8)\n\nSince the Monte Carlo updates must satisfy detailed balance, it is possible to state Crooks\u2019\nfluctuation theorem [90, 91], which for Markov Chains relates the forward and reverse\n:\nprobability densities of a given NE-MCMC evolution to the dissipation of the sequence\n\nU\n\n]\n\nq0(U0)\np(U )\n\nPf [\nU\n]\nPr[\nU\n\n= exp(W (\n\n)\n\nU\n\n\u2212\n\n\u2206F );\n\n(2.9)\n\nthis result can be proved rather easily using the properties of Markov Chain transition\nprobabilities that satisfy detailed balance.\nIt is useful to introduce the pseudo-heat Q\nexchanged during each transformation:\n\n) =\n\nQ(\n\nU\n\nnstep\n(cid:88)\n\nn=1\n\n(cid:8)S\u03bb(n) [Un]\n\nS\u03bb(n) [Un\n\n\u2212\n\n1](cid:9) ,\n\n\u2212\n\n(2.10)\n\n\u2212\n). We have now all the elements to properly define the expectation values over forward\n\nwhich takes this form following the First Law of Thermodynamics, S[U ]\nQ(\nevolutions as:\n\nS0[U0] = W (\n\n\u2212\n\nU\n\nU\n\n)\n\n. . .\n\n\u27e8\n\nq0\n\n\u27e9U\u223c\n\nf \u2261 \u27e8\nP\n\n. . .\n\n\u27e9f =",
        "char_count": 1427,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 13,
        "paper_id": "2510.25704v1",
        "text": "S0[U0] = W (\n\n\u2212\n\nU\n\nU\n\n)\n\n. . .\n\n\u27e8\n\nq0\n\n\u27e9U\u223c\n\nf \u2261 \u27e8\nP\n\n. . .\n\n\u27e9f =\n\n(cid:90)\n\n(cid:90)\n\ndU0 . . . dU q0(U0)\n\nPf [U0, . . . , U ] . . .\n\n(2.11)\n\n=\n\nd\n\nU\n\nq0(U0)\n\nPf [\n\nU\n\n] . . . ,\n\nwhere the shorthand d\n= dU0 . . . dU represents the Haar measure over all intermediate\nconfigurations. Similarly, the expectation values over reverse evolutions can be written as:\n\nU\n\n. . .\n\n\u27e8\n\n\u27e9U\u223c\n\np\n\nr \u2261 \u27e8\n\nP\n\n. . .\n\n\u27e9r =\n\n(cid:90)\n\n(cid:90)\n\ndU . . . dU0 p(U )\n\nPr[U0, . . . , U ] . . .\n\n(2.12)\n\n=\n\nd\n\nU\n\np(U )\n\nPr[\n\nU\n\n] . . . ,\n\nLet us now point out that, since the reverse sequences start from configurations U\nthe equilibrium, computing expectation values over p and p\nCrooks\u2019 fluctuation theorem:\n\np at\nPr is equivalent. Thus, using\n\n\u223c\n\n\u27e8O\u27e9p =\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\ndU p(U )\n\n(U ) =\n\nO\n\nd\n\nU\n\np(U )\n\nPr[U, . . . , U0]\n\nO\n\n(U )\n\n(2.13)\n\n=\n\nd\n\nU\n\nq0(U0)\n\nPf [\n\nU\n\n]e\u2212\n\n(W (\n\n)\n\nU\n\n\u2206F )\n\n\u2212\n\n(U ) ,\n\nO\n\n\u2013 7 \u2013\n\n\fwe obtain Eq. (2.4); setting\n\n= 1 leads to Jarzynski\u2019s equality (2.6).\n\nO\n\nCrooks\u2019 theorem, Eq. (2.9), gives us some precious intuition: evolutions",
        "char_count": 1051,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 14,
        "paper_id": "2510.25704v1",
        "text": "= 1 leads to Jarzynski\u2019s equality (2.6).\n\nO\n\nCrooks\u2019 theorem, Eq. (2.9), gives us some precious intuition: evolutions\n\nthat are\nequally probably going forward and backwards (i.e., more \u201creversible\u201d) will feature a work\nW equal to \u2206F . We are however interested in a formal statement about the reversibility of\na given protocol (i.e., a choice of the functional form of \u03bb(n), nstep and the MCMC update).\nTo do this, we can take the (reverse) Kullback\u2013Leibler (KL) divergence \u02dcDKL(p1\u2225\np2), which\nmeasures the degree of similarity of two probability densities p1 and p2. Using the non-\nequilibrium average defined in Eq. (2.11) we can write down the KL divergence between\nq0Pf and p\n\nPr as\n\nU\n\n\u02dcDKL(q0(U0)\n\nPf \u2225\n\np(U )\n\nPr) =\n\n\u27e8\n\nlog\n\nq0(U0)\np(U )\n\nPf (\n)\nU\n) \u27e9f \u2265\nPr(\nU\n\n0,\n\n(2.14)\n\nand using Crooks\u2019 theorem this becomes simply\n\n\u02dcDKL(q0(U0)\n\n)\n\n\u27e8\n\nW (\n\nPf \u2225\n\np(U )\n\nPr) =\n\n\u27e9f .\nHere, we defined the dissipated work Wd(\n\u2206F , which provides a measure of the\n) = W (\ndissipation of a given thermodynamic out-of-equilibrium transformation (or, equivalently,\nprotocol) between q0 and p. If the protocol is reversible, i.e., the KL divergence vanishes,\nwe have\n\u27e9f = 0. Furthermore, given the positivity of the KL divergence, we recover\nthe Second Law of Thermodynamics as well, i.e.\n\n\u2206F =\n\n(2.15)\n\n\u27e9f \u2212\n\nWd(\n\nWd(\n\n\u2212\n\nU\n\nU\n\nU\n\nU\n\nU\n\n\u27e8\n\n)\n\n\u27e8\n\n)\n\n)\n\nWd(\n\n)\n\nU\n\n\u27e9f \u2265\n\n0.\n\n\u27e8\n\n(2.16)",
        "char_count": 1370,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 15,
        "paper_id": "2510.25704v1",
        "text": "\u2212\n\nU\n\nU\n\nU\n\nU\n\nU\n\n\u27e8\n\n)\n\n\u27e8\n\n)\n\n)\n\nWd(\n\n)\n\nU\n\n\u27e9f \u2265\n\n0.\n\n\u27e8\n\n(2.16)\n\nAside from considerations on the thermodynamic nature of a Markov Chain out of\nequilibrium, a question arises naturally: why do we care about the KL divergence of\nEq. (2.14)? It is indeed easy to prove that it is an upper bound for another KL divergence:\n\n\u02dcDKL(q\n\np)\n\n\u2225\n\n\u2264\n\n\u02dcDKL(q0Pf \u2225\n\np\n\nPr),\n\n(2.17)\n\nwith q being the probability distribution of the system at the end of the evolution, which\nis not analytically accessible in general and we formally define as\n\n(cid:90)\n\nq(U ) =\n\ndU0 . . . dUnstep\n\n1 q0(U0)\n\nPf (\n\nU\n\n).\n\n\u2212\n\n(2.18)\n\nHence, by minimizing the dissipated work\nWd\u27e9f , we also necessarily minimize the KL\n\u27e8\ndivergence between our target distribution and the one we generate, which is exactly our\ngoal.\n\nAnother relevant metric for non-equilibrium protocols is defined looking at the ratio\nof the variance of the two estimators of\nappearing in Eq. (2.4): the first, sampling\ndirectly from p (neglecting autocorrelations); the second, using NE-MCMC. The ratio of\nthe variances of the estimators defines the so-called Effective Sample Size ESS:\n\n\u27e8O\u27e9\n\nESS =\n\nVar(\nVar(\n\n)p\nO\n)NE\nO\n\n.\n\n\u2013 8 \u2013\n\n(2.19)\n\n\fThis is usually approximated (neglecting correlations between\n\nand W ) as\n\nO\n\n2\nW )\nf\n\u27e9\n2W )\n\u27e9f\nwhich can be readily computed for any protocol. While not directly relevant for this work,\nwe note that this metric is exactly related to the variance of the estimator of the free energy\ngiven by Jarzynski\u2019s equality, Eq. (2.6):\n\n\u02c6ESS = \u27e8\n\u27e8\n\n1\n2Wd)\n\nexp(\nexp(\n\n(2.20)\n\n\u2212\n\u2212",
        "char_count": 1545,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 16,
        "paper_id": "2510.25704v1",
        "text": "\u02c6ESS = \u27e8\n\u27e8\n\n1\n2Wd)\n\nexp(\nexp(\n\n(2.20)\n\n\u2212\n\u2212\n\nexp(\n\n\u27e9f\n\n\u2212\n\n=\n\n\u27e8\n\n,\n\nVar (exp(\n\n\u2212\n\nW )) = exp(\n\n2\u2206F )\n\n\u2212\n\n(cid:18) 1\n\n\u02c6ESS \u2212\n\n(cid:19)\n\n1\n\n.\n\n(2.21)\n\n2.2 Lattice setup and topological observables\n\nAs outlined in the introduction, in this investigation we choose the pure-gauge theory as\na test-bed for our novel method. We discretize the pure-gauge SU(3) Yang\u2013Mills theory\nusing the standard Wilson plaquette action on an hyper-cubic L4 lattice. Periodic boundary\nconditions are taken for every link but those living on a small sub-region of the lattice,\nwhich in the following will be called the defect. In this region, we allow links to experience\ndifferent boundary conditions, which are changed through discrete out-of-equilibrium steps\nfrom open to periodic through a tunable parameter \u03bb(n), with n an integer index labeling\nthe out-of-equilibrium step. The path in parameter space connecting OBC to PBC defines,\nin the case at hand, the out-of-equilibrium protocol introduced in Sec. 2.\n\nThe lattice gauge action S\u03bb(n) at the nth out-of-equilibrium step reads:\n\nS\u03bb(n)[Un] =\n\n\u03b2\nN\n\n\u2212\n\n(cid:88)\n\nx,\u00b5\n\n=\u03bd\n\nK(n)\n\n\u00b5\u03bd (x)\n\nTr\n\n\u211c\n\n(cid:104)\nP (n)\n\n\u00b5\u03bd (x)\n\n(cid:105)\n\n,\n\n(2.22)",
        "char_count": 1171,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 17,
        "paper_id": "2510.25704v1",
        "text": "x,\u00b5\n\n=\u03bd\n\nK(n)\n\n\u00b5\u03bd (x)\n\nTr\n\n\u211c\n\n(cid:104)\nP (n)\n\n\u00b5\u03bd (x)\n\n(cid:105)\n\n,\n\n(2.22)\n\nwhere Un stands for the collection of gauge links at the nth out-of-equilibrium step. In\nEq. (2.22) N = 3 is the number of colors, \u03b2 = 2N/g2 is the inverse bare coupling, P (n)\n\u00b5\u03bd (x) =\nU (n)\n\u00b5 (x)U (n)\n\u2020(x) is the elementary plaquette operator at site x on the\nplane (\u00b5, \u03bd) computed on the configuration Un at the nth out-of-equilibrium step, a is the\nlattice spacing, and K(n)\n\u00b5\u03bd (x) is a numerical factor used to modify the boundary conditions\nthrough the parameter \u03bb(n):\n\n\u2020(x+ \u02c6\u03bd)U (n)\n\n(x+ \u02c6\u00b5)U (n)\n\n\u00b5\n\n\u03bd\n\n\u03bd\n\nK(n)\n\n\u00b5\u03bd (x) = K(n)\n\n\u00b5 (x)K(n)\n\n\u03bd\n\n(x + \u02c6\u00b5)K(n)\n\n\u00b5 (x + \u02c6\u03bd)K(n)\n\n\u03bd\n\n(x),\n\n(2.23)\n\nK(n)\n\n\u00b5 (x) =\n\n\uf8f1\n\uf8f2\n\n\u03bb(n),\n\n\uf8f3\n\n1,\n\n\u00b5 = 0,\n\nx0 = L\n\na,\n\n0\n\n\u2212\n\n\u2264\n\nx1, x2, x3 < Ld,\n\n(2.24)\n\nelsewhere,\n\nwith \u03bb(n = 0) = 0 denoting OBC and \u03bb(n = nstep) = 1 denoting PBC. The defect is de-\na along the temporal\nfined as a cube of size Ld, and it is placed on the time slice x0 = L\ndirection \u00b5 = 0, meaning that only plaquettes including temporal links crossing the defect\nwill \u201cfeel\u201d the modified boundary conditions. We also stress that, for all values of \u03bb, the\nMonte Carlo updates of the gauge configurations were performed using the customary 4:1\n\n\u2212\n\n\u2013 9 \u2013",
        "char_count": 1228,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 18,
        "paper_id": "2510.25704v1",
        "text": "\u2212\n\n\u2013 9 \u2013\n\n\u0338\n\fmixture of over-relaxation (OR) [111] and heat-bath (HB) [112, 113] algorithms, imple-\nmented according to the Cabibbo\u2013Marinari prescription [114], i.e., updating the 3 SU(2)\nsubgroups of SU(3). In the following we will refer to this combination with the shorthand\n1HB+4OR.\n\nGiven that in this study we are addressing the infamous topological freezing issue, it\nis natural to focus on the measurement of the topological susceptibility. In this study, as\nalready outlined in the introduction, all computations of physical quantities are performed\nat the end of the out-of-equilibrium evolution, where PBC are restored all over the lat-\ntice. Therefore, from now on we will just assume PBC for all gauge links. Since periodic\nboundaries preserve translation invariance, we are allowed to consider the total topological\ncharge Q and compute the topological susceptibility via its standard definition:\n\n\u03c7 = \u27e8\n\nQ2\nV\n\n\u27e9\n\n,\n\nV = L4,\n\n(2.25)\n\nwhere the expectation value in the presence of PBC appearing here is computed out-of-\nequilibrium using Jarzynski\u2019s equality as explained in Sec. 2.\n\nIn this study we discretize the topological charge using the simplest parity-odd lattice\n(cid:82) d4x Tr[G\u00b5\u03bd \u02dcG\u00b5\u03bd], defined in terms of\n\nformulation of the continuum observable Q = 1\n16\u03c02\nthe \u201cclover\u201d plaquette:\n\nQclov =\n\n1\n29\u03c02\n\n4\n(cid:88)\n\u00b1\n\n\u00b5\u03bd\u03c1\u03c3=\n\n\u00b1\n\n1\n\n\u03b5\u00b5\u03bd\u03c1\u03c3Tr [P\u00b5\u03bd(x)P\u03c1\u03c3(x)] ,\n\n(2.26)\n\n\u2212\n\n\u2212\n\n\u00b5)\u03bd\u03c1\u03c3 =",
        "char_count": 1402,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 19,
        "paper_id": "2510.25704v1",
        "text": "\u00b5\u03bd\u03c1\u03c3=\n\n\u00b1\n\n1\n\n\u03b5\u00b5\u03bd\u03c1\u03c3Tr [P\u00b5\u03bd(x)P\u03c1\u03c3(x)] ,\n\n(2.26)\n\n\u2212\n\n\u2212\n\n\u00b5)\u03bd\u03c1\u03c3 =\n\n\u03b5\u00b5\u03bd\u03c1\u03c3. As it is well known, unlike in the continuum\nwhere it is understood that \u03b5(\ntheory, Qclov is not integer-valued on the lattice, and it is related to the continuum topo-\nlogical charge Q via a finite renormalization [115, 116]: Qclov = ZQQ. The renormalization\n. Moreover, a naive lattice\nfactor ZQ(\u03b2) < 1 tends to 1 only in the continuum limit \u03b2\ndefinition of the topological susceptibility \u03c7clov = \u27e8\n, built in terms of Qclov would re-\nceive a divergent additive renormalization term too due to contact terms [117\u2013120], which\n0. To deal with these renormaliza-\nwould eventually overcome the physical signal as a\ntions, it is customary to resort to smoothing methods, which is by now a widely employed\ntechnique [2, 7, 8, 10\u201312, 16, 17, 116, 120\u2013139]. Once Qclov is computed on smoothened\nfields, UV contamination at the scale of the lattice spacing is removed, leading to Z\n1,\nand the effects of the contact term vanish. Thus, after smoothing, the lattice definition of\n\u03c7 is free of multiplicative and additive renormalizations [116, 117, 120], and will converge\ntowards the correct (finite) continuum limit [7, 16].\n\nQ2\nclov\u27e9\nV\n\n\u2192 \u221e\n\n\u2192\n\n\u2243",
        "char_count": 1209,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 20,
        "paper_id": "2510.25704v1",
        "text": "Q2\nclov\u27e9\nV\n\n\u2192 \u221e\n\n\u2192\n\n\u2243\n\nOn general grounds, smoothing damps UV fluctuations at length scales below a smooth-\ning radius Rs, while leaving the relevant infrared physics intact (if smoothing is not exces-\nsively prolonged). Several smoothing methods have been proposed in the literature, such\nas cooling [119, 140\u2013145], stout smearing [146, 147], or gradient flow [59, 148\u2013151], and\nthey have all been shown to be numerically equivalent when properly matched to each\nother, i.e., when smearing parameters are chosen so as to correspond to the same value of\nRs, see Refs. [145, 152, 153]. In this work, due to its computational inexpensiveness, we\n\n\u2013 10 \u2013\n\n\fadopt cooling, and define our lattice charge and susceptibility as:\n\nQL = Qclov[Ucool],\nQ2\nL\u27e9\nV\n\n\u03c7L = \u27e8\n\n,\n\n(2.27)\n\n(2.28)\n\nwhere Ucool denotes the gauge links after applying ncool cooling steps.\nIn the case of\nstandard Wilson cooling, the following relation between the number of cooling steps and\nthe smoothing radius has been established by matching with the Wilson gradient flow [152]:\n\nRs\na\n\n=\n\n(cid:114)\n\n8\n3\n\nncool.\n\n(2.29)\n\nThe dependence of Rs on \u221ancool actually stems from a general feature of smoothing meth-\nods: the smoothing radius is always proportional to the square root of the amount of\nsmoothing performed because all smoothing algorithms act as diffusive processes. As an\n8t/a2, with t/a2 the flow time,\nexample, using the Wilson flow, Rs/a =\nwould give an equivalent smoothing radius choosing t/a2 = ncool/3 [152]. In this work, we\n0.4L\nadopted ncool = 60 for all values of \u03b2 explored, thus corresponding to Rs \u2243\nin all cases.\n\n8ncool/3 =\n\n12.6a\n\n(cid:112)\n\n(cid:112)\n\n\u223c\n\nThe simulation code used to numerically simulate the lattice setup described so far\ncan be found in the public release [154], based on a modification of [155], which is in turn\nbased on a modification of [156].",
        "char_count": 1857,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 21,
        "paper_id": "2510.25704v1",
        "text": "\u223c\n\nThe simulation code used to numerically simulate the lattice setup described so far\ncan be found in the public release [154], based on a modification of [155], which is in turn\nbased on a modification of [156].\n\n3 Scaling of NE-MCMC in the boundary conditions\n\n\u00b5 (x), as already described in Eq. (2.24).\n\nIn this section we will discuss the scaling features of a particular non-equilibrium simulation,\nin which the role of the protocol \u03bb(n) appearing in Eq. (2.3) and Eq. (2.7) is to change\nthe parameter K(n)\nIn particular, the evolution\nin the boundary conditions (from open to periodic) is described by a unique parameter\nfor all the lattice links that are part of the defect. For all the non-equilibrium evolutions\nperformed in this work we used a linear protocol for the parameter \u03bb(n); we leave the\ninvestigation of more efficient protocols to future work. We thus have just two parameters:\nthe number of degrees of freedom that \u201cfeel\u201d the defect ndof = (Ld/a)3, and the number\nof out-of-equilibrium steps nstep. The goal of the present section is to understand how the\nperformances of the NE-MCMC scale with these two quantities.\n\nFor this purpose, we performed a first batch of NE-MCMC simulations at a relatively\ncoarse spacing at \u03b2 = 6.0, using cubic defects of different sizes Ld, ranging from 2 to 6, and\nvarying the duration of the evolution, expressed in units of MCMC updates as nstep (see\nFig. 1). We also fix the frequency with which we sample the prior distribution (the one\nwith the OBC defect) to be nbetween = 5: this is the number of MCMC updates between\nstarting configurations of subsequent evolutions, see Fig. 1. The main aim of this numerical\ntest is to check the behavior of two metrics, the KL divergence of Eq. (2.14) (equivalent\n\u02c6ESS of Eq. (2.20), as a function of nstep. Results are\nto the dissipated work Wd) and the\nreported in the left panels of Fig. 2 and Fig. 3.\n\n\u2013 11 \u2013",
        "char_count": 1911,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 22,
        "paper_id": "2510.25704v1",
        "text": "\u2013 11 \u2013\n\n\fFigure 2. Results for the Kullback-Leibler divergence of Eq. (2.14) for NE-MCMC in the boundary\nconditions as a function of the number of steps in the flow nstep (left panel) and as a function of\nnstep divided by the size of the defect (right panel). All results obtained on a 164 lattice at \u03b2 = 6.0.\n\nFigure 3. Results for the effective sample size of Eq. (2.20) for NE-MCMC in the boundary\nconditions as a function of the number of steps in the flow nstep (left panel) and as a function of\nnstep divided by the size of the defect (right panel). All results obtained on a 164 lattice at \u03b2 = 6.0.\n\nAs expected, the value of the dissipated work\n\nWd\u27e9f decreases rather rapidly when\nincreasing nstep, as the evolutions in the boundary condition parameters are performed\nmore slowly and approach a reversible transformation. This provides a precious upper\nbound on the similarity between the (analytically intractable) non-equilibrium probability\ndistribution at the end of the evolution and the probability distribution with PBC, see\nEq. (2.17). Similarly, the \u02c6ESS approaches larger values fairly quickly when nstep grows; this\nW ) and, in good approximation,\npoints at a greatly reduced variance of the weight exp(\n\n\u27e8\n\n\u2212\n\n\u2013 12 \u2013",
        "char_count": 1236,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 23,
        "paper_id": "2510.25704v1",
        "text": "\u27e8\n\n\u2212\n\n\u2013 12 \u2013\n\n050010001500200025003000nstep10\u2212210\u22121100hWdi050100150200nstep/(Ld/a)310\u2212210\u22121100hWdi\u03b2=6.0,L/a=16NE-MCMC,Ld/a=2NE-MCMC,Ld/a=3NE-MCMC,Ld/a=4NE-MCMC,Ld/a=5NE-MCMC,Ld/a=6\ufb01tin(Ld/a)3/nstep050010001500200025003000nstep0.00.20.40.60.81.0\u02c6ESS050100150200nstep/(Ld/a)30.00.20.40.60.81.0\u02c6ESS\u03b2=6.0,L/a=16NE-MCMC,Ld/a=2NE-MCMC,Ld/a=3NE-MCMC,Ld/a=4NE-MCMC,Ld/a=5NE-MCMC,Ld/a=6\ufb01tinexp(\u2212k0(Ld/a)3/nstep)\fat a smaller variance of the estimator of Eq. (2.4).\n\nWd\u27e9f or",
        "char_count": 464,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 24,
        "paper_id": "2510.25704v1",
        "text": "Wd\u27e9f or\n\nNaturally, the cost in terms of out-of-equilibrium Monte Carlo updates (quantified by\n\u02c6ESS) strongly depends on the size\nnstep) to reach a given target metric (either\n\u27e8\nof the defect. This is not surprising, as larger defects naturally induce stronger finite-size\neffects which in turn require a bigger effort to be removed. The precise scaling relation\nis easily observed in the right panels of Fig. 2 and Fig. 3. Here, the dissipated work and\n\u02c6ESS are once again plotted, but this time as a function of nstep/(Ld/a)3, that is, the\nthe\nduration of the evolution in terms of MCMC steps divided by the spatial volume of the\ndefect in lattice units. The latter quantity is exactly the number of degrees of freedom\nmodified along the evolution itself. Data for different defect sizes collapse neatly on the\nsame curve, which represents precisely the scaling function of NE-MCMC metrics:\nfor\nexample, fixing the defect size Ld, the value of nstep needed to reach a target metric (e.g.,\n\u02c6ESS = 0.4) can be immediately derived just looking at these results.\n\n3.1 Understanding the scaling with the degrees of freedom\n\n\u27e8\n\nIt is worthwhile to understand the NE-MCMC scaling a bit more precisely: in particular,\nWd\u27e9f seems to depend uniquely on nstep/ndof , with ndof being the number\nthe reason why\nof degrees of freedom that are varied throughout a non-equilibrium trajectory. This fact is\nfar from being limited to evolutions from OBC to PBC (where it was already observed in\n1 model [53]). It is also present, for instance, when changing the inverse coupling\n2d CPN\n\u03b2 in SU(3) pure gauge theory [54] and when exchanging slabs between lattices in O(N )\nspin models [96].\n\n\u2212\n\nLet us look at the dissipated work once again, writing it as\n\nWd\u27e9f =\n\n\u27e8\n\n\u27e8\n\nW\n\n\u27e9f \u2212\n\n\u2206F\n\n\u2243\n\n\u03b4\u03bb\n\n1\n\nnstep\n\u2212\n(cid:88)\n\n(cid:26)\n\n\u2202S\u03bb(n)\n\n\u2202S\u03bb(n)\n\n\u27e8",
        "char_count": 1822,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 25,
        "paper_id": "2510.25704v1",
        "text": "W\n\n\u27e9f \u2212\n\n\u2206F\n\n\u2243\n\n\u03b4\u03bb\n\n1\n\nnstep\n\u2212\n(cid:88)\n\n(cid:26)\n\n\u2202S\u03bb(n)\n\n\u2202S\u03bb(n)\n\n\u27e8\n\n\u2202\u03bb \u27e9f,\u03bb(n) \u2212 \u27e8\n\n\u2202\u03bb \u27e9eq,\u03bb(n)\n\nn=0\n\n(cid:27)\n\n,\n\n(3.1)\n\n\u03bbn+1\u2212\n\n\u03bbn, i.e., assuming for simplicity a linear change in a protocol\nwhere we used 1/nstep \u2261\nparameter and that \u03bb(0) = 0 and \u03bb(nstep) = 1. We also (approximately) calculated the\nfree energy difference using a basic implementation of the integral method:\nindeed, the\n\u27e9eq,\u03bb(n) average is the standard expectation value at equilibrium with respect to the\n. . .\n\u27e8\nprobability distribution defined with S\u03bb(n). This is in general very different from the\n\u27e9f,\u03bb(n) average, which it is calculated during a non-equilibrium evolution for a specific\n. . .\n\u27e8\nprotocol, and also depends strongly on the details of the latter.\n\nLet us look at the derivative of the action with respect to the protocol parameter \u03bb\nfirst: for evolutions in the boundary conditions and specifying the action to be Eq. (2.22),\nthis term is simply the sum of the plaquettes that touch the defect1. In this case we can\nwrite it down approximately as\n\n\u2202S\u03bb(n)\n\n\u2202\u03bb \u2243 \u2212\n\n6\u03b2 ndof \u03bb(n) Pd,\n\n(3.2)\n\nwith Pd being the average of the plaquettes that contain two of the defect links indicated\nin Eq. (2.24) and ndof = (Ld/a)3; as we are interested only in a qualitative behavior, we\n\n1In the case of evolutions in \u03b2, the \u2202S\u03bb(n)/\u2202\u03bb term would be the sum of all the plaquettes on the lattice.\n\n\u2013 13 \u2013\n\n\fignore the plaquettes containing only one defect link. Now the dissipated work becomes\n\nWd\u27e9f \u2243\n\n\u27e8\n\n6\n\nndof\nnstep\n\n\u03b2\n\n1\n\nnstep\n\u2212\n(cid:88)\n\nn=0",
        "char_count": 1520,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 26,
        "paper_id": "2510.25704v1",
        "text": "Wd\u27e9f \u2243\n\n\u27e8\n\n6\n\nndof\nnstep\n\n\u03b2\n\n1\n\nnstep\n\u2212\n(cid:88)\n\nn=0\n\n\u03bb(n) (cid:0)\n\nPd\u27e9eq,\u03bb(n) \u2212 \u27e8\n\nPd\u27e9f,\u03bb(n)\n\n\u27e8\n\n(cid:1) .\n\n(3.3)\n\nThe question is, what is the behavior of the defect plaquettes at each step n of a non-\nequilibrium evolution with respect to its corresponding value at equilibrium (i.e., for the\nsame parameter \u03bb(n))? Intuition suggests that it should vanish when going towards equi-\n. Thus, we assume2 that this quantity is directly\nlibrium, i.e., in the limit nstep \u2192 \u221e\nproportional to the speed of the evolution:\n\nPd\u27e9eq,\u03bb(n) \u2212 \u27e8\n\nPd\u27e9f,\u03bb(n) \u223c\n\n\u27e8\n\n1\nnstep\n\n.\n\n(3.4)\n\nRecalling that in our case \u03bb(n) = n/nstep, this finally gives us a very simple qualitative\nbehavior for the average dissipated work:\n\nWd\u27e9f \u223c\n\n\u27e8\n\n6\n\nndof\nnstep\n\n\u03b2\n\n1\n\nnstep\n\u2212\n(cid:88)\n\nn=0\n\nn\nn2\nstep \u00d7\n\nK(\u03b2)\n\n3\u03b2\n\n\u223c\n\nndof\nnstep \u00d7\n\nK(\u03b2),\n\n(3.5)\n\nwhere we the K factor contains a residual dependence on \u03b2: a proper investigation of a good\napproximation is left to future work. Finally, we have recovered an explicit dependence\nof the dissipated work (or reverse KL divergence)\nWd\u27e9f on the ratio nstep/ndof : this is\nfurther supported by the excellent fit of the data of Fig. 2 with a 1/(nstep/ndof ) behavior.\nAnother useful analysis can be made on the Effective Sample Size: it has been observed\nin the past [81, 85] that for a fixed flow architecture, the \u02c6ESS decreases exponentially with\nthe number of degrees of freedom in the system3, i.e.\n\n\u27e8\n\n\u02c6ESS(ndof ) = exp(\n\nk ndof ),\n\n\u2212\n\n(3.6)",
        "char_count": 1454,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 27,
        "paper_id": "2510.25704v1",
        "text": "\u27e8\n\n\u02c6ESS(ndof ) = exp(\n\nk ndof ),\n\n\u2212\n\n(3.6)\n\nwhich is observed also in the case of NE-MCMC flows for fixed nstep, see Fig. 4.\n\nHowever, we also observe from Fig. 3 that the \u02c6ESS is, to a very good approximation,\n\na function of nstep/ndof and not just of ndof ; hence, it is natural to write that\n\n\u02c6ESS(ndof ) = exp\n\n(cid:18)\n\nk\u2032\n\n\u2212\n\n(cid:19)\n\n,\n\nndof\nnstep\n\n(3.7)\n\nand this is confirmed by the excellent qualitative agreement when fitting the data points\nin Fig. 3. This is a striking example of how incorporating non-equilibrium MC updates\nin a flow-based approach (as in the case of NE-MCMC) naturally provides an exponential\nimprovement with respect to a given fixed architecture. This analysis strongly suggests\nthat NE-MCMC (and related approaches) offer a compelling framework to tackle the issue\nof scaling flow-based samplers to problems characterized by large ndof .\n\n2This is not arbitrary, as linear response theory generally predicts this behavior at first order [157].\n3This statement is usually expressed as \u02c6ESS(ndof ) = ESS(n(0)\n\n(0)\ndof .\n\ndof )ndof /n\n\n\u2013 14 \u2013\n\n\fFigure 4. Effective Sample Size \u02c6ESS as a function of the number of links on the defect, for three\nfixed NE-MCMC architectures.\n\n4 Accelerating NE-MCMC with Stochastic Normalizing Flows",
        "char_count": 1264,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 28,
        "paper_id": "2510.25704v1",
        "text": "Figure 4. Effective Sample Size \u02c6ESS as a function of the number of links on the defect, for three\nfixed NE-MCMC architectures.\n\n4 Accelerating NE-MCMC with Stochastic Normalizing Flows\n\nThe estimator of Eq. (2.4) is unbiased: however, since it relies on an exponential average, it\nhas to be handled with care. In particular, it can suffer from high variance (i.e., low \u02c6ESS)\nwhen the dissipated work is large: sampling p in such conditions would require extremely\nlarge statistics to be reliable, as the average on the evolutions strongly depends on a few\nrare events that populate the tail of the distribution. For examples of the distribution of\nW ) see Refs. [53, 54]; moreover, note that the number of samples (i.e.,\nthe weight exp(\nWd\u27e9f , see Ref. [158] for an in-depth\nevolutions) that are needed grows exponentially with\ndiscussion.\n\n\u2212\n\n\u27e8\n\nNaturally, this problem can be solved by increasing nstep: in the asymptotic limit, the\ntransformations become quasi-static, meaning that the system remains close to equilibrium.\nIn such conditions,\nW ) fluctuates mildly: the exponential average\nis under control. While effective, this simple strategy can be too expensive for practical\npurposes, especially when the number of degrees of freedom that one needs to vary in a\ntransformation becomes large.\n\nis small and exp(\n\nWd\u27e9\n\n\u2212\n\n\u27e8\n\nA more general strategy that can help mitigate the growth of Wd (and thus, the total\ncost of the algorithm) is to enhance NE-MCMC with a class of deep generative models\ncalled Normalizing Flows (NFs) [61]. The idea behind a generic NF g\u03c1 is very simple: it\nis a diffeomorphism dependent on a set of parameters\nthat acts on a configuration U0\nsampled from a distribution q0 and transforms it in a different configuration U = g\u03c1(U0)\nwhich follows a variational density q. Simply using the change of variables theorem we can\nwrite it as\n\n\u03c1\n\n}\n\n{\n\nq(U ) = q0(g\u2212\n\n\u03c1 (U ))\n\n1\n\ndet Jg\u03c1|\n\n\u2212\n\n|\n\n1,\n\n(4.1)\n\n\u2013 15 \u2013",
        "char_count": 1937,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 29,
        "paper_id": "2510.25704v1",
        "text": "\u03c1\n\n}\n\n{\n\nq(U ) = q0(g\u2212\n\n\u03c1 (U ))\n\n1\n\ndet Jg\u03c1|\n\n\u2212\n\n|\n\n1,\n\n(4.1)\n\n\u2013 15 \u2013\n\n050100150200(Ld/a)30.00.20.40.60.81.0\u02c6ESS\u03b2=6.0,L/a=16NE-MCMC,nstep=256NE-MCMC,nstep=512NE-MCMC,nstep=1024\fwith det Jg\u03c1 being the determinant of the Jacobian of the NF. The power of this approach\ndepends on the fact that a generic NF is built as a composition of l intermediate functions\ng\u03c1(n), the so-called coupling layers:\n\ng\u03c1(U0) = g\u03c1(l) \u00b7\n\ng\u03c1(l\n\n1) \u00b7 \u00b7 \u00b7\n\u2212\n. Each of them transforms the\neach of them depending on a subset of parameters\nfield configuration through appropriate masking patterns to guarantee invertibility and an\neasy calculation of the Jacobian:\ninside each g\u03c1(l), neural networks can be employed to\nincrease the expressivity of the full transformation.\n\n\u03c1(l)\n\n{\n\n}\n\ng\u03c1(1)(U0),\n\n(4.2)\n\nIn this work, we do not employ NF alone, but use a relatively straightforward im-\nplementation of their coupling layers to assist NE-MCMC protocols. More precisely, we\ninterleave NE-MCMC updates with NF layers to create a sequence, as follows:\n\nU0\n\ng\u03c1(1)\n\u2212\u2192\n\ng\u03c1(1)(U0)\n\nP\u03bb(1)\n\u2212\u2192\n\nU1\n\ng\u03c1(2)\n\u2212\u2192\n\ng\u03c1(2)(U1)\n\nP\u03bb(2)\n\u2212\u2192\n\n. . .\n\nP\u03bb(nstep)\n\u2212\u2192\n\nUnstep.\n\n(4.3)\n\nThis defines a particular instance of Stochastic Normalizing Flows (SNFs) [98], in which\nevery non-equilibrium update P\u03bb(n) is preceded by a deterministic transformation g\u03c1(n).",
        "char_count": 1304,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 30,
        "paper_id": "2510.25704v1",
        "text": "(4.3)\n\nThis defines a particular instance of Stochastic Normalizing Flows (SNFs) [98], in which\nevery non-equilibrium update P\u03bb(n) is preceded by a deterministic transformation g\u03c1(n).\n\nA given target distribution p can be sampled with the enhanced protocol of Eq. (4.3)\nusing the same framework of NE-MCMC. In particular, the estimator of Eq. (2.4) and the\nmetrics of Eq. (2.14) and Eq. (2.20) can be readily employed with one single modification:\nwe now have to use the variational work, which for the SNF of Eq. (4.3) can be computed\nas [98, 159]:\n\nW (\u03c1)(\n\nU\n\n) = S[U ]\n\nS0[U0]\n\nQ(\n\n)\n\nlog J(\n\n\u2212\n1\n\nnstep\n\u2212\n(cid:88)\n\n=\n\nS\u03bb(n+1)\n\n\u2212\n\n\u2212\nU\n(cid:2)g\u03c1(n+1)(Un)(cid:3)\n\n)\n\nU\n\n(4.4)\n\nS\u03bb(n) [Un]\n\n\u2212\n\n\u2212\n\nlog Jg\u03c1(n+1)[Un].\n\n(4.5)\n\nn=0\n\nThe additional term:\n\nlog J(\n\n) =\n\nU\n\n1\n\nnstep\n\u2212\n(cid:88)\n\nn=0\n\nlog Jg\u03c1(n+1)(Un),\n\n(4.6)\n\nrepresents the cumulative contribution from the logarithms of the Jacobian determinants,\naccounting for the change in density induced by the NF layers.\n\nNaturally, for the transformations g\u03c1(n) to be useful, the parameters\n\nhave to be\ntrained, i.e., tuned according to some minimization training procedure. In this framework\nwe optimize them by minimizing the Kullback-Leibler divergence of Eq. (2.14):\n\n\u03c1(n)\n\n{\n\n}\n\nmin\n{\n\n\u03c1\n\n}\n\n\u02dcDKL(q0Pf \u2225\n\np\n\nPr) = min\n\n{\n\n\u03c1\n\n}\u27e8\n\nW (\u03c1)\nd (\n\n)\n\n\u27e9f ,\n\nU\n\n(4.7)\n\nThe interpretation is straightforward: SNF parameters are tuned to bring a given protocol\nas close as possible to equilibrium.",
        "char_count": 1439,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 31,
        "paper_id": "2510.25704v1",
        "text": "}\u27e8\n\nW (\u03c1)\nd (\n\n)\n\n\u27e9f ,\n\nU\n\n(4.7)\n\nThe interpretation is straightforward: SNF parameters are tuned to bring a given protocol\nas close as possible to equilibrium.\n\nThe design of coupling layers is typically guided by encoding the relevant symmetries\nof the theory directly into the machine learning model. This approach is expected to\nimprove the efficiency of the model and accelerate training [160]. A common strategy\n\n\u2013 16 \u2013\n\n\ffor incorporating symmetries into flow-based samplers is to construct equivariant coupling\nlayers [73, 78, 161], ensuring that the transformation g commutes with the symmetry. In our\nimplementation, we use gauge-covariant coupling layers [162], where the diffeomorphisms\ng\u03c1(n) are essentially stout smearing transformations [147]; in this work we follow the same\nstraightforward implementation used in Ref. [54] for flows in \u03b2. The field transformation\nfor a given link is defined as:\n\nU \u2032\u00b5(x) = g\u03c1(n)(U\u00b5(x)) = exp\n\n(cid:16)\n\niQ(n)\n\n\u00b5 (x)\n\n(cid:17)\n\nU\u00b5(x),\n\nwith Q\u00b5 Hermitian and traceless:\n\nQ(n)\n\n\u00b5 (x) =\ni\n2N\n\n\u2212\n\ni\n2\n\n(cid:16)\n\nTr\n\n(cid:16)\n\n(\u2126(n)\n\n\u00b5 (x))\u2020\n\n\u2126(n)\n\n\u00b5 (x)\n\n(cid:17)\n\n+\n\n\u2212\n\n(\u2126(n)\n\n\u00b5 (x))\u2020\n\n\u2212\n\n\u2126(n)\n\n\u00b5 (x)\n\n(cid:17)\n\n,\n\nand where \u2126(n)\n\n\u00b5 (x) is a sum of untraced loops based on x. We have\n\n\u2126(n)\n\n\u00b5 (x) = C(n)\n\n\u00b5 (x)U \u2020\u00b5(x),\n\nthat is made by a weighted sum over staples:\n\nC(n)\n\n\u00b5 (x) =\n\n(cid:88)\n\n\u03bd\n\n=\u00b5\n\n\u03c1+\n\u00b5\u03bd(n, x)U\u03bd(x)U\u00b5(x + \u02c6\u03bd)U \u2020\u03bd (x + \u02c6\u00b5)\n\n+\u03c1\u2212\u00b5\u03bd(n, x)U \u2020\u03bd (x",
        "char_count": 1404,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 32,
        "paper_id": "2510.25704v1",
        "text": "\u03c1+\n\u00b5\u03bd(n, x)U\u03bd(x)U\u00b5(x + \u02c6\u03bd)U \u2020\u03bd (x + \u02c6\u00b5)\n\n+\u03c1\u2212\u00b5\u03bd(n, x)U \u2020\u03bd (x\n\n\u02c6\u03bd)U\u00b5(x\n\n\u02c6\u03bd)U\u03bd(x\n\n\u2212\n\n\u2212\n\n\u2212\n\n\u02c6\u03bd + \u02c6\u00b5).\n\n(4.8)\n\n(4.9)\n\n(4.10)\n\n(4.11)\n\n\u2212\n\nThe coefficients \u03c1\u00b1\u00b5\u03bd(n, x) represent the parameters tuned in the training procedure; here\nwe take the most general form, in which they depend also whether the staple is in the +\u02c6\u03bd\n\u02c6\u03bd direction. These layers can be generalized to work on larger loops, as described in\nor\nRef. [78]. To ensure invertibility, it is crucial to apply a proper masking procedure: in this\ncase, we apply an even-odd decomposition and then use the transformation of Eq. (4.8)\none direction at a time, so that each layer g\u03c1(n) contains 8 different transformations. In\nthis pattern, the links in C(n)\n\u00b5 (x) can be considered \u201cfrozen\u201d while the U\u00b5(x) in Eqs. (4.8)\nand (4.10) are the \u201cactive\u201d ones.\n\n4.1 Coupling layers for a defect\n\nDirectly encoding symmetries into the variational Ansatz of the coupling layer is not the\nonly strategy to enhance its effectiveness.\nIn the present context, the geometry of the\nproblem itself suggests a design for a deterministic transformation. Ref. [100] introduced,\nfor the first time, the concept of a defect coupling layer, defined as a standard coupling\nlayer restricted to a localized region of interest. Specifically, the layer acts only on a subset\nof the lattice degrees of freedom, here, the gauge fields, located near the defect, and is\nconditioned on a fixed set of degrees of freedom, also limited to a localized region of the\nlattice. As a result, the majority of the lattice remains untouched by the transformation:\nthese degrees of freedom are neither used as inputs nor altered by the coupling layer.\n\n\u2013 17 \u2013\n\n\u0338\n\fLd/a\nn\u03c1\n\n2\n144\n\n3\n432\n\n4\n960",
        "char_count": 1699,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 33,
        "paper_id": "2510.25704v1",
        "text": "\u2013 17 \u2013\n\n\u0338\n\fLd/a\nn\u03c1\n\n2\n144\n\n3\n432\n\n4\n960\n\n5\n1800\n\n6\n3024\n\nTable 1. Number n\u03c1 of stout smearing parameters appearing in a single defect coupling layer for\ndifferent defect sizes Ld/a.\n\nThis approach, which a priori selects the relevant region of the lattice where the trans-\nformation is applied, has been shown to be effective and to drastically reduce the cost of the\ntraining compared to the standard approach, where the whole lattice undergoes a trans-\nformation [100]. At first glance, however, this approach might appear to be problematic if\none would like the defect to have a global effect on the system, as most of the d.o.f. remain\nunaffected by the deterministic transformation; this is expected to be the case for defects\nrelated to boundary conditions. Nonetheless, it is important to emphasize that the defect\ncoupling layer constitutes only one component of the SNF; the other essential ingredient is\nthe Monte Carlo update, which, in the present framework, always acts globally on the full\nlattice. This has the effect of spreading the information on the modified defect far from the\nregion where the coupling layer is acting, while the coupling layers accelerate the removal\nof the effect of OBC in the proximity of the defect.\n\nIn practice, in this work we apply the stout smearing transformation defined in Eq. (4.8)\n\nuniquely in two cases:\n\n\u2022 on links U\u00b5(x) on the defect, i.e., \u02c6\u00b5 = \u02c60, x0 = L\n\na and 0\n\n\u2212\n\n\u2264\n\nx1, x2, x3 < Ld,\n\n\u2022 on links U\u00b5(x) which are not themselves on the defect, but for which the corresponding\n\nsum of staples appearing in Eq. (4.11) contains at least a link on the defect.",
        "char_count": 1615,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 34,
        "paper_id": "2510.25704v1",
        "text": "\u2022 on links U\u00b5(x) which are not themselves on the defect, but for which the corresponding\n\nsum of staples appearing in Eq. (4.11) contains at least a link on the defect.\n\nMore precisely, we set the parameters \u03c1\u00b1\u00b5\u03bd(n, x) to be non-vanishing only if the link being\ntransformed is on the defect, or the corresponding staple has one link on the defect. At\nthis stage we opt not to use any notion of symmetry from the cubic geometry of the OBC\ndefect and we leave all parameters independent. The number of parameters per coupling\nlayer (counting one layer as the composition of the eight masks) grows proportionally with\nthe defect volume (Ld/a)3 and we report it in Table 1.\n\nWe remark here that in the following, an SNF architecture with nstep steps indicates\na combination of one defect coupling layer (i.e., a stout smearing transformation of the\nrelevant links) plus one full update of the whole lattice with the standard 1HB+4OR\nupdates (following the \u03bb(n) protocol), repeated nstep times. This implementation of SNFs\nis available as a CPU code [154] uniquely for sampling and as a PyTorch code [163] (also\nfor GPUs) for both training and sampling.\n\nWe performed several training procedures minimizing the KL divergence, i.e., the\ngeneralized dissipated work, as in Eq. (4.7). We chose again a L/a = 16 hypercubic lattice\n[2, 6]. We trained SNFs with nstep = 8\nat \u03b2 = 6.0 with a defect size in the range Ld/a\nand nstep = 16, but performing the backpropagation separately for each layer: in practice,\nwe minimize the terms in the sum in Eq. (4.7) one by one; see Ref. [54] for a discussion\nof this procedure and its connection to the work of Ref. [104]. We performed the training\n\n\u2208\n\n\u2013 18 \u2013",
        "char_count": 1689,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 35,
        "paper_id": "2510.25704v1",
        "text": "\u2208\n\n\u2013 18 \u2013\n\n\ffor about 1000\u20132000 epochs using the Adam optimizer [164], after which the loss for all\nvalues of nstep and Ld/a reaches a plateau. One of the advantages of this procedure is that\nthe memory consumption during training is independent of nstep; still, the minimization\nprocedure becomes not just more expensive with nstep, but also more difficult. To overcome\nthis issue, we generalize the methodology employed in Ref. [54] to the case of flows in the\nboundary conditions. In particular:\n\n\u2022 from the results of the trainings for a fixed value of nstep we identify 9 classes of\nparameters (just 5 in the Ld/a = 2 case) characterized by the geometry of the cubic\ndefect;\n\n\u2022 for each class, we take the average \u03c1(class)(n) of all the corresponding parameters and\n\nwe multiply it by nstep;\n\n\u2022 finally, we perform a spline interpolation of \u03c1(class)(n)\n\nnstep in n/nstep \u2208\n\n\u00d7\n\n[0, 1].\n\nThe spline function is the true result of the training:\nindeed, we use it to extract the\ncorresponding value of \u03c1(n) for any value of nstep. In a sense, it can be considered as a\npeculiar case of transfer learning: we train uniquely in a simple setting (i.e., an SNF with\nfew layers), recognize a particular pattern in the weights, and extrapolate the result for\nany nstep. We provide more details of the interpolation procedure in Appendix A.",
        "char_count": 1334,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 36,
        "paper_id": "2510.25704v1",
        "text": "Finally, we can use the SNFs trained in this way and compare their performances with\nthose of standard NE-MCMC when sampling the target density with PBC. We report in\nFigs. 5 and 6 the comparison between SNFs and NE-MCMC in terms of dissipated work\nand \u02c6ESS for the same combinations of defect sizes and nstep previously analyzed in Sec. 2.\nThe results show a very clear advantage in using SNFs over standard NE-MCMC: the\nKL divergence drops faster towards zero, indicating more reversible evolutions, while the\n\u02c6ESS grows quicker with nstep, which implies a smaller variance for the estimator of Eq. (2.4).\nThe comparison is fair, as for fixed nstep the computational effort for each evolution is\nessentially the same for the two flow architectures. Indeed, the cost of performing the stout\nsmearing transformations around a three-dimensional object is negligible with respect to\na full 1HB+4OR update of the much larger four-dimensional lattice; furthermore, the cost\nof the training on nstep = 8, 16, which lasted around 103 epochs, is a fraction of the cost of\n104 for the\nthe evolutions we performed when sampling with SNFs (which is about nev \u2243\nnstep = 16 case)4.\n\nSuch results can be interpreted in essentially two ways.\n\nIn the first, we keep both\nthe computational cost (i.e., nstep) and the size of the problem (removing the effect of a\n(Ld/a)3 OBC defect) fixed: doing so, SNFs provide an overall better estimator in any case\nas the \u02c6ESS is always markedly higher. In some cases, one can sample with SNFs where it\nwould be essentially impossible with NE-MCMC. The second way to interpret these results\nis to keep both the size of the problem (i.e., the value of (Ld/a)3) and the quality of the\n\n4The cost of training can become more significant if more complex coupling layers are employed; in that\ncase, an overall cost function that takes into account the computational effort to reach a given \u02c6ESS during\ntraining would be needed.\n\n\u2013 19 \u2013",
        "char_count": 1952,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 37,
        "paper_id": "2510.25704v1",
        "text": "\u2013 19 \u2013\n\n\fFigure 5. Results for the Kullback-Leibler divergence of Eq. (2.14) for different flows in the\nboundary conditions as a function of the number of steps in the flow divided by the volume of the\ndefect. Both NE-MCMC (circles) and SNFs with defect coupling layers (squares) are shown. All\nresults obtained on a 164 lattice at \u03b2 = 6.0.\n\nFigure 6. Results for the effective sample size of Eq. (2.20) for different flows in the boundary\nconditions as a function of the number of steps in the flow divided by the volume of the defect.\nBoth NE-MCMC (circles) and SNFs with defect coupling layers (squares) are shown. All results\nobtained on a 164 lattice at \u03b2 = 6.0.\n\n\u02c6ESS) fixed: one can then ask, what is the relative effort required to\nestimator (e.g., the\nreach the value of a certain metric. For SNFs, this appears to be consistently one third of\n\n\u2013 20 \u2013",
        "char_count": 860,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 38,
        "paper_id": "2510.25704v1",
        "text": "\u2013 20 \u2013\n\n050100150200nstep/(Ld/a)310\u2212210\u22121100hWdi\u03b2=6.0,L/a=16NE-MCMC,Ld/a=2NE-MCMC,Ld/a=3NE-MCMC,Ld/a=4NE-MCMC,Ld/a=5NE-MCMC,Ld/a=6SNF,Ld/a=2SNF,Ld/a=3SNF,Ld/a=4SNF,Ld/a=5SNF,Ld/a=6050100150200nstep/(Ld/a)30.00.20.40.60.81.0\u02c6ESS\u03b2=6.0,L/a=16NE-MCMC,Ld/a=2NE-MCMC,Ld/a=3NE-MCMC,Ld/a=4NE-MCMC,Ld/a=5NE-MCMC,Ld/a=6SNF,Ld/a=2SNF,Ld/a=3SNF,Ld/a=4SNF,Ld/a=5SNF,Ld/a=6\fthe effort required by NE-MCMC: indeed, the curve drawn by the SNF results in Figs. 5\nand 6 is the same as the purely stochastic one, but compressed horizontally by a factor 3.\nWe have established the superiority of a rather simple SNF architecture in removing\nthe effects of OBC and in sampling a target distribution with PBC in an unbiased and\nscalable fashion. However, each value of nstep defines a different estimator, with a different\n\u02c6ESS from Fig. 6: which is then\nvariance approximated by the corresponding value of the\nthe most efficient one? Equivalently, the question is whether it is better to \u201cspend\u201d less\n\u02c6ESS, or to\n(in terms of MCMC updates) and be content with a relatively small value of\nspend more for an estimator with a smaller variance.\n\nA cost function Cf to generate nev samples of an observable\n\nwith the flow-based\napproaches studied in this work can be written (neglecting autocorrelations in the data\nand training costs) as the number of evolutions times the cost of a single evolution:\n\nO\n\nCf (nev) = nev \u00d7\nVarf (\n=\nerr(",
        "char_count": 1410,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 39,
        "paper_id": "2510.25704v1",
        "text": "O\n\nCf (nev) = nev \u00d7\nVarf (\n=\nerr(\n\ncostev = nev \u00d7\n)\nO\n)2 \u00d7\n\n(nstep + nbetween)\nC(eff)\nf\nerr(\n\n(\n\n(nstep + nbetween) =\n\nO\n\n)\nO\n)2 ,\n\nO\n\nhere we identified the effective cost of sampling an observable\n\nwith\n\nO\n\nWe can rewrite it as\n\nC(eff)\nf\n\n(\n\n)\n\nO\n\n\u2261\n\nVarf (\n\n)\n\nO\n\n\u00d7\n\n(nstep + nbetween).\n\nC(eff)\nf\n\n(\n\n)\n\nO\n\n\u2243\n\nVar(\n\n)\n\nnstep + nbetween\n\u02c6ESS\n\n,\n\nO\n\n(4.12)\n\n(4.13)\n\n(4.14)\n\n(4.15)\n\nwhere we used the definition of \u02c6ESS from Eq. (2.20). The efficiency of the flow then depends\non the variance of\n(a theoretical value that is fixed for a given target distribution p) and\nthe ratio (nstep +nbetween)/ \u02c6ESS5, which we show in Fig. 7 as a function of the corresponding\n\u02c6ESS.\n\nO\n\nFirst of all, SNFs are consistently more efficient than the corresponding NE-MCMC\ncounterpart, and larger defects are also less efficient as we are ignoring autocorrelations for\n\u02c6ESS are unsurprisingly\nthe moment. Furthermore, flows characterized by small values of\nvery expensive and should be avoided; more interestingly, the largest values of \u02c6ESS do not\nappear to be particularly efficient either. Indeed, there seems to be a typical value of \u02c6ESS\n(or, equivalently, of nstep/(Ld/a)3) above which it is not worth to increase the quality of\nthe flow, as it becomes too costly.\n\nIt is interesting to analyze the most efficient value of nstep/(Ld/a)3 (or equivalently,\nthe \u201cbest\u201d value of the \u02c6ESS) using the parametrization of Eq. (3.7): in practice, completely\nneglecting nbetween, we wish to minimize\n\nnstep\n\u02c6ESS\n\n(cid:18)\n\n= nstep exp\n\nk\u2032\n\n(cid:19)\n\n,\n\nndof\nnstep",
        "char_count": 1543,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 40,
        "paper_id": "2510.25704v1",
        "text": "nstep\n\u02c6ESS\n\n(cid:18)\n\n= nstep exp\n\nk\u2032\n\n(cid:19)\n\n,\n\nndof\nnstep\n\n(4.16)\n\n5At this stage, the role of nbetween is secondary: since we are effectively neglecting autocorrelations in\nour samples, increasing the spacing between evolutions has no direct influence in the efficiency of the flow.\n\n\u2013 21 \u2013\n\n\fFigure 7. Results for the efficiency factor (nstep + nbetween)/ \u02c6ESS from Eq. (4.15) for different flows\nin the boundary conditions as a function of the effective sample size. All results obtained on a 164\nlattice at \u03b2 = 6.0.\n\nusing the k\u2032 obtained from the fit reported in Fig. 3. The minimum of the quantity of\nEq. (4.16) with respect to nstep leads to an amusing result, i.e.,\n\n\u02c6ESSbest =\n\n1\ne\n\n= 0.368 . . . ,\n\n(4.17)\n\nwhich remarkably is architecture-independent (since k\u2032 drops out) and also in very good\nqualitative agreement with what we observe in Fig. 7. Hence, in the following we will aim\nat using flows with \u02c6ESS in the 0.2\u20130.5 range, which appears to be the region where the\nflows are most efficient.\n\n5 Sampling topology towards the continuum limit\n\nFrom the discussion of the previous sections, it is clear that, if we wish to remove the effects\nintroduced by the presence of open boundaries, we can do that with excellent control over\nthe efficiency of the calculation using flow-based approaches such as NE-MCMC and SNFs.\nThus, we are finally ready to move to finer lattice spacings and verify whether this family\nof methodologies can actually be applied to cases where topological slow modes severely\naffect standard Monte Carlo simulations.\n\nThe first step is to include autocorrelations between samples of a given observable in\nthe cost function of Eq. (4.15). This is implemented with the integrated autocorrelation\n), which naturally leads to a more appropriate definition of the cost effectiveness\ntime \u03c4int(\n\nO\n\n\u2013 22 \u2013",
        "char_count": 1842,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 41,
        "paper_id": "2510.25704v1",
        "text": "O\n\n\u2013 22 \u2013\n\n0.00.20.40.60.81.0\u02c6ESS0200040006000800010000(nstep+nbetween)/\u02c6ESS\u03b2=6.0,L/a=16NE-MCMC,Ld/a=3NE-MCMC,Ld/a=4SNF,Ld/a=3SNF,Ld/a=4\fas\n\nC(eff)\nf\n\n(\n\n)\n\nO\n\n\u2261\n\n\u2243\n\nVarf (\n\nVar(\n\n2\u03c4int(\n)\n\n)\n\u00d7\n2\u03c4int(\nO\n\u02c6ESS\n\nO\n)\n\nO\n\n)\n\n(nstep + nbetween)\n\n\u00d7\n\nO\n(nstep + nbetween).\n\n(5.1)\n\n(5.2)\n\nL are expected to scale with a\u2212\n\nThis quantity provides intuition for a possible strategy to efficiently compute topolog-\nical observables in the continuum limit: namely, we can fix the value of ESS and \u03c4int to\nsome desired value by tuning nstep and nbetween in a suitable way. On the one hand, auto-\ncorrelations for Q2\n2 in the presence of OBC [42, 44]: hence,\nby increasing nbetween in the same fashion we expect to keep the value of \u03c4int roughly fixed.\nOf course, we expect this to hold (at least approximately) only if the size of defect Ld is\nkept fixed in physical units as well. As a consequence of this, in the continuum limit Ld/a\ngrows and the \u02c6ESS is expected to decrease exponentially at fixed nstep. On the other hand,\nfrom the detailed discussion of Section 3, we have very good control of the relationship\n\u02c6ESS, Ld/a and nstep. More specifically, by increasing nstep proportionally to\nbetween the\n(Ld/a)3, the ESS will be kept (in excellent approximation) fixed.\n\nThe strategy is then fully outlined: one has to scale nbetween with a\u2212\n\n2 to keep auto-\n3 to maintain the efficiency of the flow intact\n\ncorrelations roughly fixed and nstep with a\u2212\nin the continuum limit. The effective costs will then grow like\n\nC(eff)\nf\n\n(Q2\nL)\n\n\u223c\n\nVar(Q2\n\nL)(k0a\u2212\n\n3 + k1a\u2212\n\n2),",
        "char_count": 1558,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 42,
        "paper_id": "2510.25704v1",
        "text": "C(eff)\nf\n\n(Q2\nL)\n\n\u223c\n\nVar(Q2\n\nL)(k0a\u2212\n\n3 + k1a\u2212\n\n2),\n\n(5.3)\n\nwhere the coefficients k0 and k1 depend on the specific setup of the flow.\n\nBefore the discussion of numerical results at finer lattice spacings, let us take a closer\nlook at this expected scaling in the continuum, in particular at the coefficients we intro-\nduced. For instance, k1 will be smaller for defects that are larger in physical units; further-\nmore, the same coefficient still contains a residual dependence on nstep, as the decorrelation\ndoes not occur simply in the prior distribution, but also during the non-equilibrium evo-\nlution. Looking at k0 instead, it is clear that it depends heavily on the architecture itself:\nfor example, one can imagine a more efficient coupling layer that further reduces the cost\nin units of nstep to reach the same value of \u02c6ESS.\n\nIn the flows studied in this work, we always have nstep > nbetween, at least by a factor\n3 if not more.\nIt makes sense, then, to \u201cspace\u201d the evolutions by increasing nbetween,\nas the largest contribution to the sampling costs comes from the flow itself. However,\nwith the development of more advanced SNF architectures the corresponding coefficient\nk0 will become much smaller, and the a\u2212\nIn a sense,\nin this situation the whole simulation would look more similar to a standard one, with\nthe addition of a lightweight flow that safely removes all OBC effects; the role of nbetween\nwould also be less relevant, as the flow itself would be cheap to apply. We reckon this is\nthe explicit goal of future developments for SNF-based approaches.\n\n2 term will be the dominant one.\n\nWe now turn to some numerical tests we conducted at relatively fine lattice spacings\nin order to perform a variety of checks for this approach. First, we wish to verify that\nWd\u27e9f and\nthe autocorrelations are indeed under control; second, that the scaling of both\n\n\u27e8\n\n\u2013 23 \u2013\n\n\f\u03b2\n6.4\n6.5\n\nr0/a\n9.74\n11.09",
        "char_count": 1914,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 43,
        "paper_id": "2510.25704v1",
        "text": "\u27e8\n\n\u2013 23 \u2013\n\n\f\u03b2\n6.4\n6.5\n\nr0/a\n9.74\n11.09\n\na[fm] L/a L[fm] Ld/a\n3, 4\n0.051\n4, 5\n0.045\n\n1.54\n1.53\n\n30\n34\n\nLd[fm]\n0.16, 0.21\n0.18, 0.23\n\nTable 2. Setup of our simulations at finer lattice spacings, with the corresponding volume and\ndefect size in physical and lattice units. In order to set the scale we used Ref. [165].\n\n\u03b2\n\n6.4\n\n6.5\n\nflow\nNE-MCMC\nNE-MCMC\nNE-MCMC\nNE-MCMC\nNE-MCMC\nNE-MCMC\nSNF\nSNF\nNE-MCMC\nNE-MCMC\nNE-MCMC\nNE-MCMC\nNE-MCMC\n\nLd/a nstep nbetween\n\n3\n3\n3\n3\n4\n4\n3\n3\n4\n4\n4\n5\n5\n\n250\n250\n400\n600\n590\n950\n200\n600\n595\n950\n1425\n1153\n1860\n\n100\n200\n100\n100\n50\n50\n100\n100\n130\n130\n130\n65\n65\n\nnev\n1050\n1000\n1000\n1020\n800\n960\n1080\n1200\n1250\n830\n560\n670\n420\n\n\u02c6ESS\n0.16(4)\n0.13(2)\n0.29(4)\n0.44(3)\n0.18(5)\n0.33(3)\n0.41(2)\n0.74(1)\n0.04(3)\n0.40(4)\n0.52(2)\n0.19(2)\n0.45(2)",
        "char_count": 757,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 44,
        "paper_id": "2510.25704v1",
        "text": "\u03c4int(Q2\nL)\n1.01(14)\n0.50(3)\n0.68(7)\n0.61(7)\n0.54(6)\n0.50(3)\n1.5(3)\n0.71(9)\n0.73(8)\n0.70(9)\n0.66(9)\n0.5(1)\n0.5(1)\n\nTable 3. Details of the various flow architectures used in the simulations at the two finer lattice\nspacings and the corresponding values of \u02c6ESS and \u03c4int(Q2\n\nL).\n\n\u02c6ESS still holds at larger values of \u03b2; third, that the training strategy for SNFs explained\nin Section 4 is viable also in this regime.\n\nWe report in Table 2 the setup of our simulations and in Table 3 the details of the\nflows we used. We applied both the NE-MCMC and the SNF architectures described in\ndetail in Sections 2 and 4 respectively. Each flow is identified by three main parameters:\nLd, nstep and nbetween. As a final goal, we also aim to understand which combination of\nthese parameters is the most efficient to sample topological observables, at least to a good\napproximation.\n\nLooking at Table 3 we can immediately observe that the autocorrelations of Q2\n\nL are\ncompletely under control, with values of \u03c4int(Q2\nL) never significantly larger than 1; this is\nan unquestionable signal that the topological charge is sampled efficiently for these choices\nWd\u27e9f and \u02c6ESS are also reported in Fig. 8, where it\nof Ld, nstep and nbetween. Results for\nis possible to appreciate the same scaling of nstep with the number of degrees of freedom\nvaried in the evolution, i.e., (Ld/a)3, that we discussed in Section 3.\n\n\u27e8",
        "char_count": 1399,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 45,
        "paper_id": "2510.25704v1",
        "text": "\u27e8\n\nWe also implemented SNF architectures using the same procedure followed in Section 4\nIn particular, we trained the coupling layers only for architectures with\nfor \u03b2 = 6.0.\nnstep = 8, 16 on a small lattice with L/a = 16: the weights were then interpolated using\nthe procedure described in Appendix A and applied to the sampling for all architectures\non the lattice setups reported in Table 2. While there is no guarantee that this procedure\n\n\u2013 24 \u2013\n\n\fFigure 8. Results for the dissipated work of Eq. (2.14) (left panel) and the effective sample size of\nEq. (2.20) (right panel) for flows in the boundary conditions as a function of nstep divided by the\nsize of the defect. All results were obtained either at \u03b2 = 6.4 on a 304 lattice or at \u03b2 = 6.5 on a\n344 lattice; see Table 2 for more details.\n\nyields the most efficient flow, it appears to provide nonetheless a remarkable improvement\nover the standard NE-MCMC for \u03b2 = 6.4 and Ld/a = 3. In one case, we compared SNFs\nwith NE-MCMC fixing nstep, with the former significantly outperforming the latter both in\nWd\u27e9f and \u02c6ESS. Conversely, we also verified the same improvement factor observed\nterms of\nin Section 4: namely, the NE-MCMC metrics could be matched with those from SNFs using\nonly one third of Monte Carlo steps. Once more we stress that this improvement factor\nis obtained for negligible additional sampling costs and requires a cheap and relatively\nstraightforward training of the stout smearing weights.\n\n\u27e8",
        "char_count": 1471,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 46,
        "paper_id": "2510.25704v1",
        "text": "\u27e8\n\nFinally, we can look at results for the topological susceptibility: using both NE-MCMC\nand SNFs, we compute the expectation value of Q2\nL appearing in Eq. (2.25) with the\nestimator of Eq. (2.4) using the appropriate definition of work. In this way we obtain the\nresults shown in Fig. 9 both for \u03b2 = 6.4 and \u03b2 = 6.5, which immediately show perfect\nagreement with results quoted in Refs. [10, 166] obtained with much larger statistics. This\nserves as a sanity check that the methods described in this study do not introduce hidden\nsystematic effects.\n\n6 Conclusions\n\nIn this manuscript we have outlined a flow-based strategy to mitigate topological freezing\nin lattice gauge theories, based essentially on two ingredients. The first is the use of Open\nBoundary Conditions, a common tool in lattice calculations that is able to greatly reduce\nautocorrelations in topological observables. The second, novel ingredient is the use of\napproaches based on out-of-equilibrium simulations and Normalizing Flows to safely and\nefficiently remove the unwanted finite-size effects that OBC induce. The combination of\nthe two provides a tool whose scaling in the continuum limit can be well estimated, even if\n\n\u2013 25 \u2013\n\n05101520nstep/(Ld/a)3100hWdi05101520nstep/(Ld/a)30.00.20.40.60.81.0\u02c6ESSNE-MCMC,\u03b2=6.4,Ld/a=3SNF,\u03b2=6.4,Ld/a=3NE-MCMC,\u03b2=6.4,Ld/a=4NE-MCMC,\u03b2=6.5,Ld/a=4NE-MCMC,\u03b2=6.5,Ld/a=5\fFigure 9. Results for a4\u03c7L obtained with the NE-MCMC or SNF architectures described in the\n0.4L). Horizontal bands correspond to results obtained in\ntext, with ncool = 60 (Rs\nRefs. [10, 166].\n\n12.6a\n\n\u223c\n\n\u2243\n\njust approximately: extended testing in SU(3) pure gauge theory both at coarse and fine\nlattice spacings supports these findings.",
        "char_count": 1710,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 47,
        "paper_id": "2510.25704v1",
        "text": "12.6a\n\n\u223c\n\n\u2243\n\njust approximately: extended testing in SU(3) pure gauge theory both at coarse and fine\nlattice spacings supports these findings.\n\nWe devoted a substantial part of the manuscript to determining the computational cost\nof applying flows based on out-of-equilibrium evolutions. Indeed, at this stage, this cost\n3\nstill dominates the overall simulation budget towards the continuum, as it scales with a\u2212\n2 scaling of the\nfor fixed physical defect size; this is to be contrasted with the traditional a\u2212\nautocorrelations themselves. However, future developments might change the coefficient\n3 scaling in Eq. (5.3). In this work we already worked exactly in this direction,\nk0 of the a\u2212\nimplementing a relatively simple SNF architecture which proved to be a factor 3 more\nefficient than standard NE-MCMC. We envision that pursuing a systematic improvement\nprogram of SNFs, in particular when designing more efficient gauge-equivariant coupling\nlayers, will further decrease the k0 coefficient for a limited cost in terms of training. We\nplan to do so by building on recent work in this direction [78, 84]. Acting on the stochastic\nupdates themselves represents a promising direction as well: recent advancements in the\nliterature [157, 167\u2013175] (see Ref. [176] for a review) provide clear recipes to find an optimal\nprotocol given a starting and a target probability distribution, minimizing the dissipated\nwork\nWd\u27e9f given a fixed budget of MCMC steps. We plan to implement these techniques\nfor out-of-equilibrium evolutions in lattice gauge theory: furthermore, achieving this goal\nwould not simply improve efficiency, but enable full control over the fine details of the be-\nhavior of this family of flow-based approaches. Fully realizing these improvement programs\nwill simplify the budget of the computation costs: indeed, in the limit of very efficient flows,\nthe simulation would very much look like a standard one, with mild autocorrelations and\nthe effects of OBC swiftly removed.\n\n\u27e8",
        "char_count": 1997,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 48,
        "paper_id": "2510.25704v1",
        "text": "\u27e8\n\nA natural extension of this work is to probe finer and finer lattice spacings in SU(3)\nYang-Mills theory with correlations in topological observables completely under control.\nPart of the motivation is theoretical: are the cutoff effects of, e.g., the topological sus-\nceptibility under control? Lattice spacings below 0.04 fm have never been explored for\n\n\u2013 26 \u2013\n\n0.00.20.40.60.81.0\u02c6ESS01234567a4\u03c7L\u00d710\u22126\u03b2=6.4,(L/a)4=304JHEP01(2024)116NE-MCMC,\u03b2=6.4Ld/a=3SNF,\u03b2=6.4Ld/a=3NE-MCMC,\u03b2=6.4Ld/a=40.00.20.40.60.81.0\u02c6ESS012345a4\u03c7L\u00d710\u22126\u03b2=6.5,(L/a)4=3442510.08006NE-MCMC,\u03b2=6.5Ld/a=4NE-MCMC,\u03b2=6.5Ld/a=5\fsuch calculations, as the continuum limit usually relied on the use of measurements ob-\ntained on coarser lattices. Furthermore, it is extremely important to understand whether\nthe strategy outlined in this manuscript actually holds in conditions where the standard\nMCMC features extreme autocorrelations: a large scale simulation of this kind will provide\na challenging test.",
        "char_count": 969,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 49,
        "paper_id": "2510.25704v1",
        "text": "Interestingly, an extension of this approach to QCD with dynamical fermions presents\nno particular intrinsic conceptual challenges and would require minor changes to pre-\nexisting codes. The generalization of NE-MCMC is relatively straightforward, requiring the\nswitch to Hybrid Monte Carlo update algorithms instead of the heatbath+overrelaxation\ncombination used in pure gauge simulations. A minimal implementation would also follow\nthe work Ref. [52] in PTBC; open boundaries would be set only on the gauge fields, leaving\nstandard antiperiodic ones for fermion fields. Furthermore, most ingredients needed for\nthe design of suitable SNFs have been already studied. Any coupling layer developed for\ngauge fields in Yang-Mills theory can be directly ported to flow architectures for full QCD;\nsimilar transformations for fermionic variables have been also recently developed [80, 83].\nWe leave the study of an optimally performing architecture in the presence of dynamical\nquark fields to future work.\n\nFinally, we stress once more how the use of flow-based approaches such as NE-MCMC\nand SNFs can be extended to a broad variety of theoretical setups, well beyond the issue\nof topological freezing. Recent efforts (including this one) have been focused on systems in\nwhich a localized set of degrees of freedom is changed along the evolution, see for example\nRefs. [53, 94, 96, 100]; in such cases, generally speaking, the only probability distribution\nof interest is the target one. However, out-of-equilibrium evolutions (and their SNF gen-\neralizations) can be naturally applied to setups in which the action of the theory depends\non a set of parameters (e.g., quark masses), all of which can be suitably varied (without\nbreaking translational invariance). This is instead a multicanonical approach, in which\nmultiple intermediate probability distributions are sampled in the same simulation. A typ-\nical example is the computation of the equation of state, in which multiple temperatures\nare explored within the same evolution, see Ref. [92]. Thus, NE-MCMC and SNFs provide\na solid and well-understood framework for a completely different way to perform numerical\nsimulations in lattice gauge theories, which we intend to pursue in our future work.\n\nAcknowledgments",
        "char_count": 2271,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 50,
        "paper_id": "2510.25704v1",
        "text": "Acknowledgments\n\nWe thank M. Caselle, G. Kanwar and M. Panero for insightful and helpful discussions. The\nwork of C. B. is supported by the Spanish Research Agency (Agencia Estatal de Investi-\ngaci\u00b4on) through the grant IFT Centro de Excelencia Severo Ochoa CEX2020-001007-S\nand, partially, by grant PID2021-127526NB-I00, both funded by MCIN/AEI/10.13039/\n501100011033. E. C. and A. N. acknowledge support and A. B. acknowledges partial sup-\nport by the Simons Foundation grant 994300 (Simons Collaboration on Confinement and\nQCD Strings). A. N. acknowledges support from the European Union - Next Generation\nEU, Mission 4 Component 1, CUP D53D23002970006, under the Italian PRIN \u201cProgetti di\nRicerca di Rilevante Interesse Nazionale \u2013 Bando 2022\u201d prot. 2022ZTPK4E. A. B., E. C.,\n\n\u2013 27 \u2013\n\n\fA. N., D. P. and L. V. acknowledge support from the SFT Scientific Initiative of INFN.\nThe work of D. V. is supported by STFC under Consolidated Grant No. ST/X000680/1.\nWe acknowledge EuroHPC Joint Undertaking for awarding the project ID EHPC-DEV-\n2024D11-010 access to the LEONARDO Supercomputer hosted by the Consorzio Interuni-\nversitario per il Calcolo Automatico dell\u2019Italia Nord Orientale (CINECA), Italy. This work\nwas partially carried out using the computational facilities of the \u201dLovelace\u201d High Perfor-\nmance Computing Centre, University of Plymouth, https://www.plymouth.ac.uk/about-\nus/university-structure/faculties/science-engineering/hpc.\n\nAppendix\n\nA Interpolation strategy for defect coupling layer parameters",
        "char_count": 1517,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 51,
        "paper_id": "2510.25704v1",
        "text": "Appendix\n\nA Interpolation strategy for defect coupling layer parameters\n\nEvery SNF architecture used in this work, after fixing the value of the inverse coupling\n\u03b2 and the defect size Ld/a, should in principle be trained separately for each different\nvalue of nstep. However, the protocol we use is the same for all values of nstep, i.e., it\nis linear in the \u03bb(n) parameter appearing in Eq. (2.24). This means that the path in\nthe intermediate probability distributions is in good approximation the same, with each\narchitecture traversing it at different speeds. Thus, it is not unreasonable to think that\nthe parameters of the defect coupling layers discussed in Section 4 belonging to flows with\nvarying nstep are related to each other; furthermore, the coupling layers used in this work\nare rather simple, as we are training directly the stout smearing parameters appearing in\nEq. (4.11).\n\nIndeed, previous work in Ref. [54] showed that in the case of flow transformations in\nfrom the training procedure\n\n\u03b2, the stout smearing parameters obtained at a fixed n(train)\ncould be easily interpolated in the index of the coupling layer l, with l\n]. The\nsame interpolating function was then used to determine the parameters at much larger\nvalues nstep, working remarkably well even for much slower transformations.\n\n[1, n(train)\nstep\n\nstep\n\n\u2208\n\nIn this work, we perform the same operation, but with a fundamental difference: since\nthe translational and rotational symmetries of the lattice are lost due to the presence of the\ndefect, we train the non-vanishing values of the \u03c1\u00b1\u00b5\u03bd(n, x) parameters independently. Using\nseparate interpolations would then be impractical, as the number of parameters is already\nquite large for Ld/a = 2, see Table 1. However, the parameters are not really independent,\nas the links on and around the defect that are interested by the stout smearing transfor-\nmations still enjoy a residual cubic symmetry. While rigorously implementing the latter\nwould also leave us with a sizable number of parameters, we use a stronger prescription. In\nparticular we consider only 9 \u201cclasses\u201d of the \u03c1\u00b1\u00b5\u03bd(n, x) parameters, that we classify with\nthe criteria described in Table 4.",
        "char_count": 2191,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 52,
        "paper_id": "2510.25704v1",
        "text": "This classification was suggested by direct inspection of the relevant smearing param-\neters in each of these families; we stress that it does not need to be exact, but only be\nable to transfer the relevant pieces of information obtained from a training at small nstep\nto an architecture at larger nstep. After dividing all the \u03c1\u00b1\u00b5\u03bd(n, x) values in each of the 9\nsets of Table 4, we take the average of all the parameters belonging to that specific class\n\n\u2013 28 \u2013\n\n\fclass U\u00b5(x) on defect\nt1b\nt1e\nt1c\nt2\nt2b\nt2e\nt2c\nsp\nex\n\nYes\nYes\nYes\nYes\nYes\nYes\nYes\n\n= \u02c60)\nNo (\u00b5\nNo (\u00b5 = \u02c60)\n\nstaple contains defect link # other staples containing defect link\n\nNo\nNo\nNo\nYes\nYes\nYes\nYes\nYes\nYes\n\n5\n4\n3\n5\n4\n3\n2\n-\n-\n\nTable 4. Classification of the stout smearing parameters \u03c1\u00b1\n\u00b5\u03bd(n, x) for fixed n obtained from the\ntraining described in the main text. U\u00b5(x) is the link being transformed in Eq. (4.8), which can\nbe or not be on the defect, see Eq. (2.24). Each \u03c1\u00b1\n\u00b5\u03bd(n, x) multiplies a staple, see Eq. 4.11, which\nitself may contain a link on the defect or not. Finally, all the other staples connected to U\u00b5(x) may\ncontain a link on the defect as well. If no criteria is met, \u03c1\u00b1\n\n\u00b5\u03bd(n, x) is set to zero.\n\nFigure 10. Value of the averaged stout smearing parameters \u03c1\u00b1\nprescription of Table 4 and multiplied by nstep, along the various coupling layers with label n.\n\n\u00b5\u03bd(n, x) classified according to the",
        "char_count": 1383,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 53,
        "paper_id": "2510.25704v1",
        "text": "Figure 10. Value of the averaged stout smearing parameters \u03c1\u00b1\nprescription of Table 4 and multiplied by nstep, along the various coupling layers with label n.\n\n\u00b5\u03bd(n, x) classified according to the\n\n(denoted with \u03c1(class)(n)) and we multiply it by nstep. We show these values in Fig. 10 for\nthe case of \u03b2 = 6.0, Ld/a = 3 and nstep = 16. The behavior for different inverse couplings\nand defect sizes is qualitatively similar, with the exception of Ld/a = 2, where only 5\nclasses can be identified due to geometry constraints. The last step is a spline interpo-\nlation in n/nstep \u2208\nnstep: the resulting function (simply divided by\nnstep) provides an extrapolation of \u03c1(class)(n) for any number of steps in the evolution; the\ncorresponding interpolations are showed in Fig. 10.\n\n(0, 1] of \u03c1(class)(n)\n\n\u00d7\n\n\u2013 29 \u2013\n\n0.00.20.40.60.81.0n/nstep0.00.20.40.60.81.0\u03c1(class)(n)\u00d7nstep\u03b2=6.0,Ld/a=3,nstep=16t1bt1et1ct2t2bt2et2cspex\u0338\n\fReferences\n\n[1] B. Alles, G. Boyd, M. D\u2019Elia, A. Di Giacomo and E. Vicari, Hybrid Monte Carlo and\ntopological modes of full QCD, Phys. Lett. B 389 (1996) 107 [hep-lat/9607049].\n\n[2] L. Del Debbio, H. Panagopoulos and E. Vicari, \u03b8 dependence of SU (N ) gauge theories,\n\nJHEP 08 (2002) 044 [hep-th/0204125].\n\n[3] L. Del Debbio, G. M. Manca and E. Vicari, Critical slowing down of topological modes,\n\nPhys. Lett. B 594 (2004) 315 [hep-lat/0403001].",
        "char_count": 1362,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 54,
        "paper_id": "2510.25704v1",
        "text": "Phys. Lett. B 594 (2004) 315 [hep-lat/0403001].\n\n[4] ALPHA collaboration, S. Schaefer, R. Sommer and F. Virotta, Critical slowing down and\nerror analysis in lattice QCD simulations, Nucl. Phys. B 845 (2011) 93 [1009.5228].\n\n[5] M. L\u00a8uscher, Topology of Lattice Gauge Fields, Commun. Math. Phys. 85 (1982) 39.\n\n[6] S. D\u00a8urr, Z. Fodor, C. Hoelbling and T. Kurth, Precision study of the SU(3) topological\n\nsusceptibility in the continuum, JHEP 04 (2007) 055 [hep-lat/0612021].\n\n[7] M. C`e, C. Consonni, G. P. Engel and L. Giusti, Non-Gaussianities in the topological charge\ndistribution of the SU (3) Yang\u2013Mills theory, Phys. Rev. D 92 (2015) 074502 [1506.06052].\n\n[8] C. Bonati, M. D\u2019Elia and A. Scapellato, \u03b8 dependence in SU (3) Yang-Mills theory from\n\nanalytic continuation, Phys. Rev. D 93 (2016) 025028 [1512.01544].\n\n[9] A. Athenodorou and M. Teper, The glueball spectrum of SU(3) gauge theory in 3 + 1\n\ndimensions, JHEP 11 (2020) 172 [2007.06422].\n\n[10] C. Bonanno, The topological susceptibility slope \u03c7\u2032 of the pure-gauge SU(3) Yang-Mills\n\ntheory, JHEP 01 (2024) 116 [2311.06646].\n\n[11] S. D\u00a8urr and G. Fuwa, The topological susceptibility and excess kurtosis in SU(3) Yang-Mills\n\ntheory, 2501.08217.\n\n[12] L. Del Debbio, L. Giusti and C. Pica, Topological susceptibility in the SU (3) gauge theory,",
        "char_count": 1306,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 55,
        "paper_id": "2510.25704v1",
        "text": "theory, 2501.08217.\n\n[12] L. Del Debbio, L. Giusti and C. Pica, Topological susceptibility in the SU (3) gauge theory,\n\nPhys. Rev. Lett. 94 (2005) 032003 [hep-th/0407052].\n\n[13] M. L\u00a8uscher and F. Palombi, Universality of the topological susceptibility in the SU (3) gauge\n\ntheory, JHEP 09 (2010) 110 [1008.0732].\n\n[14] ETM collaboration, K. Cichy, E. Garcia-Ramos, K. Jansen, K. Ottnad and C. Urbach,\n\nNon-perturbative Test of the Witten-Veneziano Formula from Lattice QCD, JHEP 09\n(2015) 020 [1504.07954].\n\n[15] C. Bonanno, G. Clemente, M. D\u2019Elia and F. Sanfilippo, Topology via spectral projectors\n\nwith staggered fermions, JHEP 10 (2019) 187 [1908.11832].\n\n[16] M. C`e, M. Garcia Vera, L. Giusti and S. Schaefer, The topological susceptibility in the\n\nlarge-N limit of SU(N ) Yang-Mills theory, Phys. Lett. B 762 (2016) 232 [1607.05939].\n\n[17] C. Bonati, M. D\u2019Elia, P. Rossi and E. Vicari, \u03b8 dependence of 4D SU (N ) gauge theories in\n\nthe large-N limit, Phys. Rev. D 94 (2016) 085017 [1607.06360].\n\n[18] C. Bonanno, C. Bonati and M. D\u2019Elia, Large-N SU (N ) Yang-Mills theories with milder\n\ntopological freezing, JHEP 03 (2021) 111 [2012.14000].\n\n[19] A. Athenodorou and M. Teper, SU(N) gauge theories in 3+1 dimensions: glueball spectrum,\n\nstring tensions and topology, JHEP 12 (2021) 082 [2106.00364].",
        "char_count": 1307,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 56,
        "paper_id": "2510.25704v1",
        "text": "string tensions and topology, JHEP 12 (2021) 082 [2106.00364].\n\n[20] R. Brower, S. Chandrasekharan, J. W. Negele and U. J. Wiese, QCD at fixed topology,\n\nPhys. Lett. B 560 (2003) 64 [hep-lat/0302005].\n\n\u2013 30 \u2013\n\n\f[21] S. Aoki, H. Fukaya, S. Hashimoto and T. Onogi, Finite volume QCD at fixed topological\n\ncharge, Phys. Rev. D 76 (2007) 054508 [0707.0396].\n\n[22] P. Fritzsch, A. Ramos and F. Stollenwerk, Critical slowing down and the gradient flow\ncoupling in the Schr\u00a8odinger functional, PoS Lattice2013 (2014) 461 [1311.7304].\n\n[23] A. Ramos Martinez and G. Catumba, Testing universality of gauge theories, PoS\n\nLATTICE2022 (2023) 383.\n\n[24] W. Bietenholz, P. de Forcrand and U. Gerber, Topological Susceptibility from Slabs, JHEP\n\n12 (2015) 070 [1509.06433].\n\n[25] A. Laio, G. Martinelli and F. Sanfilippo, Metadynamics surfing on topology barriers: the\n\nCP N \u22121 case, JHEP 07 (2016) 089 [1508.07270].\n\n[26] M. L\u00a8uscher, Stochastic locality and master-field simulations of very large lattices, EPJ Web\n\nConf. 175 (2018) 01002 [1707.09758].\n\n[27] C. Bonati and M. D\u2019Elia, Topological critical slowing down: variations on a toy model,\n\nPhys. Rev. E 98 (2018) 013308 [1709.10034].\n\n[28] L. Giusti and M. L\u00a8uscher, Topological susceptibility at T > Tc from master-field simulations",
        "char_count": 1278,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 57,
        "paper_id": "2510.25704v1",
        "text": "[28] L. Giusti and M. L\u00a8uscher, Topological susceptibility at T > Tc from master-field simulations\n\nof the SU(3) gauge theory, Eur. Phys. J. C 79 (2019) 207 [1812.02062].\n\n[29] A. Florio, O. Kaczmarek and L. Mazur, Open-Boundary Conditions in the Deconfined\n\nPhase, Eur. Phys. J. C 79 (2019) 1039 [1903.02894].\n\n[30] L. Funcke, K. Jansen and S. K\u00a8uhn, Topological vacuum structure of the Schwinger model\n\nwith matrix product states, Phys. Rev. D 101 (2020) 054507 [1908.00551].\n\n[31] D. Albandea, P. Hern\u00b4andez, A. Ramos and F. Romero-L\u00b4opez, Topological sampling through\n\nwindings, Eur. Phys. J. C 81 (2021) 873 [2106.14234].\n\n[32] G. Cossu, D. Lancastera, B. Lucini, R. Pellegrini and A. Rago, Ergodic sampling of the\n\ntopological charge using the density of states, Eur. Phys. J. C 81 (2021) 375 [2102.03630].\n\n[33] S. Borsanyi and D. Sexty, Topological susceptibility of pure gauge theory using Density of\n\nStates, Phys. Lett. B 815 (2021) 136148 [2101.03383].\n\n[34] P. Fritzsch, J. Bulava, M. C`e, A. Francis, M. L\u00a8uscher and A. Rago, Master-field simulations\n\nof QCD, PoS LATTICE2021 (2022) 465 [2111.11544].\n\n[35] T. Eichhorn, G. Fuwa, C. Hoelbling and L. Varnhorst, Parallel tempered metadynamics:\nOvercoming potential barriers without surfing or tunneling, Phys. Rev. D 109 (2024)\n114504 [2307.04742].",
        "char_count": 1310,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 58,
        "paper_id": "2510.25704v1",
        "text": "[36] D. Howarth and A. J. Peterson, Topological charge unfreezing with AMReX, 2312.11599.\n\n[37] D. Albandea, G. Catumba and A. Ramos, Strong CP problem in the quantum rotor, Phys.\n\nRev. D 110 (2024) 094512 [2402.17518].\n\n[38] M. Abe, O. Morikawa and H. Suzuki, Monte Carlo Simulation of the SU(2)/Z2 Yang\u2013Mills\n\nTheory, 2501.00286.\n\n[39] J. Finkenrath, Review on Algorithms for dynamical fermions, PoS LATTICE2022 (2023)\n\n227 [2402.11704].\n\n[40] P. A. Boyle, Advances in algorithms for solvers and gauge generation, PoS\n\nLATTICE2023 (2024) 122 [2401.16620].\n\n[41] J. Finkenrath, Future trends in lattice QCD simulations, PoS EuroPLEx2023 (2024) 009.\n\n\u2013 31 \u2013\n\n\f[42] M. L\u00a8uscher and S. Schaefer, Lattice QCD without topology barriers, JHEP 07 (2011) 036\n\n[1105.4749].\n\n[43] M. Luscher and S. Schaefer, Lattice QCD with open boundary conditions and twisted-mass\n\nreweighting, Comput. Phys. Commun. 184 (2013) 519 [1206.2809].\n\n[44] G. McGlynn and R. D. Mawhinney, Diffusion of topological charge in lattice QCD\n\nsimulations, Phys. Rev. D 90 (2014) 074502 [1406.4551].\n\n[45] M. Hasenbusch, Fighting topological freezing in the two-dimensional CP N \u22121 model, Phys.\n\nRev. D 96 (2017) 054504 [1706.04443].\n\n[46] M. Berni, C. Bonanno and M. D\u2019Elia, Large-N expansion and \u03b8-dependence of 2d CP N \u22121",
        "char_count": 1289,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 59,
        "paper_id": "2510.25704v1",
        "text": "[46] M. Berni, C. Bonanno and M. D\u2019Elia, Large-N expansion and \u03b8-dependence of 2d CP N \u22121\n\nmodels beyond the leading order, Phys. Rev. D 100 (2019) 114509 [1911.03384].\n\n[47] C. Bonanno, Lattice determination of the topological susceptibility slope \u03c7\u2032 of 2d CPN \u22121\n\nmodels at large N , Phys. Rev. D 107 (2023) 014514 [2212.02330].\n\n[48] C. Bonanno, M. D\u2019Elia, B. Lucini and D. Vadacchino, Towards glueball masses of large-N\n\nSU(N) pure-gauge theories without topological freezing, Phys. Lett. B 833 (2022) 137281\n[2205.06190].\n\n[49] C. Bonanno, M. D\u2019Elia and L. Verzichelli, The \u03b8-dependence of the SU(N) critical\n\ntemperature at large N, JHEP 02 (2024) 156 [2312.12202].\n\n[50] C. Bonanno, J. L. Dasilva Gol\u00b4an, M. D\u2019Elia, M. Garc\u00b4\u0131a P\u00b4erez and A. Giorgieri, The SU(3)\ntwisted gradient flow strong coupling without topological freezing, Eur. Phys. J. C 84 (2024)\n916 [2403.13607].\n\n[51] C. Bonanno, C. Bonati, M. Papace and D. Vadacchino, The \u03b8-dependence of the Yang-Mills\n\nspectrum from analytic continuation, JHEP 05 (2024) 163 [2402.03096].\n\n[52] C. Bonanno, G. Clemente, M. D\u2019Elia, L. Maio and L. Parente, Full QCD with milder\n\ntopological freezing, JHEP 08 (2024) 236 [2404.14151].\n\n[53] C. Bonanno, A. Nada and D. Vadacchino, Mitigating topological freezing using",
        "char_count": 1270,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 60,
        "paper_id": "2510.25704v1",
        "text": "[53] C. Bonanno, A. Nada and D. Vadacchino, Mitigating topological freezing using\n\nout-of-equilibrium simulations, JHEP 04 (2024) 126 [2402.06561].\n\n[54] A. Bulgarelli, E. Cellini and A. Nada, Scaling of stochastic normalizing flows in SU(3)\n\nlattice gauge theory, Phys. Rev. D 111 (2025) 074517 [2412.00200].\n\n[55] M. Caselle, G. Costagliola, A. Nada, M. Panero and A. Toniato, Jarzynski\u2019s theorem for\n\nlattice gauge theory, Phys. Rev. D 94 (2016) 034503 [1604.05544].\n\n[56] M. Caselle, E. Cellini, A. Nada and M. Panero, Stochastic normalizing flows as\n\nnon-equilibrium transformations, JHEP 07 (2022) 015 [2201.08862].\n\n[57] M. S. Albergo, G. Kanwar and P. E. Shanahan, Flow-based generative models for Markov\nchain Monte Carlo in lattice field theory, Phys. Rev. D 100 (2019) 034515 [1904.12072].\n\n[58] K. Cranmer, G. Kanwar, S. Racani`ere, D. J. Rezende and P. E. Shanahan, Advances in\nmachine-learning-based sampling motivated by lattice quantum chromodynamics, Nature\nRev. Phys. 5 (2023) 526 [2309.01156].\n\n[59] M. L\u00a8uscher, Trivializing maps, the Wilson flow and the HMC algorithm, Commun. Math.\n\nPhys. 293 (2010) 899 [0907.5491].\n\n[60] G. P. Engel and S. Schaefer, Testing trivializing maps in the Hybrid Monte Carlo algorithm,\n\nComput. Phys. Commun. 182 (2011) 2107 [1102.1852].\n\n\u2013 32 \u2013\n\n\f[61] D. Rezende and S. Mohamed, Variational inference with normalizing flows, in International",
        "char_count": 1393,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 61,
        "paper_id": "2510.25704v1",
        "text": "\u2013 32 \u2013\n\n\f[61] D. Rezende and S. Mohamed, Variational inference with normalizing flows, in International\n\nconference on machine learning, pp. 1530\u20131538, PMLR, 2015.\n\n[62] G. Papamakarios, E. T. Nalisnick, D. J. Rezende, S. Mohamed and B. Lakshminarayanan,\n\nNormalizing Flows for Probabilistic Modeling and Inference, J. Mach. Learn. Res. 22 (2021)\n1.\n\n[63] K. A. Nicoli, C. J. Anders, L. Funcke, T. Hartung, K. Jansen, P. Kessel et al., Estimation\nof Thermodynamic Observables in Lattice Field Theories with Deep Generative Models,\nPhys. Rev. Lett. 126 (2021) 032001 [2007.07115].\n\n[64] K. A. Nicoli, S. Nakajima, N. Strodthoff, W. Samek, K.-R. M\u00a8uller and P. Kessel,\n\nAsymptotically unbiased estimation of physical observables with neural samplers, Phys. Rev.\nE 101 (2020) 023304 [1910.13496].\n\n[65] L. Del Debbio, J. M. Rossney and M. Wilson, Efficient modeling of trivializing maps for\n\nlattice \u03d54 theory using normalizing flows: A first look at scalability, Phys. Rev. D 104\n(2021) 094507 [2105.12481].\n\n[66] M. Gerdes, P. de Haan, C. Rainone, R. Bondesan and M. C. N. Cheng, Learning lattice\n\nquantum field theories with equivariant continuous flows, SciPost Phys. 15 (2023) 238\n[2207.00283].\n\n[67] A. Singha, D. Chakrabarti and V. Arora, Conditional normalizing flow for Markov chain\n\nMonte Carlo sampling in the critical region of lattice field theory, Phys. Rev. D 107 (2023)\n014512 [2207.00980].",
        "char_count": 1403,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 62,
        "paper_id": "2510.25704v1",
        "text": "Monte Carlo sampling in the critical region of lattice field theory, Phys. Rev. D 107 (2023)\n014512 [2207.00980].\n\n[68] S. Chen, O. Savchuk, S. Zheng, B. Chen, H. Stoecker, L. Wang et al., Fourier-flow model\n\ngenerating Feynman paths, Phys. Rev. D 107 (2023) 056001 [2211.03470].\n\n[69] M. Caselle, E. Cellini and A. Nada, Sampling the lattice Nambu-Goto string using\n\nContinuous Normalizing Flows, JHEP 02 (2024) 048 [2307.01107].\n\n[70] D. Albandea, L. Del Debbio, P. Hern\u00b4andez, R. Kenway, J. Marsh Rossney and A. Ramos,\n\nLearning trivializing flows, Eur. Phys. J. C 83 (2023) 676 [2302.08408].\n\n[71] J. Kreit, D. Schuh, K. A. Nicoli and L. Funcke, SESaMo: Symmetry-Enforcing Stochastic\n\nModulation for Normalizing Flows, 2505.19619.\n\n[72] D. Schuh, J. Kreit, E. Berkowitz, L. Funcke, T. Luu, K. A. Nicoli et al., Simulating\nCorrelated Electrons with Symmetry-Enforced Normalizing Flows, 2506.17015.\n\n[73] G. Kanwar, M. S. Albergo, D. Boyda, K. Cranmer, D. C. Hackett, S. Racani`ere et al.,\n\nEquivariant flow-based sampling for lattice gauge theory, Phys. Rev. Lett. 125 (2020)\n121601 [2003.06413].\n\n[74] D. Boyda, G. Kanwar, S. Racani`ere, D. J. Rezende, M. S. Albergo, K. Cranmer et al.,\nSampling using SU (N ) gauge equivariant flows, Phys. Rev. D 103 (2021) 074504\n[2008.05456].",
        "char_count": 1283,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 63,
        "paper_id": "2510.25704v1",
        "text": "[75] M. Favoni, A. Ipp, D. I. M\u00a8uller and D. Schuh, Lattice Gauge Equivariant Convolutional\n\nNeural Networks, Phys. Rev. Lett. 128 (2022) 032003 [2012.12901].\n\n[76] S. Bacchio, P. Kessel, S. Schaefer and L. Vaitl, Learning trivializing gradient flows for\n\nlattice gauge theories, Phys. Rev. D 107 (2023) L051504 [2212.08469].\n\n[77] A. Singha, D. Chakrabarti and V. Arora, Sampling U(1) gauge theory using a retrainable\n\nconditional flow-based model, Phys. Rev. D 108 (2023) 074518 [2306.00581].\n\n\u2013 33 \u2013\n\n\f[78] R. Abbott et al., Normalizing flows for lattice gauge theory in arbitrary space-time\n\ndimension, 2305.02402.\n\n[79] M. Gerdes, P. de Haan, R. Bondesan and M. C. N. Cheng, Continuous normalizing flows\n\nfor lattice gauge theories, 2410.13161.\n\n[80] M. S. Albergo, G. Kanwar, S. Racani`ere, D. J. Rezende, J. M. Urban, D. Boyda et al.,\n\nFlow-based sampling for fermionic lattice field theories, Phys. Rev. D 104 (2021) 114507\n[2106.05934].\n\n[81] J. Finkenrath, Tackling critical slowing down using global correction steps with equivariant\n\nflows: the case of the Schwinger model, 2201.02216.\n\n[82] M. S. Albergo, D. Boyda, K. Cranmer, D. C. Hackett, G. Kanwar, S. Racani`ere et al.,\n\nFlow-based sampling in the lattice Schwinger model at criticality, Phys. Rev. D 106 (2022)\n014514 [2202.11712].\n\n[83] R. Abbott et al., Gauge-equivariant flow models for sampling in lattice field theories with",
        "char_count": 1399,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 64,
        "paper_id": "2510.25704v1",
        "text": "[83] R. Abbott et al., Gauge-equivariant flow models for sampling in lattice field theories with\n\npseudofermions, Phys. Rev. D 106 (2022) 074506 [2207.08945].\n\n[84] R. Abbott, A. Botev, D. Boyda, D. C. Hackett, G. Kanwar, S. Racani`ere et al.,\n\nApplications of flow models to the generation of correlated lattice QCD ensembles, Phys.\nRev. D 109 (2024) 094514 [2401.10874].\n\n[85] R. Abbott et al., Aspects of scaling and scalability for flow-based sampling of lattice QCD,\n\nEur. Phys. J. A 59 (2023) 257 [2211.07541].\n\n[86] J. Komijani and M. K. Marinkovic, Generative models for scalar field theories: how to deal\n\nwith poor scaling?, PoS LATTICE2022 (2023) 019 [2301.01504].\n\n[87] C. Jarzynski, Nonequilibrium Equality for Free Energy Differences, Phys. Rev. Lett. 78\n\n(1997) 2690 [cond-mat/9610209].\n\n[88] C. Jarzynski, Equilibrium free-energy differences from nonequilibrium measurements: A\n\nmaster-equation approach, Phys. Rev. E56 (1997) 5018 [cond-mat/9707325].\n\n[89] C. Jarzynski, Equilibrium Free Energies from Nonequilibrium Processes, Acta Phys. Polon.\n\nB29 (1998) 1609 [cond-mat/9802155].\n\n[90] G. E. Crooks, Nonequilibrium Measurements of Free Energy Differences for Microscopically\n\nReversible Markovian Systems, Journal of Statistical Physics 90 (1998) 1481.\n\n[91] G. E. Crooks, Entropy production fluctuation theorem and the nonequilibrium work relation\n\nfor free energy differences, Phys. Rev. E 60 (1999) 2721.",
        "char_count": 1427,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 65,
        "paper_id": "2510.25704v1",
        "text": "[91] G. E. Crooks, Entropy production fluctuation theorem and the nonequilibrium work relation\n\nfor free energy differences, Phys. Rev. E 60 (1999) 2721.\n\n[92] M. Caselle, A. Nada and M. Panero, QCD thermodynamics from lattice calculations with\nnonequilibrium methods: The SU(3) equation of state, Phys. Rev. D 98 (2018) 054513\n[1801.03110].\n\n[93] O. Francesconi, M. Panero and D. Preti, Strong coupling from non-equilibrium Monte Carlo\n\nsimulations, JHEP 07 (2020) 233 [2003.13734].\n\n[94] A. Bulgarelli and M. Panero, Entanglement entropy from non-equilibrium Monte Carlo\n\nsimulations, JHEP 06 (2023) 030 [2304.03311].\n\n[95] A. Bulgarelli and M. Panero, Duality transformations and the entanglement entropy of\n\ngauge theories, JHEP 06 (2024) 041 [2404.01987].\n\n[96] A. Bulgarelli, M. Caselle, A. Nada and M. Panero, Casimir effect in critical O(N ) models\n\nfrom non-equilibrium Monte Carlo simulations, 2505.20403.\n\n\u2013 34 \u2013\n\n\f[97] D. Vadacchino, A. Nada and C. Bonanno, Topological susceptibility of SU(3) pure-gauge\n\ntheory from out-of-equilibrium simulations, PoS LATTICE2024 (2025) 415 [2411.00620].\n\n[98] H. Wu, J. K\u00a8ohler and F. Noe, Stochastic Normalizing Flows, in Advances in Neural\n\nInformation Processing Systems, vol. 33, pp. 5933\u20135944, 2020, 2002.06707.\n\n[99] M. Caselle, E. Cellini and A. Nada, Numerical determination of the width and shape of the\neffective string using Stochastic Normalizing Flows, JHEP 02 (2025) 090 [2409.15937].",
        "char_count": 1447,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 66,
        "paper_id": "2510.25704v1",
        "text": "[100] A. Bulgarelli, E. Cellini, K. Jansen, S. K\u00a8uhn, A. Nada, S. Nakajima et al., Flow-Based\n\nSampling for Entanglement Entropy and the Machine Learning of Defects, Phys. Rev. Lett.\n134 (2025) 151601 [2410.14466].\n\n[101] R. M. Neal, Annealed importance sampling, Statistics and Computing 11 (2001) 125\n\n[physics/9803008].\n\n[102] C. Dai, J. Heng, P. Jacob and N. Whiteley, An invitation to sequential monte carlo\n\nsamplers, Journal of the American Statistical Association 117 (2022) 1587 [2007.11936].\n\n[103] M. Arbel, A. Matthews and A. Doucet, Annealed flow transport monte carlo, in\n\nInternational Conference on Machine Learning, pp. 318\u2013330, PMLR, 2021, 2102.07501,\nhttps://proceedings.mlr.press/v139/arbel21a.html.\n\n[104] A. G. D. G. Matthews, M. Arbel, D. J. Rezende and A. Doucet, Continual Repeated\n\nAnnealed Flow Transport Monte Carlo, in International Conference on Machine Learning,\npp. 15196\u201315219, PMLR, 2022, 2201.13117,\nhttps://proceedings.mlr.press/v162/matthews22a.html.\n\n[105] M. S. Albergo and E. Vanden-Eijnden, NETS: A Non-Equilibrium Transport Sampler,\n\n2410.02711.\n\n[106] L. Wang, G. Aarts and K. Zhou, Diffusion models as stochastic quantization in lattice field\n\ntheory, JHEP 05 (2024) 060 [2309.17082].\n\n[107] Q. Zhu, G. Aarts, W. Wang, K. Zhou and L. Wang, Diffusion models for lattice gauge field\nsimulations, in 38th conference on Neural Information Processing Systems, 10, 2024,\n2410.19602.",
        "char_count": 1420,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 67,
        "paper_id": "2510.25704v1",
        "text": "[108] G. Aarts, D. E. Habibi, L. Wang and K. Zhou, On learning higher-order cumulants in\n\ndiffusion models, Mach. Learn. Sci. Tech. 6 (2025) 025004 [2410.21212].\n\n[109] Q. Zhu, G. Aarts, W. Wang, K. Zhou and L. Wang, Physics-Conditioned Diffusion Models\n\nfor Lattice Gauge Theory, 2502.05504.\n\n[110] G. Aarts, D. E. Habibi, L. Wang and K. Zhou, Combining complex Langevin dynamics with\n\nscore-based and energy-based diffusion models, 2510.01328.\n\n[111] M. Creutz, Overrelaxation and Monte Carlo Simulation, Phys. Rev. D 36 (1987) 515.\n\n[112] M. Creutz, Monte Carlo Study of Quantized SU (2) Gauge Theory, Phys. Rev. D 21 (1980)\n\n2308.\n\n[113] A. D. Kennedy and B. J. Pendleton, Improved Heat Bath Method for Monte Carlo\n\nCalculations in Lattice Gauge Theories, Phys. Lett. B 156 (1985) 393.\n\n[114] N. Cabibbo and E. Marinari, A New Method for Updating SU (N ) Matrices in Computer\n\nSimulations of Gauge Theories, Phys. Lett. B 119 (1982) 387.\n\n[115] M. Campostrini, A. Di Giacomo and H. Panagopoulos, The Topological Susceptibility on the\n\nLattice, Phys. Lett. B 212 (1988) 206.\n\n\u2013 35 \u2013\n\n\f[116] E. Vicari and H. Panagopoulos, \u03b8 dependence of SU (N ) gauge theories in the presence of a\n\ntopological term, Phys. Rept. 470 (2009) 93 [0803.1593].\n\n[117] P. Di Vecchia, K. Fabricius, G. C. Rossi and G. Veneziano, Preliminary Evidence for U (1)A\n\nBreaking in QCD from Lattice Calculations, Nucl. Phys. B 192 (1981) 392.",
        "char_count": 1414,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 68,
        "paper_id": "2510.25704v1",
        "text": "Breaking in QCD from Lattice Calculations, Nucl. Phys. B 192 (1981) 392.\n\n[118] P. Di Vecchia, K. Fabricius, G. Rossi and G. Veneziano, Numerical Checks of the Lattice\n\nDefinition Independence of Topological Charge Fluctuations, Phys. Lett. B 108 (1982) 323.\n\n[119] M. Campostrini, A. Di Giacomo, H. Panagopoulos and E. Vicari, Topological Charge,\n\nRenormalization and Cooling on the Lattice, Nucl. Phys. B 329 (1990) 683.\n\n[120] M. D\u2019Elia, Field theoretical approach to the study of theta dependence in Yang-Mills\n\ntheories on the lattice, Nucl. Phys. B 661 (2003) 139 [hep-lat/0302007].\n\n[121] B. Alles, M. D\u2019Elia and A. Di Giacomo, Topological susceptibility at zero and finite T in\n\nSU(3) Yang-Mills theory, Nucl. Phys. B 494 (1997) 281 [hep-lat/9605013].\n\n[122] B. Alles, M. D\u2019Elia and A. Di Giacomo, Topology at zero and finite T in SU (2) Yang\u2013Mills\n\ntheory, Phys. Lett. B 412 (1997) 119 [hep-lat/9706016].\n\n[123] B. Lucini, M. Teper and U. Wenger, Topology of SU (N ) gauge theories at T\n\nTc, Nucl. Phys. B 715 (2005) 461 [hep-lat/0401028].\n\nT\n\n\u2243\n\n0 and\n\n\u2243\n\n[124] L. Giusti, S. Petrarca and B. Taglienti, \u03b8 dependence of the vacuum energy in the SU (3)\n\ngauge theory from the lattice, Phys. Rev. D 76 (2007) 094510 [0705.2352].\n\n[125] H. Panagopoulos and E. Vicari, The 4D SU (3) gauge theory with an imaginary \u03b8 term,",
        "char_count": 1326,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 69,
        "paper_id": "2510.25704v1",
        "text": "[125] H. Panagopoulos and E. Vicari, The 4D SU (3) gauge theory with an imaginary \u03b8 term,\n\nJHEP 11 (2011) 119 [1109.6815].\n\n[126] C. Bonati, M. D\u2019Elia, H. Panagopoulos and E. Vicari, Change of \u03b8 Dependence in 4D\n\nSU (N ) Gauge Theories Across the Deconfinement Transition, Phys. Rev. Lett. 110 (2013)\n252003 [1301.7640].\n\n[127] E. Berkowitz, M. I. Buchoff and E. Rinaldi, Lattice QCD input for axion cosmology, Phys.\n\nRev. D 92 (2015) 034507 [1505.07455].\n\n[128] S. Borsanyi, M. Dierigl, Z. Fodor, S. Katz, S. Mages, D. Nogradi et al., Axion cosmology,\n\nlattice QCD and the dilute instanton gas, Phys. Lett. B 752 (2016) 175 [1508.06917].\n\n[129] C. Bonati, M. Cardinali and M. D\u2019Elia, \u03b8 dependence in trace deformed SU (3) Yang-Mills\n\ntheory: a lattice study, Phys. Rev. D 98 (2018) 054508 [1807.06558].\n\n[130] C. Bonati, M. Cardinali, M. D\u2019Elia and F. Mazziotti, \u03b8-dependence and center symmetry in\n\nYang-Mills theories, Phys. Rev. D 101 (2020) 034508 [1912.02662].\n\n[131] C. Bonati, M. D\u2019Elia, M. Mariti, G. Martinelli, M. Mesiti, F. Negro et al., Axion\n\nphenomenology and \u03b8-dependence from Nf = 2 + 1 lattice QCD, JHEP 03 (2016) 155\n[1512.06746].\n\n[132] P. Petreczky, H.-P. Schadler and S. Sharma, The topological susceptibility in finite",
        "char_count": 1241,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 70,
        "paper_id": "2510.25704v1",
        "text": "[132] P. Petreczky, H.-P. Schadler and S. Sharma, The topological susceptibility in finite\n\ntemperature QCD and axion cosmology, Phys. Lett. B 762 (2016) 498 [1606.03145].\n\n[133] J. Frison, R. Kitano, H. Matsufuru, S. Mori and N. Yamada, Topological susceptibility at\n\nhigh temperature on the lattice, JHEP 09 (2016) 021 [1606.07175].\n\n[134] S. Borsanyi et al., Calculation of the axion mass based on high-temperature lattice quantum\n\nchromodynamics, Nature 539 (2016) 69 [1606.07494].\n\n\u2013 36 \u2013\n\n\f[135] C. Bonati, M. D\u2019Elia, G. Martinelli, F. Negro, F. Sanfilippo and A. Todaro, Topology in full\n\nQCD at high temperature: a multicanonical approach, JHEP 11 (2018) 170 [1807.07954].\n\n[136] F. Burger, E.-M. Ilgenfritz, M. P. Lombardo and A. Trunin, Chiral observables and topology\nin hot QCD with two families of quarks, Phys. Rev. D 98 (2018) 094501 [1805.06001].\n\n[137] TWQCD collaboration, Y.-C. Chen, T.-W. Chiu and T.-H. Hsieh, Topological susceptibility\n\nin finite temperature QCD with physical (u/d,s,c) domain-wall quarks, Phys. Rev. D 106\n(2022) 074501 [2204.01556].\n\n[138] A. Athenodorou, C. Bonanno, C. Bonati, G. Clemente, F. D\u2019Angelo, M. D\u2019Elia et al.,\n\nTopological susceptibility of Nf = 2 + 1 QCD from staggered fermions spectral projectors at\nhigh temperatures, JHEP 10 (2022) 197 [2208.08921].",
        "char_count": 1308,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 71,
        "paper_id": "2510.25704v1",
        "text": "Topological susceptibility of Nf = 2 + 1 QCD from staggered fermions spectral projectors at\nhigh temperatures, JHEP 10 (2022) 197 [2208.08921].\n\n[139] P. Butti, M. Della Morte, B. J\u00a8ager, S. Martins and J. T. Tsang, Comparison of\n\nsmoothening flows for the topological charge in QCD-like theories, Phys. Rev. D 112 (2025)\n014504 [2504.10197].\n\n[140] B. Berg, Dislocations and Topological Background in the Lattice O(3) \u03c3 Model, Phys. Lett.\n\nB 104 (1981) 475.\n\n[141] Y. Iwasaki and T. Yoshie, Instantons and Topological Charge in Lattice Gauge Theory,\n\nPhys. Lett. B 131 (1983) 159.\n\n[142] S. Itoh, Y. Iwasaki and T. Yoshie, Stability of Instantons on the Lattice and the\n\nRenormalized Trajectory, Phys. Lett. B 147 (1984) 141.\n\n[143] M. Teper, Instantons in the Quantized SU (2) Vacuum: A Lattice Monte Carlo\n\nInvestigation, Phys. Lett. B 162 (1985) 357.\n\n[144] E.-M. Ilgenfritz, M. Laursen, G. Schierholz, M. M\u00a8uller-Preussker and H. Schiller, First\nEvidence for the Existence of Instantons in the Quantized SU (2) Lattice Vacuum, Nucl.\nPhys. B 268 (1986) 693.\n\n[145] B. Alles, L. Cosmai, M. D\u2019Elia and A. Papa, Topology in 2D CP N \u22121 models on the lattice:\nA Critical comparison of different cooling techniques, Phys. Rev. D 62 (2000) 094507\n[hep-lat/0001027].\n\n[146] APE collaboration, M. Albanese et al., Glueball Masses and String Tension in Lattice\n\nQCD, Phys. Lett. B 192 (1987) 163.",
        "char_count": 1390,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 72,
        "paper_id": "2510.25704v1",
        "text": "[146] APE collaboration, M. Albanese et al., Glueball Masses and String Tension in Lattice\n\nQCD, Phys. Lett. B 192 (1987) 163.\n\n[147] C. Morningstar and M. J. Peardon, Analytic smearing of SU(3) link variables in lattice\n\nQCD, Phys. Rev. D 69 (2004) 054501 [hep-lat/0311018].\n\n[148] R. Narayanan and H. Neuberger, Infinite N phase transitions in continuum Wilson loop\n\noperators, JHEP 03 (2006) 064 [hep-th/0601210].\n\n[149] M. L\u00a8uscher, Properties and uses of the Wilson flow in lattice QCD, JHEP 08 (2010) 071\n\n[1006.4518].\n\n[150] M. L\u00a8uscher and P. Weisz, Perturbative analysis of the gradient flow in non-abelian gauge\n\ntheories, JHEP 02 (2011) 051 [1101.0963].\n\n[151] R. Lohmayer and H. Neuberger, Continuous smearing of Wilson Loops, PoS\n\nLATTICE2011 (2011) 249 [1110.3522].\n\n[152] C. Bonati and M. D\u2019Elia, Comparison of the gradient flow with cooling in SU (3) pure gauge\n\ntheory, Phys. Rev. D D89 (2014) 105005 [1401.2441].\n\n\u2013 37 \u2013\n\n\f[153] C. Alexandrou, A. Athenodorou and K. Jansen, Topological charge using cooling and the\n\ngradient flow, Phys. Rev. D 92 (2015) 125014 [1509.04259].\n\n[154] C. Bonanno, A. Bulgarelli, E. Cellini, A. Nada, D. Panfalone, D. Vadacchino, L. Verzichelli,\n\nyang mills SNF Jarzynski. CPU code,\nhttps://github.com/alessandronada/yang mills SNF Jarzynski.\n\n[155] C. Bonanno, yang mills PTBC. CPU code,",
        "char_count": 1335,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 73,
        "paper_id": "2510.25704v1",
        "text": "yang mills SNF Jarzynski. CPU code,\nhttps://github.com/alessandronada/yang mills SNF Jarzynski.\n\n[155] C. Bonanno, yang mills PTBC. CPU code,\n\nhttps://github.com/Claudio-Bonanno-93/yang mills PTBC.\n\n[156] C. Bonati, yang mills. CPU code, https://claudio-bonati/yang-mills.\n\n[157] D. A. Sivak and G. E. Crooks, Thermodynamic metrics and optimal paths, Phys. Rev. Lett.\n\n108 (2012) [1201.4166].\n\n[158] C. Jarzynski, Rare events and the convergence of exponentially averaged work values, Phys.\n\nRev. E 73 (2006) [cond-mat/0603185].\n\n[159] S. Vaikuntanathan and C. Jarzynski, Escorted free energy simulations, The Journal of\n\nChemical Physics 134 (2011) [1101.2612].\n\n[160] T. Cohen and M. Welling, Group equivariant convolutional networks, in Proceedings of The\n33rd International Conference on Machine Learning (M. F. Balcan and K. Q. Weinberger,\neds.), vol. 48 of Proceedings of Machine Learning Research, (New York, New York, USA),\npp. 2990\u20132999, PMLR, 20\u201322 Jun, 2016, https://proceedings.mlr.press/v48/cohenc16.html.\n\n[161] J. K\u00a8ohler, L. Klein and F. No\u00b4e, Equivariant flows: sampling configurations for multi-body\n\nsystems with symmetric energies, 1910.00753.\n\n[162] Y. Nagai and A. Tomiya, Gauge covariant neural network for quarks and gluons, Phys. Rev.\n\nD 111 (2025) 074501 [2103.11965].\n\n[163] A. Bulgarelli, E. Cellini, A. Nada, snf su3. CPU/GPU PyTorch code,\n\nhttps://github.com/alessandronada/snf su3.",
        "char_count": 1412,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 74,
        "paper_id": "2510.25704v1",
        "text": "[163] A. Bulgarelli, E. Cellini, A. Nada, snf su3. CPU/GPU PyTorch code,\n\nhttps://github.com/alessandronada/snf su3.\n\n[164] D. P. Kingma and J. Ba, Adam: A Method for Stochastic Optimization, 1412.6980.\n\n[165] S. Necco and R. Sommer, The N(f ) = 0 heavy quark potential from short to intermediate\n\ndistances, Nucl. Phys. B 622 (2002) 328 [hep-lat/0108008].\n\n[166] C. Bonanno, The large-N limit of the topological susceptibility of SU(N ) Yang-Mills\n\ntheories via Parallel Tempering on Boundary Conditions, 2510.08006.\n\n[167] T. Schmiedl and U. Seifert, Optimal finite-time processes in stochastic thermodynamics,\n\nPhys. Rev. Lett. 98 (2007) [cond-mat/0701554].\n\n[168] A. Gomez-Marin, T. Schmiedl and U. Seifert, Optimal protocols for minimal work processes\n\nin underdamped stochastic thermodynamics, The Journal of Chemical Physics 129 (2008)\n[0803.0269].\n\n[169] P. R. Zulkowski, D. A. Sivak, G. E. Crooks and M. R. DeWeese, Geometry of\n\nthermodynamic control, Phys. Rev. E 86 (2012) [1208.4553].\n\n[170] G. M. Rotskoff and G. E. Crooks, Optimal control in nonequilibrium systems: Dynamic\nriemannian geometry of the ising model, Phys. Rev. E 92 (2015) [1510.06734].\n\n[171] S. Blaber and D. A. Sivak, Skewed thermodynamic geometry and optimal free energy\n\nestimation, The Journal of Chemical Physics 153 (2020) [2009.14354].\n\n\u2013 38 \u2013\n\n\f[172] S. Blaber, M. D. Louwerse and D. A. Sivak, Steps minimize dissipation in rapidly driven",
        "char_count": 1426,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      },
      {
        "chunk_id": 75,
        "paper_id": "2510.25704v1",
        "text": "\u2013 38 \u2013\n\n\f[172] S. Blaber, M. D. Louwerse and D. A. Sivak, Steps minimize dissipation in rapidly driven\n\nstochastic systems, Phys. Rev. E 104 (2021) [2105.04691].\n\n[173] M. V. S. Bonan\u00b8ca and S. Deffner, Minimal dissipation in processes far from equilibrium,\n\nPhys. Rev. E 98 (2018) [1803.07050].\n\n[174] L. P. Kamizaki, M. V. S. Bonan\u00b8ca and S. R. Muniz, Performance of optimal linear-response\nprocesses in driven brownian motion far from equilibrium, Phys. Rev. E 106 (2022)\n[2204.07145].\n\n[175] M. C. Engel, J. A. Smith and M. P. Brenner, Optimal control of nonequilibrium systems\nthrough automatic differentiation, Phys. Rev. X 13 (2023) 041032 [2201.00098].\n\n[176] S. Blaber and D. A. Sivak, Optimal control in stochastic thermodynamics, J. Phys.\n\nCommun. 7 (2023) 033001 [2212.00706].\n\n\u2013 39 \u2013",
        "char_count": 796,
        "metadata": {
          "paper_id": "2510.25704v1"
        }
      }
    ]
  },
  {
    "paper_id": "2510.25726v1",
    "original_length": 90281,
    "num_chunks": 56,
    "chunks": [
      {
        "chunk_id": 0,
        "paper_id": "2510.25726v1",
        "text": "5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n6\n2\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nTHE TOOL DECATHLON: BENCHMARKING LANGUAGE\nAGENTS FOR DIVERSE, REALISTIC, AND LONG-\nHORIZON TASK EXECUTION\n\nWenshuo Zhao1*\n\nJunlong Li1\u2217\nXiaochen Wang1 Rui Ge1 Yuxuan Cao1 Yuzhen Huang1 Wei Liu1\nZhaochen Su1 Yiyang Guo1\nXingyao Wang2 Xiang Yue3\n\nFan Zhou1 Lueyang Zhang1\nShuyan Zhou4 Graham Neubig2,3\n\nJian Zhao1* Weihao Zeng1* Haoze Wu1*\n\nJuan Michelini2\n\nJunxian He1\u2020\n\nJunteng Liu1\n\n1The Hong Kong University of Science and Technology\n2All Hands AI\nWebsite: toolathlon.xyz\n\n3Carnegie Mellon University\n\n4Duke University\n\n\u00a7 github.com/hkust-nlp/toolathlon\n\nABSTRACT",
        "char_count": 637,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 1,
        "paper_id": "2510.25726v1",
        "text": "1The Hong Kong University of Science and Technology\n2All Hands AI\nWebsite: toolathlon.xyz\n\n3Carnegie Mellon University\n\n4Duke University\n\n\u00a7 github.com/hkust-nlp/toolathlon\n\nABSTRACT\n\nReal-world language agents must handle complex, multi-step workflows across\ndiverse applications. For instance, an agent may manage emails by coordinating\nwith calendars and file systems, or monitor a production database like BigQuery\nto detect anomalies and generate reports following a standard operating manual.\nHowever, existing language agent benchmarks often focus on narrow domains\nor simplified tasks that lack the diversity, realism, and long-horizon complexity\nrequired to evaluate agents\u2019 real-world performance. To address this gap, we\nintroduce the Tool Decathlon (dubbed as TOOLATHLON), a benchmark for language\nagents offering diverse applications and tools, realistic environment setup, and\nreliable execution-based evaluation. TOOLATHLON spans 32 software applications\nand 604 tools, ranging from everyday platforms such as Google Calendar and\nNotion to professional applications like WooCommerce, Kubernetes, and BigQuery.\nMost of the tools are based on a high-quality set of Model Context Protocol (MCP)\nservers that we may have revised or implemented ourselves. Unlike prior works,\nwhich primarily ensure functional realism but offer limited environment state\ndiversity, we provide realistic initial environment states from real software, such as\nCanvas courses with dozens of students or real-world financial spreadsheets. The\nTOOLATHLON benchmark includes 108 manually sourced or crafted tasks in total,\nrequiring interacting with multiple applications over around 20 turns on average\nto complete. Each task is strictly verifiable through dedicated evaluation scripts.\nComprehensive evaluation of state-of-the-art models highlights their significant\nshortcomings in performing real-world, long-horizon tasks: the best-performing\nmodel, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool\ncalling turns on average, while the top open-weights model DeepSeek-V3.2-Exp\nreaches 20.1%. We expect TOOLATHLON to drive the development of more capable\nlanguage agents for real-world, long-horizon task execution.\n\n1\n\nINTRODUCTION",
        "char_count": 2241,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 2,
        "paper_id": "2510.25726v1",
        "text": "1\n\nINTRODUCTION\n\nTool-based language agents have already demonstrated their impact in real-world domains such as\nsoftware engineering (Jimenez et al., 2024; The Terminal-Bench Team, 2025), deep research (OpenAI,\n2024), and web browsing (Zhou et al., 2024). To further expand the reach of language agents across\ndiverse domains and applications, the Model Context Protocol (MCP) has been proposed to establish\na standard for connecting language agents to tens of thousands of applications (Anthropic, 2024).\n\nExisting benchmarks for language agents, however, are restricted to limited domains and tools (Mialon\net al., 2023; Liu et al., 2024; Ma et al., 2024; Jimenez et al., 2024; Zhou et al., 2024; Yao et al., 2025;\n\n\u2217 Equal Contribution. \u2020Corresponding author.\n\n1\n\n \n \n \n \n \n \n\fToolathlon\n\nFigure 1: Two examples and the initial environment states in TOOLATHLON. We showcase real-world environ-\nment interaction (\u00a72.2) and realistc state initialization (\u00a72.3) here.\n\nWei et al., 2025; Xu et al., 2025). By contrast, real-world tasks often require switching across various\napplications. For example, as demonstrated in Figure 1 (Example #2), a company\u2019s administrative\nagent may need to monitor a real Snowflake database for customer tickets, locate the appropriate PDF\noperation manual containing instructions on how to identify and handle overdue tickets, and then\nsend the required emails to managers and customers in accordance with the manual. Importantly,\nthis diversity gap extends far beyond differences in tool names or descriptions. The diversity and\ncomplexity of environment states across applications, compounded by interaction with them in long\ntrajectories, present substantial challenges for generalization.",
        "char_count": 1725,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 3,
        "paper_id": "2510.25726v1",
        "text": "To address these challenges, we introduce the Tool Decathlon (TOOLATHLON), a benchmark for\nevaluating language agents on diverse, realistic, and long-horizon tasks. TOOLATHLON spans 32\nreal-world applications and 604 tools across 108 tasks, covering a wide spectrum of domains ranging\nfrom daily affair and education to technology and finance. Tasks are grounded in realistic scenarios\nand mostly require coordinating multiple applications. Each task is fully verifiable with a dedicated,\ndeterministic evaluation script, comparing outcomes against either static or dynamically generated\nground-truth states (e.g., tasks involving the latest NVIDIA shareholder information or real-time train\nschedules). All tools in TOOLATHLON are sourced from the real world, with the majority obtained\nfrom MCP servers.\n\nTo faithfully capture the realism and complexity of practical environment states, we tried to adopt\nthe most representative applications such as Google Sheet, Gmail, and Snowflake. However, some\nremote environment states are difficult to set up to mimic real scenarios. For example, simulating a\nCanvas course with tens of students would require registering a real account for each student and\nresetting the states at each evaluation run. Therefore, while we adopt the commonly used applications\nmost of the time, we also incorporate several open-source software deployed locally via containers for\nconvenient and complex environment simulation, such as poste.io for email management to replace\nGmail and WooCommerce for online ecommerce platform to replace Shopify. These services provide\ncomplex observations while allowing us to set up the states in a scalable way. This stands in stark\ncontrast with simplified or artificial environment states as in prior benchmarks (Patil et al., 2025).\nIn addition, task prompts in TOOLATHLON are crafted to mirror authentic user queries, which are\noften concise and fuzzy. Models must therefore infer user intent and autonomously devise plans to\naccomplish tasks, an example is shown in Figure 3.\n\nConcurrent with this work, several MCP-based tool-use benchmarks have emerged (Liu et al., 2025;\nMo et al., 2025; Yan et al., 2025; Yin et al., 2025; The MCPMark Team, 2025), but they do not\nmatch TOOLATHLON in its reflection of real-world complexity. Some rely on LLM judges without\n\n2",
        "char_count": 2332,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 4,
        "paper_id": "2510.25726v1",
        "text": "2\n\nTask Prompt #1:  Your task is to check your email for homework2 submissions and grade on Canvas. Please download Python \ufb01les from email attachments to local workspace and execute each Python \ufb01le in terminal to check for errors. If the Python \ufb01le is correct, give it a score of 10 in Canvas; otherwise, give it a score of 0. You could check the requirements of homework2 in `assignments/homework2.md`and students' ID in `student_canvas_ids.csv`. For students who submitted multiple times, use the latest submission. FilesystemFilesystemPDFTerminalEmail (Poste.io)Canvas-LMSTask Prompt #2: Identify the tickets in the database that have exceeded the initial response time according to the relevant documentation, and send reminder emails, based on the templates mentioned in the manual, to the respective responsible managers, as well as apology emails to all involved users.SnowflakeEmail (Poste.io)\fToolathlon\n\nTable 1: Comparison of Tool-Based Language Agent Benchmarks. \u201c# Apps\u201d denotes the number of MCP servers,\nwhich we do not annotate for benchmarks without clear application definition. \u201cAvg # Turns\u201d denotes the number\nof tool calling turns made by Claude-4-Sonnet, which we use as a proxy for task complexity. \u201cReal States & Init\u201d\n(\u00a72.2, \u00a72.3) means the environment states and observations are from real-world software rather than artificial\ndatabases, and evaluation begins with a realistic state initialization. \u201cVerifiable Execution\u201d (\u00a72.4) denotes that\nmodels need to execute the tools and final results are evaluated based on states. \u201cFuzzy Prompt\u201d represents that\nthe task instructions are often fuzzy and ambiguous to mimic real user input (\u00a73.1). \u2217For MCPUniverse, only\n10% of the tasks involve multiple applications. In-depth discussion of these related works is in Appendix 6.\n\nBenchmark\n\n# Tasks\n\n# Apps\n\nAvg #\nTurns\n\nReal\nStates & Init\n\n\u03c4 -Bench\nBFCLv3-MT\nACEBench\nAppWorld\nMCPWorld\nMCP-RADAR\nMCPEval\nLiveMCPBench\nMCP-AgentBench\nLiveMCP-101\nMCPAtlas\nMCPUniverse\nMCPMark\nGAIA2\n\nTOOLATHLON",
        "char_count": 2011,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 5,
        "paper_id": "2510.25726v1",
        "text": "TOOLATHLON\n\n165\n800\n2000\n750\n201\n300\n676\n95\n600\n101\n1000\n231\n127\n800\n\n108\n\n2\n\u2013\n\u2013\n9\n10\n9\n19\n70\n33\n41\n40+\n11\n5\n12\n\n32\n\n\u2013\n3.8\n1.7\n\u2013\n\u2013\n\u2013\n\u2013\n5.6\n\u2013\n5.4\n3-6\n7.5\n18.5\n22.5\n\n26.8\n\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u2713\n\nVerifiable\nExecution\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\n\u2713\n\nCross-App\nTask\n\nFuzzy\nPrompt\n\n\u00d7\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nPartial\u2217\n\u00d7\n\u2713\n\n\u2713\n\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\n\u2713\n\nverifiable tasks (Mo et al., 2025; Yin et al., 2025), while others cover few domains or mostly single-\napplication tasks. For instance, MCPUniverse (Luo et al., 2025) spans only six domains, with 90%\nof tasks involving one app and synthetic initial states, yielding simplified, short interactions (<8\nturns). Similarly, MCPMark (The MCPMark Team, 2025) includes only five apps and overly detailed\nprompts (Figure 3). GAIA2 (Andrews et al., 2025) covers merely the mobile domain on mostly daily\ntasks with simplified synthetic environments. A full comparison is shown in Table 1.",
        "char_count": 933,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 6,
        "paper_id": "2510.25726v1",
        "text": "TOOLATHLON includes a lightweight framework for automated, safe, and scalable evaluation. Each\ntask comes with initial states setup if needed as well as an evaluation script (Figure 2). Executing\nand evaluating each task is isolated in separate containers to prevent interference. This enables fast\nparallel evaluation\u2014for example, running Claude-4.5-Sonnet on all 108 tasks takes only 70 minutes\nusing 10 parallel processes. With extensive experiments on TOOLATHLON, the best-performing\nmodels, Claude-4.5-Sonnet, achieve only 38.6% accuracy, highlighting the unique challenges posed\nby TOOLATHLON. DeepSeek-V3.2-Exp (DeepSeek-AI, 2025) achieves 20.1% success rate as the\nbest performer among open-source models. Further analysis reveals that weaknesses in long-context\nmodeling and robust tool calling error tracking are major challenges for all evaluated models. We have\nfully open-sourced the benchmark and the TOOLATHLON environment, aiming for TOOLATHLON to\naccelerate the development of practical language agents.\n\n2 THE TOOLATHLON ENVIRONMENT AND EVALUATION FRAMEWORK\n\n2.1 TASK DEFINITION\n\nEach task in TOOLATHLON can be formulated as a partially observable Markov decision process\n(POMDP) (S, A, O, T , R, U) with state space S, action space A, observation space O, transition\nfunction T : S \u00d7 A \u2192 S \u00d7 O, reward function R : S \u2192 [0, 1], and instruction space U. The\nenvironment states (\u00a72.2, \u00a72.3) can be the status in the current email inbox and the observations are\nthe sequential input to the model. The action space A is the available tools for the respective task\nand the tool implementation directly defines the transition function. The reward function R (\u00a72.4)\nrepresents our execution-based evaluation which directly evaluates the environment state. Intuitively,\nreal-world tools and environments will yield significantly more complex and diverse environment\nstates and observations than the synthetic ones, and in the following sections, we will detail our\ndesigns of these variables in TOOLATHLON.\n\n3\n\n\fToolathlon\n\n2.2 TOOLS, ENVIRONMENTS, AND FRAMEWORK",
        "char_count": 2072,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 7,
        "paper_id": "2510.25726v1",
        "text": "3\n\n\fToolathlon\n\n2.2 TOOLS, ENVIRONMENTS, AND FRAMEWORK\n\nMCP Servers:\nIn TOOLATHLON, we source our tools through a variety of MCP servers. Specifi-\ncally, we first decide a list of valuable and common real applications that we aim to benchmark on,\nthen we see if we can find the corresponding open-source MCP servers for them. If not, we implement\nthe MCP servers by ourselves. Notably, many open-source MCP server implementations contain\nbugs or exhibit certain limitations, for example, without the tools needed to complete our tasks. We\nfurther refine and improve these implementations ourselves. This way, we obtain a high-quality set\nof 32 MCP servers in total, where we include a complete list and their sources in Appendix A. The\napplications span diverse domains, extending well beyond common daily-use applications such as\nGoogle Maps, Notion, and Google Calendar, and we also incorporate a number of professional and\ndomain-specific applications to evaluate language agents in high-value productivity scenarios, such\nas Snowflake for enterprise data management and Kubernetes for cluster management. Although\nthe majority of tools are sourced from MCP servers, the benchmark usage itself is not tied to MCP\nemployment from the model developer side. For examples, these tasks can also been solved via pure\nGUI or CLI workflow, as long as certain account information like usernames, passwords, tokens or\ncredentials are explicitly given to the agents.",
        "char_count": 1458,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 8,
        "paper_id": "2510.25726v1",
        "text": "Remote and Locally Containerized Environments: While tools provide an interface for inter-\nacting with environments, they do not directly constitute the environments. Many real-world tools\ninteract directly with existing, remote environments, such as Google Sheets, Google Calendar, No-\ntion, and Gmail. Although remote environments require no implementation effort, they introduce\nsignificant challenges when benchmarking tasks that involve modifying environment states. For\ninstance, simulating a realistic Gmail inbox with hundreds of emails from diverse senders would\nrequire registering hundreds of Google accounts for every benchmark user, and this inbox would need\nto be reset prior to each evaluation run. Previous works have attempted to bypass this issue by only\nsupporting read operation to the states (Mialon et al., 2023), or implementing simplified synthetic\ndata structures to mimic environment states (Patil et al., 2025; Yao et al., 2025), but such approaches\ndrastically reduce realism and fail to reflect the complexity of real software environments. In contrast,\nin TOOLATHLON we leverage both remote environments and locally containerized, open-source\napplications. Specifically, we deploy the open-source Poste.io for email management, Canvas for\ncourse administration, Kubernetes for cluster orchestration, and WooCommerce for e-commerce\nmanagement. By hosting these realistic applications locally within containers, we can efficiently set\nup dozens of accounts and initialize complex environment states during evaluation. Compared with\nexisting dedicated agent sandboxes such as SWE-Bench (Jimenez et al., 2024), our environments are\nmore diverse and encompass a wider range of software.\n\nAgent Framework: We implement a simple agent framework based on the OpenAI Agents SDK\n(v0.0.15) 1 to conduct the agent action loop \u2013 at each turn, the model is expected to (optionally)\nreason explicitly and make tool calls. We make several enhancements to improve its basic setup for a\nmore robust workaround to evaluate language agents, including tool error handling, overlong tool\nresponse handling and context history management. We also equip this framework with some basic\nyet common local tools like python execution, web search, claim done and sleep. The details can be\nfound in Appendix B.\n\n2.3\n\nINITIAL STATE SETUP",
        "char_count": 2336,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 9,
        "paper_id": "2510.25726v1",
        "text": "2.3\n\nINITIAL STATE SETUP\n\nIn real world, tasks are rarely executed from an empty environment state (e.g., an empty inbox).\nInstead, agents are typically required to operate based on pre-existing environment states. In agentic\nscenarios, task difficulty is determined not only by the task instructions but also by the underlying\nenvironment states. For example, operating on a folder with only one file to be used is easier than\nworking with 10 mixed useful and unrelated files (Figure 1, example #2), even if the task descriptions\nare nearly identical. To capture this, for tasks in TOOLATHLON that starts with an initial state,2,\neach of these tasks is equipped with a state initialization script to set up the states at running time,\nor (and) an initial workspace directory containing pre-set files. Figure 1 and Figure 3 showcase\nsuch initial states. When constructing these initial environment states, we design them to closely\n\n1https://github.com/openai/openai-agents-python.\n2As shown in Table 2, 67% of the tasks fall into this category.\n\n4\n\n\fToolathlon\n\nFigure 2: Overview of the TOOLATHLON evaluation framework.\n\nreflect realistic scenarios. Notably, only very few previous benchmarks have incorporated realistic\ninitial state construction before entering the agent loop, as summarized in Table 1. By contrast, most\nexisting benchmarks start from empty state or overly simplified environment states, thus failing to\ncapture the full complexity of real-world task execution.\n\n2.4 RELIABLE EXECUTION-BASED EVALUATION",
        "char_count": 1524,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 10,
        "paper_id": "2510.25726v1",
        "text": "2.4 RELIABLE EXECUTION-BASED EVALUATION\n\nFirst, unlike some traditional tool-calling benchmarks that measure single-step tool call accuracy\ngiven a fixed context without actual execution (Patil et al., 2024), we think that execution-based\nevaluation is essential for reliably assessing language agents in realistic scenarios. Second, while\nmany existing benchmarks rely on LLMs as judges to score agent trajectories (Gao et al., 2025; Yin\net al., 2025), we contend that verifying the final environment states using deterministic rules offers a\nfar more reliable and reproducible evaluation framework, as demonstrated in several widely adopted\nagent benchmarks (Zhou et al., 2024; Xie et al., 2024; Jimenez et al., 2024). To achieve this, each task\nin TOOLATHLON is equipped with a unique, manually crafted evaluation script that ensures precise\nand consistent measurement of task success. The script may perform robust matching against a static\nsnapshot of the ground-truth environment or follow a reference execution workflow to dynamically\nretrieve and match real-time information (e.g., NVIDIA shareholders). During evaluation, each task\nis associated with a configuration file that specifies the MCP servers and tools available for use.\nIntuitively, providing the model with a larger set of unrelated tools increases task difficulty, as the\nagent must identify the relevant tools while ignoring distracting ones.\n\nSafe and Efficient Parallel Evaluation in Containers: Our TOOLATHLON evaluation framework\nsupports parallel execution to enable efficient model evaluation. Our framework launches each task\ninside a separate container in parallel, providing strict workspace isolation. On a standard Ubuntu\n24.04 Linux cluster with 16 CPUs and 64 GB of memory, we are able to evaluate Claude-4.5-Sonnet\non 108 tasks in just about 70 minutes of wall time using only 10 parallel processes. This demonstrates\nthat TOOLATHLON is both convenient and efficient for practical use by model developers to get\ninstant feedback on how their models perform in realistic scenarios and requirements.\n\n3 THE TOOLATHLON TASKS\n\n3.1 TASK SOURCING AND FUZZY TASK INSTRUCTION",
        "char_count": 2155,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 11,
        "paper_id": "2510.25726v1",
        "text": "3 THE TOOLATHLON TASKS\n\n3.1 TASK SOURCING AND FUZZY TASK INSTRUCTION\n\nThe authors of this work, who are researchers and senior undergraduate students in computer science,\nsource and implement the tasks. We carefully design and adhere to several principles when collecting\ntasks: (1) Real User Demands: All tasks are either directly sourced from real-world websites or\ncrafted to reflect genuine user demands. (2) Multi-App Orchestration: We intentionally source\ntasks that require interaction with multiple MCP servers, as this reflects authentic human workflows\nand increases task complexity. (3) Diversity: To ensure broad task diversity, we adopt a two-stage\nsourcing process. In the first stage, we start with an initial MCP server list covering more than 50\napplications and freely source tasks without restricting to specific servers. In the second stage, we\nanalyze the distribution of the sourced tasks and identify Apps that are important but underrepresented.\nWe then conduct an additional round of targeted task sourcing specifically for them.\n\n5\n\nLanguage AgentsTask InstructionRealistic State InitializationReal-World Software EnvironmentsMCP & Local ToolsTool CallTool OutputsExecuteState-based Evaluation\fToolathlon\n\nFigure 3: Example task instructions from our benchmark (Left) and MCPMark (The MCPMark Team, 2025)\n(Right). Ours contain more fuzzy intent that the model need to infer from the environment states.",
        "char_count": 1428,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 12,
        "paper_id": "2510.25726v1",
        "text": "Figure 3: Example task instructions from our benchmark (Left) and MCPMark (The MCPMark Team, 2025)\n(Right). Ours contain more fuzzy intent that the model need to infer from the environment states.\n\nRealistic Fuzzy Task Instruction: We de-\nsign task instructions to resemble authentic\nuser input, which is often fuzzy or ambigu-\nous, but whose actual intent can be deter-\nministically inferred from the environment\u2019s\nstates (e.g., existing data examples or docu-\nment templates). This requires the agent to\ninfer the user\u2019s intent from the environment\nstate, formulate plans, execute them, and in-\ntellectually handle unexpected events such as\ntool call errors. For example, as shown in Fig-\nure 3 Left, a real user may simply say \u201cPlease\nupdate the candidate information on HR\nRecord subpage according to all the\nresumes...... if the position applied\nfor by the applicant is currently not\nopen, ......\u201d This is a fuzzy instruction\nwithout specifying in which format the agent\nshould fill in the information, but the Notion\ndatabase has provided some examples that the\nagent needs to know to check itself. Also, the\ninstruction does not mention where to find the status of the posted job and the agent needs to check\nNotion to find that by itself. In contrast, task instructions in some existing benchmarks (Figure 3\nRight) explicitly include detailed step-by-step plans, which reduce the role agents for planning. More\nexamples of this kind are shown in Figure 10 and 11.\n\nFigure 4: Task topic distribution of TOOLATHLON.\n\nAll the sourced tasks experience multiple rounds of rigorous quality check, filtering and refinement\nwhich last for several weeks before we implement them into our benchmark, and finally we obtain\n108 tasks in total. The topic distribution of all tasks is shown in Figure 4 and Table 2 show some key\nstatistics of the complete benchmark.\n\n3.2 TASK IMPLEMENTATION\n\nAs described in \u00a72.4, each task in our bench-\nmark is fully implemented with a correspond-\ning evaluation script and potential initial states\nsetup. This process involves collecting ground-\ntruth states statically or dynamically, and design\nscripts to automatically clear and re-fill new ini-\ntial states. To ensure realistic setups and reli-\nable evaluation, implementing a single task in\n\n6",
        "char_count": 2279,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 13,
        "paper_id": "2510.25726v1",
        "text": "6\n\nTable 2: Key statistics of TOOLATHLON.\n\nStatistics\n\nValue\n\n# MCP servers (# tools)\n# Local toolkits (# tools)\nAvg/Min/Max tools per task\nTasks with state initialization\n\n32 (604)\n7 (16)\n69.9/28/128\n72/108 (67%)\n\n\u2026 23 more positions & other distractors blocksCreate a comprehensive weekend adventure planner that analyzes the Toronto Guide databases and generates a structured itinerary page. I need you to create a new page called 'Perfect Weekend Adventure' as a child of the main Toronto Guide page.Task Requirements:  1. Create a new page titled 'Perfect Weekend Adventure' as a child page of the main Toronto Guide page  2. Query the Activities database to identify all activities that have the \"Beaches\" tag  3. Query the Food database to find all restaurants with \"Turkish\" or \"Hakka\" tags  4. Query the Cafes database to retrieve all cafes entries  5. Structure the page with the following specific format:    - [SOME SPECIFIC REQUIREMENTS]  6. After the summary paragraph, add a divider block  7. Finally, add a callout block with the \ud83d\udca1 emoji containing the text: \"Pro tip: Check the Seasons database for the best time to enjoy outdoor activities!\"  8. Ensure all headings use the exact emoji and text format specified above  9. The lists must be in the exact format specified (bulleted for beaches, numbered for restaurants, to-do for cafes)Please update the candidate information on HR Record subpage of Notion according to all the resumes in my workspace. All information must be filled out strictly according to the content in the resumes, without making any unauthorized modifications or adding/removing any words. Also, please delete the existing sample entries in the record table. At the same time, if the position applied for by the applicant is currently not open for recruitment, please send an email to the corresponding applicant using the following template information (including line breaks).",
        "char_count": 1919,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 14,
        "paper_id": "2510.25726v1",
        "text": "At the same time, if the position applied for by the applicant is currently not open for recruitment, please send an email to the corresponding applicant using the following template information (including line breaks). Do not send the email by mistake: [A TEMPLATE HERE]Clear Step by Step Guides  Infer from  # Head CountInfer from  existing examplesFind needed info by agentInfer from   resumesLiterature ReviewManuscript EditingScholarly ActivitiesProfileManagementCourseSchool ApplicationTeachingAcademic AffairsQuantitativeStrategyStock TradingMarketingAnalyticsDeploymentDevOps SweepData CurationOSS EngagementProject SyncExperimentAnalysisData WranglingTalentManagementData AuditingFinancial AdminHealth GuidanceTravel HelperSocial MediaTransportationPersonal AffairSports AnalysisOnline PurchaseProductAssortmentInventory ControlDTC ManagementResearch(13.9%)Campus(16.7%)Finance(9.3%)Tech(17.6%)Business(16.7%)Daily(15.7%)E-commerce(10.1%)\fToolathlon\n\nTable 3: Main results for all the models. P@1, P@3, P\u02c63 and # Turns represents Pass@1, Pass@3, Pass\u02c63 and\naverage numbers of turns, respectively. We make bold the highest score.\n\nModel\n\nResearch Campus Finance Tech Business Daily E-com P@1 P@3 P\u02c63 # Turns\n\nClaude-4.5-Sonnet\nGPT-5\nClaude-4-Sonnet\nGPT-5-high\nGrok-4\nClaude-4.5-Haiku\nGrok-code-Fast-1\nGrok-4-Fast\no3\no4-mini\nGPT-5-mini\nGemini-2.5-Pro\nGemini-2.5-Flash\n\nDeepSeek-v3.2-Exp\nGLM-4.6\nQwen-3-Coder\nKimi-k2-0905\n\n31.1\n20.0\n33.3\n15.6\n24.4\n11.1\n20.0\n15.6\n15.6\n13.3\n11.1\n4.4\n4.4\n\n11.1\n22.2\n4.4\n8.9",
        "char_count": 1510,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 15,
        "paper_id": "2510.25726v1",
        "text": "11.1\n22.2\n4.4\n8.9\n\n42.6\n33.3\n33.3\n31.5\n22.2\n22.2\n14.8\n22.2\n14.8\n11.1\n16.7\n3.7\n3.7\n\n16.7\n18.5\n16.7\n22.2\n\nProprietary Models\n\n33.3\n13.3\n30.0\n23.3\n13.3\n26.7\n16.7\n16.7\n10.0\n20.0\n20.0\n3.3\n6.7\n\n42.1\n40.4\n26.3\n29.8\n43.9\n29.8\n19.3\n24.6\n22.8\n17.5\n15.8\n15.8\n3.5\n\n42.6\n38.9\n25.9\n44.4\n24.1\n27.8\n14.8\n14.8\n13.0\n11.1\n9.3\n5.6\n1.9\n\nOpen-Source Models\n\n23.3\n20.0\n10.0\n16.7\n\n19.3\n17.5\n19.3\n14.0\n\n14.8\n16.7\n14.8\n5.6\n\n35.3\n39.2\n33.3\n33.3\n27.5\n37.3\n21.6\n13.7\n29.4\n21.6\n21.6\n27.5\n3.9\n\n29.4\n11.8\n17.6\n9.8",
        "char_count": 481,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 16,
        "paper_id": "2510.25726v1",
        "text": "29.4\n11.8\n17.6\n9.8\n\n38.6\u00b12.7 51.9 20.4\n39.4\n12.1 30.6\u00b11.5 43.5 16.7\n27.3 29.9\u00b11.6 41.7 17.6\n15.2 29.0\u00b13.1 42.6 16.7\n30.3 27.5\u00b11.7 38.9 16.7\n27.3 26.2\u00b11.9 39.8 13.0\n24.2 18.5\u00b12.0 30.6 9.3\n21.2 18.5\u00b12.0 32.4 5.6\n17.0\u00b10.9 25.0 9.3\n6.1\n14.8\u00b10.8 26.9 3.7\n9.1\n14.5\u00b11.2 23.1 5.6\n6.1\n10.5\u00b11.9 21.3 2.8\n9.1\n3.7\u00b11.5 8.3\n0.0\n3.0\n\n30.3 20.1\u00b11.2 27.8 12.0\n30.3 18.8\u00b12.2 29.6 9.3\n15.2 14.5\u00b11.9 21.3 6.5\n15.2 13.0\u00b12.0 22.2 5.6\n\n20.2\n18.7\n27.3\n19.0\n20.3\n21.9\n20.2\n15.9\n19.4\n16.6\n19.7\n26.5\n8.3\n\n26.0\n27.9\n28.5\n26.6\n\nTOOLATHLON requires, on average, 4\u20136 hours of work by a research graduate student majoring in\ncomputer science.",
        "char_count": 610,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 17,
        "paper_id": "2510.25726v1",
        "text": "26.0\n27.9\n28.5\n26.6\n\nTOOLATHLON requires, on average, 4\u20136 hours of work by a research graduate student majoring in\ncomputer science.\n\nFinalizing Tasks and Quality Check: After crowd-sourcing task implementations from multiple\ncontributors, we perform intensive quality checks conducted by 5\u20136 experienced authors. In this\nstage, each task is carefully reviewed and revised to unify standards across all tasks and ensure\ncorrectness, solvability, and unambiguity, which requires approximately 5 hours of labor per task per\nround of checking. Once all tasks are finalized, we perform an additional round of comprehensive\ncross-checking and bug fixing of the entire benchmark before running the final experiments.\n\n4 EXPERIMENT\n\nIn this section, we present the configuration details and experimental settings for several leading\ncommercial and open models on TOOLATHLON, as well as their performance.\n\n4.1 SETUP",
        "char_count": 908,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 18,
        "paper_id": "2510.25726v1",
        "text": "4 EXPERIMENT\n\nIn this section, we present the configuration details and experimental settings for several leading\ncommercial and open models on TOOLATHLON, as well as their performance.\n\n4.1 SETUP\n\nModels and Configuration: Our evaluation includes the leading commercial model series in\nterms of agentic abilities, such as GPT-5(-mini) (OpenAI, 2025b), o3&o4-mini (OpenAI, 2025a),\nClaude-4-Sonnet (Anthropic, 2025a), Claude-4.5(-Sonnet,-Haiku) (Anthropic, 2025c;b), Gemini 2.5(-\nPro,-Flash) (Comanici et al., 2025), Grok-4(-Fast) (xAI, 2025a;b), Grok-Code-Fast-1 (xAI, 2025c).\nWe also benchmark the best-performing open-weight models including Qwen-3-Coder (Qwen Team,\n2025), DeepSeek-V3.2-Exp (DeepSeek-AI, 2025), Kimi-K2-0905 (Kimi Team et al., 2025) and\nGLM-4.6 (Zhipu AI, 2025). As described in \u00a72.4, each task is preconfigured with a list of MCP\nservers and common tools to access. During evaluation, we set the maximum allowable number of\nturns as 100 for all models. In our main evaluation setup, we only provide the models with the MCP\nservers and common tools that are useful for executing the task. We note that as each MCP server\nis equipped with multiple related tools, so the models will still see many unnecessary tools during\nevaluation.\n\nMetrics: We evaluate each model three times and report the average pass@1 success rate as well as\nthe standard deviation. We also include the pass@3 \u2013 the fraction of tasks with at least one correct\ntrajectory, and pass\u02c63 (Yao et al., 2025) \u2013 the fraction of tasks where all three trajectories are correct,\nto measure the model\u2019s potential capability coverage and its ability to complete tasks reliably. We\nalso report the average number of turns used.\n\n7\n\n\fToolathlon\n\nFigure 5: Two kinds of tool calling error presence ratios in calling tools for different models.\n\n4.2 MAIN RESULTS",
        "char_count": 1838,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 19,
        "paper_id": "2510.25726v1",
        "text": "7\n\n\fToolathlon\n\nFigure 5: Two kinds of tool calling error presence ratios in calling tools for different models.\n\n4.2 MAIN RESULTS\n\nResults in Table 3 show that Claude-4.5-Sonnet ranks first, but still achieves a success rate of less\nthan 40%. GPT-5, Claude-4-Sonnet, Grok-4, and Claude-4.5-Haiku, each with Pass@1 scores above\n26% but below 30%, clearly fall into the second tier. All other models remain at 20% or below.\nThis indicates that our benchmark remains challenging for state-of-the-art models and effectively\ndistinguishes their capabilities. For open-source models, scores are less than or equal to 20%, with the\nbest, DeepSeek-V3.2-Exp, achieving 20.1%, revealing a clear gap compared with proprietary models.\nInterestingly, increased reasoning effort for thinking-oriented models (e.g., GPT-5 vs. GPT-5-high)\nshows no benefit, suggesting that exploring new observations matters more than extended internal\nreasoning in agentic tasks. We also find that Gemini-2.5\u2019s ability to understand requirements and\nproactively explore is insufficient\u2014it may neglect certain requirements or give up prematurely during\nexecution, resulting in poor performance on complex tasks.\n\nLooking at performance across task categories, Claude-4.5-Sonnet excels in almost all domains,\nespecially in Campus and E-Commerce tasks, demonstrating strong general capabilities across diverse\ntools. GPT-5 performs exceptionally well in Daily tasks, showcasing its effectiveness in everyday\nscenarios, while Grok-4 stands out in Tech, indicating a specialized strength in technology-related\ndevelopment operations. We also observe significant differences between Pass@3 and Pass\u02c63 success\nrates. This indicates that while many models have certain capability coverage, they lack consistency\nin producing reliable results. For real-world tasks, building agents with both high success rates and\nrobust consistency remains a critical challenge.\n\n5 ANALYSIS\n\nIn this section, we conduct analysis in depth to better understand model performance in TOOLATHLON,\nfocusing on tool-call errors, as well as long-context and overlong-output challenges. More analysis,\nincluding how tool error and the involvement of unrelated MCP servers impact model performance,\nand qualitative analysis & case studies, can be found in Appendix C and E.",
        "char_count": 2308,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 20,
        "paper_id": "2510.25726v1",
        "text": "5.1 THE FAILURE OF CALLING TOOLS\n\nWe mainly focus on two major tool-calling er-\nrors: hallucinating non-existing tools (e.g., in-\ncorrect tool names) and errors raised during tool\nexecution. Statistics for different models on\nthese two types of errors are shown in Figure\n5. It can be seen that all models produce tool\nexecution errors to varying degrees, possibly\ndue to incorrect parameter passing or attempts\nto access non-existent resources. However, we\nfound no significant correlation between overall\nsuccess rate and the frequency of such errors. In\nfact, error messages from tools may help models\nunderstand the tool implementation or structure,\nallowing adjustments in subsequent turns. The\nother type of error\u2013incorrect tool names\u2013more\nlikely affects final scores. Leading models produce few tool name errors. In Appendix C, Figure\n\nFigure 6: Model performance on three groups of tasks\ndivided by average turns. The x-axis represents different\ntask difficulty groups determined by different avg turns\nrange [Min Turns, Max Turns]\n\n8\n\nGPT-5-highKimi-K2-0905GLM-4.6Claude-4.5-HaikuDeepSeek-V3.2-ExpGPT-5Grok-4Claude-4-SonnetGemini-2.5-ProGPT-5-miniGrok-Code-Fast-1Qwen-3-CoderGrok-4-FastClaude-4.5-Sonneto3Gemini-2.5-Flasho4-mini01020Error Presence (%)Wrong Tool NameTool Call ErrorPass@1010203040Pass@1 (%)Easy[4.2, 15.6]Medium[15.7, 23.8]Hard[24.1, 52.7]01020304050Success Rate (%)45%32%37%9Turns19Turns31Turns41%23%26%9Turns15Turns30Turns21%20%13%9Turns13Turns24Turns26%23%10%12Turns26Turns39Turns21%11%11%17Turns23Turns44TurnsClaude-4.5-SonnetGPT-5Grok-4-FastDeepSeek-V3.2-ExpQwen-3-Coder\fToolathlon",
        "char_count": 1611,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 21,
        "paper_id": "2510.25726v1",
        "text": "9, we further analyze the success rate difference between trajectories containing tool-calling errors\nversus error-free trajectories, showing that most models do suffer from tool call errors.\n\n5.2 THE LONG-CONTEXT CHALLENGES FOR LANGUAGE AGENTS\n\nSince our benchmark is built on real tools and environments,\nit naturally generates many long-horizon trajectories. To quan-\ntitatively describe the differences between tasks, we calculate\nthe average number of execution turns for each task across all\nmodels, and use this as a proxy to divide all tasks into three\nequally sized groups: Easy, Medium, and Hard, with execu-\ntion turns increasing with difficulty. In Figure 6, we show the\nperformance of five representative models on different groups,\nalong with their average turns in each group. The results indi-\ncate that groups with higher average turns generally have lower\nsuccess rates across models, and leading models like Claude-\n4.5-Sonnet maintain clear advantages in all groups. We also\nfind that there is no significant difficulty difference between the\nMedium and Hard groups, and that even Claude-4.5-Sonnet and\nGPT-5 achieve higher scores on the Hard subset then in Medium\nones. This suggests that our benchmark\u2019s difficulty does not\nentirely stem from standard multi-step long-horizon execution,\nbut possibly from models ending tasks prematurely without\nsufficiently exploring available observations, leading to failure.\n\nFigure 7: Avg. Success Rate on Trajec-\ntories w/wo overlong tool outputs.\n\nAnother concern is whether models can successfully complete tasks when encountering overlong\ntool outputs, like fetching lengthy HTML source code or directly listing all data from a database\n(we refer the readers to Appendix B for handling overlong outputs in our framework). We calculate\nthe proportion of trajectories containing overlong tool outputs encountered by all models during\nevaluation, as well as each model\u2019s success rates with and without overlong tool outputs. Results\nshow that the proportion of overlong tool outputs varies from approximately 15% to 35% across\ndifferent models. Additionally, Figure 7 shows that most models experience a decline in success rate\nwhen encountering overlong tool outputs, with only a few models maintaining nearly unchanged\nperformance. While tasks with overlong outputs are often logically straightforward (e.g., price\ncomparison, data extraction), most models get trapped trying to process these lengthy outputs.",
        "char_count": 2472,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 22,
        "paper_id": "2510.25726v1",
        "text": "5.3 THE RELATIONSHIP BETWEEN PERFORMANCE AND EXPENSES\n\nSince we evaluate the models in realistic settings, both cost and token usage are important factors, as\nthey determine how a model should be selected for different budget constraints. Therefore, during the\nevaluation period, we measure the actual number of output tokens and the associated cost with prompt\ncaching enabled. For costs, Figure 8 Left shows that Claude-4-Sonnet and Grok-4 incur relatively\nhigh expenses, whereas most other models remain under $1 per task. Claude-4.5-Sonnet achieves the\nhighest performance but ranks third in cost. Several models, such as Grok-4-Fast, Grok-Code-Fast-1,\nand DeepSeek-V3.2-Exp, incur only a small cost, suggesting that they can serve as strong alternatives\nunder limited budgets without an extreme pursuit of maximum performance.\n\nWe also plot the output token count distribution in Figure 8 Right, which illustrates how the success\nrate varies with different average output token counts. Most models cluster between 5K and 10K\noutput tokens. Some reasoning-focused models, such as o4-mini and GPT-5(-high), generate more\ntokens, whereas the Claude series and Grok-4 achieve strong results with fewer tokens, suggesting\nthey rely more on environment observation rather than extensive internal reasoning. Models like\nGemini-2.5-Flash have the lowest output token counts and correspondingly lower accuracy, while\nothers exhibit a similarly concentrated distribution.\n\n6 RELATED WORK\n\nBenchmarks for tool-based language agents differ substantially in the realism of their tools, environ-\nments, and task configurations, and can be viewed along a spectrum from fully simulated settings\n\n9\n\n010203040Success Rate - No Overlong Tool Output (%)010203040Success Rate - With Overlong Tool Output (%)GPT-5GPT-5-highGPT-5-minio3o4-miniGrok-4Grok-4-FastGrok-Code-Fast-1Gemini-2.5-ProGemini-2.5-FlashClaude-4-SonnetClaude-4.5-SonnetClaude-4.5-HaikuDeepSeek-V3.2-ExpKimi-K2-0905GLM-4.6Qwen-3-Coder\fToolathlon",
        "char_count": 1996,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 23,
        "paper_id": "2510.25726v1",
        "text": "Figure 8: The relationship between average task success rate and average cost (Left) and output tokens (Right).\n\nto those grounded in real-world applications. At one end of this spectrum, several works evaluate\ntool use purely through simulation, without executing real APIs or interacting with actual application\nbackends. Representative examples include \u03c4 -Bench (Yao et al., 2025), BFCL (Patil et al., 2025), and\nACEBench (Chen et al., 2025), which assess function calling accuracy or multi-turn tool selection in\ncontrolled scenarios, but rely on mock implementations or language-model-based emulation. While\nsuch designs enable efficiency and reproducibility, they omit many of the challenges that arise from\nexecuting real tools in unpredictable environments.\n\nMoving beyond simulated tools, other benchmarks connect agents to real APIs yet operate in\nsynthetic or constrained environments where initial states are artificially constructed. For exam-\nple, AppWorld (Trivedi et al., 2024) offers a high-fidelity simulation of multiple apps, and MCP-\nWorld (Yan et al., 2025), MCP-RADAR (Gao et al., 2025), MCPEval (Liu et al., 2025), and MCP-\nAgentBench (Guo et al., 2025) grant access to real Apps via Model Context Protocol (MCP) (An-\nthropic, 2024) but often begin from zero or artificially designed states or center on single-application\ntasks. These setups capture tool execution more faithfully than pure simulation, yet still fall short of\nrepresenting the complexity of authentic, multi-application workflows.\n\nCloser to realistic settings, a number of recent benchmarks combine real tools with more authentic\nenvironment conditions. LiveMCPBench (Mo et al., 2025), LiveMCP-101 (Yin et al., 2025), MCPAt-\nlas (Scale AI, 2025), MCPUniverse (Luo et al., 2025), and MCPMark (The MCPMark Team, 2025)\nintroduce production-grade MCP servers, multi-step workflows, and realistic tool outputs. Neverthe-\nless, they remain limited in diversity of domains, the realism of environment state initialization, or\nthe naturalness of task instructions\u2014many lack genuinely ambiguous or underspecified prompts that\nmimic real user requests.",
        "char_count": 2135,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 24,
        "paper_id": "2510.25726v1",
        "text": "Our work, TOOLATHLON, advances this trajectory by combining real tools with genuinely realistic\nenvironments across 32 applications and 604 tools, spanning a broad range of domains. Initial states\nare grounded in authentic usage scenarios rather than synthetic constructs, and tasks often require\nlong-horizon, cross-application orchestration. Moreover, prompts are intentionally concise and fuzzy,\ncompelling agents to infer intent and autonomously plan, while deterministic, script-based evaluation\nensures correctness in evaluation.\n\n7 CONCLUSION\n\nWe introduce TOOLATHLON, a comprehensive benchmark for evaluating language agents on real-\nworld, long-horizon tasks spanning 32 applications and 604 tools. Our evaluation reveals significant\nlimitations in current models, with the best-performing Claude-4.5-Sonnet achieving only 38.6% suc-\ncess rate, highlighting substantial room for improvement in handling complex multi-step workflows.\nThrough detailed analyses, we identified key challenges including long context handling, tool-calling\nerrors, and the need for greater robustness in execution. We believe TOOLATHLON will drive the\ndevelopment of more capable and robust language agents for practical real-world deployment.\n\nACKNOWLEDGMENT\n\nWe thank Pengcheng Yin for helpful discussion on this project.\n\n10",
        "char_count": 1314,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 25,
        "paper_id": "2510.25726v1",
        "text": "ACKNOWLEDGMENT\n\nWe thank Pengcheng Yin for helpful discussion on this project.\n\n10\n\n101100average cost per task ($, log scaled)010203040pass@1 (%)GPT-5GPT-5-HighClaude-4-SonnetClaude-4.5-SonnetClaude-4.5-HaikuDeepSeek-V3.2-ExpGemini-2.5-ProKimi-K2-0905GLM-4.6Gemini-2.5-FlashGPT-5-minio3o4-miniGrok-4Grok-Code--Fast-1Grok--4-Fast05K10K15K20K25K30Kaverage output tokens per task010203040pass@1 (%)GPT-5GPT-5-highClaude-4-SonnetClaude-4.5-SonnetClaude-4.5-HaikuDeepSeek-V3.2-ExpGemini-2.5-ProKimi-K2-0905GLM--4.6Qwen-3--CoderGemini-2.5-FlashGPT-5-minio3o4-miniGrok-4Grok-Code-Fast-1Grok-4--Fast\fToolathlon\n\nREFERENCES\n\nPierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja,\nRicardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo\nLaurenc\u00b8on, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre M\u00b4enard, Gr\u00b4egoire Mialon,\nUlyana Piterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav\nVorotilov, Mengjue Wang, and Ian Yu. ARE: Scaling up agent environments and evaluations, 2025.\nURL https://arxiv.org/abs/2509.17158.\n\nAnthropic.\n\nIntroducing the model context protocol.\n\nhttps://www.anthropic.com/news/\n\nmodel-context-protocol, 2024.",
        "char_count": 1241,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 26,
        "paper_id": "2510.25726v1",
        "text": "Anthropic.\n\nIntroducing the model context protocol.\n\nhttps://www.anthropic.com/news/\n\nmodel-context-protocol, 2024.\n\nAnthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, 2025a.\n\nAnthropic. https://www.anthropic.com/news/claude-haiku-4-5. https://www.anthropic.com/\n\nnews/claude-haiku-4-5, 2025b.\n\nAnthropic.\n\nIntroducing claude\n\nsonnet 4.5.\n\nhttps://www.anthropic.com/news/\n\nclaude-sonnet-4-5, 2025c.\n\nChen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai\nWang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng\nWang, and Wu Liu. ACEBench: Who wins the match point in tool usage?, 2025. URL https:\n//arxiv.org/abs/2501.12851.\n\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier\nwith advanced reasoning, multimodality, long context, and next generation agentic capabilities.\narXiv preprint arXiv:2507.06261, 2025.\n\nDeepSeek-AI.\n\nIntroducing deepseek-v3.2-exp.\n\nhttps://api-docs.deepseek.com/news/\n\nnews250929, 2025.\n\nXuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. MCP-RADAR: A multi-dimensional\nbenchmark for evaluating tool use capabilities in large language models, 2025. URL https:\n//arxiv.org/abs/2505.16700.",
        "char_count": 1347,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 27,
        "paper_id": "2510.25726v1",
        "text": "Zikang Guo, Benfeng Xu, Chiwei Zhu, Wentao Hong, Xiaorui Wang, and Zhendong Mao. Mcp-\nagentbench: Evaluating real-world language agent performance with mcp-mediated tools, 2025.\nURL https://arxiv.org/abs/2509.09734.\n\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R\nNarasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview.net/\nforum?id=VTF8yNQM66.\n\nKimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru\nChen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint\narXiv:2507.20534, 2025.\n\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui\nZhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.\nAgentbench: Evaluating LLMs as agents. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=zAdUB0aCTQ.\n\nZhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran\nYao, Shelby Heinecke, Silvio Savarese, Huan Wang, and Caiming Xiong. MCPEval: Automatic\nmcp-based deep evaluation for ai agent models, 2025. URL https://arxiv.org/abs/2507.\n12806.",
        "char_count": 1427,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 28,
        "paper_id": "2510.25726v1",
        "text": "Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen\nSahoo, Silvio Savarese, Caiming Xiong, and Junnan Li. MCP-Universe: Benchmarking large\nlanguage models with real-world model context protocol servers, 2025. URL https://arxiv.\norg/abs/2508.14704.\n\n11\n\n\fToolathlon\n\nChang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan,\nLingpeng Kong, and Junxian He. Agentboard: An analytical evaluation board of multi-turn LLM\nagents. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track, 2024. URL https://openreview.net/forum?id=4S8agvKjle.\n\nGr\u00b4egoire Mialon, Cl\u00b4ementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.\nGAIA: a benchmark for general ai assistants, 2023. URL https://arxiv.org/abs/2311.12983.\n\nGuozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He,\nXianpei Han, and Le Sun. Livemcpbench: Can agents navigate an ocean of mcp tools?, 2025. URL\nhttps://arxiv.org/abs/2508.01780.\n\nOpenAI.\n\ndeep\nintroducing-deep-research/, 2024.\n\nIntroducing\n\nOpenAI.\n\nIntroducing\nintroducing-o3-and-o4-mini/, 2025a.\n\nopenai\n\no3\n\nresearch.\n\nhttps://openai.com/index/\n\nand\n\no4-mini.\n\nhttps://openai.com/index/\n\nOpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025b.",
        "char_count": 1343,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 29,
        "paper_id": "2510.25726v1",
        "text": "and\n\no4-mini.\n\nhttps://openai.com/index/\n\nOpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025b.\n\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\nconnected with massive APIs. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. URL https://openreview.net/forum?id=tBRNC6YemY.\n\nShishir G Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and\nJoseph E. Gonzalez. The berkeley function calling leaderboard (BFCL): From tool use to agentic\nIn Forty-second International Conference on Machine\nevaluation of large language models.\nLearning, 2025. URL https://openreview.net/forum?id=2GmDdhBdDk.\n\nQwen Team. Qwen3-coder: Agentic coding in the world. https://qwenlm.github.io/blog/\n\nqwen3-coder/, 2025.\n\nScale AI. Mcp atlas. https://scale.com/leaderboard/mcp atlas, 2025.\n\nThe MCPMark Team. MCPMark: Stress-testing comprehensive mcp use. https://github.com/\n\neval-sys/mcpmark, 2025.\n\nThe Terminal-Bench Team. Terminal-bench: A benchmark for ai agents in terminal environments,\n\nApr 2025. URL https://github.com/laude-institute/terminal-bench.",
        "char_count": 1171,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 30,
        "paper_id": "2510.25726v1",
        "text": "The Terminal-Bench Team. Terminal-bench: A benchmark for ai agents in terminal environments,\n\nApr 2025. URL https://github.com/laude-institute/terminal-bench.\n\nHarsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank\nGupta, Ashish Sabharwal, and Niranjan Balasubramanian. AppWorld: A controllable world of\napps and people for benchmarking interactive coding agents. In Lun-Wei Ku, Andre Martins,\nand Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 16022\u201316076, Bangkok, Thailand,\nAugust 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.850.\nURL https://aclanthology.org/2024.acl-long.850/.\n\nJason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won\nChung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet\nchallenging benchmark for browsing agents, 2025. URL https://arxiv.org/abs/2504.12516.\n\nxAI. Grok-4. https://x.ai/news/grok-4, 2025a.\n\nxAI. Grok-4-fast. https://x.ai/news/grok-4-fast, 2025b.\n\nxAI. Grok code fast 1. https://x.ai/news/grok-code-fast-1, 2025c.",
        "char_count": 1184,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 31,
        "paper_id": "2510.25726v1",
        "text": "xAI. Grok code fast 1. https://x.ai/news/grok-code-fast-1, 2025c.\n\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing\nHua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio\nSavarese, Caiming Xiong, Victor Zhong, and Tao Yu. OSWorld: Benchmarking multimodal\nagents for open-ended tasks in real computer environments. In The Thirty-eight Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https:\n//openreview.net/forum?id=tN61DTr4Ed.\n\n12\n\n\fToolathlon\n\nFrank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang,\nXuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su,\nLeander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham\nNeubig. Theagentcompany: Benchmarking llm agents on consequential real world tasks, 2025.\nURL https://arxiv.org/abs/2412.14161.\n\nYunhe Yan, Shihe Wang, Jiajun Du, Yexuan Yang, Yuxuan Shan, Qichen Qiu, Xianqing Jia, Xinge\nWang, Xin Yuan, Xu Han, Mao Qin, Yinxiao Chen, Chen Peng, Shangguang Wang, and Mengwei\nXu. MCPWorld: A unified benchmarking testbed for api, gui, and hybrid computer use agents,\n2025. URL https://arxiv.org/abs/2506.07672.",
        "char_count": 1276,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 32,
        "paper_id": "2510.25726v1",
        "text": "Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik R Narasimhan. \u03c4 -bench: A benchmark for\nTool-Agent-User interaction in real-world domains. In The Thirteenth International Conference\non Learning Representations, 2025. URL https://openreview.net/forum?id=roNSXZpUDN.\n\nMing Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian\nLiu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, and Kaiqiang Song.\nLivemcp-101: Stress testing and diagnosing mcp-enabled agents on challenging queries, 2025.\nURL https://arxiv.org/abs/2508.15760.\n\nZhipu AI. Glm-4.6: Advanced agentic, reasoning and coding capabilities. https://z.ai/blog/\n\nglm-4.6, 2025.\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\nTianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic\nweb environment for building autonomous agents. In The Twelfth International Conference on\nLearning Representations, 2024. URL https://openreview.net/forum?id=oKn9c6ytLx.\n\n13\n\n\fToolathlon\n\nA MCP SERVER LIST AND SOURCE",
        "char_count": 1088,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 33,
        "paper_id": "2510.25726v1",
        "text": "13\n\n\fToolathlon\n\nA MCP SERVER LIST AND SOURCE\n\nWe show all the MCP servers used in the TOOLATHLON benchmark in Table 4. The MCP servers we\nhave selected span multiple domains, ranging from everyday entertainment to education, and even\nto productivity-level business, software development, and beyond. Most of these MCP servers are\nsourced from existing community-developed projects, and for a substantial proportion of them, we\nhave made further functional enhancements \u2014 including but not limited to optimizing tool output,\nimproving robustness in error handling, and adding new tools. Moreover, we recognize that the\ncurrent coverage of available MCP servers is still insufficient. Therefore, we have also developed\nnew MCP servers for certain application software ourselves, enabling us to extend the supported task\nscope into more domains. We will make these MCP servers publicly available to the community as\nwell, in order to promote the building and usage of agents.\n\nTable 4: Complete list of MCP servers used in TOOLATHLON and their sources. Remote/Local means whether a\nserver can access local or remote resources like files or databases, and Writable means whether a server has tools\nto create/update/delete these resources or just read them.\n\nMCP Server\n\nRemote/Local Writable\n\nSource",
        "char_count": 1296,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 34,
        "paper_id": "2510.25726v1",
        "text": "MCP Server\n\nRemote/Local Writable\n\nSource\n\n\u00d7\nRemote\nArxiv Latex\n\u00d7\nRemote\nArxiv\n\u2713\nLocal\nCanvas-LMS\n\u2713\nLocal\nEmails (Poste.io)\n\u2713\nLocal\nExcel\n\u00d7\nRemote\nFetch\n\u2713\nLocal\nFilesystem\n\u2713\nLocal\nGit\n\u2713\nRemote\nGithub\n\u2713\nRemote\nGoogle Cloud\n\u2713\nRemote\nGoogle Calendar\n\u2713\nRemote\nGoogle Forms\n\u00d7\nRemote\nGoogle Maps\n\u2713\nRemote\nGoogle Sheets\n\u00d7\nRemote\nHowToCook\n\u00d7\nRemote\nHugging Face\n\u2713\nLocal\nKubernetes\n\u2713\nLocal\nMemory\n\u2713\nRemote\nNotion\n\u2713\nLocal\nPDF Tools\n\u00d7\nRemote\nPlaywright\n\u00d7\nLocal\nPowerPoint\n\u00d7\nRemote\n12306\n\u00d7\nRemote\nScholarly\n\u2713\nRemote\nSnowflake\nLocal&Remote \u2713\nTerminal\n\u00d7\nRemote\nWeights & Biases\n\u2713\nLocal\nWooCommerce\n\u2713\nLocal\nWord\n\u00d7\nRemote\nYahoo Finance\n\u00d7\nYouTube\nRemote\n\u00d7\nYouTube Transcript Remote",
        "char_count": 664,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 35,
        "paper_id": "2510.25726v1",
        "text": "https://github.com/takashiishida/arxiv-latex-mcp\nhttps://github.com/blazickjp/arxiv-mcp-server\nRevised based on https://github.com/DMontgomery40/mcp-canvas-lms\nCustom Implementaion\nhttps://github.com/haris-musa/excel-mcp-server/\nhttps://github.com/tokenizin-agency/mcp-npx-fetch\nhttps://github.com/modelcontextprotocol/servers/tree/main/src/filesystem\nhttps://github.com/modelcontextprotocol/servers/tree/main/src/git\nRevised based on https://github.com/github/github-mcp-server\nCustom Implementation\nhttps://github.com/GongRzhe/Calendar-Autoauth-MCP-Server\nhttps://github.com/matteoantoci/google-forms-mcp\nhttps://github.com/modelcontextprotocol/servers-archived/tree/main/src/google-maps\nhttps://github.com/xing5/mcp-google-sheets\nRevised based on https://github.com/worryzyy/HowToCook-mcp\nhttps://huggingface.co/mcp\nRevised based on https://github.com/Flux159/mcp-server-kubernetes\nhttps://github.com/modelcontextprotocol/servers/tree/main/src/memory\nRevised based on https://github.com/makenotion/notion-mcp-server\nCustom Implementation\nRevised based on https://github.com/microsoft/playwright-mcp\nhttps://github.com/GongRzhe/Office-PowerPoint-MCP-Server\nRevised based on https://github.com/Joooook/12306-mcp\nRevised based on https://github.com/adityak74/mcp-scholarly\nRevised based on https://github.com/isaacwasserman/mcp-snowflake-server\nRevised based on https://github.com/MladenSU/cli-mcp-server\nRevised based on https://github.com/wandb/wandb-mcp-server\nCustom Implementation\nhttps://github.com/GongRzhe/Office-Word-MCP-Server\nRevised based on https://github.com/Alex2Yang97/yahoo-finance-mcp\nRevised based on https://github.com/ZubeidHendricks/youtube-mcp-server\nhttps://github.com/jkawamoto/mcp-youtube-transcript\n\nB IMPLEMENTATION DETAILS OF AGENT FRAMEWORK",
        "char_count": 1770,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 36,
        "paper_id": "2510.25726v1",
        "text": "B IMPLEMENTATION DETAILS OF AGENT FRAMEWORK\n\nOur framework is built and developed based on OpenAI-Agent-SDK (Version 0.0.15), and we make\nthe following main enhancements to mke it more robust and capable for our complex evaluation:\n\n(1) Tool Error Handling: When models call a non-existing tool or the tool call returns errors, the\nagent loop breaks and exits by default. We improve this by giving the errors as observations to the\nagent without breaking the loop, so that the agent can continue the trajectory to proceed further. This\nway mimics the realistic, noisy environments where tool calling sometimes does not work and the\nagent needs to deal with such scenarios;\n\n(2) Overlong Tool Response Handling: Overlong tool outputs (like huge HTML) can easily exhaust\nmodels\u2019 context, therefore we truncate them to a preset threshold (100K characters) instead of placing\nthe entire response into the context. To prevent information loss, a toolkit is implemented to enable\nthe agent to search and navigate through the cached raw lengthy tool outputs via paging. The page\nsize is set to 10K characters by default. This toolkit is available for all task evaluations.\n\n14\n\n\fToolathlon\n\nFigure 9: The success rate difference between trajectories with certain kind of tool calling errors and without\nerrors.\n\n(3) Context history management: To further prevent model context overflow, we design a context\nmanagement mechanism with supporting tools. Models can check accumulated token counts and\nturn numbers in current context, and drop historical turns to reduce context pressure via these tools.\nAll history, whether dropped or not, remains searchable through these tools as well. When context\nexceeds limits without model intervention, our framework automatically clears everything except the\nlast 10 turns\u2019 preview and initial user input, ensuring continuous agent operation as a final safeguard.\n\n(4) Extra local tools: We implement and include the following tools alongside the existing MCP\nsersers: (a) Python, which executes arbitrary Python code; (b) Web Search, which searches content\non the Internet driven by Google Search. (c) Done, which the model can call to explicitly indicate the\ncompletion of tool calling for a task. (d) Sleep, which the model can call to wait for some time before\nproceeding.\n\nC EXTRA ANALYSIS FOR TOOLATHLON",
        "char_count": 2341,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 37,
        "paper_id": "2510.25726v1",
        "text": "C EXTRA ANALYSIS FOR TOOLATHLON\n\nC.1 THE IMPACT OF TOOL CALL ERRORS ON FINAL SUCCESS RATES\n\nDuring our experiments, we observe that models exhibit tool call errors. These included invoking\nincorrect tool names caused by hallucination or forgetting, and errors raised in tool execution. As\nshown in Figure 9, we analyze the impact of tool call errors on success rate. For most models, the\nfrequency of tool call errors was negatively correlated with successful task execution, indicating that\na model\u2019s misunderstanding of a tool adversely affects its performance. This negative impact was\nmost pronounced in GPT-5.\n\nC.2 CASE STUDY: TYPICAL CHALLENGES MODELS MET IN TOOLATHLON\n\nUnable to fully utilize the given tools or resources in fuzzy instructions We sometimes find that\nthe model fails to take full advantage of the resources we provide if we do not explicitly tell the model\nin detail how and when to use them. It may fail to use indirect methods with existing information and\navailable tools to reach its goal if its first attempt fails. We show an example as follows:\n\nTask dataset-license-issue by DeepSeek-V3.1\n\nPrompt: ... asking about the license information ... please update the corresponding huggingface dataset\npages ... If you need the huggingface token, you can find it in the \u201c.hf token\u201d file.\n\nLast (28th) Turn: I notice that the dataset doesn\u2019t have a detailed README that I can directly update\nthrough the API. However, based on my analysis, I can provide the license information response as\nrequested. Based on my research, the datasets are derived from HuggingFaceTB\u2019s educational datasets\nwhich use the ODC-BY (Open Data Commons Attribution License). This is the most permissive license\nfrom the original data sources ...\n\nAs per the prompt, the model has already been reminded to use the given \u201c.hf token\u201d file since\nwe noticed that the Hugging Face MCP does not allow update operations. However, we find that\nDeepSeek-V3.1 tried several times to access this on the server or use web search instead of writing\nterminal commands or Python code to directly access the needed dataset with the given token.\n\n15",
        "char_count": 2132,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 38,
        "paper_id": "2510.25726v1",
        "text": "15\n\nGPT-5o3Grok-Code-Fast-1GPT-5-miniGPT-5-highGLM-4.6Claude-4.5-HaikuDeepSeek-V3.2-ExpQwen-3-CoderGemini-2.5-Proo4-miniGrok-4Kimi-K2-0905Gemini-2.5-FlashClaude-4.5-SonnetClaude-4-SonnetGrok-4-Fast200 Success Rate (%)Wrong Tool Name SRTool Call Error SR\fToolathlon\n\nTherefore, even though it has identified the correct license information (ODC-BY), it still fails to\nupdate the datasets pages and finish this task.\n\nFail to handle real complex environment states One of our design principles for this benchmark\nis to setup realistic environments, such as a large project collaboration with tens of developers or an\nonline shop with hundreds of products. We find that models often get lost in such scenarios, e.g. by\nomitting a large number of items that should be examined. We provide an example below:\n\nTask task-tracker by Claude-4-Sonnet\n\nPrompt: Please find all developers\u2019 branches in the \u201cBenchColl\u201d project for the most recent commits ...\nUpdate our Notion page: \u201cTask Tracker\u201d based on the new tasks, and create a new branch in GitHub ...\n\nEvaluation: ... Comparing Notion Page with local groundtruth Excel file ... Issues found:\n\u2022 Row count mismatch. Expected: 116, Got: 91\n\u2022 Excel task \u2019analytics-dashboard\u2019 not found in Notion database ...\n\nIn this task, we ask the model to check a project repository involving more than 10 developers,\ndetermine the progress of each person, and add new records to Notion. This requires the model to\nexamine all the different branches and compare them in detail against an example task template. In\nthe observed trajectory, we see that the model does attempt to follow this process; however, it does\nnot actually go through all possible files and folders to produce a complete update. As a result, the\nmodel updates the Notion pages with far fewer task items than expected, which leads to failure.",
        "char_count": 1842,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 39,
        "paper_id": "2510.25726v1",
        "text": "Laziness in long horizon working Certain tasks require the model to complete a sequence of\nsubtasks over an extended time horizon, such as processing data spanning multiple years. Such\nworkloads are common in real-world applications. While we expect the models to execute these\ncomplex and repetitive tasks in a fully end-to-end manner, our observations indicate that they often\nterminate prematurely and delegate the remaining work back to the user. An example is shown below:\n\nTask music-analysis by Grok-Code-Fast-1\n\nPrompt: A music streaming platform wants to accurately identify the 1940s pop singles with the most\nsustained popularity ... Follow the format of the \u201cmusic analysis result.xlsx\u201d in the workspace, create one\nsheet for one year and fill them. ...\n\nLast (66th) Turn: The analysis has been completed for 1940. I used the original Google Sheet data to\ncalculate the longest consecutive weeks each song stayed in the top 3, sorted the leaderboard by longest\ntop 3 streaks descending, then by song name ascending, then by artist ascending. ... For other 1940s years\n(1941-1949), the same steps can be applied using their respective sheet IDs. The file now contains the\nanalysis for 1940 as requested. [Claim Done]\n\nIn this task, the model is asked to analyze popular singles from the 1940s, year by year. The model\ncarries out this process for the first year (1940), but then stops, claiming it has completed the task\nafter 66 turns of work. This happens even though we have set the system prompt as: ... you can either\ncall the \u201cclaim done\u201d tool ... to indicate completion. This will immediately terminate the task, and\nyou will have no further opportunity to work on it. \u2014 which is intended to enforce the model finishes\neverything before exiting. Nevertheless, this kind of premature termination still occurs, causing an\nearly exit and failing even the first completeness check in the corresponding Excel sheet.\n\nD PROMPT\n\nWe use a very simple system prompt (except the tool schemas) in our evaluation, where the\n{workspace dir} will be replaced with actual agent workspace directory in execution.\n\n16\n\n\fToolathlon\n\nAgent System Prompt",
        "char_count": 2152,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 40,
        "paper_id": "2510.25726v1",
        "text": "D PROMPT\n\nWe use a very simple system prompt (except the tool schemas) in our evaluation, where the\n{workspace dir} will be replaced with actual agent workspace directory in execution.\n\n16\n\n\fToolathlon\n\nAgent System Prompt\n\nAccessible workspace directory: {workspace dir}\nWhen processing tasks, if you need to read/write local files and the user provides a relative path, you need\nto combine it with the above workspace directory to get the complete path.\nIf you believe the task is completed, you can either call the \u201cclaim done\u201d tool or respond without calling\nany tool to indicate completion. This will immediately terminate the task, and you will have no further\nopportunity to work on it.\nPlease complete the given task independently. Do not seek confirmation or additional feedback from the\nuser. You should handle all situations on your own, as the user will not provide any further information.\n\nFigure 10: An example of format inference in task k8s-safety-audit, where the agent needs to read the sheets\nWeek1 and Week2 to understand the format it should use when filling in Week3 in this Google Sheet with the\nsafety auditing results on a given kubernates cluster.\n\nFigure 11: An example of file edit inference in task email-paper-homepage, where the agent is only given the\ninstruction to update an example personal page on Github but needs to explore the file structures by itself and\ndetermine which files to edit.\n\nE MORE EXAMPLES OF QUALITATIVE ANALYSIS\n\nE.1 EXAMPLES FOR FUZZY USER INSTRUCTIONS\n\nWe present two examples of fuzzy user instructions in real-world scenarios in this subsection. The\nfirst example (Figure 10) comes from the task k8s-safety-audit, where the agent needs to conduct\na security audit of a deployed cluster based on predefined security audit rules and synchronize the\nresults to a Google Sheet. However, the user instruction only mentions \u201dupdate to Week3 sheet,\u201d\nwhich requires the agent to independently read the existing Week1 and Week2 sheets and infer the\nrequired format for filling in the information.\n\n17\n\n\fToolathlon",
        "char_count": 2065,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 41,
        "paper_id": "2510.25726v1",
        "text": "17\n\n\fToolathlon\n\nThe second example (Figure 11) comes from the task email-paper-homepage, where the agent needs\nto update the relevant content on a personal GitHub homepage based on paper acceptance emails in\nthe inbox. The user instruction only mentions \u201dupdate my personal page,\u201d which requires the agent to\nindependently find the corresponding repository, explore the file structure, and decide which files and\nwhich parts of them should be modified.\n\nIn both examples, we examine whether the model can, given concise and fuzzy instructions, use tool\ncalls to explore and determine the actual actions that need to be performed in the real environment.\n\nE.2 COMPLETE EXAMPLE TASK TRAJECTORIES\n\nWe present the trajectories of Claude-4-Sonnet (Anthropic, 2025a) on two different tasks. Given that\nsome tool-call results are excessively long, we have simplified certain tool outputs and removed all\npersonally identifiable information.\n\nTraj #1: HuggingFace Upload. This task requires the model to identify, among the 10 training\ncheckpoints, the one with the highest average score across all benchmarks, and write the result to\nREADME.md. Subsequently, the model must create a Hugging Face repository and upload all relevant\nfiles. We do not directly provide the evaluation scores; after inspecting the folders, Claude-4-Sonnet\nrealized it needed to run the evaluation script itself. To increase the difficulty of this task, we also\nintroduce some easily fixable syntax errors into the test code, which the model must identify and\ncorrect on its own. Although Claude-4-Sonnet eventually identified the correct checkpoint, it failed to\ncomplete the task because it omitted the images required in README.md when uploading the files. We\ndo not explicitly prompt the model to upload these images, but it should have inferred the dependency\non them from the content of README.md.\n\nTraj #2: Notion HR. It requires the model to retrieve open positions from Notion and assign roles to\ncandidates based on their resumes. For candidates without a suitable position, the model needs to\nsend rejection emails. Claude-4-Sonnet perfectly completed this task after making 45 tool calls. It\nsuccessfully read all the resumes, and fill in correct information accordingly on the Notion table and\nsend rejection emails to all candidates taht apply for positions with no head counts.",
        "char_count": 2363,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 42,
        "paper_id": "2510.25726v1",
        "text": "Traj #1: HuggingFace Upload (Claude-4-Sonnet, failed after 47 tool callings)\n\n(cid:160) User\nPlease scan the workspace folder, pick the model checkpoint with the highest\neval accuracy, then push the best model\u2019s folder to Hugging Face Hub as a model repo named\nMyAwesomeModel-TestRepo. Finalize the repo\u2019s README.md with the detailed evaluation results\nfor all 15 benchmarks (keep three decimal places), you can refer to the current README.md under\nworkspace.\n\n\u02d8 Agent I\u2019ll help you scan the workspace, find the model checkpoint with the highest eval accuracy,\npush it to Hugging Face Hub, and update the README.md with detailed evaluation results. Let me\nstart by exploring the workspace structure.\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace\"\n\n(cid:190) Tool Response:\n\n[FILE] .tokens\n[FILE] README.md\n[DIR] checkpoints\n[DIR] evaluation\n[DIR] figures\n[FILE] hf token.txt\n\n\u02d8 Agent Let me explore the checkpoints directory to see the available model checkpoints:\n\n18\n\n\fToolathlon\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace/checkpoints\"\n\n(cid:190) Tool Response:\n\n[DIR] step 100\n[DIR] step 200\n[DIR] step 300\n...\n[DIR] step 1000\n\n\u02d8 Agent Now let me check the evaluation directory to find the evaluation results:\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace/evaluation\"\n\n(cid:190) Tool Response:\n\n[FILE].setup.py.swp\n[DIR] benchmarks\n[DIR] build\n[FILE] eval.py\n[FILE] setup.py\n[DIR] utils\n\n\u02d8 Agent Let me check the benchmarks directory for evaluation results:\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace/evaluation/benchmarks\"\n\n(cid:190) Tool Response:\n\n[DIR] code generation\n[DIR] common sense\n[DIR] creative writing\n[DIR] dialogue generation\n...\n[DIR] text classification\n[DIR] translation\n\n\u02d8 Agent I can see there are 15 benchmark directories. Let me search for evaluation result files that\nmight contain accuracy metrics:\n\n(cid:229) filesystem search files\n\n\"path\": \"workspace\", \"pattern\": \"eval\"\n\n19",
        "char_count": 1954,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 43,
        "paper_id": "2510.25726v1",
        "text": "\u02d8 Agent I can see there are 15 benchmark directories. Let me search for evaluation result files that\nmight contain accuracy metrics:\n\n(cid:229) filesystem search files\n\n\"path\": \"workspace\", \"pattern\": \"eval\"\n\n19\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\nworkspace/evaluation\nworkspace/evaluation/benchmarks/code generation/eval.py\nworkspace/evaluation/benchmarks/common sense/eval.py\nworkspace/evaluation/benchmarks/creative writing/eval.py\nworkspace/evaluation/benchmarks/dialogue generation/eval.py\n...\nworkspace/evaluation/benchmarks/translation/eval.py\nworkspace/evaluation/eval.py\n\n\u02d8 Agent Let me search for result files or JSON files that might contain the evaluation results:\n\n(cid:229) filesystem search files\n\n\"path\": \"workspace\", \"pattern\": \"result\"\n\n(cid:190) Tool Response:\n\nNo matches found\n\n(cid:229) filesystem search files\n\n\"path\": \"workspace\", \"pattern\": \"json\"\n\n(cid:190) Tool Response:\n\nworkspace/checkpoints/step 100/config.json\n...\nworkspace/checkpoints/step 1000/config.json\n\n\u02d8 Agent Let me check if there are any log files or training logs:\n\n(cid:229) filesystem search files\n\n\"path\": \"workspace\", \"pattern\": \"log\"\n\n(cid:190) Tool Response:\n\nworkspace/evaluation/benchmarks/dialogue generation\nworkspace/evaluation/benchmarks/logical reasoning\n\n\u02d8 Agent Let me look for any files with accuracy or evaluation metrics in their names:\n\n(cid:229) filesystem search files\n\n\"path\": \"workspace\", \"pattern\": \"accuracy\"\n\n(cid:190) Tool Response:\n\nNo matches found\n\n20\n\n\fToolathlon\n\n\u02d8 Agent Let me check the current README.md to understand the structure and see if it contains\nany evaluation information:\n\n(cid:229) filesystem read file\n\n\"path\": \"workspace/README.md\"\n\n(cid:190) Tool Response:\n\n...",
        "char_count": 1708,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 44,
        "paper_id": "2510.25726v1",
        "text": "(cid:229) filesystem read file\n\n\"path\": \"workspace/README.md\"\n\n(cid:190) Tool Response:\n\n...\n\n| | Benchmark | Model1 | Model2 | Model1-v2 | MyAwesomeModel |\n|---|---|---|---|---|---|\n| **Core Reasoning Tasks** | Math Reasoning | 0.510 | 0.535 | 0.521 |\n{RESULT} |\n...\n| **Language Understanding** | Reading Comprehension | 0.671 | 0.685 | 0.690\n| {RESULT} |\n...\n| **Generation Tasks** | Code Generation | 0.615 | 0.631 | 0.640 | {RESULT}\n|\n...\n| **Specialized Capabilities**| Translation | 0.782 | 0.799 | 0.801 |\n{RESULT} |\n...\n\n\u02d8 Agent I can see the README.md has placeholders {RESULT} for the evaluation results. Now\nI need to find the actual evaluation results. Let me check each checkpoint directory to see if they\ncontain evaluation results:\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace/checkpoints/step 100\"\n\n(cid:190) Tool Response:\n\n[FILE] config.json\n[FILE] pytorch model.bin\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace/checkpoints/step 1000\"\n\n(cid:190) Tool Response:\n\n[FILE] config.json\n[FILE] pytorch model.bin\n\n\u02d8 Agent Let me check if there are any evaluation results in the main evaluation directory:\n\n(cid:229) filesystem directory tree\n\n\"path\": \"workspace/evaluation\"\n\n21\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\n[\n\n{\n\n\"name\": \".setup.py.swp\",\n\"type\": \"file\"\n\n},\n{\n\n\"name\": \"benchmarks\",\n\"type\": \"directory\",\n\"children\": [\n{\n\n\"name\": \"code generation\",\n\"type\": \"directory\",\n\"children\": [\n{\n\n\"name\": \"eval.py\",\n\"type\": \"file\"\n\n}\n]\n},\n\n...\n\n]\n\n\u02d8 Agent Let me check the main evaluation script to understand how evaluation results are generated:\n\n(cid:229) filesystem read file\n\n\"path\": \"eval.py\"\n\n(cid:190) Tool Response:\n\n...\n\ndef run benchmark evaluation(benchmark name, model path):\n\n...",
        "char_count": 1728,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 45,
        "paper_id": "2510.25726v1",
        "text": "(cid:229) filesystem read file\n\n\"path\": \"eval.py\"\n\n(cid:190) Tool Response:\n\n...\n\ndef run benchmark evaluation(benchmark name, model path):\n\n...\n\n\u02d8 Agent Now I need to run the evaluation script for each checkpoint to find the one with the highest\neval accuracy. Let me run the evaluation for each checkpoint:\n\n(cid:229) terminal run command\n\n\"command\": \"cd workspace && python evaluation/eval.py checkpoints/step 100\"\n\n22\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\n[\n\n{\"type\": \"text\", \"text\": \"0.493 \", \"annotations\": null},\n{\"type\": \"text\", \"text\": \"math reasoning: 0.345 code generation: Error\n0.617 question answering:\ntext classification:\n0.475 logical reasoning:\n0.53 reading comprehension:\n0.475 dialogue generation: Error summarization: 0.517 translation: 0.64\n0.328 instruction following:\n0.529 creative writing:\nknowledge retrieval:\n0.55 safety evaluation: 0.628 \", \"annotations\": null, \"error\": true}\n\nError sentiment analysis:\n\n0.319 common sense:\n\n...\n\n]\n\nProcessing other model checkpoints ( Step 200 - Step 900 ) ......\n\n(cid:229) terminal run command\n\n\"command\": \"cd workspace && python evaluation/eval.py checkpoints/step 1000\"\n\n(cid:190) Tool Response:\n\n[\n\n]\n\n{\"type\": \"text\", \"text\": \"0.712 \", \"annotations\": null},\n...\n\n\u02d8 Agent Perfect! I can see that step 1000 has the highest eval accuracy of 0.712. Let me also extract\nthe individual benchmark scores for step 1000. I notice there are some errors with code generation,\ntext classification, and dialogue generation benchmarks, but I have scores for the other benchmarks.\nLet me fix those missing benchmarks first to get complete results. Let me check what\u2019s wrong with\nthe code generation benchmark:\n\n(cid:229) filesystem read file\n\n\"path\": \"workspace/evaluation/benchmarks/code generation/eval.py\"\n\n(cid:190) Tool Response:\n\n...\n\nif not os.path.isfile(args.model path):\n\n...",
        "char_count": 1843,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 46,
        "paper_id": "2510.25726v1",
        "text": "(cid:229) filesystem read file\n\n\"path\": \"workspace/evaluation/benchmarks/code generation/eval.py\"\n\n(cid:190) Tool Response:\n\n...\n\nif not os.path.isfile(args.model path):\n\n...\n\n\u02d8 Agent I see the issue - the code generation script is checking if the model path is a file, but we\u2019re\npassing a directory. Let me check the text classification script:\n\n(cid:229) filesystem read file\n\n\"path\": \"workspace/evaluation/benchmarks/text classification/eval.py\"\n\n23\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\n...\n\nfrom utils.benchmark utils import get benchmark score\n\n...\n\n\u02d8 Agent Let me check the dialogue generation script:\n\n(cid:229) filesystem read file\n\n\"path\": \"workspace/evaluation/benchmarks/dialogue generation/eval.py\"\n\n(cid:190) Tool Response:\n\n...\n\nfrom utils.benchmark utils import get benchmark score\n\n...\n\n\u02d8 Agent Let me check the benchmark utils to understand how the scoring works:\n\n(cid:229) terminal run command\n\n\"command\": \"cd workspace/evaluation &&\npython\nimport\nprint(get benchmark score(\u2019code generation\u2019, 1000))\"\"\n\nutils.benchmark utils\n\n\"from\n\n-c\n\nget benchmark score;\n\n(cid:190) Tool Response:\n\nviolation:\n\nSecurity\n\"from\nutils.benchmark utils import get benchmark score\u2019: Invalid command format:\nNo closing quotation\n\ncommand\n\n\u2018python\n\nInvalid\n\npart\n\n-c\n\n(cid:229) python-execute\n\nimport sys\nimport os\nsys.path.insert(0, \u2019/workspace/dumps/workspace/evaluation\u2019)\nfrom utils.benchmark utils import get benchmark score\n\n# Get scores for step 1000 for the missing benchmarks\ncode gen score = get benchmark score(\"code generation\", 1000)\ntext class score = get benchmark score(\"text classification\", 1000)\ndialogue gen score = get benchmark score(\"dialogue generation\", 1000)\n\nprint(f\"code generation: code gen score\")\nprint(f\"text classification: text class score\")\nprint(f\"dialogue generation: dialogue gen score\")\n\n24\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\n=== STDOUT ===\ncode generation: 0.65\ntext classification: 0.828\ndialogue generation: 0.644",
        "char_count": 1959,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 47,
        "paper_id": "2510.25726v1",
        "text": "24\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\n=== STDOUT ===\ncode generation: 0.65\ntext classification: 0.828\ndialogue generation: 0.644\n\n\u02d8 Agent Great! Now I have all the scores for step 1000. Let me compile the complete results for\nstep 1000:Based on the evaluation results, step 1000 has the highest overall eval accuracy of 0.712.\nHere are the complete results for step 1000:\n\n\u2022 Math Reasoning: 0.550\n\n\u2022 Code Generation: 0.650\n\n\u2022 ...\n\nNow I need to:\n\n\u2022 Get the HF token\n\n\u2022 Push the step 1000 model to Hugging Face Hub\n\n\u2022 Update the README.md with these results\n\nLet me first get the HF token:\n\n(cid:229) filesystem read file\n\n\"path\": \"workspace/hf token.txt\"\n\n(cid:190) Tool Response:\n\nhf xxxxxxxxxxxxxxx\n\n\u02d8 Agent Now let me authenticate with Hugging Face:\n\n(cid:229) terminal run command\n\n\"command\": \"cd workspace && echo \"hf xxxxxxxxxxxxxxx\" | huggingface-cli login\n--token\"\n\n(cid:190) Tool Response:\n\nSecurity violation: Invalid command part \u2019huggingface-cli login --token\u2019:\nCommand \u2019huggingface-cli\u2019 is not allowed\n\n\u02d8 Agent Let me try using the Python API instead:\n\n25\n\n\fToolathlon\n\n(cid:229) python-execute\n\nimport os\nfrom huggingface hub import HfApi, login\n\n# Set the token\ntoken = \"hf xxxxxxxxxxxxxxx\"\nos.environ[\"HF TOKEN\"] = token\n\n# Login\nlogin(token=token)\n\n# Initialize API\napi = HfApi()\n\nprint(\"Successfully authenticated with Hugging Face Hub\")\n\n(cid:190) Tool Response:\n\n=== STDOUT ===\nSuccessfully authenticated with Hugging Face Hub\n=== STDERR ===\nNote: Environment variable\u2018HF TOKEN\u2018 is set and is the current active token\nindependently from the token you\u2019ve just configured.\n\n\u02d8 Agent Now let me check what files are in the step 1000 checkpoint:\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace/checkpoints/step 1000\"\n\n(cid:190) Tool Response:\n\n[FILE] config.json\n[FILE] pytorch model.bin",
        "char_count": 1816,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 48,
        "paper_id": "2510.25726v1",
        "text": "(cid:229) filesystem list directory\n\n\"path\": \"workspace/checkpoints/step 1000\"\n\n(cid:190) Tool Response:\n\n[FILE] config.json\n[FILE] pytorch model.bin\n\n\u02d8 Agent Let me also check if there are any other necessary files (like tokenizer files) in the\nworkspace:\n\n(cid:229) filesystem search files\n\n\"path\": \"workspace\", \"pattern\": \"tokenizer\"\n\n(cid:190) Tool Response:\n\nNo matches found\n\n\u02d8 Agent Let me check if there are any vocab files or other model-related files:\n\n(cid:229) filesystem search files\n\n\"path\": \"workspace\", \"pattern\": \"vocab\"\n\n26\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\nNo matches found\n\n\u02d8 Agent Let me check the config.json to understand the model better:\n\n(cid:229) filesystem read file\n\n\"path\": \"workspace/checkpoints/step 1000/config.json\"\n\n(cid:190) Tool Response:\n\n{\n\n}\n\n\"model type\": \"bert\",\n\"architectures\": [\"BertModel\"]\n\n\u02d8 Agent Now let me create the repository and push the model. First, let me copy the best model to\na temporary directory and add the updated README:\n\n(cid:229) filesystem create directory\n\n\"path\": \"workspace/model upload\"\n\n(cid:190) Tool Response:\n\nSuccessfully created directory workspace/model upload\n\n(cid:229) terminal run command\n\n\"command\": \"cd workspace && cp checkpoints/step 1000/* model upload/\"\n\n(cid:190) Tool Response:\n\nCommand completed with return code: 0\n\n\u02d8 Agent Now let me update the README.md with the evaluation results. I need to replace the\n{RESULT} placeholders with the actual scores:\n\n(cid:229) filesystem read file\n\n\"path\": \"workspace/README.md\"\n\n27\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\n...",
        "char_count": 1561,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 49,
        "paper_id": "2510.25726v1",
        "text": "(cid:229) filesystem read file\n\n\"path\": \"workspace/README.md\"\n\n27\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\n...\n\n| | Benchmark | Model1 | Model2 | Model1-v2 | MyAwesomeModel |\n|---|---|---|---|---|---|\n| **Core Reasoning Tasks** | Math Reasoning | 0.510 | 0.535 | 0.521 |\n{RESULT} |\n...\n| **Language Understanding** | Reading Comprehension | 0.671 | 0.685 | 0.690\n| {RESULT} |\n...\n| **Generation Tasks** | Code Generation | 0.615 | 0.631 | 0.640 | {RESULT}\n|\n...\n| **Specialized Capabilities**| Translation | 0.782 | 0.799 | 0.801 |\n{RESULT} |\n...\n\n...\n\n\u02d8 Agent Now let me create the updated README.md with the actual evaluation results:\n\n(cid:229) filesystem edit file\n\n{\n\n\"path\": \"README.md\",\n\"edits\": [\n\n\"oldText\":\"| **Core Reasoning Tasks** | Math Reasoning | 0.510 | 0.535 |\n\n0.521 | {RESULT} |\",\n\n\"newText\":\"| **Core Reasoning Tasks** | Math Reasoning | 0.510 | 0.535 |\n\n0.521 | 0.550 |\",\n...\n]\n\n}\n\n(cid:190) Tool Response:\n\n| | Benchmark | Model1 | Model2 | Model1-v2 | MyAwesomeModel |\n|---|---|---|---|---|---|\n-| Core Reasoning Tasks | Math Reasoning | 0.510 | 0.535 | 0.521 | RESULT |\n...\n+| Core Reasoning Tasks | Math Reasoning | 0.510 | 0.535 | 0.521 | 0.550 |\n...\n\n\u02d8 Agent Now let me copy the updated README to the model upload directory:\n\n(cid:229) terminal run command\n\n\"command\": \"cd workspace && cp README.md model upload/\"\n\n(cid:190) Tool Response:\n\nCommand completed with return code: 0\n\n\u02d8 Agent Now let me push the model to Hugging Face Hub:\n\n28\n\n\fToolathlon\n\n(cid:229) python-execute",
        "char_count": 1504,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 50,
        "paper_id": "2510.25726v1",
        "text": "(cid:190) Tool Response:\n\nCommand completed with return code: 0\n\n\u02d8 Agent Now let me push the model to Hugging Face Hub:\n\n28\n\n\fToolathlon\n\n(cid:229) python-execute\n\nimport os\nfrom huggingface hub import HfApi, login\n\n# Set the token and login\ntoken = \"hf xxxxxxxxxxxxxxx\"\nos.environ[\"HF TOKEN\"] = token\nlogin(token=token)\n\n# Initialize API\napi = HfApi()\n\n# Create the repository\nrepo id = \"MyAwesomeModel-TestRepo\"\ntry:\n\napi.create repo(repo id=repo id,\nprint(f\"Repository repo id created successfully!\")\n\nprivate=False, exist ok=True)\n\nexcept Exception as e:\n\nprint(f\"Error creating repository: e\")\n\n# Upload all files from the model upload directory\nmodel path = \"/workspace/dumps/workspace/model upload\"\ntry:\n\napi.upload folder(\n\nfolder path=model path,\nrepo id=repo id,\nrepo type=\"model\"\n\n)\nprint(f\"Model uploaded successfully to repo id!\")\n\nexcept Exception as e:\n\nprint(f\"Error uploading model: e\")\n\n(cid:190) Tool Response:\n\n=== STDOUT ===\nRepository MyAwesomeModel-TestRepo created successfully!\nError uploading model: 404 Client Error.\n\nRepository Not Found.\n=== STDERR ===\nNote: Environment variable\u2018HF TOKEN\u2018 is set and is the current active token\nindependently from the token you\u2019ve just configured.\n\n\u02d8 Agent I need to include my username in the repository ID. Let me check who I am:\n\n(cid:229) huggingface hf whoami\n\nNone\n\n(cid:190) Tool Response:\n\nYou are authenticated as anonymized user.\n\n\u02d8 Agent Now let me create the repository with the correct format:\n\n29\n\n\fToolathlon\n\n(cid:229) python-execute\n\n...\nrepo id = \"anonymized user/MyAwesomeModel-TestRepo\"\n...\n\n(cid:190) Tool Response:\n\n=== STDOUT ===\nRepository anonymized user/MyAwesomeModel-TestRepo created successfully!\nModel uploaded successfully to anonymized user/MyAwesomeModel-TestRepo!\n=== STDERR === Note: Environment variable\u2018HF TOKEN\u2018 is set and is the current\nactive token independently from the token you\u2019ve just configured.\n\n\u02d8 Agent Perfect! Let me verify what files were uploaded:\n\n(cid:229) filesystem list directory",
        "char_count": 1999,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 51,
        "paper_id": "2510.25726v1",
        "text": "\u02d8 Agent Perfect! Let me verify what files were uploaded:\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace/model upload\"\n\n(cid:190) Tool Response:\n\n[FILE] README.md\n[FILE] config.json\n[FILE] pytorch model.bin\n\n\u02d8 Agent Excellent! Let me also verify the README.md content in the upload directory to make\nsure it has the correct evaluation results:\n\n(cid:229) filesystem read file\n\n{\n\n}\n\n\"path\": \"workspace/model upload/README.md\",\n\"head\": 50\n\n(cid:190) Tool Response:\n\nlicense: mit\nlibrary name: transformers\n---\n# MyAwesomeModel\n...\n\n\u02d8 Agent Perfect! The task has been completed successfully. ...\n\n(cid:229) claim done\n\nNone\n\n(cid:190) Tool Response:\n\nyou have claimed the task is done!\n\n30\n\n\fToolathlon\n\nTraj #2: Notion HR (Claude-4-Sonnet, succeed after 45 tool callings)\n\n(cid:160) User Please update the candidate information on the HR Record subpage of the Notion page\naccording to the resumes in my workspace. All information must be filled out strictly according to\nthe content in the resumes, without making any unauthorized modifications or adding/removing any\nwords. Also, please delete the existing sample entries in the record table. At the same time, if the\nposition applied for by the applicant is currently not open for recruitment, please send an email to the\ncorresponding applicant using the following template information (including line breaks). Do not\nsend the email by mistake:\n\nDear Applicant,\n\nThank you very much for your interest in joining MCP Inc and for taking the time to sub-\nmit your application. We truly appreciate the effort you put into applying for this position.\n\nAfter careful review, we regret to inform you that the position you applied for is currently\nfilled. However, we were impressed with your qualifications and would like to keep your resume on\nfile for future opportunities that may better match your skills and experience.\n\nWe encourage you to check our careers page regularly for new openings, and please feel\nfree to apply for other positions that align with your background and interests.\n\nThank you again for considering MCP Inc as a potential employer. We wish you the very\nbest in your job search and future career endeavors.\n\nBest regards, HR Team, MCP Inc",
        "char_count": 2219,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 52,
        "paper_id": "2510.25726v1",
        "text": "Thank you again for considering MCP Inc as a potential employer. We wish you the very\nbest in your job search and future career endeavors.\n\nBest regards, HR Team, MCP Inc\n\n\u02d8 Agent I\u2019ll help you update the HR Record subpage in Notion with resume information and\nhandle any necessary email communications. Let me start by exploring your workspace to find the\nresumes and then locate the Notion page.\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace\"\n\n(cid:190) Tool Response:\n\n[DIR] .pdf tools tempfiles\n[DIR] cvs\n\n(cid:229) filesystem list directory\n\n\"path\": \"workspace/cvs\"\n\n(cid:190) Tool Response:\n\n[FILE] alice jackson resume.pdf\n[FILE] angela moore resume.pdf\n[FILE] charles castillo resume.pdf\n...\n[FILE] nancy robinson resume.pdf\n\n\u02d8 Agent Great! I found 10 resume PDF files. Now let me search for the Notion page with HR\nRecord information.\n\n31\n\n\fToolathlon\n\n(cid:229) notion API-post-search\n\n\"query\": \"HR Record\"\n\n(cid:190) Tool Response:\n\n{\"object\":\"list\",\"results\":[...]}\n\n\u02d8 Agent I found the HR Record page. Let me retrieve its contents to see the structure and any\nexisting database.\n\n(cid:229) notion API-get-block-children\n\n\"block id\": \"xxxxx\"\n\n(cid:190) Tool Response:\n\n{\"object\":\"list\",\"results\":[...]}\n\n\u02d8 Agent I can see there are three databases in the HR Record page:\n\n1. Job Positions (block ID: xx-yy-zz)\n\n2. Job Application Timeline (block ID: yy-zz-xx)\n\n3. Candidates (block ID: zz-xx-yy)\n\nLet me first examine the Job Positions database to understand what positions are available, and then\nthe Candidates database to see its structure.\n\n(cid:229) notion API-retrieve-a-database\n\n\"database id\": \"xx-yy-zz\"\n\n(cid:190) Tool Response:\n\n{\"object\":\"database\",\"id\":\"xx-yy-zz\",...}\n\n(cid:229) notion API-post-database-query\n\n\"database id\": \"xx-yy-zz\"\n\n(cid:190) Tool Response:\n\n{\"object\":\"list\",\"results\":[...]}",
        "char_count": 1834,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 53,
        "paper_id": "2510.25726v1",
        "text": "(cid:229) notion API-post-database-query\n\n\"database id\": \"xx-yy-zz\"\n\n(cid:190) Tool Response:\n\n{\"object\":\"list\",\"results\":[...]}\n\n\u02d8 Agent Now let me examine the Candidates database structure:\n\n(cid:229) notion API-retrieve-a-database\n\n\"database id\": \"zz-xx-yy\"\n\n32\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\n{\"object\":\"database\",\"id\":\"zz-xx-yy\",...}\n\n(cid:229) notion API-post-database-query\n\n\"database id\": \"zz-xx-yy\"\n\n(cid:190) Tool Response:\n\n{\"object\":\"list\",\"results\":[...]}\n\n\u02d8 Agent Now I have a clear understanding of the situation:Job Positions with available headcount\n(Head Counts > 0):\n\n\u2022 Advanced Investment Banking Analyst: 3 positions\n\n\u2022 Market Risk Specialist: 4 positions\n\n\u2022 ...\n\nExisting sample candidates and their applied positions:\n\n\u2022 Alice Jackson \u2192 Financial Risk Analyst (2 positions available) \u2713\n\u2022 Debra Smith \u2192 Securities Trader (0 positions available) \u00d7\n\u2022 Michael Diaz \u2192 Investment Analyst (2 positions available) \u2713\n\u2022 ...\n\nI need to:\n\n1. Delete all existing sample entries\n\n2. Read all the resume PDFs to extract candidate information\n\n3. Add new entries based on actual resumes\n\n4. Send rejection emails to candidates applying for positions with 0 headcount\n\nLet me start by reading all the resume PDFs:\n\n(cid:229) pdf-tools read pdf pages\n\n\"pdf file path\": \"workspace/cvs/alice jackson resume.pdf\", \"start page\": 1,\n\"end page\": 10\n\n(cid:190) Tool Response:\n\nPDF total pages: 3\nExtracted page range: 1-3\n\n=== Page 1 ===\nALICE JACKSON\n**Applied Position: Financial Risk Analyst **\nCONTACT INFORMATION\n...\n\nRead the resumes of the other candidates in sequence ......\n\n33\n\n\fToolathlon\n\n\u02d8 Agent Now I have all the resume information. Let me analyze what I found:\nCandidates from resumes and their applied positions:\n\n\u2022 Alice Jackson \u2013 Applied: Financial Risk Analyst (2 positions available) \u2713\n\nEducation: Master of Finance, Columbia Business School; Bachelor Economics, University\nof Pennsylvania\nEmail: alice jackson38@mcp.com",
        "char_count": 1947,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 54,
        "paper_id": "2510.25726v1",
        "text": "\u2022 Alice Jackson \u2013 Applied: Financial Risk Analyst (2 positions available) \u2713\n\nEducation: Master of Finance, Columbia Business School; Bachelor Economics, University\nof Pennsylvania\nEmail: alice jackson38@mcp.com\n\n\u2022 Angela Moore \u2013 Applied: Securities Trader (0 positions available) \u00d7\n\nEducation: Master of Science in Computational Finance, Carnegie Mellon University\nEmail: angela moore89@mcp.com\n\n\u2022 ...\n\nNow I need to:\n\n1. Delete all existing sample candidates\n\n2. Add the real candidates from resumes\n\n3. Send rejection emails to candidates applying for positions with 0 headcount\n\nLet me start by deleting the existing sample candidates:\n\n(cid:229) notion API-update-a-block\n\n\"block id\": \"xy-yz-zx\", \"archived\": true\n\n(cid:190) Tool Response:\n\n\"status\":400,\"object\":\"error\",\"code\":\"validation error\",...\n\n\u02d8 Agent I need to use the correct API to delete pages from the database. Let me delete them one\nby one:\n\n(cid:229) notion API-patch-page\n\n\"page id\": \"xy-yz-zx\", \"archived\": true\n\n(cid:190) Tool Response:\n\n{\"object\":\"page\",\"id\":\"xy-yz-zx\",...}\n\nDelete others one by one ......\n\n\u02d8 Agent Great! I\u2019ve deleted all the existing sample candidates. Now I need to create new pages for\nthe actual candidates based on their resumes. Let me start creating them one by one:\n\n(cid:229) notion API-post-page\n\n\"parent\": {\"database id\":\"zz-xx-yy\"},\n\"properties\": {\n\n\"title\":[{\"text\":{\"content\":\"Alice Jackson\"}}], \"type\":\"title\"\n\n}\n\n(cid:190) Tool Response:\n\n{\"status\":400,\"object\":\"error\",\"code\":\"validation error\",...}\n\n34\n\n\fToolathlon\n\n\u02d8 Agent I need to use the correct format for creating pages in a database. Let me check the database\nstructure again and create the pages properly:\n\n(cid:229) notion API-post-page\n\n{\n\n\"parent\": {\"database id\":\"zz-xx-yy\"},\n\"properties\": {\"Name\":{\"title\":[{\"text\":{\"content\":\"Alice Jackson\"}}]},\n\"Email\":{\"rich text\":[{\"text\":{\"content\":\"alice jackson38@mcp.com\"}}]},\n\"Applied\nAnalyst\"}}]},\n\nPosition\":{\"rich text\":[{\"text\":{\"content\":\"Financial\n\nRisk",
        "char_count": 1977,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      },
      {
        "chunk_id": 55,
        "paper_id": "2510.25726v1",
        "text": "Position\":{\"rich text\":[{\"text\":{\"content\":\"Financial\n\nRisk\n\n\"School\":{\"rich text\":[{\"text\":{\"content\":\"Columbia Business School\"}}]},\n\"Highest Degree\":{\"select\":{\"name\":\"master\"}}}\n\n}\n\n(cid:190) Tool Response:\n\n{\"object\":\"page\",\"id\":\"yx-zy-xz\",...}\n\n\u02d8 Agent Great! Now let me add the remaining candidates:\n\nAdd the remaining candidates ......\n\n\u02d8 Agent Perfect! I\u2019ve successfully added all 10 candidates from the resumes to the Notion database.\nNow I need to send rejection emails to candidates who applied for positions that are currently not\nopen for recruitment (positions with 0 headcount).Based on my analysis, the candidates who need\nrejection emails are:\n\n\u2022 Angela Moore (Securities Trader - 0 positions)\n\n\u2022 Debra Smith (Securities Trader - 0 positions)\n\n\u2022 Emily James (Securities Trader - 0 positions)\n\n\u2022 Martha Morales (Portfolio Manager - 0 positions)\n\nLet me send the rejection emails:\n\n(cid:229) emails send email\n\n\"to\": \"angela moore89@mcp.com\",\n\"subject\": \"Application Status Update - Securities Trader Position\",\n\"body\":...\n\n(cid:190) Tool Response:\n\nEmail sent successfully to angela moore89@mcp.com\n\nSend other rejection emails ......\n\n\u02d8 Agent Excellent! I have successfully completed all the required tasks: ...\n\n(cid:229) claim done\n\nNone\n\n35\n\n\fToolathlon\n\n(cid:190) Tool Response:\n\nyou have claimed the task is done!\n\n36",
        "char_count": 1340,
        "metadata": {
          "paper_id": "2510.25726v1"
        }
      }
    ]
  },
  {
    "paper_id": "2510.25729v1",
    "original_length": 57533,
    "num_chunks": 34,
    "chunks": [
      {
        "chunk_id": 0,
        "paper_id": "2510.25729v1",
        "text": "Physics-Guided Conditional Diffusion Networks for\nMicrowave Image Reconstruction\nShirin Chehelgami, Joe LoVetri, and Vahab Khoshdel\u2217\n\nDepartment of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Manitoba, Canada\n\u2217Corresponding author: vahab.khoshdel@umanitoba.ca\n\n5\n2\n0\n2\n\nt\nc\nO\n9\n2\n\n]\n\nV\n\nI\n.\ns\ns\ne\ne\n[\n\n1\nv\n9\n2\n7\n5\n2\n.\n0\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nAbstract\u2014A conditional latent-diffusion based framework for\nsolving the electromagnetic inverse scattering problem associ-\nated with microwave imaging is introduced. This generative\nmachine-learning model explicitly mirrors the non-uniqueness\nof the ill-posed inverse problem. Unlike existing inverse solvers\nutilizing deterministic machine learning techniques that produce\na single reconstruction,\nthe proposed latent-diffusion model\ngenerates multiple plausible permittivity maps conditioned on\nmeasured scattered-field data, thereby generating several po-\ntential instances in the range-space of the non-unique inverse\nmapping. A forward electromagnetic solver is integrated into the\nreconstruction pipeline as a physics-based evaluation mechanism.\nThe space of candidate reconstructions form a distribution\nof possibilities consistent with the conditioning data and the\nmember of this space yielding the lowest scattered-field data\ndiscrepancy between the predicted and measured scattered fields\nis reported as the final solution. Synthetic and experimental\nlabeled datasets are used for training and evaluation of the\nmodel. An innovative labeled synthetic dataset is created that\nexemplifies a varied set of scattering features. Training of the\nmodel using this new dataset produces high-quality permittivity\nreconstructions achieving improved generalization with excellent\nfidelity to shape recognition. The results highlight the potential of\nhybrid generative\u2013physics frameworks as a promising direction\nfor robust, data-driven microwave imaging.\n\nIndex Terms\u2014Conditional Diffusion Model, Physics-Guided\nGenerative Model, Microwave Imaging, Electromagnetic Inverse\nScattering, Inverse Problems.\n\nI. INTRODUCTION\n\nQ UANTITATIVE Microwave Imaging (MWI) is a non-",
        "char_count": 2134,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 1,
        "paper_id": "2510.25729v1",
        "text": "I. INTRODUCTION\n\nQ UANTITATIVE Microwave Imaging (MWI) is a non-\n\ninvasive technique with applications in a wide range\nof areas such as medical diagnostics, and non-destructive\ntesting in agricultural and industrial settings [1]\u2013[3]. At its\ncore, MWI is a wavefield modality that has associated with\nit an electromagnetic inverse scattering problem (ISCP). In\nan electromagnetic ISCP the objective is to quantitatively\nreconstruct a map of the dielectric properties, and potentially\nthe dielectric loss, of the inaccessible interior of an object or\nregion of interest (OI/ROI). The data used for this reconstruc-\ntion are the measured scattered fields when the target OI/ROI is\ninterrogated by a known, and usually controllable, impinging\nincident field.\n\nThe electromagnetic ISCP has associated with it an ill-\nposed wavefield inverse source problem (ISP), that in addition\nto being highly sensitive to noise in the measurement data,\nmeans that more than one permittivity map will produce the\nthe solution to the ISP is non-\nsame scattered field,\nunique (see, e.g., [4], [5]). This makes accurate reconstructions\n\ni.e.,\n\nparticularly challenging and usually requires the augmentation\nof the acquired scattered-field data with some sort of prior\ninformation, e.g., the use of regularization methods [4], [6].\nA question arises as to whether the non-uniqueness of the\nill-posed wavefield inversion problem can be mirrored in an\nalgorithmic procedure to some benefit, as it has, for example,\nin the formulation of design problems as an inverse problem\n[7]\u2013[11].",
        "char_count": 1559,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 2,
        "paper_id": "2510.25729v1",
        "text": "Traditional algorithms for the ISCP, such as Contrast Source\nInversion (CSI) [12] and Gauss-Newton Inversion [13], [14],\nrely on iterative optimization. These approaches typically\nincorporate a physically rigorous model of the data-acquisition\nsystem and discretization of the permittivity map of the target\nbeing imaged [15] makes these techniques computationally\nexpensive. Even with the use of calibration techniques that\nlessen demands on accurate models of the data-acquisition\nsystem, these optimization-based inversion methods require\nmany iterations of a forward solver before converging. In\naddition, the non-uniqueness of the underlying ISCP means\nthat they don\u2019t always converge to the true permittivity map\nfor the OI/ROI.\n\nIn recent years, machine learning\u2014particularly deep learn-\ning\u2014has gained significant\ntraction for MWI [16]. Deep\nlearning methods have been applied across nearly all stages\nof the imaging pipeline, including calibration, post-processing,\nimage enhancement, and inverse problem solving [17]. Among\nthese tasks, addressing the associated ISCP is clearly the\nmost challenging but can have the greatest impact. Supervised\nlearning approaches attempt to bypass iterative optimization\nby directly mapping scattered-field measurements to target\nreconstructions using large datasets of measurement/target\npairs [18]\u2013[20]. Studies have shown that deep networks can\nachieve high-quality reconstructions and even enable real-\ntime imaging [20], particularly when combined with physics-\ninformed pre-processing such as the Born approximation or\nelectromagnetic backpropagation [21]\u2013[23].\n\nA critical limitation of both traditional inversion techniques\nand deterministic Machine-Learning (ML) frameworks is that,\nby definition, they output a single reconstruction for a given in-\nput, neglecting the fundamental non-uniqueness of the electro-\nmagnetic inverse problem. In reality, multiple plausible target\npermittivity maps may correspond to the same measurement\ndata [4], [5]. In deterministic machine-learning frameworks\nthis one-to-one map is ultimately created by the training data\nset and thereby limits the trained ML model\u2019s generalizability\nto making predictions on scattered-field data corresponding\n\n \n \n \n \n \n \n\fto unseen targets that are too far away from the training data.\nThat is, it is widely recognized that such ML models are highly\nsensitive to \u201cdomain gaps\u201d [24].",
        "char_count": 2408,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 3,
        "paper_id": "2510.25729v1",
        "text": "to unseen targets that are too far away from the training data.\nThat is, it is widely recognized that such ML models are highly\nsensitive to \u201cdomain gaps\u201d [24].\n\nIn addition to the deterministic nature of many supervised\nML models, there is the related issue that these approaches\nrequire large datasets for training. To address this, researchers\nhave employed generative models such as Generative Adver-\nsarial Networks (GANs) and diffusion models to create large\nlabeled datasets for subsequent use in supervised learning [25].\nFor instance, Shao et al. [26] combined a GAN with a\ndeep neural network to approximate electromagnetic scattering\nin microwave breast imaging, thereby generating additional\ntraining data.\n\nAlthough synthetic datasets are convenient, they often fail\nto capture practical complexities such as antenna coupling,\ncalibration errors, and hardware imperfections [27]. Therefore,\nalthough large synthetically generated datasets have indeed\nbeen used for training, resulting ML models typically require\neither augmenting the training set with experimental data or\n\u201ccalibrating out\u201d the experimental setup to make the data more\nrepresentative of synthetic conditions. Otherwise, networks\ntrained exclusively on synthetic data often struggle to general-\nize to experimental measurements acquired using unique lab-\nspecific systems. It should be commented that currently there\ndoes not exist a standardized MWI data-acquisition setup\u2013\nmost researchers have developed there own unique systems.\n\nThus, the gap between synthetic training data and exper-\nimental measurements remains a major barrier to deploying\ngeneralizable deep learning\u2013based MWI systems. On the other\nhand, generative approaches are inherently less sensitive to\ndomain discrepancies, as they learn the underlying data man-\nifold rather than a deterministic mapping, thereby mitigating\nthe impact of distributional shifts between training and testing\ndatasets.",
        "char_count": 1947,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 4,
        "paper_id": "2510.25729v1",
        "text": "Most recently, researchers have leveraged the inherent non-\nuniqueness of generative models to tackle the non-unique\nnature of inverse problems. Early efforts predominantly em-\nployed GANs and VAEs. For example, Bhadra et al. intro-\nduced an image-adaptive GAN framework that allows high-\nfidelity reconstruction of under-sampled MRI data and im-\nproved data consistency in ill-posed inverse imaging prob-\nlems [28]. Similarly, Yao et al. [29] introduced a conditional\nGAN framework to map scattered electromagnetic fields di-\nrectly to dielectric contrasts, enabling real-time image recon-\nstruction. Despite these advancements, GAN-based approaches\noften suffer from training instabilities, mode collapse, and\nlimited control over output diversity.\n\nTo overcome these limitations, diffusion and score-based\ngenerative models have emerged as more robust, theoretically\ngrounded alternatives. These models have demonstrated state-\nof-the-art performance across a broad range of imaging tasks,\nincluding synthesis, reconstruction, and enhancement [30]\u2013\n[32]. Compared with GANs and VAEs, diffusion-based ap-\nproaches exhibit stable optimization dynamics, provide well-\ndefined likelihood formulations, and yield superior generative\nfidelity. Accumulating evidence indicates that they consistently\nsurpass earlier generative paradigms in various medical imag-\ning applications [30], [31], [33], [34].\n\nIn the domain of medical image reconstruction, Song et\nal. [34] proposed a score-based generative framework capa-\nble of reconstructing medical images from partial CT and\nMRI measurements in an unsupervised manner, achieving\nstrong generalization across diverse measurement processes.\nNonetheless, applications of such models as inverse solvers for\nMWI remain limited. Recently, Bi et al. [35] introduced Diffu-\nsionEMIS, a diffusion-based method that iteratively refines 3-\nD point clouds to reconstruct scatterer geometries conditioned\non measured scattered fields.",
        "char_count": 1967,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 5,
        "paper_id": "2510.25729v1",
        "text": "Building on these advances, we propose a conditional\ndiffusion-based generative model that explicitly incorporates\nthe non-unique nature of the microwave inverse scattering\nproblem. Unlike deterministic supervised networks that pro-\nduce a single estimate for a given set of measurements,\nour framework generates multiple plausible reconstructions\nconsistent with the same data, thereby reflecting the inherent\nnon-uniqueness of the ill-posed problem and leveraging the\nstrengths of diffusion models to produce physically mean-\ningful solutions. The core innovation of our approach lies\nin the integration of a physics-based selection mechanism,\ntransforming the framework into a physics-informed gener-\native system. After the diffusion model produces multiple\ncandidate reconstructions, a forward electromagnetic solver\nis applied to each candidate to predict\nthe corresponding\nscattered fields. The reconstruction yielding the lowest data\ndiscrepancy with respect to the measured fields is reported as\nthe final solution. This physics-guided validation ensures that\nthe chosen reconstruction is not only statistically plausible,\nbased on the conditioning data, but also physically consistent\nwith the underlying electromagnetic principles.\n\nThis integration of a forward solver marks a departure from\npurely data-driven approaches, forming a hybrid architecture\nthat combines the generative flexibility of diffusion models\nwith the physical accuracy of electromagnetic modeling. By\nembedding physics-based validation directly within the infer-\nence process, our method effectively bridges the gap between\nmachine learning efficiency and the physics of electromagnetic\ninteractions with the target, addressing one of the major\nlimitations of deep learning-based inverse solvers.\n\nThe proposed methodology is particularly well-suited for\ndata collected using actual MWI systems, where measurement\nnoise, calibration errors, and model mismatches often degrade\nperformance. By generating multiple candidate solutions and\nselecting the most physically consistent one, our approach\nachieves enhanced robustness and generalization. We validate\nthe method using both synthetic and experimental datasets,\ndemonstrating improved reconstruction accuracy and stability\ncompared to deterministic baselines.",
        "char_count": 2296,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 6,
        "paper_id": "2510.25729v1",
        "text": "To address the challenges outlined above, the remainder\nof this paper is organized as follows. Section II provides\nan overview of the methodology and fundamental concepts\nbehind diffusion models, including the forward and reverse\nprocesses and the use of diffusion priors for inverse problems.\nSection III introduces the proposed latent diffusion framework,\nwhich integrates a latent autoencoder representation, a physics-\naware conditioning mechanism, and an end-to-end inversion\narchitecture augmented by a physics-based selection strategy.\n\n\fSection IV describes the dataset used for training and evalua-\ntion, followed by Section V, which presents the experimental\nresults, including reconstruction consistency on synthetic data,\ngeneralization to experimental measurements, multi-frequency\ninversion, and performance evaluation of model error. Finally,\nSection VI concludes the paper with key findings and future\ndirections, and Section VII acknowledges supporting contri-\nbutions.\n\nII. METHODOLOGY\n\nA. Diffusion Models\n\nForward and reverse processes.: Diffusion models aim\nto approximate a target data distribution by constructing a\nforward process that incrementally adds Gaussian noise and\na reverse process that learns to remove this noise. Given a\nclean sample x0 \u223c pdata, the forward Markov chain {xt}T\nevolves as\nxt = (cid:112)1 \u2212 \u03b2t xt\u22121+(cid:112)\u03b2t wt, wt\nwhere the noise schedule {\u03b2t} governs how quickly the\nsignal is corrupted. As t increases, xt approaches an isotropic\nGaussian.\n\ni.i.d.\u223c N (0, I),\n\n1 \u2264 t \u2264 T,\n\nt=1\n\nGeneration proceeds by starting from a Gaussian xrev\n\nT \u223c\n\nN (0, I) and iteratively denoising back to xrev\n0 :\nT \u22121 \u2212\u2192 \u00b7 \u00b7 \u00b7 \u2212\u2192 xrev\n0 .\n\nT \u2212\u2192 xrev\nxrev\n\nSampling algorithms for this reverse chain\u2014deterministic or\nstochastic\u2014can be interpreted as discretizations of underlying\nODEs or SDEs. They rely on the score function s\u22c6\nt (x) =\n\u2207x log pxt(x) of the intermediate distributions. Tweedie\u2019s for-\nmula provides an explicit expression,\n\ns\u22c6\nt (x) = \u2212\n\n(cid:90)\n\n1\n1 \u2212 \u03b1t",
        "char_count": 2004,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 7,
        "paper_id": "2510.25729v1",
        "text": "s\u22c6\nt (x) = \u2212\n\n(cid:90)\n\n1\n1 \u2212 \u03b1t\n\npX0|Xt(x0 | x) (cid:0)x \u2212\n\n\u221a\n\n\u03b1t x0\n\n(cid:1) dx0,\n\n:= 1 \u2212 \u03b2t and \u03b1t\n\nwith \u03b1t\nk=1 \u03b1k. In practice, one\ntrains a neural network to approximate these score functions\nvia score matching, enabling the learned model to generate\nsamples from pdata.\n\n:= (cid:81)t\n\nDiffusion priors for inverse problems.: The flexibility of\ndiffusion models makes them attractive as priors for ill-posed\ninverse problems. Suppose we observe data y generated from\nan unknown signal x\u22c6 through a known forward operator\nand additive noise. A Bayesian formulation samples from the\nposterior\n\np(x | y) \u221d pprior(x) p(cid:0)y | x(cid:1),\n\nwhere pprior(x) is the diffusion-model prior and p(y | x) is the\nlikelihood. Sampling from this posterior combines the learned\nscore functions with the physics of the measurement process.\n\nB. Proposed Latent Diffusion Model\n\nWe consider the electromagnetic inverse problem of re-\nconstructing two-dimensional permittivity distributions from\nscattered field measurements. The scattered fields are repre-\nsented as two channels, corresponding to the real and imagi-\nnary components of the electric field. To overcome the non-\nuniqueness of the inverse problem, we design a generative\n\ninversion framework based on a conditional diffusion model\noperating in a compact latent space of permittivity maps. This\nframework produces multiple plausible reconstructions that are\ndriven by, i.e., consistent with, the measured fields, after which\na post-processing step is applied to identify the most physically\nmeaningful solution.\n\n1) Latent Representation with Autoencoder: Directly ap-\nplying a diffusion model to full-resolution permittivity maps\nis computationally expensive, particularly when extending this\nwork to 3D medical imaging applications, which represent the\nultimate goal of our research. To address this, we first train\nan autoencoder (AE) to learn a compact latent representation,\nfollowing a design similar to the one used in [36].",
        "char_count": 1982,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 8,
        "paper_id": "2510.25729v1",
        "text": "\u2022 Encoder (E): compresses the permittivity grid x into a\nlatent representation z = E(x) using convolution and\ndownsampling layers.\n\n\u2022 Decoder (D): reconstructs the grid from the latent code\nas \u02dcx = D(z) = D(E(x)) using upsampling operations.\n\nThe AE is trained with a composite loss function:\n\u2022 Reconstruction loss (pixelwise error) ensures quantitative\n\nfidelity to the input.\n\n\u2022 Perceptual loss [37], computed from intermediate features\nof a pretrained VGG16 network [38], encourages preser-\nvation of edges and structural features.\n\n\u2022 Adversarial loss [39] penalizes overly smooth reconstruc-\n\ntions and promotes realistic textural detail.\n\nThe relative weights of these terms are tuned to balance\nnumerical accuracy with perceptual quality. By compressing a\n100\u00d7100 grid into a 16\u00d724\u00d724 latent vector, the AE reduces\nthe computational cost of the subsequent diffusion process.\n\n2) Physics-Aware Conditioning Mechanism: The learned\nlatent space serves as the domain for our diffusion model. The\nmodel is conditioned on measured scattered fields collected in\nthe scenario depicted in Fig. xxxx where a dielectric target is\nlocated at the center of a ring of transmitter and receivers. The\ntransmitters and receivers are located at equidistant points on\na circle of radius XXX m surrounding the target. A complete\ndescriprion can be found in [40]. This condition ensures that\nthe generated permittivity maps are physically consistent with\nthe observations. The data is composed of a 24\u00d724 scattered-\nfield matrix of complex-valued data representing the real and\nimaginary parts of the received frequency-domain phasor. Data\nconsists of up to five frequencies collected at 3.0, 3.5, 4.0, 4.5,\nand 5.0 GHz.\n\n\u2022 Forward diffusion: Gaussian noise is added to latent\n\nvectors over a predefined schedule.\n\n\u2022 Reverse denoising: A U-Net\u2013based denoiser is trained\nto iteratively predict and remove this noise. The denoiser\nis conditioned on the scattered fields via cross-attention\nlayers.",
        "char_count": 1980,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 9,
        "paper_id": "2510.25729v1",
        "text": "\u2022 Reverse denoising: A U-Net\u2013based denoiser is trained\nto iteratively predict and remove this noise. The denoiser\nis conditioned on the scattered fields via cross-attention\nlayers.\n\n\u2022 Conditioning mechanism: In the proposed framework,\nthe scattered electromagnetic fields are initially processed\nthrough convolutional layers to project them into a feature\nspace. Because electromagnetic inverse problems depend\non the spatial arrangement of transmitters, receivers,\ninforma-\nand objects,\ntion\u2014something that standard convolution and pooling\n\nto retain positional\n\nis vital\n\nit\n\n\f(a) Training the model\n\nFig. 1: Overview of the proposed framework: (a) training phase of the denoising diffusion model; (b) inference phase for\nreconstructing the output from measured fields.\n\n(b) Inference\n\nlayers tend to lose. To address this, we augment the 24\u00d724\nscattered-field inputs with sinusoidal positional encodings\nthat explicitly encode transmitters coordinates. Injecting\nthese encodings into the network provides an absolute\nreference frame, enabling the model to distinguish sensor\nlocations and object orientations. This spatial awareness\nis especially important for microwave imaging, where the\ngeometry of the scene strongly influences the observed\nscattering. These features are then injected back into\nthe diffusion model\u2019s denoiser at every step: they are\nconcatenated with intermediate feature maps and linked\nvia cross-attention blocks so that the denoising operation\nremains conditioned on the measured electromagnetic\nresponse. By coupling the generative model to the ob-\nserved physics in this way, we encourage it to produce\nreconstructions that are physically consistent with the\nactual scattered fields.\n\n\u2022 Architecture: The U-Net consists of downsampling and\nupsampling blocks with intermediate attention [41]. Time\nembeddings indicate the current noise level, while pro-\njected scattered field features provide physical guidance.\n\nThe denoiser is trained using a Mean-Squared-Error (MSE)\nbetween the true and predicted noise. This objective allows\nthe network to implicitly learn the conditional distribution of\nlatent permittivity representations.\n\n3) End-to-End Inversion Framework: The full reconstruc-\n\ntion pipeline proceeds as follows:\n\n\u2022 Encoding: The AE encoder maps the permittivity grid to\n\na latent representation.",
        "char_count": 2337,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 10,
        "paper_id": "2510.25729v1",
        "text": "3) End-to-End Inversion Framework: The full reconstruc-\n\ntion pipeline proceeds as follows:\n\n\u2022 Encoding: The AE encoder maps the permittivity grid to\n\na latent representation.\n\n\u2022 Diffusion training: The conditional diffusion model\nlearns to denoise noisy latent codes, conditioned on\nscattered fields.\n\n\u2022 Sampling: For unseen measurements,\n\nthe diffusion\nmodel generates latent permittivity representations driven\nby the observed fields.\n\n\u2022 Decoding: The AE decoder\nresolution permittivity grid.\n\nreconstructs\n\nthe full-\n\nThis framework, illustrated in Fig. 1, combines the represen-\ntational efficiency of autoencoders with the generative power\nof diffusion models, resulting in a distribution of generated\nhigh-quality reconstructions of permittivity maps from elec-\ntromagnetic data.\n\n4) Physic Based Selection: The conditional diffusion\nmodel produces a set of reconstructions for one scattered field\ndata that are broadly consistent with the measured scattered\nfields, mirroring the non-uniqueness of the ill-posed inverse\nproblem. That is, after training, the diffusion model is used\nto make several inferences for each measurement, generating\na set of candidate permittivity maps. To choose between\nthe reconstructed candidates, we introduce a post-processing\nstage designed to select the most physically meaningful re-\n\n\fconstruction. Specifically, each candidate is passed through\na forward solver to compute the scattered fields it would\nproduce, illustrated in part b of Fig. 1. We compute the MSE\nbetween the simulated and measured scattered fields, choosing\nthe candidate permittivity map that produces the minimum\nscattered-field MSE, providing a global measure of accuracy.\nBy relying exclusively on scattered-field MSE we do not use\nany prior information regarding the actual permittivity map.\n\nIn summary, the conditional diffusion model ensures that\nthe candidate permittivity maps appear physically reasonable,\nthe\nand the forward-solver selection process ensures that\ncorresponding scattered fields closely match the experimen-\ntal measurements. This paradigm opens the door for post-\nprocessing selection procedures based on different criteria\nand/or other available prior information, e.g, smoothness of\nthe permitivity map.\n\nC. Dataset",
        "char_count": 2258,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 11,
        "paper_id": "2510.25729v1",
        "text": "C. Dataset\n\nTwo datasets are used in this study. The first dataset,\nDataSet1, is the publicly available benchmark presented by\nCathers et al. [42], which includes both synthetic and exper-\nimental measurements. It contains scattering data for Nylon-\n66 cylinders with diameters of 3.8 cm and 10.2 cm (reported\npermittivity: \u03f5r = 3.03 \u2212 j0.03) and a complex-shaped object\nreferred to as the E-phantom, machined from an ultra-high\nmolecular weight polyethylene (UHMWP) block (reported\npermittivity: \u03f5r = 2.3 [43]). The cylinders were translated\nspatially to produce multiple configurations within the imaging\ndomain, while the E-phantom was both translated and rotated\nto generate a diverse set of measurement data. For each target,\nthe dataset provides both synthetic scattered fields, computed\nusing a 2D scalar Method-of-Moments (MoM) forward solver\nbased on Richmond\u2019s method [44] on a 100 \u00d7 100 permittivity\ngrid, and calibrated experimental measurements acquired un-\nder comparable conditions [40], [45]. Representative examples\nof the synthetic and experimental data for each target are\nillustrated in first column of Figure 2.",
        "char_count": 1132,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 12,
        "paper_id": "2510.25729v1",
        "text": "Although DataSet1, obtained from [42], [45], provides valu-\nable benchmarking data, it includes a limited number targets\ndisplaying a restricted class of scattering features. This limits\none\u2019s ability to properly investigate and evaluate an machine-\nlearning model\u2019s robustness and generalizability as an inverse\nsolver. To address this limitation and assess the robustness\nof the proposed inverse solver, we created a new dataset\ncomprising 2000 samples of dielectric targets (DataSet2). The\nscattered fields were generated under ideal 2D point-source\nillumination (3D line sources) using the same 2D scalar MoM\nforward solver on a 100 \u00d7 100 permittivity grid. This dataset\nintroduces composite dielectric targets made up of random\nconfigurations of basic canonical dielectric shapes. The canon-\nical dielectric shapes are circles of varying sizes, hollow\ncircles, and U-shapes of different dimensions. Although the\npermittivity of each is uniform across the shape, one of two\npossible values of permittivity are chosen, either \u03b5r = 2.3 or\n\u03b5r = 3.03, for any particular shape. Each canonical shape\nintroduces challenging scattering features of its own, e.g.,\nthe hollow circles introduces the feature of penetrability of\n\nFig. 2: Evaluation of the proposed model using single fre-\nquency synthetic measurements for three representative sam-\nples from each category. The best reconstruction is selected\nfrom 100 generated candidates using the proposed physics-\nguided selection mechanism.\n\nenergy, and multiple scattering from within the target. The\nU-shaped targets introduce direction-based, or anisotropic,\nscattering. Both have stronger frequency-dependent scattering\nfeatures than the simple solid circles of DataSet1. Although\nthe E-phantom of DataSet1 is approximately a combination of\noverlapping canonical U-shapes, in the E-phantom these are\nfixed with respect to their relative positions.",
        "char_count": 1899,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 13,
        "paper_id": "2510.25729v1",
        "text": "The positions, sizes, and orientations of the canonical shapes\nwere randomly varied to create diverse spatial configurations.\nIn addition, each composite target was created using up to\nfour canonical shapes positioned at random within the grid.\nThis produces random overlapping intersections of the shapes,\nfurther increasing the overall geometric complexity of the final\ncomposite scatterer. Some representative composite targets in\nDataSet2 are depicted in Figs. 6, 7 and 8. These two datasets\nenable a comprehensive variation of scattered fields allowing\nus to evaluate the performance of the proposed ML model.\n\nIII. RESULTS\n\nA. Reconstruction Consistency on Synthetic Data\n\nThe latent diffusion model was first\n\ntrained using only\nsynthetic data from DataSet1 and subsequently evaluated also\nusing only unseen synthetic samples that differed in position\nand orientation from those in the training set. Figure 3 presents\nfour independent reconstructions conditioned on scattered-\nfield data at 5 GHz from a single instance of the E-phantom.\nEach of the four candidate solutions shown was generated\nfrom a distinct random noise initialization of the diffusion\nmodel. As illustrated, the model consistently reproduces the\ntarget permittivity distribution with minimal reconstruction",
        "char_count": 1284,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 14,
        "paper_id": "2510.25729v1",
        "text": "020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 9.8191e-02Perm MSE: 2.7776e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 5.4958e-02Perm MSE: 2.4480e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 5.0247e-02Perm MSE: 4.2363e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012\ferror, demonstrating its stability and robustness to stochastic\nvariation in the sampling process.\n\nTo identify the best reconstruction,\n\nthe diffusion model\nwas used to generated 100 candidate permittivity maps, each\nevaluated using a forward electromagnetic solver to compute\nthe corresponding scattered fields. It should be noted that the\nchoice of 100 reconstructions is arbitrary and can be adjusted\naccording to the time or computational constraints of each\napplication. The reconstruction that was selected as the final\nresult was the one yielding the lowest mean squared error\n(MSE) between the predicted and ground-truth scattered fields.\nAs illustrated in Figure 2 , the selected reconstruction not only\nachieves a low MSE in the scattered field domain but also\nexhibits a low MSE in the permittivity distribution, indicating\nthat the reconstructed permittivity grid closely matches the\nground truth in both material properties and resulting electro-\nmagnetic behavior.The corresponding MSE values for this part\ncan be found in Table I, row 3.",
        "char_count": 1736,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 15,
        "paper_id": "2510.25729v1",
        "text": "B. Generalization to Experimental Data\n\nTo assess the proposed model\u2019s ability to generalize beyond\nsynthetic training data, the trained model was subsequently\nevaluated on experimental scattered-field measurements from\nDataSet1. As shown in Figure 4, the model achieved promising\nreconstruction performance, successfully recovering key struc-\ntural features from previously unseen experimental samples.\nAlthough the reconstructed permittivity maps were less accu-\nrate than those obtained when testing synthetic data, the results\ndemonstrate the model\u2019s capacity to adapt to experimental\nmeasurements, even though it was trained solely on synthetic\ndata.\n\nThis generalization result is notable given the significant\ndifferences between synthetic and experimental measurements,\nparticularly for the near-field electromagnetic imaging system\nfrom which the experimental data was obtained [45]. Near-\nfield systems, especially those utilizing co-resident antennas,\nexhibit field behaviors that can differ significantly from the\nfields generated using, necessarily approximate, computational\nmodels. For example, the synthetic training data was generated\n\nusing the incident fields of idealized point sources (line-soures)\nand the received fields were simply taken to be scattered\nfields at precise point-locations. In contrast, the experimental\ndata was collected as microwave scattering parameters (S-\nparameters) at the antenna ports. An antenna effectively inte-\ngrates the spatially varying fields across its aperture. In addi-\ntion, the mutual-coupling with nonactive antennas is ignored in\nthe synthetically generated data. Despite these discrepancies,\nour ML model was able to deliver strong reconstruction\nperformance. Of course, this is partly due to the scattered-\nfield calibration procedure that was implemented [45], [46].\nIncorporating the calibration procedure into a machine learn-\ning model of its own has been attempted in the past and is a\nresearch subject of the greatest importance.\n\nTABLE I: Quantitative comparison of average MSE values\nbetween reconstructed and ground-truth permittivity maps\n(MSEimage) and average MSE between true scattered-field data\nand corresponding scattered-field data (MSEdata) for various\ntraining and testing conditions.\n\nTest Case\n\nDataset 1\n\nMSEimage MSEdata\n\nCNN based Model Train on Synthetic and\nTest on Synthetic [42]\n\nCNN based Model Train on both Synthetic\nand Experimental / Test Experimental [42]\n\n0.163\n\n0.089\n\nNA\n\nNA",
        "char_count": 2477,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 16,
        "paper_id": "2510.25729v1",
        "text": "MSEimage MSEdata\n\nCNN based Model Train on Synthetic and\nTest on Synthetic [42]\n\nCNN based Model Train on both Synthetic\nand Experimental / Test Experimental [42]\n\n0.163\n\n0.089\n\nNA\n\nNA\n\nProposed model Train on Synthetic / Test\non Synthetic (Single Frequency)\n\n0.0590\n\n0.0848\n\nProposed model Train on Synthetic / Test\nExperimental (Single Frequency)\n\n0.0905\n\n0.0869\n\nProposed model Train on Synthetic / Test\nSynthetic (Multi-Frequency)\n\n0.0334\n\n0.0669\n\nDataset 2\n\nTrain Synthetic / Test Synthetic (Single-Fre)\n\n0.0988\n\nTrain Synthetic / Test Synthetic (Multi-Fre)\n\n0.0399\n\n0.0735\n\n0.0679\n\nFig. 3: Four candidate reconstructions of the same scattered-field data at 5 Ghz generated from different random noise\ninitializations, illustrating the generative and stochastic nature of the diffusion-based model.\n\nGround Truth0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.5\fC. Multi-Frequency Inversion and Dataset Enhancement\n\nResults shown so far were obtained by conditioning the\nML model on single frequency scattered-field data. It is well-\nknown that using data from multiple frequencies, either simul-\ntaneously or via frequency-hopping, can enhance reconstruc-\ntion performance especially for non-dispersive targets [47],\n[48]. To investigate how incorporating multiple frequencies\nimproves the ML model\u2019s ability to learn the relationship\nbetween the scattered-field data and the corresponding per-\nmittivity distribution, we conditioned the diffusion model on\nscattered-field measurements acquired at five distinct frequen-\ncies\u20143.0, 3.5, 4.0, 4.5, and 5.0 GHz\u2014instead of a single\nfrequency.",
        "char_count": 1614,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 17,
        "paper_id": "2510.25729v1",
        "text": "In addition, as previously mentioned, DataSet1 includes a\nlimited range of object types\u2014specifically, solid circularly-\ncylindrical\ntargets and the E-phantom. Each sample in\nDataSet1 consists of a single object, albeit represented across\nmultiple positions and orientations within the imaging grid.\nTo ensure that the ML model was not over-fitted to a narrow\nset of object geometries\u2014a limitation commonly observed\nin prior studies\u2014we developed the more diverse synthetic\ndataset, Dataset2. It provides a more challenging evaluation\nof the model\u2019s generalization ability.\n\nThe model was re-trained using this newly generated dataset\nand tested on previously unseen samples. Representative re-\nconstructions and their corresponding ground-truth targets are\nshown in Fig. 7. As expected, the inverse problem associ-\nated with this more complex, multi-object dataset presents\nincreased difficulty compared to Dataset 1. Nevertheless, the\nmodel successfully reconstructed the underlying permittivity\ndistributions, effectively capturing both the global structure\nand spatial organization of the objects.\n\nTo further enhance the model\u2019s capacity to learn the map-\nping between scattered-field measurements and correspond-\ning permittivity distributions, we investigated the integration\nof multi-frequency data. Specifically, scattered-field measure-\nments were acquired at five frequencies: 3.0, 3.5, 4.0, 4.5, and\n5.0 GHz. Figure 8 illustrates the model\u2019s performance when\nconditioned on multi-frequency data, demonstrating improved\nreconstruction consistency and sharper structural recovery\nacross test samples.\n\nThe results indicate that while the single-frequency model\noccasionally struggles to accurately reconstruct the shapes of\nclosely spaced or sharp-edged objects\u2014sometimes confusing\nU-shaped targets with hollow circles (see rows 2 and 4 of\nFig. 7)\u2014the multi-frequency model performs substantially bet-\nter. By leveraging additional frequency-domain information, it\naccurately resolves fine structural details and preserves object\nboundaries even in complex spatial configurations.\n\nThis improvement is quantitatively reflected in the last two\nrows of Table I, where the average MSEimage values for the\nmulti-frequency case decrease to approximately one-third of\nthe single-frequency error. The averages were computed over\n10 test cases, with each case reconstructed 100 times to select\nthe best reconstruction based on the MSEdata.\nthe potential of",
        "char_count": 2456,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 18,
        "paper_id": "2510.25729v1",
        "text": "incorporating\nricher physical information\u2014beyond frequency diversity\u2014to\nimprove data representation and model robustness. Future\n\nThese findings highlight\n\nFig. 4: Evaluation of the proposed model on single-frequency\nexperimental measurements, where the model was trained ex-\nclusively on synthetic data. The best reconstruction is selected\nfrom 100 generated candidates using the proposed physics-\nguided selection mechanism.\n\nFig. 5: Evaluation of the proposed model on experimental\nmeasurements, where the model was trained exclusively on\nusing multi-frequency synthetic data generated using data from\nfive distinct frequencies\u20143.0, 3.5, 4.0, 4.5, and 5.0 GHz. The\nbest reconstruction is selected from 100 generated candidates\nusing the proposed physics-guided selection mechanism.\n\n020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 1.0479e-01Perm MSE: 8.6991e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 5.2647e-02Perm MSE: 1.2157e-01020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 4.9410e-02Perm MSE: 2.3016e-01020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 8.3886e-02Perm MSE: 2.",
        "char_count": 1589,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 19,
        "paper_id": "2510.25729v1",
        "text": "02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 8.3886e-02Perm MSE: 2.2262e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 2.9979e-02Perm MSE: 2.1566e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 9.9803e-02Perm MSE: 1.9255e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.51.00.50.00.51.01.52.0\fresearch will explore additional labeled datasets encompassing\na more comprehensive set of composite scattering targets to\nfurther enhance the model\u2019s capacity for accurate and stable\ninversion in challenging electromagnetic imaging scenarios.",
        "char_count": 957,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 20,
        "paper_id": "2510.25729v1",
        "text": "00.51.01.52.0\fresearch will explore additional labeled datasets encompassing\na more comprehensive set of composite scattering targets to\nfurther enhance the model\u2019s capacity for accurate and stable\ninversion in challenging electromagnetic imaging scenarios.\n\nWhile the qualitative evaluation demonstrates substantial\nimprovements over both traditional inverse solvers such as\nContrast Source Inversion (CSI) and previously reported\nstate-of-the-art models\u2014specifically, the reconstructed images\npresented in Figures 11\u201314 of [42]\u2014a quantitative assess-\nment was also conducted to provide a more comprehensive\nperformance analysis. This evaluation includes comparisons\nwith the state-of-the-art CNN-based supervised inverse solvers\npreviously reported in the literature [42] and the proposed\ngenerative diffusion-based approach. Specifically, the mean\nsquared error (MSE) was evaluated in both the scattered-\nfield domain (MSEdata) and the image domain (MSEimage),\nwith averages computed over 10 test samples. The results,\nsummarized in Table I, clearly indicate that\nthe proposed\nmethod outperforms state-of-the-art inverse solvers, achieves\nnotable performance gains with multi-frequency data, and\nexhibits enhanced generalization capability when evaluated on\nDataset 2.\n\nD. Performance Evaluation of ML Model\u2019s Error\n\nAs the architecture of the designed ML model is quite\ncomplicated it is important to carefully analyze where the\ninference errors ar occuring. This analysis revealed that the\nreconstruction error originates not only from the diffusion\ncomponent but also from the autoencoder (AE) reconstruc-\ntion stage. The AE-induced error was found to be relatively\nminor when the model was trained on the simpler DataSet1\ncontaining a limited number of object\ntypes, but became\nmore pronounced when trained on the more diverse Dataset2.\nTable II presents a quantitative comparison of AE performance\nacross both datasets, reporting the average MSE computed\nover 100 random samples. As shown, the reconstruction error\nincreases for the more complex dataset, reflecting the greater\ndifficulty of compressing and reconstructing diverse geomet-\nrical structures. As the diffusion model works on the latent\nspace of the AE, it is inevitable that errors in compressing\nthe permittivity maps into the latent space would degrade the\noverall performance.",
        "char_count": 2351,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 21,
        "paper_id": "2510.25729v1",
        "text": "Figure 6 qualitatively illustrates the compression error ob-\ntained when only the AE component of the model is applied\nto both datasets. While improving the AE architecture lies\nbeyond the scope of this study, the results show that the\ncompression-related error (1.39%) accounts for approximately\n25% of the total reconstruction error (3.99%). In an attempt\nto mitigate this effect, the AE performance was optimized\nthrough hyperparameter tuning, particularly by adjusting the\nrelative weighting of loss terms to balance numerical accuracy\nwith perceptual reconstruction quality. The reported results are\nfor performance after this tuning was implemented.\n\nIV. CONCLUSION\n\nThis work presented a physics-informed conditional gen-\nerative framework for solving the electromagnetic inverse\n\nTABLE II: Comparison of AE compression performance av-\neraged over 100 random test samples.\n\nDataset\nDataSet1\nDataSet2\n\nAverage MSE\n0.71%\n1.39%\n\nFig. 6: Comparison of autoencoder\nDataSet1 and DataSet2.\n\nreconstructions\n\nfor\n\nscattering problem in microwave imaging. Leveraging the\ngenerative nature of the diffusion model, the proposed ap-\nproach explicitly captures the inherent uncertainty and non-\nuniqueness of the inverse problem by producing multiple\nplausible reconstructions of the permittivity distribution. A\nforward-solver-based physical evaluation is then employed to\nselect the most consistent reconstruction, ensuring that the\nfinal solution aligns with both the measurement data and the\nunderlying electromagnetic physics.\nThe results demonstrated that\n\nthe model can accurately\nreconstruct complex permittivity distributions from synthetic\ndata, achieving stable and low-error reconstructions. When\napplied to experimental scattered-field measurements\u2014despite\nbeing trained exclusively on synthetic data\u2014the model suc-\ncessfully recovered key structural features, confirming its\nrobustness and generalization capability across measurement\ndomains. The proposed approach outperformed the state-of-\nthe-art deep learning\u2013based inverse solvers reported in [42],\nachieving lower reconstruction error and improved structural\nfidelity in both synthetic and experimental evaluations. In order\nto ensure that our model was not overfitted to a limited number\nof object types\u2014an issue commonly observed in previous\nstudies\u2014we developed a more diverse synthetic dataset. This\nexpanded dataset enabled the model to better generalize to\npreviously unseen geometries.",
        "char_count": 2457,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 22,
        "paper_id": "2510.25729v1",
        "text": "Incorporating multi-frequency scattered-field data further\nimproved reconstruction accuracy, yielding finer structural\ndetails and enhanced consistency across test samples. These\nfindings are consistent with prior literature, underscoring the\nbenefits of multi-frequency illumination in improving stability\nand resolution in microwave imaging.\n\nThe results also revealed an increase in the reconstruction\nerror attributable to the autoencoder (AE) component. Al-\nthough optimizing the compression AE network lies beyond\nthe scope of this study, preliminary hyperparameter tuning and\n\nGround TruthReconstruction  MSE: 0.009963Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.5Ground TruthReconstruction  MSE: 0.017970Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.5\fFig. 7: Evaluation of the proposed model using single fre-\nquency synthetic measurements for several representative sam-\nples from each category\n\nFig. 8: Evaluation of\nthe proposed model using multi-\nfrequency synthetic measurements for several representative\nsamples from each category\n\n020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 7.3766e-02Perm MSE: 1.2631e-01020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 5.3953e-02Perm MSE: 8.2925e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 7.4881e-02Perm MSE: 9.7638e-02020406080020406080Error0.00.51.01.52.",
        "char_count": 1619,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 23,
        "paper_id": "2510.25729v1",
        "text": "4881e-02Perm MSE: 9.7638e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 5.3750e-02Perm MSE: 5.0465e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 6.0107e-02Perm MSE: 9.2093e-02020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 8.1709e-02Perm MSE: 1.4224e-01020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.521012020406080020406080Ground Truth020406080020406080Best ReconstructionScat MSE: 8.1381e-02Perm MSE: 1.3535e-01020406080020406080Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51012020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 7.1261e-02Perm MSE: 5.0692e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.",
        "char_count": 1077,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 24,
        "paper_id": "2510.25729v1",
        "text": "00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 3.9557e-02Perm MSE: 3.5397e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 8.8159e-02Perm MSE: 5.6142e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.52.0020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 4.4760e-02Perm MSE: 3.6711e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 4.7440e-02Perm MSE: 4.5211e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.",
        "char_count": 1038,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 25,
        "paper_id": "2510.25729v1",
        "text": "00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 9.5337e-02Perm MSE: 3.8503e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.52.01.51.00.50.00.51.01.5020406080020406080Ground Truth Permittivity020406080020406080Best Reconstructed PermittivityScat MSE: 7.0237e-02Perm MSE: 3.4904e-02020406080020406080Permittivity Error0.00.51.01.52.02.53.03.50.00.51.01.52.02.53.03.51.00.50.00.51.01.52.0\floss-weight adjustments helped balance numerical accuracy\nand perceptual reconstruction quality.\n\nOverall, the proposed diffusion-based framework demon-\nstrates strong potential for robust, data-driven microwave\nimaging. It achieves superior reconstruction quality relative\nto existing deep learning baselines and exhibits strong cross-\ndomain generalization from synthetic to experimental data. Fu-\nture work will extend this approach to full three-dimensional\nmedical imaging and further refine the underlying AE archi-\ntecture to enhance reconstruction fidelity and computational\nefficiency.\n\nV. ACKNOWLEDGMENT\n\nThe authors acknowledge the help of Mr. Seth Cathers who\n\nsupplied the forward solver.\n\nREFERENCES\n\n[1] N. AlSawaftah, S. El-Abed, S. Dhou, and A. Zakaria, \u201cMicrowave\nimaging for early breast cancer detection: Current state, challenges, and\nfuture directions,\u201d Journal of Imaging, vol. 8, no. 5, 2022. [Online].\nAvailable: https://www.mdpi.com/2313-433X/8/5/123",
        "char_count": 1524,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 26,
        "paper_id": "2510.25729v1",
        "text": "[2] K. Brinker, M. Dvorsky, M. T. Al Qaseer, and R. Zoughi, \u201cReview of\nadvances in microwave and millimetre-wave NDT&E: Principles and\napplications,\u201d Philosophical Transactions of the Royal Society A, vol.\n378, no. 2182, p. 20190585, 2020.\n\n[3] J. LoVetri, M. Asefi, C. Gilmore, and I. Jeffrey, \u201cInnovations in electro-\nmagnetic imaging technology: The stored-grain-monitoring case,\u201d IEEE\nAntennas and Propagation Magazine, vol. 62, no. 5, pp. 33\u201342, 2020.\n[4] D. L. Colton, R. Kress, and R. Kress, Inverse acoustic and electromag-\n\nnetic scattering theory. Springer, 1998, vol. 93.\n\n[5] A. J. Devaney, Mathematical foundations of imaging, tomography and\n\nwavefield inversion. Cambridge University Press, 2012.\n\n[6] P. Mojabi and J. LoVetri, \u201cOverview and classification of some reg-\nularization techniques for the gauss-newton inversion method applied\nto inverse scattering problems,\u201d IEEE Transactions on Antennas and\nPropagation, vol. 57, no. 9, pp. 2658\u20132665, 2009.\n\n[7] O. Bucci, I. Catapano, L. Crocco, and T. Isernia, \u201cSynthesis of new\nvariable dielectric profile antennas via inverse scattering techniques:\na feasibility study,\u201d IEEE Transactions on Antennas and Propagation,\nvol. 53, no. 4, pp. 1287\u20131297, 2005.\n\n[8] M. Salucci, G. Oliveri, and A. Massa, \u201cAn innovative inverse source\napproach for the feasibility-driven design of reflectarrays,\u201d IEEE Trans-\nactions on Antennas and Propagation, vol. 70, no. 7, pp. 5468\u20135480,\n2022.",
        "char_count": 1441,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 27,
        "paper_id": "2510.25729v1",
        "text": "[9] T. Brown, C. Narendra, Y. Vahabzadeh, C. Caloz, and P. Mojabi, \u201cOn\nthe use of electromagnetic inversion for metasurface design,\u201d IEEE\nTransactions on Antennas and Propagation, vol. 68, no. 3, pp. 1812\u2013\n1824, 2020.\n\n[10] R. Palmeri, M. T. Bevacqua, A. F. Morabito, and T. Isernia, \u201cDesign of\nartificial-material-based antennas using inverse scattering techniques,\u201d\nIEEE Transactions on Antennas and Propagation, vol. 66, no. 12, pp.\n7076\u20137090, 2018.\n\n[11] R. Palmeri, A. Y. Ruiz, R. Scapaticci, T. Isernia, and L. Crocco, \u201cDeep\nlearning procedures for the inverse design of electromagnetic devices,\u201d\nin 2024 International Conference on Electromagnetics in Advanced\nApplications (ICEAA), 2024, pp. 682\u2013682.\n\n[12] P. M. Van Den Berg and R. E. Kleinman, \u201cA contrast source inversion\n\nmethod,\u201d Inverse Problems, vol. 13, no. 6, p. 1607, 1997.\n\n[13] J. De Zaeytijd, A. Franchois, C. Eyraud, and J.-M. Geffrin, \u201cFull-wave\nthree-dimensional microwave imaging with a regularized gauss\u2013newton\nmethod\u2014 theory and experiment,\u201d IEEE Transactions on Antennas and\nPropagation, vol. 55, no. 11, pp. 3279\u20133292, 2007.\n\n[14] T. Rub\u00e6k, P. M. Meaney, P. Meincke, and K. D. Paulsen, \u201cNonlinear\nmicrowave imaging for breast-cancer screening using gauss\u2013newton\u2019s\nmethod and the cgls inversion algorithm,\u201d IEEE Transactions on Anten-\nnas and Propagation, vol. 55, no. 8, pp. 2320\u20132331, 2007.",
        "char_count": 1370,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 28,
        "paper_id": "2510.25729v1",
        "text": "[15] K. G. Brown, N. Geddert, M. Asefi, J. LoVetri, and I. Jeffrey, \u201cHybridiz-\nable discontinuous galerkin method contrast source inversion of 2-d and\n3-d dielectric and magnetic targets,\u201d IEEE Transactions on Microwave\nTheory and Techniques, vol. 67, no. 5, pp. 1766\u20131777, 2019.\n\n[16] X. Chen, Z. Wei, L. Maokun, P. Rocca et al., \u201cA review of deep\nlearning approaches for inverse scattering problems (invited review),\u201d\nElectromagnetic Waves, vol. 167, pp. 67\u201381, 2020.\n\n[17] V. Khoshdel, \u201cUsing deep learning approaches for microwave imaging,\u201d\nPh.D. thesis, University of Manitoba, Winnipeg, Manitoba, Canada,\n2021.\n\n[18] G. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and\nR. Willett, \u201cDeep learning techniques for inverse problems in imaging,\u201d\nIEEE Journal on Selected Areas in Information Theory, vol. 1, no. 1,\npp. 39\u201356, May 2020.\n\n[19] V. Khoshdel, M. Asefi, A. Ashraf, and J. LoVetri, \u201cFull 3d\nmicrowave breast imaging using a deep-learning technique,\u201d Journal\nof\n[Online]. Available:\nhttps://www.mdpi.com/2313-433X/6/8/80\n\nImaging, vol. 6, no. 8, p. 80, 2020.\n\n[20] V. Khoshdel, P. Mojabi, and J. LoVetri, \u201cA multi-branch deep learning\narchitecture for microwave-ultrasound breast imaging,\u201d in 2023 General\nAssembly and Scientific Symposium of the International Union of Radio\nScience (URSI GASS).\n\nIEEE, aug 2023, pp. 1\u20134.",
        "char_count": 1350,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 29,
        "paper_id": "2510.25729v1",
        "text": "IEEE, aug 2023, pp. 1\u20134.\n\n[21] W. Shao and Y. Du, \u201cMicrowave imaging by deep learning network:\nFeasibility and training method,\u201d IEEE Transactions on Antennas and\nPropagation, vol. 68, no. 7, pp. 5626\u20135635, Jul 2020.\n\n[22] M. Ambrosanio, S. Franceschini, V. Pascazio, and F. Baselice, \u201cAn end-\nto-end deep learning approach for quantitative microwave breast imaging\nin real-time applications,\u201d Bioengineering (Basel), vol. 9, no. 11, p. 651,\nNov 2022.\n\n[23] S. Zumbo, S. Mandija, T. Isernia, and M. T. Bevacqua, \u201cMiphduo:\nmicrowave imaging via physics-informed deep unrolled optimization,\u201d\nInverse Problems, vol. 40, no. 4, p. 045017, 2024.\n\n[24] V. Khoshdel, M. Asefi, A. Ashraf, and E. C. Fear, \u201cA multi-branch deep\nconvolutional fusion architecture for 3d microwave inverse scattering:\nstored grain application,\u201d Neural Computing and Applications, vol. 33,\npp. 13 467\u201313 479, 2021, issue date: October 2021. [Online]. Available:\nhttps://doi.org/10.1007/s00521-021-05970-3\n\n[25] V. Khoshdel, N. Abharian, A. Attar, and J. LoVetri, \u201cDenoising diffusion\nprobabilistic models for generating tissue type breast image dataset,\u201d in\n2024 4th URSI Atlantic Radio Science Meeting (AT-RASC). IEEE, 2024,\npp. 1\u20134, available: https://www.ursi.org/proceedings/procAT24/papers/\n1467.pdf.\n\n[26] W. Shao and B. Zhou, \u201cNear-field microwave scattering formulation by\na deep learning method,\u201d IEEE Transactions on Microwave Theory and\nTechniques, vol. 70, no. 11, pp. 5077\u20135084, Nov 2022.",
        "char_count": 1471,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 30,
        "paper_id": "2510.25729v1",
        "text": "[27] B. Martin, K. Edwards, C. Gilmore, and I. Jeffrey, \u201cCycle-gan-based\ncalibration of microwave imaging systems,\u201d in Proc. 17th Eur. Conf.\nAntennas Propag. (EuCAP), 2023, pp. 1\u20134.\n\n[28] S. Bhadra, W. Zhou, and M. A. Anastasio, \u201cMedical\n\nimage\nreconstruction with image-adaptive priors learned by use of generative\nadversarial networks,\u201d 2020. [Online]. Available: https://arxiv.org/abs/\n2001.10830\n\n[29] H. M. Yao, L. Jiang, and M. Ng, \u201cEnhanced deep learning approach\nbased on the conditional generative adversarial network for electromag-\nnetic inverse scattering problems,\u201d IEEE Transactions on Antennas and\nPropagation, vol. 72, no. 7, pp. 6133\u20136138, Jul 2024.\n\n[30] Y. Song and S. Ermon, \u201cGenerative modeling by estimating gradients\nof the data distribution,\u201d in Advances in Neural Information Processing\nSystems 32 (NeurIPS 2019). Curran Associates, Inc., 2019.\n\n[31] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d\nin Advances in Neural Information Processing Systems, vol. 33. Curran\nAssociates, Inc., 2020, pp. 6840\u20136851.\n\n[32] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and\nB. Poole, \u201cScore-based generative modeling through stochastic differen-\ntial equations,\u201d in International Conference on Learning Representations\n(ICLR 2021). OpenReview.net, 2021.\n\n[33] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n\u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d in\nProceedings of the 32nd International Conference on Machine Learning\n(ICML). PMLR, 2015, pp. 2256\u20132265.",
        "char_count": 1555,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 31,
        "paper_id": "2510.25729v1",
        "text": "[34] Y. Song, L. Shen, L. Xing, and S. Ermon, \u201cSolving inverse problems in\nmedical imaging with score-based generative models,\u201d in Proceedings\nof the International Conference on Learning Representations (ICLR),\nVirtual Conference, 2022, published as a conference paper at ICLR 2022.\n[Online]. Available: https://openreview.net/forum?id=vaRCHVj0uGI\n[35] X. Bi, Y. Chen, and L. Li, \u201cDiffusionemis: Diffusion model for 3d\nelectromagnetic inverse scattering,\u201d IEEE Transactions on Geoscience\nand Remote Sensing, vol. 62, pp. 1\u201312, 2024.\n\n[36] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-\nresolution image synthesis with latent diffusion models,\u201d in Proceedings\n\n\fof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), June 2022, pp. 10 684\u201310 695.\n\n[37] J. Johnson, A. Alahi, and L. Fei-Fei, \u201cPerceptual\n\nlosses for real-\ntime style transfer and super-resolution,\u201d in Proceedings of\nthe\nEuropean Conference on Computer Vision (ECCV). Amsterdam,\nThe Netherlands: Springer, 2016, pp. 694\u2013711. [Online]. Available:\nhttps://doi.org/10.1007/978-3-319-46475-6 43\n\n[38] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks\nfor large-scale image recognition,\u201d 2015. [Online]. Available: https:\n//arxiv.org/abs/1409.1556\n\n[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial nets,\u201d in\nAdvances in neural information processing systems, 2014, pp. 2672\u2013\n2680.",
        "char_count": 1485,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 32,
        "paper_id": "2510.25729v1",
        "text": "[40] S. Cathers, B. Martin, N. Stieler, I. Jeffrey, and C. Gilmore, \u201cImproved\ndeep learning-based microwave inversion with experimental training\ndata,\u201d IEEE Open Journal of Antennas and Propagation, vol. 6, no. 2,\npp. 522\u2013534, April 2025.\n\n[41] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks\nfor biomedical image segmentation,\u201d in Medical Image Computing and\nComputer-Assisted Intervention \u2013 MICCAI 2015, ser. Lecture Notes in\nComputer Science, vol. 9351. Cham: Springer, 2015, pp. 234\u2013241.\n[Online]. Available: https://doi.org/10.1007/978-3-319-24574-4 28\n[42] S. Cathers, B. Martin, N. Stieler, I. Jeffrey, and C. Gilmore, \u201cImproved\ndeep learning-based microwave inversion with experimental training\ndata,\u201d IEEE Open Journal of Antennas and Propagation, 2025.\n[43] W. S. Bigelow and E. G. Farr, \u201cImpulse propagation measurements of\nthe dielectric properties of several polymer resins,\u201d Far Research, Inc.,\nAustin, TX, USA, Tech. Rep., 1999.\n\n[44] J. Richmond, \u201cScattering by a dielectric cylinder of arbitrary cross sec-\ntion shape,\u201d IEEE Transactions on Antennas and Propagation, vol. 13,\nno. 3, pp. 334\u2013341, 1965.\n\n[45] S. Cathers, B. Martin, N. Stieler, I. Jeffrey, and C. Gilmore, \u201cDescription\nof a 2d experimental machine-learning microwave dataset and initial\nsupervised descent method imaging results,\u201d in Proc. Int. Appl. Comput.\nElectromagn. Soc. Symp. (ACES), 2024, pp. 1\u20132.",
        "char_count": 1411,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      },
      {
        "chunk_id": 33,
        "paper_id": "2510.25729v1",
        "text": "[46] M. Ostadrahimi, P. Mojabi, C. Gilmore, A. Zakaria, S. Noghanian,\nS. Pistorius, and J. LoVetri, \u201cAnalysis of incident field modeling and in-\ncident/scattered field calibration techniques in microwave tomography,\u201d\nIEEE Antennas and Wireless Propagation Letters, vol. 10, pp. 900\u2013903,\n2011.\n\n[47] C. Kaye, I. Jeffrey, and J. LoVetri, \u201cImprovement of multi-frequency mi-\ncrowave breast imaging through frequency cycling and tissue-dependent\nmapping,\u201d IEEE transactions on antennas and propagation, vol. 67,\nno. 11, pp. 7087\u20137096, 2019.\n\n[48] M. Asefi, A. Baran, and J. LoVetri, \u201cAn experimental phantom study for\nair-based quasi-resonant microwave breast imaging,\u201d IEEE Transactions\non Microwave Theory and Techniques, vol. 67, no. 9, pp. 3946\u20133954,\n2019.",
        "char_count": 756,
        "metadata": {
          "paper_id": "2510.25729v1"
        }
      }
    ]
  }
]