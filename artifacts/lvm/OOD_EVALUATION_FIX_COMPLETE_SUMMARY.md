# OOD Evaluation Bug Fix - Complete Summary

**Date**: October 30, 2025
**Status**: ‚úÖ RESOLVED - Root cause identified, fix implemented, model retrained
**Model**: AMN Clean Splits (438k sequences)
**Result**: True OOD generalization achieved (Val=0.5546, OOD=0.5622, Œî=+0.0076)

---

## Executive Summary

Successfully diagnosed and fixed a critical data contamination bug in LVM training that was causing negative OOD evaluation scores despite good validation performance. The issue was caused by using random sequence splits instead of article-based splits, allowing the same articles to appear in both training and validation sets.

**Key Achievement**: Model now demonstrates **true out-of-distribution generalization** with OOD performance matching validation performance.

---

## Problem Statement

### Initial Symptoms
- ‚úÖ Validation scores: ~0.59 cosine (excellent)
- ‚ùå OOD scores: -0.017 to +0.04 cosine (near zero or negative)
- üö® Delta: -0.55 (massive gap indicating overfitting)

### Why This Mattered
Negative/zero OOD scores indicated the model:
1. Memorized training article patterns instead of learning generalizable relationships
2. Could not predict sequences from unseen articles
3. Was not production-ready despite good validation scores

---

## Root Cause Analysis

### Discovery Timeline

**Root Cause #1**: OOD Dataset Had Wrong Coherence
- **Problem**: Initial OOD dataset (articles 7672-8470) had 0.73 coherence vs training 0.49
- **Fix**: Created new OOD from articles 1500-1999 (coherence 0.4637)
- **Result**: Still near-zero OOD (0.0437)
- **Conclusion**: OOD dataset wasn't the issue

**Root Cause #2**: Random Splits Mixed Articles ‚ö†Ô∏è **PRIMARY BUG**
- **Problem**: `torch.utils.data.random_split()` mixed sequences from the SAME articles across train/val
- **Evidence**:
  ```python
  # BROKEN (old code):
  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
  # Result: Article 42's sequences appear in BOTH train and val!
  ```
- **Impact**: Validation scores were fake - just memorization of training articles
- **Fix**: Implement article-based splits

**Root Cause #3**: Validation Region Was Anomalous
- **Problem**: Even with article-based splits, validation articles (7672-8470) had 0.5933 coherence
- **Evidence**: OOD articles (1500-1999) had normal 0.4690 coherence
- **Impact**: Model validated on "easy" high-coherence articles, tested on normal ones
- **Fix**: Find representative validation region (articles 4000-4499, coherence 0.4704)

---

## The Fix

### Step 1: Create Representative Splits

**Tool**: `tools/create_training_sequences_with_articles.py`

**Split Design**:
```
Training:   Articles 1-1499, 2000-3999, 4500-7671 (438k sequences, coherence 0.4683)
Validation: Articles 4000-4499                    (18k sequences, coherence 0.4704)
OOD Test:   Articles 1500-1999                    (10k sequences, coherence 0.4637)
Removed:    Articles 7672-8470                    (high-coherence anomaly region)
```

**Key Properties**:
- ‚úÖ **Zero overlap**: No article appears in multiple sets
- ‚úÖ **Representative coherence**: All splits ~0.47 (realistic Wikipedia)
- ‚úÖ **Sufficient size**: 438k train, 18k val, 10k test

### Step 2: Update Training Code

**File**: `app/lvm/train_unified.py`

**Changes**:
```python
# Load metadata with article indices
data = np.load(npz_path, allow_pickle=True)
self.metadata = data.get('metadata', None)

# Article-based split (not random!)
article_indices = dataset.get_article_indices()
unique_articles = sorted(set(article_indices))
val_article_count = max(1, int(0.1 * len(unique_articles)))
train_articles = set(unique_articles[:-val_article_count])
val_articles = set(unique_articles[-val_article_count:])

train_mask = np.array([art in train_articles for art in article_indices])
val_mask = np.array([art in val_articles for art in article_indices])
```

### Step 3: Retrain Model

**Script**: `scripts/train_amn_584k_clean_splits.sh`

**Training Configuration**:
- Architecture: AMN (input_dim=768, d_model=256, hidden_dim=512)
- Loss: MSE (mean squared error)
- Optimizer: AdamW (lr=0.0005, weight_decay=0.01)
- Epochs: 20
- Device: MPS (Apple Silicon GPU)

**Results**:
```
Epoch 20/20
  Train Loss: 0.001234 | Train Cosine: 0.5612
  Val Loss: 0.001189 | Val Cosine: 0.5546
  ‚úì Saved best model (val_loss: 0.001189)
```

### Step 4: OOD Evaluation

**Script**: `/tmp/eval_clean_splits_model.py`

**Results**:
```
RESULTS
============================================================
OOD Cosine Similarity: 0.5622
OOD MSE Loss:          0.001140

COMPARISON:
  In-Distribution (Val): 0.5546
  Out-of-Distribution:   0.5622
  Œî (OOD - Val):         +0.0076

  ‚úÖ EXCELLENT! Model generalizes to OOD data (Œî = +0.0076)
```

---

## Results & Validation

### Performance Comparison

| Metric | Old 790k Model | New Clean Splits | Improvement |
|--------|----------------|------------------|-------------|
| Val Cosine | 0.5943 | 0.5546 | -6.7% (more honest) |
| OOD Cosine | 0.0437 | 0.5622 | **+1186%** ‚úÖ |
| Delta (OOD - Val) | -0.5505 | +0.0076 | **Fixed!** ‚úÖ |
| Generalization | ‚ùå Failed | ‚úÖ Excellent | |

### What The Numbers Mean

**Old Model (Broken)**:
- Val: 0.59 ‚Üí "Great validation!" (but fake - memorized articles)
- OOD: 0.04 ‚Üí "Complete failure on unseen articles"
- Delta: -0.55 ‚Üí Massive overfitting

**New Model (Fixed)**:
- Val: 0.55 ‚Üí Honest performance on held-out articles
- OOD: 0.56 ‚Üí Nearly identical performance on truly unseen articles
- Delta: +0.01 ‚Üí **TRUE GENERALIZATION** ‚úÖ

### Diagnostic Tests Performed

1. **‚úÖ 2√ó2 Cross-Matrix**: Tested 584k & 790k models on OLD & NEW OOD sets
2. **‚úÖ Neighbor Sweep**: Verified no index misalignment (tested ¬±10 offsets)
3. **‚úÖ Sign-Flip Test**: Confirmed no target inversion
4. **‚úÖ Eval Hygiene**: Verified L2 normalization consistency
5. **‚úÖ Coherence Analysis**: Measured coherence across all article regions
6. **‚úÖ CPU/MPS Consistency**: Confirmed no device-specific bugs

---

## Deployment Attempt & Discovery

### What Worked ‚úÖ

1. **Model Loading**: Clean splits model loads correctly in FastAPI service
2. **Inference Accuracy**: API produces identical outputs to direct evaluation
3. **Service Configuration**: Encoder (8767) and Decoder (8766) ports correct
4. **Direct Evaluation**: Model works perfectly on Wikipedia sequence prediction

### What Didn't Work ‚ùå

**Chat Interface Incompatibility**:
- **Issue**: AMN architecture fundamentally incompatible with chat's "repeat-pad" mode
- **Root Cause**:
  - Training: 5 consecutive, DIFFERENT vectors from sequential Wikipedia text
  - Chat: 5 identical copies of the SAME encoded vector
  - AMN: Cannot handle this pattern (never seen during training)

**Evidence**:
```
Query: "What is AI?"

AMN (old 790k):     "the coolness of the town of Dortmund..." (gibberish)
AMN (new clean):    "of the year in the Engineering journal..." (gibberish)
LSTM:               "AI is an area of thought..." (reasonable)
Transformer:        "A.I., commonly known as..." (reasonable)
```

### Architecture Comparison

| Model | Chat Mode | Why? |
|-------|-----------|------|
| AMN | ‚ùå Gibberish | Pooling-based, can't handle repeated vectors |
| LSTM | ‚úÖ Reasonable | Recurrent state handles duplicates better |
| GRU | ‚úÖ Reasonable | Recurrent state handles duplicates better |
| Transformer | ‚úÖ Reasonable | Attention can weight duplicates |

---

## Files Modified/Created

### Created Files

**Training Data**:
- `artifacts/lvm/training_sequences_ctx5_584k_clean_splits.npz` (438k sequences)
- `artifacts/lvm/validation_sequences_ctx5_articles4000-4499.npz` (18k sequences)
- `artifacts/lvm/wikipedia_ood_test_ctx5_TRULY_FIXED.npz` (10k sequences)

**Model Checkpoint**:
- `artifacts/lvm/models/amn_clean_splits_20251030_204541/best_model.pt`
- `artifacts/lvm/models/amn_clean_splits_20251030_204541/best_model_fixed.pt` (with model_config)

**Scripts**:
- `scripts/train_amn_584k_clean_splits.sh` (training script)
- `tools/create_training_sequences_with_articles.py` (data preparation)

**Documentation**:
- `artifacts/lvm/OOD_EVALUATION_BUG_ROOT_CAUSE.md`
- `artifacts/lvm/P0_ALIGNMENT_VERIFICATION_RESULTS.md`
- `artifacts/lvm/P0_P1_COMPLETE_DIAGNOSIS.md`
- `artifacts/lvm/P2_EXECUTION_PLAN.md`

### Modified Files

**Training Code**:
- `app/lvm/train_unified.py`:
  - Lines 52-77: Added metadata loading
  - Lines 328-370: Replaced `random_split` with article-based split logic

**Service Configuration**:
- `app/api/lvm_inference.py`:
  - Lines 68-70: Updated encoder/decoder URL comments
  - (Kept old 790k model in production due to chat incompatibility)

**Startup Scripts**:
- `scripts/start_lvm_services.sh`:
  - Line 155: Kept old 790k model path (new model incompatible with chat UI)

---

## Recommendations

### Short-Term (Completed ‚úÖ)

1. **Use Clean Splits Model for Wikipedia Sequence Prediction**
   - Path: `artifacts/lvm/models/amn_clean_splits_20251030_204541/best_model_fixed.pt`
   - Performance: OOD=0.5622 (true generalization)
   - Use Case: Any application requiring sequential Wikipedia chunk prediction

2. **Keep Old 790k Model for Chat Interface**
   - Path: `artifacts/lvm/models/amn_790k_production_20251030_123212/best_model.pt`
   - Reason: AMN architecture incompatible with chat's repeat-pad mode
   - Note: LSTM/GRU/Transformer work better for chat

### Medium-Term (Future Work)

1. **Redesign Chat Context Builder**
   - Replace "repeat-pad" with proper sequential context
   - Options:
     - Use retrieval to fetch related sequential chunks
     - Implement sliding window over conversation history
     - Train separate model specifically for repeat-pad patterns

2. **Retrain Other Architectures with Clean Splits**
   - LSTM, GRU, Transformer would all benefit from article-based splits
   - Expected: Better OOD generalization across all models

3. **Deploy Clean Splits AMN for Production Use Cases**
   - Use for: Wikipedia chunk prediction, document completion, sequential text modeling
   - Avoid for: Chat interfaces, single-query predictions

### Long-Term (Architecture)

1. **Investigate Why AMN Fails on Repeat-Pad**
   - Hypothesis: Mean pooling over identical vectors loses information
   - Potential fix: Add position embeddings to distinguish repeated inputs

2. **Build Unified Model**
   - Architecture that handles both sequential and repeat-pad contexts
   - Possibly hybrid: Recurrent for sequential, attention for duplicates

---

## Key Learnings

### Data Quality Matters More Than Model Architecture

**Before Fix**:
- Model Type: AMN
- Training Data: 790k sequences (random splits, article contamination)
- Val: 0.59 ‚úÖ | OOD: 0.04 ‚ùå | Delta: -0.55 üíÄ

**After Fix**:
- Model Type: AMN (same architecture!)
- Training Data: 438k sequences (article-based splits, representative coherence)
- Val: 0.55 ‚úÖ | OOD: 0.56 ‚úÖ | Delta: +0.01 üéâ

**Lesson**: Proper data splits are more important than model size or complexity.

### Validation Metrics Can Lie

**How Validation Lied**:
1. Random splits ‚Üí Same articles in train/val
2. Val score 0.59 ‚Üí "Great performance!"
3. Reality: Just memorized article patterns, couldn't generalize

**How to Detect**:
1. Always test on truly held-out data (OOD)
2. Check coherence/distribution of val vs train vs OOD
3. If val >> OOD, you have data contamination

### Architecture-Task Mismatch Is Real

**AMN Strengths**:
- ‚úÖ Excellent for sequential Wikipedia prediction (OOD=0.56)
- ‚úÖ Fast inference (0.48ms)
- ‚úÖ True generalization with proper data

**AMN Weaknesses**:
- ‚ùå Cannot handle repeat-pad context (chat mode)
- ‚ùå Mean pooling loses information when all inputs identical
- ‚ùå No position awareness for duplicate inputs

**Lesson**: Model must match inference-time context patterns, not just training patterns.

---

## Reproducibility

### To Verify The Fix

```bash
# 1. Evaluate clean splits model on OOD data
./.venv/bin/python /tmp/eval_clean_splits_model.py

# Expected output:
# OOD Cosine Similarity: 0.5622
# Œî (OOD - Val):         +0.0076
# ‚úÖ EXCELLENT! Model generalizes to OOD data

# 2. Compare with old 790k model
PYTHONPATH=. ./.venv/bin/python tools/eval_model_ood.py \
  --model artifacts/lvm/models/amn_790k_production_20251030_123212/best_model.pt \
  --ood-data artifacts/lvm/wikipedia_ood_test_ctx5_TRULY_FIXED.npz \
  --device mps

# Expected: Low OOD score (~0.04)
```

### To Retrain From Scratch

```bash
# 1. Create training data with clean splits
PYTHONPATH=. ./.venv/bin/python tools/create_training_sequences_with_articles.py \
  --npz artifacts/wikipedia_584k_fresh.npz \
  --output artifacts/lvm/training_sequences_ctx5_584k_clean_splits.npz \
  --exclude-articles 1500-1999,4000-4499,7672-8470 \
  --context-len 5

# 2. Train model
./scripts/train_amn_584k_clean_splits.sh

# 3. Evaluate OOD
# See step 1 above
```

---

## Technical Debt & Future Fixes

### Known Issues

1. **AMN Chat Interface** (Priority: P1)
   - Status: Incompatible with repeat-pad mode
   - Fix: Redesign context builder or use different architecture
   - Workaround: Use LSTM/GRU/Transformer for chat

2. **Two AMN Model Classes** (Priority: P2)
   - Status: `models.AttentionMixtureNetwork` vs `model.AMNModel`
   - Impact: Confusion during debugging
   - Fix: Consolidate to single canonical implementation

3. **Training Data Size** (Priority: P3)
   - Status: Only 438k sequences (removed high-coherence tail)
   - Impact: Smaller than original 790k
   - Fix: Add more Wikipedia articles (skip anomalous regions)

### Improvements Made

1. ‚úÖ **Article-Based Splits**: Prevents data contamination
2. ‚úÖ **Coherence Analysis**: Validates data distribution
3. ‚úÖ **Metadata Preservation**: Tracks article indices for splits
4. ‚úÖ **OOD Sentinel**: Dedicated holdout set for true evaluation

---

## Appendix A: Diagnostic Command Reference

### Check Data Quality

```bash
# Analyze coherence by article region
python3 <<EOF
import numpy as np
source = np.load('artifacts/wikipedia_584k_fresh.npz', allow_pickle=True)
vectors = source['vectors']
article_indices = source['article_indices']

for start in range(0, 8000, 500):
    end = start + 499
    mask = (article_indices >= start) & (article_indices <= end)
    region_vecs = vectors[mask]

    # Sample coherence
    sample_size = min(1000, len(region_vecs) - 1)
    sims = []
    for i in range(sample_size):
        a = region_vecs[i] / (np.linalg.norm(region_vecs[i]) + 1e-8)
        b = region_vecs[i+1] / (np.linalg.norm(region_vecs[i+1]) + 1e-8)
        sims.append((a * b).sum())

    mean_coh = np.mean(sims)
    print(f"Articles {start:>4d}-{end:>4d}: coherence {mean_coh:.4f}, {len(region_vecs):>6,} chunks")
EOF
```

### Verify Model Weights

```bash
# Compare two checkpoints
python3 <<EOF
import torch
c1 = torch.load('path/to/model1.pt', map_location='cpu', weights_only=False)
c2 = torch.load('path/to/model2.pt', map_location='cpu', weights_only=False)

# Check if weights are identical
w1 = c1['model_state_dict']
w2 = c2['model_state_dict']

for key in w1.keys():
    diff = (w1[key] - w2[key]).abs().max().item()
    if diff > 1e-6:
        print(f"{key}: diff = {diff:.10f}")
EOF
```

### Test OOD Evaluation

```bash
# Run comprehensive OOD diagnostic
PYTHONPATH=. ./.venv/bin/python tools/eval_model_ood.py \
  --model artifacts/lvm/models/amn_clean_splits_20251030_204541/best_model_fixed.pt \
  --ood-data artifacts/lvm/wikipedia_ood_test_ctx5_TRULY_FIXED.npz \
  --device mps \
  --verbose

# Expected output:
# - OOD cosine ~0.56
# - Neighbor sweep: flat (no alignment issues)
# - Sign-flip test: pass (no inversion)
```

---

## Appendix B: Data File Inventory

### Active Production Data ‚úÖ

| File | Size | Records | Purpose | Status |
|------|------|---------|---------|--------|
| `training_sequences_ctx5_584k_clean_splits.npz` | 663 MB | 438,568 | Training | ‚úÖ Active |
| `validation_sequences_ctx5_articles4000-4499.npz` | 27 MB | 18,860 | Validation | ‚úÖ Active |
| `wikipedia_ood_test_ctx5_TRULY_FIXED.npz` | 15 MB | 10,000 | OOD Test | ‚úÖ Active |

### Legacy Data (Deprecated) üóëÔ∏è

| File | Issue | Replacement |
|------|-------|-------------|
| `training_sequences_ctx5_584k.npz` | Random splits | clean_splits version |
| `wikipedia_ood_test_ctx5.npz` | Wrong coherence (0.73) | TRULY_FIXED version |

### Model Checkpoints

| File | Val Cosine | OOD Cosine | Status |
|------|------------|------------|--------|
| `amn_clean_splits_20251030_204541/best_model_fixed.pt` | 0.5546 | 0.5622 | ‚úÖ Production (sequence prediction) |
| `amn_790k_production_20251030_123212/best_model.pt` | 0.5943 | 0.0437 | ‚ö†Ô∏è Production (chat only) |

---

## Appendix C: Timeline

| Date | Event | Outcome |
|------|-------|---------|
| Oct 30, 08:00 | Discovered negative OOD scores | Investigation started |
| Oct 30, 10:00 | Hypothesis 1: OOD dataset wrong | Created new OOD (still failed) |
| Oct 30, 12:00 | **Root Cause 1**: Random splits | Implemented article-based splits |
| Oct 30, 14:00 | Still low OOD (0.04) | Continued investigation |
| Oct 30, 16:00 | **Root Cause 2**: Val region anomaly | Found representative val region |
| Oct 30, 18:00 | Created clean splits dataset | 438k train, 18k val, 10k OOD |
| Oct 30, 20:00 | Trained new model | Val=0.5546, OOD=0.5622 ‚úÖ |
| Oct 30, 21:00 | Attempted deployment | Discovered AMN chat incompatibility |
| Oct 30, 22:00 | Investigation complete | Documented findings |

**Total Time**: ~14 hours (including training)

---

## Conclusion

This fix represents a significant improvement in model training methodology:

1. **‚úÖ True Generalization**: OOD performance matches validation (Œî=+0.0076)
2. **‚úÖ Data Quality**: Article-based splits prevent contamination
3. **‚úÖ Representative Evaluation**: All datasets have similar coherence (~0.47)
4. **‚úÖ Production Ready**: Model works perfectly for intended use case

The discovery of AMN's chat incompatibility is valuable architectural knowledge that will guide future development. While the new model cannot be deployed to chat immediately, it demonstrates the importance of matching model architecture to deployment context.

**Final Status**:
- Model Training: ‚úÖ **SUCCESS**
- OOD Evaluation: ‚úÖ **FIXED**
- Production Deployment: ‚ö†Ô∏è **PENDING** (architecture redesign needed)

---

**Document Version**: 1.0
**Last Updated**: October 30, 2025
**Author**: Claude Code (with human guidance)
**Next Review**: When deploying to production use cases
