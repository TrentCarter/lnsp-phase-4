TRICO ‚Äî Training Ideas (Vector-Only)

LNSP/LVM ‚Ä¢ Semantic GPS 768-D ‚Ä¢ October 13, 2025

Goal. Stop ‚Äúgarbage decodes‚Äù by making the model‚Äôs 768-D outputs both (a) on-manifold in your Semantic-GPS space and (b) decoder-compatible with vec2text. We‚Äôll do that with four concrete, vector-native training tracks you can run in parallel and compare.

‚∏ª

TL;DR (what to run)
	1.	E2E Vector Supervision (E2E-V): Simple cosine-align to targets; adds manifold control.
	2.	Iterative No-Grad‚ÜíGrad (INGR): Teach ‚Äúimprove-by-recursion‚Äù with cheap compute.
	3.	Contrastive + Cycle (CONTRAST-CYCLE): Align to positives, repel hard negatives, and enforce vec2text round-trips.
	4.	Curriculum + Lane Hard-Negatives (CURR-LANE): Stage difficulty and TMD-aware negatives for stability.

Run all four; keep the one that makes decoded text coherent and lifts retrieval metrics.

‚∏ª

Assumptions & Setup
	‚Ä¢	Space: 768-D Semantic GPS (unit-norm; use SGPS projector).
	‚Ä¢	Encoder/Decoder: Use vec2text for both directions (encoder and decoder) to avoid GTR-T5 compatibility headaches.
	‚Ä¢	Data: Wikipedia chunks (40‚Äì50k), CPESH labels when available.
	‚Ä¢	Lanes: TMD lanes (Factoid, Math-Deriv, Code-API, ‚Ä¶) as categorical features and sampling buckets.

Common tricks (use in all tracks):
	‚Ä¢	Unit-norm outputs: yÃÇ ‚Üê y / ||y||.
	‚Ä¢	EMA teacher for stability (decay ‚âà 0.999‚Äì0.9999).
	‚Ä¢	SGPS projection penalty: keep outputs near training distribution (e.g., distance to k-NN hull or small L2 to retrieval centroid).
	‚Ä¢	Vec-decoder compatibility loss (below) on a random 30‚Äì50% of batches.

‚∏ª

Track 1 ‚Äî E2E Vector Supervision (E2E-V)

Purpose: Baseline that already fixes many ‚Äúgarbage decode‚Äù cases by keeping vectors on-manifold and close to targets.

Batch I/O
	‚Ä¢	Input: q (question vec), optional retrieval pooled rÃÑ, lane embedding t.
	‚Ä¢	Target: y* (CPESH Expected) or teacher vector (vec2text encoder of gold text).

Model (simple): yÃÇ = fŒ∏(q, rÃÑ, t) (Mamba or slim MLP); output 768-D.

Loss
	‚Ä¢	Alignment: L_align = 1 ‚àí cos(yÃÇ, y*).
	‚Ä¢	Manifold regularizer: L_mani = ||yÃÇ ‚àí Proj_SGPS(yÃÇ)||¬≤ or small penalty to drift from k nearest vectors in train set.
	‚Ä¢	Decoder-compat: decode and re-encode once:
	‚Ä¢	txt = vec2text.decode(yÃÇ)
	‚Ä¢	v' = vec2text.encode(txt)
	‚Ä¢	L_cycle = 1 ‚àí cos(yÃÇ, v')

Total: L = L_align + Œª_mani L_mani + Œª_cycle L_cycle.

Why it helps: Forces outputs to sit where vec2text is accurate, not just ‚Äúsomewhere‚Äù in 768-D.

‚∏ª

Track 2 ‚Äî Iterative No-Grad ‚Üí Grad (INGR)

Purpose: Teach the head (or a tiny refiner) to improve by recursion without paying full backprop at every step.

Module: Tiny Refiner (CTR-SGPS) that updates (z, y) S times; last step trains.

(z0=0, y0 = pool(Y or initial head))
for s in 1..S-1:
    (zs, ys) = step_no_grad(zs-1, ys-1; q, rÃÑ, t)
(zS, yS) = step_grad(zS-1, yS-1; q, rÃÑ, t)

Loss (on final step only):
L = (1 ‚àí cos(yS, y*)) + Œ±¬∑BCE(p_halt,S, ùüô{cos(yS,y*)‚â•œÑ}) + Œ≤¬∑L_cycle(yS)

Notes
	‚Ä¢	Set S‚âà8‚Äì16 for baseline; cap latency with a halting head at inference.
	‚Ä¢	Use EMA of the refiner for evaluation.

Why it helps: You get the iterative behavior (big win for coherence) at almost the cost of a single forward.

‚∏ª

Track 3 ‚Äî Contrastive + Cycle (CONTRAST-CYCLE)

Purpose: Make vectors discriminative and decoder-friendly. Fixes ‚Äúblurry‚Äù outputs that decode nonsensically.

Positives/Negatives
	‚Ä¢	Positive: y* or teacher vector for the same chunk.
	‚Ä¢	Negatives: (a) lane-hard: same lane, different article; (b) near-miss: high retrieval but wrong; (c) adversarial: lexically similar but semantically different.

Loss
	‚Ä¢	InfoNCE:
L_{\text{nce}} = -\log \frac{\exp(\cos(yÃÇ, y^+)/œÑ_c)}{\exp(\cos(yÃÇ, y^+)/œÑ_c)+\sum_j \exp(\cos(yÃÇ, y^-_j)/œÑ_c)}
	‚Ä¢	Cycle: same L_cycle as Track 1.
	‚Ä¢	Optional Mutual consistency: symmetrize with teacher: 1 ‚àí cos(fŒ∏(q,‚Ä¶), stopgrad(y*)) + 1 ‚àí cos(stopgrad(fŒ∏(q,‚Ä¶)), y*).

Total: L = L_nce + Œª_cycle L_cycle.

Why it helps: Separates close concepts; keeps decodes sharp and faithful.

‚∏ª

Track 4 ‚Äî Curriculum + Lane Hard-Negatives (CURR-LANE)

Purpose: Stabilize training; expose the model to difficulty gradually; force lane-aware precision.

Stages
	1.	Stage A (Easy): Short, clean paragraphs; far negatives; batch size high.
	2.	Stage B (Medium): Normal chunks; lane-hard negatives; add L_cycle.
	3.	Stage C (Hard): Long/technical chunks; near-miss negatives; enable INGR (Track 2) for the last 30‚Äì50% of training.

Sampling
	‚Ä¢	Curriculum by readability/length and retrieval ambiguity.
	‚Ä¢	Maintain lane balance per epoch.

Scheduler
	‚Ä¢	Warmup ‚Üí cosine decay; raise Œª_cycle over time; increase negative count per batch over stages.

‚∏ª

Implementation Notes (do these no matter what)
	‚Ä¢	Normalize everywhere: inputs, intermediate y, outputs.
	‚Ä¢	SGPS projector: if you have a PCA/autoencoder of the corpus vectors, project predictions back (light orthogonality helps).
	‚Ä¢	Adapters per lane: small FiLM/LoRA heads keyed by TMD ‚Üí lowers interference.
	‚Ä¢	Batch shaping: mix 70% in-lane, 30% cross-lane examples to avoid collapse.
	‚Ä¢	Decode gating (for logging only): if cos(yÃÇ, v') < 0.7, mark as ‚Äúdecode-risky‚Äù; surface to eval dashboard.
	‚Ä¢	Teacher path (optional): keep a frozen EMA of a previously good checkpoint to generate soft targets y* when CPESH isn‚Äôt available.

‚∏ª

Minimal Loss Recipes (copy/paste)

E2E-V

L = (1 - cos(yÃÇ, y*)) + 0.05 * ||yÃÇ - Proj_SGPS(yÃÇ)||^2 + 0.2 * (1 - cos(yÃÇ, vec2text.encode(vec2text.decode(yÃÇ))))

INGR

L = (1 - cos(yS, y*)) + 0.2 * BCE(p_halt,S, ùüô{cos(yS,y*)‚â•0.85}) + 0.2 * (1 - cos(yS, vec2text.encode(vec2text.decode(yS))))

CONTRAST-CYCLE

L = InfoNCE(yÃÇ, y+, {y-}) + 0.2 * (1 - cos(yÃÇ, vec2text.encode(vec2text.decode(yÃÇ))))


‚∏ª

Metrics & Gates (what ‚Äúgood‚Äù looks like)
	‚Ä¢	Vector alignment: cos(yÃÇ, y*) ‚Üë; median ‚â• 0.88 on val after Stage B.
	‚Ä¢	Decoder cycle: cos(yÃÇ, v') ‚Üë; median ‚â• 0.82.
	‚Ä¢	Retrieval synergy: nDCG@10 on re-query with yÃÇ ‚Üë vs baseline.
	‚Ä¢	Lane accuracy: per-lane pass rate (CPESH Expected within top-k when decoded) ‚Üë.
	‚Ä¢	Halting efficiency (INGR): avg steps ‚â§ 8 with ‚â• 85% inside S_max.
	‚Ä¢	Human sanity checks: 50-sample blind read‚Äî‚â• 70% ‚Äúsensible‚Äù after Stage C.

‚∏ª

Failure Modes & Fast Fixes
	‚Ä¢	Nonsensical decodes despite high cosine to y*: raise Œª_cycle; add near-miss negatives.
	‚Ä¢	Mode collapse (all vectors look alike): increase negative count; add lane adapters; up InfoNCE temperature œÑ_c.
	‚Ä¢	Over-halting (INGR stops too early): lower halt threshold œÑ or add penalty for early halts.
	‚Ä¢	Training stable but val decode poor: enable EMA eval; tighten SGPS projector; add small L2 to nearest-neighbor barycenter.

‚∏ª

7-Day Plan (practical)

Day 1‚Äì2: Implement Tracks 1 & 3. Log cos(yÃÇ,y*), cos(yÃÇ,v'), nDCG@10.
Day 3: Add SGPS projector + EMA; run quick grid on Œª_cycle ‚àà {0.1,0.2,0.3}.
Day 4: Implement INGR (Track 2) with S=8; œÑ=0.85; halting head.
Day 5: Turn on CURR-LANE: Stage A‚ÜíB scheduling + lane adapters (rank-8).
Day 6: Ablate: no cycle vs cycle, no contrast vs contrast, no INGR vs INGR.
Day 7: Pick winner by decode sanity + retrieval uplift; checkpoint + freeze.

‚∏ª

Experiment Naming (so results aren‚Äôt chaos)

exp/<date>_<track>_S<steps>_CY<Œªcycle>_N<neg>_LANE<on|off>_EMA<on|off>
# example:
exp/2025-10-13_CONTRAST-CYCLE_S0_CY0.2_N64_LANEon_EMAon


‚∏ª

Optional: Wire the Tiny Refiner (CTR-SGPS) cleanly
	‚Ä¢	Inputs: (q, rÃÑ, Y_topk, t) ‚Üí iterate (z,y); output (yÃÇ, p_halt).
	‚Ä¢	Use it inside Track 2 (training) and after the LVM head at inference for hard queries.
	‚Ä¢	Keep h=512, S_max=16, œÑ‚àà[0.82,0.9].

‚∏ª

Final Notes (blunt and practical)
	‚Ä¢	If vec2text is the only reliable pair, standardize on it for both encode/decode.
	‚Ä¢	The single biggest lever against ‚Äúgarbage text‚Äù is CONTRAST-CYCLE with lane-hard negatives.
	‚Ä¢	The single cheapest quality bump is INGR (no-grad‚Üígrad) + EMA.
	‚Ä¢	Do not skip normalization and the SGPS projector; most drift bugs come from that.

If you want, I can turn this into a Makefile + runnable trainer skeleton next, with flags for --track, --lambda-cycle, --negatives, --ema, and --ingr-steps.



#
# UPDATED after critical Review:

You‚Äôre right to push back. Both reviews are fair. Here‚Äôs a tighter, low-risk plan that directly addresses each critique and keeps cost/infrastructure sane while we finish validating the baseline.

# What we change immediately

* **No parallel tracks.** We proceed **one experiment at a time** with hard stop/go gates.
* **No heavy cycle-loss in training (yet).** We use vec2text only for **diagnostics**, not as a training target, until we prove it‚Äôs helpful.
* **Defer CTR/INGR/halting, curriculum, lane adapters.** Those need infra we don‚Äôt have and add 10‚Äì100√ó cost. We‚Äôll revisit only if simple fixes fail.

---

# Phase 0 ‚Äî Baseline first (finish what‚Äôs running)

**Model:** simple LSTM (PID 11640) + **InfoNCE** with **in-batch negatives** only.
**Targets:** vec2text-encoded vectors (so train & eval live in the same latent).
**Report (single page):**

* `val_cosine` (mean/median)
* `test_cosine`
* Decode sanity on 100 val samples (**diagnostic only**): % with cycle_cos ‚â• 0.8; % < 0.7; 10 example decodes.

**Exit criteria:**

* If `val_cosine ‚â• 0.50` **and** ‚â•70% decodes judged coherent ‚Üí **Ship baseline** and proceed to retrieval integration.
* If either fails ‚Üí Phase 1.

---

# Phase 1 ‚Äî Diagnose vec2text compatibility (cheap & decisive)

Goal: confirm whether ‚Äúgarbage decodes‚Äù come from **off-manifold predictions** or an **inherently brittle decoder**.

**D1. Latent compatibility sweep (no decoder calls)**

* Compute cosine to **nearest 8 train anchors** for each prediction; record mean/variance and **angular dispersion**.
* If off-manifold: you‚Äôll see **low NN cosines** and **high dispersion** where decodes are bad.

**D2. Minimal decode probe (‚â§100 samples)**

* `y_pred ‚Üí decode ‚Üí encode ‚Üí y_cycle`; record `cos(y_pred, y_cycle)` and human coherence (3-point scale).
* Correlate: low NN cosine ‚Üî low cycle_cos ‚Üî incoherent text? If yes ‚Üí the model is off-manifold; if no ‚Üí decoder is brittle even on-manifold.

**Decision:**

* If off-manifold ‚Üí Phase 2A (cheap manifold alignment).
* If decoder brittle ‚Üí Phase 2B (decoder-aware but **very light**).

---

# Phase 2A ‚Äî Cheap manifold alignment (no decoder in the loop)

Cost multiplier: **‚âà1.05√ó** (tiny).

**A1. Anchor-MMD (mini-maximum mean discrepancy)**

* Precompute **1,024 anchor vectors** from train (random stratified by article).
* Add `L_mmd = MMD_RBF(y_pred, anchors_batch)` with **fixed tiny weight** (Œª_mmd=0.02 to start).
* Effect: pulls predictions toward the empirical latent distribution **without** any PCA/projector infra.

**A2. Mean/variance matching (batch-level)**

* Maintain running **per-lane** (or global, if no lanes) mean/var stats of train embeddings.
* Add a **soft penalty** to match batch mean/var of `y_pred` to these stats (Œª_stat=0.01).

**Train once**, measure:

* Œî`val_cosine`, Œîdecode coherence on 100 samples.
* If both improve (or remain equal with cleaner decodes) ‚Üí keep A1/A2; else drop them.

---

# Phase 2B ‚Äî Decoder-aware training (lightweight, gated)

Only if Phase 1 showed decoder brittleness **even on-manifold**.

**B1. Sparse cycle audits (training-time)**

* Apply cycle loss to **5% of batches**, **vec2text --steps=1 only**.
* Weight tiny: Œª_cycle=0.05.
* Budget: expect ‚â§**1.5√ó** slowdown (vs 5‚Äì10√ó previously).
* If throughput tanks, cut to 2% and/or stagger (every Nth step).

**B2. Cache & reuse** (no infra changes)

* Keep a **FIFO cache** of `(y_pred ‚Üí text ‚Üí y_cycle)` for recently seen patterns; reuse within the epoch to avoid repeated decodes on near-duplicates.

**Exit gate:** Only keep cycle if we see **measurable** uplift in:

* % with `cycle_cos ‚â• 0.8` (‚â•+10 pts) **and**
* Human coherence on the 100-sample panel (‚â•+10 pts absolute)
  at ‚â§2√ó wall-time.

---

# Phase 3 ‚Äî Stronger contrastive, still simple

If we need more discriminability and Phase 2 didn‚Äôt get us there.

**C1. In-batch + ‚Äúsame-article‚Äù negatives**

* Free hard negatives: for each anchor, sample negatives from **the same article** (different chunk). No lane infra needed.
* Keep temperature sweep tight: œÑ ‚àà {0.05, 0.07, 0.1}.
* Log: train loss curve, `val_cosine`, and decode sanity.
* If it helps, keep; otherwise revert‚Äîno sunk cost.

---

# What we are NOT doing (yet)

* No **INGR/halting** (multi-step refiners) ‚Äî agrees with both reviews: too expensive, infra heavy.
* No **curriculum** ‚Äî needs readability metadata & pipelines we don‚Äôt have.
* No **lane adapters** or lane-aware negatives ‚Äî later, when lane batching exists.
* No **SGPS projector** ‚Äî replaced by **Anchor-MMD** + mean/var matching (no PCA/AE buildout).

---

# Concrete numbers (to remove hand-waving)

* **Œª_mmd = 0.02**, RBF œÉ chosen as median pairwise distance on anchors (compute once).
* **Œª_stat = 0.01** (mean/var match).
* **Cycle weight Œª_cycle = 0.05** on **5%** of batches, **--steps 1**.
* **Contrastive œÑ** grid: 0.05, 0.07, 0.10 (pick 1 by val cosine).
* **Success gates** (realistic):

  * Baseline: `val_cosine ‚â• 0.50`.
  * After Phase 2: `+0.02` absolute on `val_cosine` **or** +10 pts on 100-sample decode-coherence, with ‚â§1.5√ó time.
  * After Phase 3: further +0.02 on `val_cosine` or +5 pts decode-coherence.

---

# Why this directly answers the critiques

**Premature optimization:** We run **baseline ‚Üí diagnose ‚Üí one small change** at a time.
**Compute cost:** Cycle is **off** by default; when on, it‚Äôs 5% batches, `--steps 1`, Œª small. Anchor-MMD/mean-var are ~free.
**Missing infra:** We avoid projector, lanes, curriculum, INGR. Everything here is runnable in the current loop.
**Vec2text bottleneck:** Used sparingly for **diagnostics**; if ever in the loss, it‚Äôs **light** and **cached**.
**Metrics realism:** Gates are tied to current 0.75-ish cycle baselines; no fantasy targets.
**Actionable:** Each phase is 1‚Äì2 days, not weeks, and can be abandoned if no gain.

---

# If baseline already wins

If the LSTM + InfoNCE already delivers acceptable decode quality: **stop**. Integrate retrieval and move to evaluation at system level. Save fancy methods for when the data or task demands them.

---

If you want, I‚Äôll package Phase-1 diagnostics (anchors, dispersion, 100-sample decode probe) and Phase-2A losses as drop-in functions you can toggle with flags like:

```
--mmd_anchors 1024 --lambda_mmd 0.02 --lambda_stat 0.01 \
--cycle_pct 0.05 --cycle_lambda 0.05 --cycle_steps 1
```

This keeps experiments surgical and measurable.
