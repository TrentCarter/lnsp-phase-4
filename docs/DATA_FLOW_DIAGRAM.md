# LNSP Data Flow Architecture

**Last Updated**: October 16, 2025
**Purpose**: Visual architecture showing data flow through the complete LNSP system

---

## ğŸ¯ System Overview

LNSP (Latent-space Next-Sequence Prediction) is a **tokenless, vector-native retrieval and prediction system** that operates entirely in semantic vector space.

**Key Components**:
1. **vecRAG**: Vector-based retrieval (FAISS + PostgreSQL)
2. **LVM**: Latent Vector Models (predict next semantic vector)
3. **Vec2Text**: Bidirectional textâ†”vector conversion
4. **GraphRAG**: (Planned) Graph-based ontology traversal via Neo4j

---

## ğŸ“Š Complete System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAW DATA SOURCES                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  âœ… Wikipedia Articles (500k)          ğŸ—‘ï¸ FactoidWiki (deprecated)         â”‚
â”‚  Location: data/datasets/wikipedia/   Location: data/factoidwiki_1k.jsonl â”‚
â”‚  Purpose: LVM training (sequential)   Purpose: DO NOT USE (taxonomic)     â”‚
â”‚                                                                             â”‚
â”‚  ğŸ”„ Ontologies (SWO, GO, DBpedia)     â”‚
â”‚  Location: artifacts/ontology_chains/  â”‚
â”‚  Purpose: GraphRAG only, NOT for LVM   â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PREPROCESSING PIPELINE                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â€¢ Chunking (semantic boundaries)      â”‚
â”‚  â€¢ CPESH generation (LLM)              â”‚
â”‚  â€¢ Metadata extraction                 â”‚
â”‚  â€¢ Quality filtering                   â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRIMARY STORAGE LAYER                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PostgreSQL (ACTIVE)             â”‚    â”‚  Neo4j (EMPTY)             â”‚   â”‚
â”‚  â”‚  âœ… 80,636 concepts               â”‚    â”‚  âš ï¸ 0 nodes                 â”‚   â”‚
â”‚  â”‚  Location: /opt/homebrew/var/    â”‚    â”‚  Location: /opt/homebrew/  â”‚   â”‚
â”‚  â”‚           postgresql@17/          â”‚    â”‚           var/neo4j/       â”‚   â”‚
â”‚  â”‚                                   â”‚    â”‚                            â”‚   â”‚
â”‚  â”‚  Tables:                          â”‚    â”‚  Planned Use:              â”‚   â”‚
â”‚  â”‚  â€¢ cpe_entry (concepts+metadata)  â”‚    â”‚  â€¢ Graph relationships     â”‚   â”‚
â”‚  â”‚  â€¢ cpe_vectors (768D vectors)     â”‚    â”‚  â€¢ Ontology traversal      â”‚   â”‚
â”‚  â”‚  â€¢ tmd_codes (domain/task/mod)    â”‚    â”‚  â€¢ 6-degree shortcuts      â”‚   â”‚
â”‚  â”‚                                   â”‚    â”‚                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GTR-T5 ENCODING LAYER                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  âœ… CORRECT: Vec2Text-Compatible        â”‚
â”‚  Method: IsolatedVecTextVectOrchestratorâ”‚
â”‚  â€¢ T5EncoderModel (transformers)       â”‚
â”‚  â€¢ Mean pooling                        â”‚
â”‚  â€¢ L2 normalization                    â”‚
â”‚  â€¢ Output: 768D vectors                â”‚
â”‚                                         â”‚
â”‚  âŒ WRONG: sentence-transformers        â”‚
â”‚  Produces incompatible vectors!        â”‚
â”‚  (9.9x worse vec2text decoding)        â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VECTOR STORAGE LAYER                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  NPZ Vector Files (ACTIVE)                                      â”‚      â”‚
â”‚  â”‚  âœ… wikipedia_500k_corrected_vectors.npz (230 MB)               â”‚      â”‚
â”‚  â”‚     â€¢ vectors: [500k, 768] float32                              â”‚      â”‚
â”‚  â”‚     â€¢ texts: [500k] strings                                     â”‚      â”‚
â”‚  â”‚     â€¢ ids: [500k] UUIDs â†’ correlates with PostgreSQL            â”‚      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â”‚  âœ… training_sequences_ctx5.npz (449 MB)                        â”‚      â”‚
â”‚  â”‚     â€¢ context_vectors: [80k, 5, 768] - LVM training input      â”‚      â”‚
â”‚  â”‚     â€¢ target_vectors: [80k, 768] - LVM training targets        â”‚      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  FAISS Index (ACTIVE)                                           â”‚      â”‚
â”‚  â”‚  âœ… wikipedia_500k_corrected_ivf_flat_ip.index (238 MB)         â”‚      â”‚
â”‚  â”‚     â€¢ Type: IVF_FLAT_IP                                         â”‚      â”‚
â”‚  â”‚     â€¢ Vectors: 500k                                             â”‚      â”‚
â”‚  â”‚     â€¢ Dimension: 768                                            â”‚      â”‚
â”‚  â”‚     â€¢ nlist: 512, nprobe: 16                                    â”‚      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                             â”‚
            â†“ (Path A: vecRAG)            â†“ (Path B: LVM Training)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  vecRAG RETRIEVAL           â”‚   â”‚  LVM TRAINING PIPELINE         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                             â”‚   â”‚                                 â”‚
â”‚  Query Text                 â”‚   â”‚  1. Create Training Sequences  â”‚
â”‚      â†“                      â”‚   â”‚     Sliding window: [5â†’1]      â”‚
â”‚  GTR-T5 Encode              â”‚   â”‚                                 â”‚
â”‚      â†“                      â”‚   â”‚  2. Train 4 Architectures:     â”‚
â”‚  Query Vector [768]         â”‚   â”‚     â€¢ AMN (1.5M params)        â”‚
â”‚      â†“                      â”‚   â”‚     â€¢ LSTM (5.1M params)       â”‚
â”‚  FAISS Search               â”‚   â”‚     â€¢ GRU (7.1M params)        â”‚
â”‚      â†“                      â”‚   â”‚     â€¢ Transformer (17.9M)      â”‚
â”‚  Top-K Neighbors            â”‚   â”‚                                 â”‚
â”‚      â†“                      â”‚   â”‚  3. Loss: MSE (regression)     â”‚
â”‚  Get CPE IDs                â”‚   â”‚     20 epochs                  â”‚
â”‚      â†“                      â”‚   â”‚                                 â”‚
â”‚  PostgreSQL Lookup          â”‚   â”‚  4. Save Best Models           â”‚
â”‚      â†“                      â”‚   â”‚                                 â”‚
â”‚  Return Concept Texts       â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                             â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â†“
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚  TRAINED LVM MODELS (ACTIVE)   â”‚
                                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                   â”‚                                 â”‚
                                   â”‚  ./artifacts/lvm/models/       â”‚
                                   â”‚                                 â”‚
                                   â”‚  âœ… AMN (0.5664 cosine)         â”‚
                                   â”‚     0.49 ms/query âš¡            â”‚
                                   â”‚                                 â”‚
                                   â”‚  âœ… LSTM (0.5758 cosine) â­      â”‚
                                   â”‚     0.56 ms/query              â”‚
                                   â”‚     RECOMMENDED                â”‚
                                   â”‚                                 â”‚
                                   â”‚  âœ… GRU (0.5754 cosine)         â”‚
                                   â”‚     2.08 ms/query              â”‚
                                   â”‚                                 â”‚
                                   â”‚  âœ… Transformer (0.5820) ğŸ†     â”‚
                                   â”‚     2.68 ms/query              â”‚
                                   â”‚     Best accuracy              â”‚
                                   â”‚                                 â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LVM INFERENCE PIPELINE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Input: 5 Context Chunks (text)                                            â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â†“                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  STEP 1: Text Encoding              â”‚    ~300 ms (3%)                   â”‚
â”‚  â”‚  GTR-T5 Encoder (vec2text method)   â”‚                                   â”‚
â”‚  â”‚  5 texts â†’ 5 vectors [5, 768]       â”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â†“                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  STEP 2: LVM Prediction             â”‚    ~0.5 ms (0.05%) âš¡             â”‚
â”‚  â”‚  LSTM Model                          â”‚                                   â”‚
â”‚  â”‚  [5, 768] â†’ [768]                   â”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â†“                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  STEP 3: Vector Decoding            â”‚    ~10,000 ms (97%) â±ï¸            â”‚
â”‚  â”‚  Vec2Text Decoder (JXE)             â”‚    ** BOTTLENECK **              â”‚
â”‚  â”‚  [768] â†’ Text                       â”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â†“                                                                      â”‚
â”‚  Output: Predicted Next Chunk (text)                                       â”‚
â”‚                                                                             â”‚
â”‚  TOTAL LATENCY: ~10,300 ms per prediction                                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Data Synchronization (PostgreSQL â†” FAISS)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INGESTION API (Port 8004)              â”‚
â”‚  Atomic writes to PostgreSQL + FAISS    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Concept Entry â”‚
        â”‚ + Metadata    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚   â”‚ FAISS       â”‚
â”‚              â”‚   â”‚             â”‚
â”‚ INSERT INTO  â”‚   â”‚ index.add() â”‚
â”‚ cpe_entry    â”‚   â”‚             â”‚
â”‚              â”‚   â”‚ index.save()â”‚ â† CRITICAL!
â”‚              â”‚   â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        âœ… Synchronized
   CPE ID links both stores
```

**Critical Rules**:
- Every concept MUST have a unique CPE ID (UUID)
- PostgreSQL entry and FAISS vector MUST be written atomically
- FAISS index MUST call `.save()` after `.add()` (Oct 4 fix)
- NPZ files correlate: vector index â†’ CPE ID â†’ concept text

**Verification**:
```bash
# Check sync status
psql -U lnsp -d lnsp -c "SELECT COUNT(*) FROM cpe_entry;"
# Result: 80,636

python3 -c "import faiss; idx = faiss.read_index('artifacts/wikipedia_500k_corrected_ivf_flat_ip.index'); print(idx.ntotal)"
# Result: ~500,000 (includes variations)
```

---

## ğŸ§  LVM Training Data Flow

```
Wikipedia Articles (500k chunks)
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sequential Ordering      â”‚
â”‚ Maintain article order   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GTR-T5 Encoding          â”‚
â”‚ Each chunk â†’ 768D vector â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ wikipedia_500k_corrected_vectors.npz â”‚
â”‚ [500k, 768] + texts + IDs            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create Training Pairs    â”‚
â”‚ Sliding window:          â”‚
â”‚ [chunk_i-5 ... chunk_i-1]â”‚
â”‚          â†“               â”‚
â”‚      chunk_i             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ training_sequences_ctx5.npz          â”‚
â”‚ â€¢ context: [80k, 5, 768]             â”‚
â”‚ â€¢ target: [80k, 768]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Train 4 LVM Models       â”‚
â”‚ MSE Loss, 20 epochs      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Best Models (Oct 16, 2025)           â”‚
â”‚ â€¢ AMN: 0.5664 (fastest)              â”‚
â”‚ â€¢ LSTM: 0.5758 (best balance) â­     â”‚
â”‚ â€¢ GRU: 0.5754                        â”‚
â”‚ â€¢ Transformer: 0.5820 (most accurate)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” vecRAG Query Flow

```
User Query: "What is machine learning?"
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GTR-T5 Encoding          â”‚
â”‚ Text â†’ [768] vector      â”‚
â”‚ (vec2text-compatible)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FAISS Similarity Search              â”‚
â”‚ Query vector vs 500k corpus          â”‚
â”‚ Inner Product (IP) similarity        â”‚
â”‚ IVF search: nlist=512, nprobe=16     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Top-K Results            â”‚
â”‚ [k] indices + distances  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Map Index â†’ CPE ID       â”‚
â”‚ Using NPZ metadata       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL Lookup                    â”‚
â”‚ SELECT * FROM cpe_entry              â”‚
â”‚ WHERE cpe_id IN (...)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Results with Full Metadata           â”‚
â”‚ â€¢ Concept text                       â”‚
â”‚ â€¢ Probe questions                    â”‚
â”‚ â€¢ Expected answers                   â”‚
â”‚ â€¢ CPESH negatives                    â”‚
â”‚ â€¢ Cosine similarity scores           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
Return to User
```

---

## ğŸ¯ LVM Inference Flow (Textâ†’Text)

```
User Provides Context:
  "Chunk 1: AI is...",
  "Chunk 2: Machine learning...",
  "Chunk 3: Neural networks...",
  "Chunk 4: Deep learning...",
  "Chunk 5: Training data..."
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ENCODING (300ms)                      â”‚
â”‚ GTR-T5 Encoder                        â”‚
â”‚ 5 texts â†’ 5 vectors [5, 768]         â”‚
â”‚ Method: IsolatedVecTextVectOrchestrator â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LVM INFERENCE (0.5ms) âš¡               â”‚
â”‚ LSTM Model                            â”‚
â”‚ Input: [1, 5, 768] (batch=1)         â”‚
â”‚ Output: [1, 768] predicted vector     â”‚
â”‚ model.eval(), torch.no_grad()         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DECODING (10,000ms) â±ï¸                 â”‚
â”‚ Vec2Text Decoder (JXE)                â”‚
â”‚ Iterative refinement (1 step default) â”‚
â”‚ Input: [768] vector                   â”‚
â”‚ Output: Text string                   â”‚
â”‚ ** THIS IS THE BOTTLENECK **          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
Predicted Next Chunk:
  "Models require supervised learning..."

QUALITY METRICS:
  â€¢ Vector cosine: 0.45-0.55 (semantic alignment)
  â€¢ ROUGE-1: 0.10-0.15 (low expected)
  â€¢ BLEU: 0.00-0.02 (very low expected)

Note: Low ROUGE/BLEU is normal!
This is semantic prediction, not text generation.
```

---

## ğŸ“¦ Storage Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PERSISTENT STORAGE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Tier 1: Structured Data                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ PostgreSQL: /opt/homebrew/var/postgresql@17/         â”‚      â”‚
â”‚  â”‚ â€¢ cpe_entry: 80,636 rows (concepts, metadata)        â”‚      â”‚
â”‚  â”‚ â€¢ cpe_vectors: 80,636 rows (768D vectors)            â”‚      â”‚
â”‚  â”‚ â€¢ Size: ~1.5 GB                                       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â”‚  Tier 2: Vector Indexes                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ FAISS: ./artifacts/*.index                            â”‚      â”‚
â”‚  â”‚ â€¢ wikipedia_500k_corrected_ivf_flat_ip.index          â”‚      â”‚
â”‚  â”‚ â€¢ Size: 238 MB                                        â”‚      â”‚
â”‚  â”‚ â€¢ Vectors: 500k                                       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â”‚  Tier 3: Raw Vectors                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ NPZ Files: ./artifacts/*.npz                          â”‚      â”‚
â”‚  â”‚ â€¢ wikipedia_500k_corrected_vectors.npz (230 MB)      â”‚      â”‚
â”‚  â”‚ â€¢ training_sequences_ctx5.npz (449 MB)               â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â”‚  Tier 4: Trained Models                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ LVM Models: ./artifacts/lvm/models/                   â”‚      â”‚
â”‚  â”‚ â€¢ 4 architectures Ã— ~50-200 MB each                  â”‚      â”‚
â”‚  â”‚ â€¢ Total: ~200 MB                                      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â”‚  Tier 5: Caches                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ SQLite: ./artifacts/*.db                              â”‚      â”‚
â”‚  â”‚ â€¢ cpesh_index.db (20 KB)                              â”‚      â”‚
â”‚  â”‚ â€¢ mlflow.db                                           â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TOTAL ACTIVE STORAGE: ~2.4 GB
```

---

## âš¡ Performance Characteristics

### Latency Breakdown by Component

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMPONENT LATENCY (single query)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  GTR-T5 Encoding (per chunk):        ~50-100 ms           â”‚
â”‚  â”œâ”€ Tokenization:                     ~5 ms               â”‚
â”‚  â”œâ”€ Transformer forward:              ~40 ms              â”‚
â”‚  â””â”€ Pooling + normalization:          ~5 ms               â”‚
â”‚                                                             â”‚
â”‚  LVM Inference:                       0.49-2.68 ms âš¡      â”‚
â”‚  â”œâ”€ AMN:                              0.49 ms (fastest)   â”‚
â”‚  â”œâ”€ LSTM:                             0.56 ms (best)      â”‚
â”‚  â”œâ”€ GRU:                              2.08 ms             â”‚
â”‚  â””â”€ Transformer:                      2.68 ms (best acc)  â”‚
â”‚                                                             â”‚
â”‚  Vec2Text Decoding (JXE):             ~10,000 ms â±ï¸        â”‚
â”‚  â”œâ”€ Iterative refinement (1 step):   ~10,000 ms          â”‚
â”‚  â””â”€ THIS IS THE BOTTLENECK            (97% of total)      â”‚
â”‚                                                             â”‚
â”‚  FAISS Search (vecRAG):               ~5-20 ms            â”‚
â”‚  â”œâ”€ IVF quantization:                 ~2 ms               â”‚
â”‚  â”œâ”€ Search nprobe clusters:           ~10 ms              â”‚
â”‚  â””â”€ Rerank top-K:                     ~5 ms               â”‚
â”‚                                                             â”‚
â”‚  PostgreSQL Lookup:                   ~1-5 ms             â”‚
â”‚  â””â”€ Indexed UUID lookup:              ~1 ms               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FULL PIPELINE LATENCY:
  Encoding (5 chunks):     ~300 ms   (3%)
  LVM Inference:           ~0.5 ms   (0.05%)
  Vec2Text Decoding:       ~10,000 ms (97%)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL:                   ~10,300 ms

OPTIMIZATION PRIORITY:
  1. ğŸ¯ Vec2text decoding (10s â†’ 2s)
  2. Encoding batching (300ms â†’ 100ms)
  3. LVM (already excellent at 0.5ms)
```

---

## ğŸ”„ Critical Data Correlations

All data must correlate via **CPE ID (UUID)**:

```
PostgreSQL cpe_entry
    â”œâ”€ cpe_id: "123e4567-e89b-12d3-a456-426614174000"
    â”œâ”€ concept_text: "Machine learning uses algorithms..."
    â””â”€ metadata: {...}
          â”‚
          â”‚ (Same UUID)
          â†“
PostgreSQL cpe_vectors
    â”œâ”€ cpe_id: "123e4567-e89b-12d3-a456-426614174000"
    â””â”€ concept_vec: [0.123, 0.456, ...] (768D)
          â”‚
          â”‚ (Same UUID)
          â†“
FAISS Index
    â”œâ”€ index position: 42
    â””â”€ vector: [0.123, 0.456, ...] (768D)
          â”‚
          â”‚ (via NPZ metadata)
          â†“
NPZ File (wikipedia_500k_corrected_vectors.npz)
    â”œâ”€ vectors[42]: [0.123, 0.456, ...] (768D)
    â”œâ”€ texts[42]: "Machine learning uses algorithms..."
    â””â”€ ids[42]: "123e4567-e89b-12d3-a456-426614174000"

WITHOUT CORRELATION: System breaks!
  âŒ vecRAG: Can't map FAISS index â†’ concept text
  âŒ LVM Training: Can't create text-aligned sequences
  âŒ LVM Inference: Can't decode vector â†’ text
```

---

## ğŸ“ Best Practices Summary

### âœ… DO:
1. **Always use vec2text-compatible GTR-T5 encoding**
   - Method: `IsolatedVecTextVectOrchestrator.encode_texts()`
   - Never use `sentence-transformers` directly

2. **Maintain CPE ID correlation across all stores**
   - PostgreSQL, FAISS, NPZ must share same UUID

3. **Use sequential data for LVM training**
   - Wikipedia âœ…, Textbooks âœ…, Papers âœ…
   - Ontologies âŒ (taxonomic, not sequential)

4. **Call FAISS `.save()` after every `.add()`**
   - Critical for persistence

5. **Monitor sync status regularly**
   ```bash
   ./scripts/verify_data_sync.sh
   ```

### âŒ DON'T:
1. **Don't use ontology data for LVM training**
   - Reason: Taxonomic hierarchies, not sequential narrative

2. **Don't use InfoNCE loss for LVM**
   - Reason: LVM is regression (MSE), not contrastive learning

3. **Don't skip vec2text encoding method**
   - sentence-transformers is 9.9x worse for vec2text decoding

4. **Don't expect high ROUGE/BLEU scores**
   - LVM does semantic prediction, not token generation
   - Low scores are expected and normal

---

## ğŸ“š Related Documentation

- **Database Locations**: `docs/DATABASE_LOCATIONS.md`
- **LVM Training Data**: `docs/LVM_DATA_MAP.md`
- **Training Rules**: `CLAUDE.md` + `LNSP_LONG_TERM_MEMORY.md`
- **Performance**: `artifacts/lvm/COMPREHENSIVE_LEADERBOARD.md`
- **Architecture**: `docs/TOKENLESS_MAMBA_ARCHITECTURE.md`

---

## ğŸš€ Quick Start Commands

```bash
# Check system status
psql -U lnsp -d lnsp -c "SELECT COUNT(*) FROM cpe_entry;"
brew services list | grep -E 'postgresql|neo4j'

# Verify FAISS index
python3 -c "import faiss; idx = faiss.read_index('artifacts/wikipedia_500k_corrected_ivf_flat_ip.index'); print(f'Vectors: {idx.ntotal}')"

# Test LVM inference
VEC2TEXT_FORCE_PROJECT_VENV=1 TOKENIZERS_PARALLELISM=false \
    ./venv/bin/python3 tools/lvm_text_output_examples.py

# Train new LVM model
./.venv/bin/python app/lvm/train_unified.py \
    --model-type lstm \
    --data artifacts/lvm/training_sequences_ctx5.npz \
    --epochs 20
```

---

**Last Updated**: October 16, 2025
**System Status**: âœ… All active components operational
**Next Review**: When adding Neo4j graph data
