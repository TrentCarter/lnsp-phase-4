# LLM Pricing Service Documentation

## Overview

The LLM Pricing Service provides **dynamic, cached pricing data** for all supported LLM providers with intelligent fallback to static pricing when APIs are unavailable.

**Location**: `services/webui/llm_pricing.py`

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   API Request                        â”‚
â”‚            (Get model pricing)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Pricing Service     â”‚
          â”‚  get_pricing()       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SQLite Cache  â”‚      â”‚  Provider API  â”‚
â”‚  (24h TTL)     â”‚      â”‚  (Live Query)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”‚    Cache Miss         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Static Fallbackâ”‚
            â”‚ (Hardcoded)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Features

### âœ… 1. Intelligent Caching (SQLite)

- **24-hour TTL** for API-sourced pricing
- **1-hour TTL** for fallback pricing
- Automatic expiration and refresh
- Persistent across service restarts

### âœ… 2. Multi-Provider Support

Supported providers:
- **OpenAI** - gpt-4, gpt-4o, gpt-3.5-turbo, gpt-5-codex
- **Anthropic** - Claude 3.5 & 4.5 series
- **Google** - Gemini 2.0 & 2.5 series
- **DeepSeek** - deepseek-r1, deepseek-chat
- **Kimi** - moonshot-v1 series

### âœ… 3. Graceful Fallback

1. Check SQLite cache first
2. If expired/missing, query provider API
3. If API fails, use static pricing
4. If no static pricing, return 0.0

### âœ… 4. Admin API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/admin/pricing/stats` | GET | Get cache statistics |
| `/api/admin/pricing/refresh` | POST | Force refresh all cached entries |
| `/api/admin/pricing/clear` | POST | Clear entire cache |

---

## Usage

### From Code

```python
from services.webui.llm_pricing import get_pricing_service

# Get singleton instance
pricing = get_pricing_service()

# Get pricing for a model (returns tuple: input_cost, output_cost per 1K tokens)
input_cost, output_cost = pricing.get_pricing('anthropic', 'claude-3-5-sonnet-20241022')

print(f"Input: ${input_cost}/1K tokens")
print(f"Output: ${output_cost}/1K tokens")
```

### From API

```bash
# Get cache statistics
curl http://localhost:6101/api/admin/pricing/stats

# Refresh all cached pricing
curl -X POST http://localhost:6101/api/admin/pricing/refresh

# Clear cache
curl -X POST http://localhost:6101/api/admin/pricing/clear
```

### From UI

Navigate to **HMI â†’ Settings â†’ System Status â†’ Pricing Cache Management**

Controls:
- **ğŸ”„ Refresh Pricing Cache** - Query all providers for latest pricing
- **ğŸ—‘ï¸ Clear Pricing Cache** - Remove all cached data (rebuilds on next request)
- **Cache Stats** - View cache hit rate, API vs fallback ratio

---

## Cache Statistics

The service tracks:
- `total_entries` - Number of cached model prices
- `from_api` - Entries fetched from provider APIs
- `from_fallback` - Entries using static fallback
- `expired` - Entries past TTL (will be refreshed on next access)
- `cache_hit_rate` - Percentage of valid cached entries

Example response:
```json
{
  "status": "ok",
  "stats": {
    "total_entries": 7,
    "from_api": 1,
    "from_fallback": 6,
    "expired": 0,
    "cache_hit_rate": "100.0%"
  }
}
```

---

## Provider-Specific Notes

### OpenAI
- No public pricing API
- Uses `/v1/models` endpoint to verify model exists
- Falls back to static pricing immediately

### Anthropic
- No public pricing API
- Verifies API key validity
- Falls back to static pricing immediately

### Google/Gemini
- No public pricing API
- Falls back to static pricing immediately

### DeepSeek
- No public pricing API
- Falls back to static pricing immediately

### Kimi/Moonshot
- No public pricing API
- Falls back to static pricing immediately

**Note**: Since no providers expose real-time pricing via API, the service currently relies on well-maintained static pricing with smart caching. Future enhancement: scrape provider pricing pages or use third-party pricing aggregators.

---

## Static Fallback Pricing

All prices are **per 1,000 tokens** (as of November 2025):

### OpenAI
- gpt-4-turbo: $0.01 / $0.03
- gpt-4: $0.03 / $0.06
- gpt-3.5-turbo: $0.0015 / $0.002
- gpt-4o: $0.0025 / $0.01
- gpt-4o-mini: $0.00015 / $0.0006
- gpt-5-codex: $0.05 / $0.15 (estimated)

### Anthropic
- claude-3-5-sonnet-20241022: $0.003 / $0.015
- claude-3-5-haiku-20241022: $0.0008 / $0.004
- claude-sonnet-4-5-20250929: $0.003 / $0.015
- claude-haiku-4-5: $0.0008 / $0.004
- claude-opus-4-5-20250929: $0.015 / $0.075

### Google
- gemini-2.5-pro: $0.00125 / $0.005
- gemini-2.5-flash: $0.000075 / $0.0003
- gemini-2.5-flash-lite: $0.00001875 / $0.000075
- gemini-2.0-flash: $0.00015 / $0.0006
- gemini-2.0-pro: $0.0015 / $0.006

### DeepSeek
- deepseek-r1: $0.00055 / $0.00219
- deepseek-chat: $0.00014 / $0.00028

### Kimi
- moonshot-v1-8k: $0.00012 / $0.00012
- moonshot-v1-32k: $0.00024 / $0.00024
- moonshot-v1-128k: $0.0006 / $0.0006

---

## Database Schema

**Location**: `artifacts/hmi/pricing_cache.db`

```sql
CREATE TABLE pricing_cache (
    provider TEXT NOT NULL,
    model_name TEXT NOT NULL,
    input_cost REAL NOT NULL,
    output_cost REAL NOT NULL,
    cached_at INTEGER NOT NULL,
    source TEXT NOT NULL,  -- 'api' or 'fallback'
    PRIMARY KEY (provider, model_name)
);
```

---

## Integration Points

### 1. HMI App (`hmi_app.py`)

The `_get_model_cost()` function now uses the pricing service:

```python
def _get_model_cost(provider, model_name, output=False):
    """Get cost per 1K tokens using dynamic pricing service"""
    from services.webui.llm_pricing import get_pricing_service

    try:
        pricing_service = get_pricing_service()
        input_cost, output_cost = pricing_service.get_pricing(provider, model_name)
        return output_cost if output else input_cost
    except Exception as e:
        logger.error(f"Pricing service error: {e}")
        return 0.0
```

### 2. API Models Endpoint

`/api/models/api-status` automatically includes pricing data:

```json
{
  "model_id": {
    "cost_per_1k_input": 0.003,
    "cost_per_1k_output": 0.015,
    "usage": {
      "total_tokens": 15234,
      "total_cost": 0.2435
    }
  }
}
```

### 3. Settings UI

Displays pricing cache stats and management controls in the System Status tab.

---

## Performance Characteristics

- **Cache lookup**: <1ms (SQLite indexed query)
- **API fetch** (when cache miss): 100-500ms depending on provider
- **Fallback lookup**: <1ms (in-memory dictionary)
- **Cache size**: ~1KB per model (negligible storage)

---

## Maintenance

### Regular Tasks

1. **Monitor cache hit rate**: Should be >90% under normal usage
2. **Review fallback ratio**: High `from_fallback` may indicate API issues
3. **Update static pricing**: Refresh quarterly from provider pricing pages
4. **Check expired entries**: Auto-cleaned on access, but can manually refresh

### Updating Static Pricing

Edit the `fallback_pricing` dictionary in `services/webui/llm_pricing.py`:

```python
self.fallback_pricing = {
    'openai': {
        'new-model': {'input': 0.005, 'output': 0.015},  # Add new model
        # ...
    }
}
```

---

## Troubleshooting

### Issue: All prices showing $0

**Cause**: Pricing service not initialized or fallback map missing model

**Solution**:
1. Check logs for `Pricing service error` messages
2. Verify model name matches exactly (case-sensitive)
3. Add model to `fallback_pricing` if missing
4. Clear cache: `curl -X POST http://localhost:6101/api/admin/pricing/clear`

### Issue: Cache hit rate <50%

**Cause**: Frequent cache expiration or new models constantly being queried

**Solution**:
1. Increase TTL (default 24h): `LLMPricingService(ttl_hours=48)`
2. Pre-warm cache with common models
3. Check for model name variations causing duplicate entries

### Issue: API fetches failing

**Cause**: Provider API changes or network issues

**Solution**:
1. Check provider API status
2. Verify API keys in `.env`
3. Service gracefully falls back to static pricing
4. Update provider-specific fetchers if API changed

---

## Future Enhancements

### Priority 1: Real-Time Pricing APIs
- Integrate with third-party pricing aggregators
- Scrape official pricing pages (with caching)
- Subscribe to provider pricing update notifications

### Priority 2: Cost Prediction
- Estimate costs based on prompt length before sending
- Historical cost trending and budgeting
- Cost alerts and anomaly detection

### Priority 3: Multi-Currency Support
- Convert pricing to user's preferred currency
- Real-time FX rates
- Regional pricing differences

---

## Testing

### Unit Tests

```bash
# Test pricing service
./.venv/bin/python -c "
from services.webui.llm_pricing import get_pricing_service

pricing = get_pricing_service()

# Test various providers
tests = [
    ('anthropic', 'claude-3-5-sonnet-20241022'),
    ('google', 'gemini-2.5-flash'),
    ('openai', 'gpt-4o'),
]

for provider, model in tests:
    input_cost, output_cost = pricing.get_pricing(provider, model)
    assert input_cost > 0, f'{provider}/{model} has zero input cost'
    assert output_cost > 0, f'{provider}/{model} has zero output cost'
    print(f'âœ“ {provider}/{model}: \${input_cost}/\${output_cost}')

print('âœ“ All tests passed')
"
```

### Integration Tests

```bash
# Test via API endpoint
curl -s http://localhost:6101/api/models/api-status | \
  python3 -c "
import sys, json
data = json.load(sys.stdin)
for model_id, model in data['models'].items():
    assert model['cost_per_1k_input'] > 0, f'{model_id} has zero input cost'
    assert model['cost_per_1k_output'] > 0, f'{model_id} has zero output cost'
    print(f'âœ“ {model_id}: \${model[\"cost_per_1k_input\"]}/\${model[\"cost_per_1k_output\"]}')
print('âœ“ All API models have valid pricing')
"
```

---

## Migration Guide

### From Static Pricing

If you were using the old hardcoded `_get_model_cost()` function:

**Before**:
```python
cost = 0.003  # Hardcoded
```

**After**:
```python
from services.webui.llm_pricing import get_pricing_service

pricing = get_pricing_service()
input_cost, output_cost = pricing.get_pricing('anthropic', 'claude-3-5-sonnet')
```

No changes needed in calling code - `_get_model_cost()` now uses the service internally.

---

## Contact

For questions or issues related to the pricing service:
- Check logs: `tail -f logs/hmi.log | grep -i pricing`
- Review cache: `sqlite3 artifacts/hmi/pricing_cache.db "SELECT * FROM pricing_cache;"`
- File issue: Include model name, provider, and error message

---

**Version**: 1.0.0
**Last Updated**: 2025-11-13
**Author**: DirEng (Claude Code)
