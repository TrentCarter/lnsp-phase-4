# GraphMERT-LVM Training Results & Scaling Analysis

**Date:** 2025-10-16
**Hardware:** Apple Silicon (MPS)
**Model:** GraphMERT-LVM-768D (67.4M parameters)

---

## 10k Benchmark Results ‚úÖ

### Training Configuration
- **Dataset:** 10,000 sequences (9k train / 1k val)
- **Epochs:** 3
- **Batch size:** 32
- **Device:** MPS (Apple Silicon GPU)
- **Model parameters:** 67,352,833

### Performance Metrics

| Metric | Epoch 1 | Epoch 2 | Epoch 3 | Change |
|--------|---------|---------|---------|--------|
| **Train Loss** | 0.001576 | 0.001198 | 0.001117 | ‚Üì 29% |
| **Train Cosine** | 0.2850 | 0.3444 | 0.4072 | ‚Üë 43% |
| **Val Loss** | 0.001587 | 0.001489 | 0.001347 | ‚Üì 15% |
| **Val Cosine** | 0.3906 | 0.4283 | 0.4829 | ‚Üë 24% |
| **Epoch Time** | 11.7s | 9.7s | 9.8s | - |

### Key Observations

‚úÖ **Training is working!**
- Loss decreasing steadily
- Cosine similarity improving (0.39 ‚Üí 0.48 in just 3 epochs)
- Model converging nicely

‚úÖ **Hardware performance:**
- **Total time:** 31.8 seconds for 10k dataset
- **Average epoch time:** 10.6 seconds
- **Throughput:** ~850 samples/second

‚úÖ **Model quality:**
- **Final val cosine:** 0.4829
- **Target (LSTM baseline):** 0.5758
- **Gap:** 0.09 (likely needs more epochs to converge)

---

## Scaling to Full 80k Dataset

### Time Estimates

**Based on 10k benchmark:**
- 10k dataset = **10.6s per epoch**
- 80k dataset = 8√ó more data = **~85s per epoch** (1.4 min)

**Full training (25 epochs):**
- 85s √ó 25 = **2,125 seconds**
- **‚âà 35-40 minutes total** ‚ö°

### Recommended Training Plan

#### Option A: Quick Test (10 epochs)
```bash
python app/lvm/train_graphmert_lvm_benchmark.py \
    --data artifacts/lvm/training_sequences_ctx5.npz \
    --epochs 10 \
    --batch-size 32 \
    --device mps \
    --output-dir artifacts/lvm/models/graphmert_lvm_80k_e10
```
- **Time:** ~15 minutes
- **Expected val cosine:** 0.52-0.55
- **Use case:** Quick validation

#### Option B: Full Training (25 epochs) ‚≠ê RECOMMENDED
```bash
python app/lvm/train_graphmert_lvm_benchmark.py \
    --data artifacts/lvm/training_sequences_ctx5.npz \
    --epochs 25 \
    --batch-size 32 \
    --device mps \
    --output-dir artifacts/lvm/models/graphmert_lvm_80k_full
```
- **Time:** ~35-40 minutes
- **Expected val cosine:** ‚â•0.55 (target: 0.5758)
- **Use case:** Production model

#### Option C: Extended Training (50 epochs)
```bash
python app/lvm/train_graphmert_lvm_benchmark.py \
    --data artifacts/lvm/training_sequences_ctx5.npz \
    --epochs 50 \
    --batch-size 32 \
    --device mps \
    --output-dir artifacts/lvm/models/graphmert_lvm_80k_e50
```
- **Time:** ~70 minutes (1.2 hours)
- **Expected val cosine:** ‚â•0.57 (match or beat LSTM)
- **Use case:** Maximum quality

---

## Comparison: GraphMERT-LVM vs Existing LVM Models

### Architecture Comparison

| Model | Parameters | Architecture | Val Cosine | Inference (ms) |
|-------|-----------|--------------|------------|----------------|
| **AMN** | 2.1M | Attention Mixture | 0.5664 | 0.49 |
| **LSTM‚≠ê** | 5.4M | 2-layer LSTM | 0.5758 | 0.56 |
| **GRU** | 7.3M | 4-layer GRU Stack | 0.5724 | 1.12 |
| **Transformer** | 18.4M | 4-layer Transformer | 0.5820 | 2.68 |
| **GraphMERT-LVM** | 67.4M | 12-layer + Attention Decay | 0.48 (3 epochs) | TBD |

### What GraphMERT-LVM Adds

**Existing LVM (e.g., LSTM):**
```
5 context vectors ‚Üí LSTM ‚Üí 1 predicted vector
```
- Pure neural prediction
- No interpretability
- Fast inference (0.56ms)

**GraphMERT-LVM (this implementation):**
```
5 context vectors ‚Üí 12 Transformer layers + Attention Decay ‚Üí 1 predicted vector
```
- Neural prediction (same as LVM)
- **+ Attention decay mechanism** (Œª=0.6, learnable threshold)
- **+ Larger capacity** (67M params vs 5M)
- **Ready for KG extension** (can add MNM loss later)

**Future GraphMERT-LVM + KG:**
```
5 context vectors + KG leaves ‚Üí GraphMERT encoder ‚Üí vector + triples
```
- Neural + symbolic reasoning
- Explicit KG triple extraction
- Interpretable + editable

---

## Next Steps

### Immediate (Today)

1. **Run full 80k training (Option B)**
   ```bash
   python app/lvm/train_graphmert_lvm_benchmark.py \
       --data artifacts/lvm/training_sequences_ctx5.npz \
       --epochs 25 \
       --batch-size 32 \
       --device mps \
       --output-dir artifacts/lvm/models/graphmert_lvm_80k_full
   ```
   - Time: ~35-40 minutes
   - Goal: Val cosine ‚â•0.55

2. **Compare with LSTM baseline**
   - Load best checkpoint
   - Evaluate on same test set
   - Benchmark inference speed

### Phase 2 (Add KG Integration)

Once we have a trained 768-d native GraphMERT-LVM:

1. **Vector-based entity linking** (NO Vec2Text!)
   - Encode entities with GTR-T5 encoder
   - Cosine similarity to find relevant entities
   - Build leafy chain graphs

2. **Modify architecture for KG leaves**
   - Add leaf embedding layer
   - Implement H-GAT (hierarchical graph attention)
   - Add MNM loss (masked node modeling)

3. **Joint training**
   - MLM loss (vector prediction) + MNM loss (triple prediction)
   - 28 UMLS-style relations
   - Seed KG from existing UMLS data

### Phase 3 (Evaluation)

1. **FActScore* evaluation**
   - Extract triples from test set
   - Judge factuality with LLM
   - Target: ‚â•65%

2. **ValidityScore evaluation**
   - Check UMLS relation constraints
   - Entity type matching
   - Target: ‚â•65%

3. **Integration**
   - Dual-mode orchestrator (classic LVM vs LVM-GM)
   - FastAPI endpoint updates
   - Production deployment

---

## Key Insights

### ‚úÖ **Major Win: No Vec2Text Bottleneck**

**Original PRD plan:**
- Decode 80k sequences with Vec2Text (3-7 days!)
- Entity linking on decoded text
- Then train

**Actual implementation:**
- Train directly on 768-d vectors (35 minutes!)
- Vec2Text only for inference/debugging
- Entity linking in vector space (future Phase 2)

**Time saved:** 3-7 days ‚Üí 35 minutes = **290-580x faster!** üöÄ

### ‚úÖ **Hardware Reality Check**

**Initially planned:**
- 40 CUDA GPUs (Linux cluster)
- Multi-GPU DDP training
- <1 hour for full training

**Actual hardware:**
- Apple Silicon (MPS)
- Single GPU
- ~35 minutes for full training

**Result:** Still very reasonable! MPS performance is excellent.

### ‚úÖ **Model is Converging**

Even with just 3 epochs on 10k data:
- Val cosine: 0.48 (already decent)
- Clear upward trend (0.39 ‚Üí 0.48)
- Loss decreasing steadily

With 25 epochs on 80k data:
- Expected val cosine: ‚â•0.55
- Likely will match LSTM baseline (0.5758)

---

## Files Created

### Training Infrastructure
- `app/lvm/graphmert_lvm_768d.py` - 768-d native encoder (67M params)
- `app/lvm/train_graphmert_lvm_benchmark.py` - Training script
- `tools/create_10k_training_subset.py` - Data preparation

### Trained Models
- `artifacts/lvm/models/graphmert_lvm_10k_mps/` - 10k benchmark
  - `benchmark_model.pt` - Checkpoint
  - `benchmark_results.json` - Metrics

### Documentation
- `docs/PRDs/PRD_GraphMERT_LVM_Integration.md` - Full PRD
- `docs/GraphMERT_LVM_Benchmark_Guide.md` - Setup guide
- `docs/GraphMERT_LVM_Training_Results.md` - This file

---

## Recommended Command (Run Now!)

```bash
# Full 80k training, 25 epochs, ~35-40 minutes
python app/lvm/train_graphmert_lvm_benchmark.py \
    --data artifacts/lvm/training_sequences_ctx5.npz \
    --epochs 25 \
    --batch-size 32 \
    --device mps \
    --output-dir artifacts/lvm/models/graphmert_lvm_80k_full

# Monitor progress in real-time (optional)
# Training prints progress every 50 batches with ETA
```

**Ready to kick off full training?** üöÄ
