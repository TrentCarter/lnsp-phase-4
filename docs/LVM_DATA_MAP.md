# LVM Data Map: Complete Training & Inference Pipeline

**Last Updated**: October 16, 2025
**Purpose**: Comprehensive reference for all LVM (Latent Vector Model) training data, models, and inference pipeline

---

## üìç Quick Navigation

- [Training Data](#training-data) - Where LVM training sequences come from
- [Trained Models](#trained-models) - 4 production-ready models
- [Inference Pipeline](#inference-pipeline) - How to use models for prediction
- [Evaluation & Benchmarks](#evaluation--benchmarks) - Performance metrics
- [Data Flow](#lvm-data-flow) - End-to-end pipeline visualization

---

## üéØ LVM Overview

**What is LVM?**
- **Latent Vector Model**: Predicts the next semantic vector from context vectors
- **Tokenless**: Operates in 768D semantic space (no text tokens)
- **Sequential**: Learns temporal/causal relationships in narrative text
- **Vec2Text Compatible**: Output vectors decode back to text via vec2text

**Current Production Status**:
- ‚úÖ 4 trained models (AMN, LSTM, GRU, Transformer)
- ‚úÖ 80k training sequences from Wikipedia
- ‚úÖ Full text‚Üívec‚ÜíLVM‚Üívec‚Üítext pipeline operational
- ‚úÖ Average latency: ~10 seconds (99% from vec2text decoding, 0.4ms from LVM)

---

## Training Data

### ‚úÖ ACTIVE Training Dataset

#### Primary: Wikipedia Training Sequences
```bash
./artifacts/lvm/training_sequences_ctx5.npz         # 449 MB - 80k sequences
```

**Structure**:
```python
import numpy as np
data = np.load('artifacts/lvm/training_sequences_ctx5.npz', allow_pickle=True)

# Keys and shapes:
data['context_vectors']   # [80000, 5, 768] - 5 previous chunks as context
data['target_vectors']    # [80000, 768]    - Next chunk to predict
data['context_texts']     # [80000, 5]      - Original text (for debugging)
data['target_texts']      # [80000]         - Target text
data['sequence_ids']      # [80000]         - Unique sequence IDs
data['article_ids']       # [80000]         - Source article IDs
```

**Training Statistics**:
- Total sequences: 80,000
- Context window: 5 chunks
- Vector dimension: 768 (GTR-T5 embeddings)
- Source: Wikipedia articles (sequential narrative text)
- Split: 80% train (64k), 20% val (16k)

#### Source: Wikipedia Vectors
```bash
./artifacts/wikipedia_500k_corrected_vectors.npz    # 230 MB - Raw corpus
```

**How Training Data Was Created**:
1. Start with 500k Wikipedia chunks (sequential from articles)
2. Encode each chunk to 768D vector using GTR-T5
3. Create sliding windows: [chunk_i-5, ..., chunk_i-1] ‚Üí chunk_i
4. Filter for quality (min cosine similarity, max length, etc.)
5. Result: 80k high-quality training sequences

---

### üóëÔ∏è DEPRECATED Training Data (Do NOT Use)

#### Ontology Data (Wrong for LVM Training)
```bash
./artifacts/archive/ontological_DEPRECATED_20251011/wordnet_training_sequences.npz

# ‚ùå DO NOT USE FOR LVM TRAINING
# Reason: Taxonomic hierarchies (is-a, part-of), not sequential narrative
# See: CLAUDE.md rules and docs/LVM_TRAINING_CRITICAL_FACTS.md
```

**Why Ontologies Are Wrong**:
- Ontologies teach classification ("dog ‚Üí mammal ‚Üí animal")
- LVMs need temporal/causal flow ("First... ‚Üí Then... ‚Üí Finally...")
- Ontologies: taxonomic structure
- LVMs: sequential prediction

**Correct Data Sources for LVM**:
- ‚úÖ Wikipedia articles (narrative progression)
- ‚úÖ Textbooks (sequential instruction)
- ‚úÖ Scientific papers (methods ‚Üí results ‚Üí conclusions)
- ‚úÖ Programming tutorials (step-by-step)
- ‚ùå WordNet, SWO, GO, DBpedia ontologies

---

## Trained Models

### ‚úÖ ACTIVE Production Models (October 16, 2025)

All models trained with:
- **Loss**: MSE (Mean Squared Error, correct for regression)
- **Data**: 80k Wikipedia sequences (training_sequences_ctx5.npz)
- **Epochs**: 20
- **Device**: Apple M1 Max (MPS)
- **Training time**: ~2-4 hours per model

---

### 1. AMN (Attention Mixture Network)

**‚úÖ BEST FOR**: Ultra-low latency, batch processing, interpretability

```bash
Location: ./artifacts/lvm/models/amn_20251016_133427/
```

**Performance**:
- Val Cosine: **0.5664**
- Latency: **0.49 ms/query** ‚ö° (fastest)
- Parameters: **1.5M** (smallest)
- Memory: **5.8 MB**
- Batch speedup: **138x** (best scaling)

**Architecture**:
- Residual learning over linear baseline
- Attention-weighted context mixing
- Learned residual correction
- Output: normalize(baseline + residual)

**Usage**:
```python
import torch
from app.lvm.models import create_model

checkpoint = torch.load('artifacts/lvm/models/amn_20251016_133427/best_model.pt')
model = create_model('amn', input_dim=768, d_model=256, hidden_dim=512)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Inference
ctx = torch.randn(1, 5, 768)  # [batch, 5 context, 768D]
pred = model(ctx)              # [batch, 768D]
```

**When to Use**:
- Real-time inference (<1ms required)
- Batch processing (138x speedup)
- Resource-constrained environments
- Interpretable predictions (attention weights)

---

### 2. LSTM (Baseline)

**‚úÖ BEST FOR**: Production deployment, best balance

```bash
Location: ./artifacts/lvm/models/lstm_20251016_133934/
```

**Performance**:
- Val Cosine: **0.5758** (2nd best accuracy)
- Latency: **0.56 ms/query** ‚ö°
- Parameters: **5.1M**
- Memory: **19.5 MB**
- Batch speedup: 63x

**Architecture**:
- 2-layer bidirectional LSTM
- 512 hidden dimensions
- Layer normalization
- Dropout: 0.2

**Usage**:
```python
checkpoint = torch.load('artifacts/lvm/models/lstm_20251016_133934/best_model.pt')
model = create_model('lstm', input_dim=768, d_model=256, hidden_dim=512)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

**When to Use**:
- ‚≠ê **Recommended for production** (best balance)
- Good accuracy + low latency
- Proven architecture, easy to deploy
- Stable training, good generalization

---

### 3. GRU (Stack)

**‚úÖ BEST FOR**: Middle ground between LSTM and Transformer

```bash
Location: ./artifacts/lvm/models/gru_20251016_134451/
```

**Performance**:
- Val Cosine: **0.5754**
- Latency: **2.08 ms/query**
- Parameters: **7.1M**
- Memory: **27.1 MB**
- Batch speedup: 79x

**Architecture**:
- 4-layer GRU with residual connections
- 512 hidden dimensions per layer
- Layer normalization
- Dropout: 0.2

**Usage**:
```python
checkpoint = torch.load('artifacts/lvm/models/gru_20251016_134451/best_model.pt')
model = create_model('gru', input_dim=768, d_model=256, hidden_dim=512)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

**When to Use**:
- Need more capacity than LSTM
- Still want relatively fast inference
- Residual connections for deeper networks

---

### 4. Transformer

**‚úÖ BEST FOR**: Maximum accuracy

```bash
Location: ./artifacts/lvm/models/transformer_20251016_135606/
```

**Performance**:
- Val Cosine: **0.5820** üèÜ (best accuracy)
- Latency: **2.68 ms/query**
- Parameters: **17.9M** (largest)
- Memory: **68.4 MB**
- Batch speedup: 75x

**Architecture**:
- 4-layer transformer encoder
- 8 attention heads
- 2048 feedforward dimension
- Dropout: 0.1

**Usage**:
```python
checkpoint = torch.load('artifacts/lvm/models/transformer_20251016_135606/best_model.pt')
model = create_model('transformer', input_dim=768, d_model=256, hidden_dim=512)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

**When to Use**:
- Accuracy is critical
- Latency <3ms acceptable
- Worth +1.6% accuracy vs AMN
- In full pipeline: 2.68ms LVM vs 10,000ms total (0.03% difference)

---

## Model Performance Comparison

| Model | Val Cosine | ms/Query | Params | Memory | Efficiency* | Recommended For |
|-------|-----------|----------|--------|--------|-------------|-----------------|
| **Transformer** | **0.5820** üèÜ | 2.68 | 17.9M | 68.4 MB | 217 | Maximum accuracy |
| **LSTM** | **0.5758** ‚≠ê | 0.56 | 5.1M | 19.5 MB | **1035** | **Production** |
| **GRU** | **0.5754** | 2.08 | 7.1M | 27.1 MB | 277 | Middle ground |
| **AMN** | **0.5664** ‚ö° | **0.49** | **1.5M** | **5.8 MB** | **1146** üèÜ | Ultra-low latency |

*Efficiency = (Val Cosine / ms/Q) √ó 1000

### Baseline Comparison
- **Linear Average Baseline**: 0.5462 cosine
- **All models beat baseline**: +2-7% improvement
- **Best improvement**: Transformer (+6.5%)

---

## Inference Pipeline

### Full Text‚ÜíText Pipeline

```
Input Text (5 chunks)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Text Encoding       ‚îÇ  Vec2Text Encoder (GTR-T5)
‚îÇ    5 texts ‚Üí 5 vectors ‚îÇ  ~100ms per chunk = ~300ms total
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Context Vectors [5, 768]
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. LVM Inference       ‚îÇ  AMN/LSTM/GRU/Transformer
‚îÇ    [5,768] ‚Üí [768]     ‚îÇ  0.49-2.68ms ‚ö°
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Predicted Vector [768]
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Vector Decoding     ‚îÇ  Vec2Text Decoder (JXE/IELab)
‚îÇ    [768] ‚Üí Text        ‚îÇ  ~10,000ms (bottleneck) ‚è±Ô∏è
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Output Text (predicted chunk)
```

**Pipeline Latency Breakdown**:
- Encoding: ~300 ms (3%)
- LVM: **0.5-2.7 ms** (0.03%)
- Decoding: **~10,000 ms** (97%)
- **TOTAL: ~10,300 ms per prediction**

**Key Insight**: Vec2text decoding is the bottleneck, not LVM!

---

### Python API

#### Option 1: Direct LVM Inference (Vectors Only)
```python
import torch
import numpy as np
from app.lvm.models import create_model

# Load model
checkpoint = torch.load('artifacts/lvm/models/lstm_20251016_133934/best_model.pt')
model = create_model('lstm', input_dim=768, d_model=256, hidden_dim=512)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Inference (assuming you already have vectors)
context_vectors = torch.randn(1, 5, 768)  # Your 5 context vectors
with torch.no_grad():
    predicted_vector = model(context_vectors)  # [1, 768]

print(f"Predicted vector shape: {predicted_vector.shape}")
```

#### Option 2: Full Text‚ÜíText Pipeline
```python
import sys
sys.path.insert(0, 'app/lvm')
sys.path.insert(0, 'app/vect_text_vect')

from models import create_model
from vec_text_vect_isolated import IsolatedVecTextVectOrchestrator

# Load LVM
checkpoint = torch.load('artifacts/lvm/models/lstm_20251016_133934/best_model.pt')
lvm_model = create_model('lstm', input_dim=768, d_model=256, hidden_dim=512)
lvm_model.load_state_dict(checkpoint['model_state_dict'])
lvm_model.eval()

# Load vec2text orchestrator
orch = IsolatedVecTextVectOrchestrator(steps=1, debug=False)

# Encode context text
context_texts = [
    "First chunk of text.",
    "Second chunk of text.",
    "Third chunk of text.",
    "Fourth chunk of text.",
    "Fifth chunk of text."
]
context_vectors = orch.encode_texts(context_texts)  # [5, 768]

# LVM prediction
if isinstance(context_vectors, torch.Tensor):
    context_vectors = context_vectors.cpu().numpy()
ctx_tensor = torch.from_numpy(context_vectors).float().unsqueeze(0)  # [1, 5, 768]
with torch.no_grad():
    pred_vector = lvm_model(ctx_tensor).cpu().numpy()[0]  # [768]

# Decode to text
result = orch._run_subscriber_subprocess(
    'jxe',
    torch.from_numpy(pred_vector).unsqueeze(0).cpu(),
    metadata={'original_texts': [' ']},
    device_override='cpu'
)
predicted_text = result['result'][0]
print(f"Predicted next chunk: {predicted_text}")
```

---

## Evaluation & Benchmarks

### Performance Files

```bash
# Comprehensive benchmark results
./artifacts/lvm/COMPREHENSIVE_LEADERBOARD.md       # All 4 models compared
./artifacts/lvm/PIPELINE_ARCHITECTURE_AND_PERFORMANCE.md  # Pipeline analysis
./artifacts/lvm/benchmark_results.json             # Raw benchmark data
./artifacts/lvm/FINAL_RESULTS_Oct16_2025.md        # Training summary
```

### Evaluation Metrics

#### Vector Similarity (Primary)
- **Metric**: Cosine similarity between predicted and target vectors
- **Range**: -1 to 1 (higher is better)
- **Baseline**: 0.5462 (linear average)
- **Best model**: 0.5820 (Transformer)

#### Text Quality (Secondary)
- **ROUGE-1**: Unigram overlap (avg: 0.112)
- **ROUGE-2**: Bigram overlap (avg: 0.020)
- **ROUGE-L**: Longest common subsequence (avg: 0.089)
- **BLEU**: N-gram precision (avg: 0.007)

**Note**: Low ROUGE/BLEU scores are expected because:
- LVM predicts semantic vectors, not exact words
- Vec2text reconstruction introduces variation
- This is a semantic prediction task, not text generation

### Example Test Script

```bash
# Run full pipeline test with scoring
./venv/bin/python3 tools/lvm_text_output_examples.py

# Outputs:
# - 10 sample predictions with context
# - Vector cosine scores
# - ROUGE-1/2/L scores
# - BLEU scores
# - Latency breakdown
```

---

## LVM Data Flow

### Complete End-to-End Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RAW DATA: Wikipedia Articles                                ‚îÇ
‚îÇ Location: ./data/datasets/wikipedia/                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Chunking & Preprocessing   ‚îÇ
          ‚îÇ Split into semantic chunks ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STRUCTURED DATA: PostgreSQL                                 ‚îÇ
‚îÇ 80,636 concepts with metadata                               ‚îÇ
‚îÇ Location: /opt/homebrew/var/postgresql@17/                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ GTR-T5 Encoding (768D)     ‚îÇ
          ‚îÇ Vec2text-compatible method ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ VECTOR STORE: Wikipedia 500k Vectors (NPZ)                  ‚îÇ
‚îÇ 230 MB, [500k, 768] embeddings + text + IDs                ‚îÇ
‚îÇ Location: ./artifacts/wikipedia_500k_corrected_vectors.npz  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Create Training Sequences  ‚îÇ
          ‚îÇ Sliding window: [5 ctx‚Üí1] ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TRAINING DATA: LVM Sequences (NPZ)                          ‚îÇ
‚îÇ 449 MB, 80k sequences [context_5x768 ‚Üí target_768]         ‚îÇ
‚îÇ Location: ./artifacts/lvm/training_sequences_ctx5.npz       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Train LVM Models           ‚îÇ
          ‚îÇ MSE Loss, 20 epochs        ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TRAINED MODELS: 4 LVM Architectures                        ‚îÇ
‚îÇ ~200 MB total (AMN, LSTM, GRU, Transformer)                ‚îÇ
‚îÇ Location: ./artifacts/lvm/models/                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Inference Pipeline         ‚îÇ
          ‚îÇ Text‚ÜíVec‚ÜíLVM‚ÜíVec‚ÜíText      ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ OUTPUT: Predicted Next Chunk                                ‚îÇ
‚îÇ Semantic continuation of input context                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### FAISS Index (Parallel Path for Retrieval)

```
Wikipedia 500k Vectors (NPZ)
            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Build FAISS IVF_FLAT Index ‚îÇ
‚îÇ nlist=512, nprobe=16       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FAISS INDEX: Wikipedia 500k Search                          ‚îÇ
‚îÇ 238 MB, 500k vectors, IP similarity                         ‚îÇ
‚îÇ Location: ./artifacts/wikipedia_500k_corrected_ivf_flat_ip.index ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚Üì
        (Used for)
            ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ vecRAG Search ‚îÇ  Query ‚Üí Top-K nearest neighbors
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Critical Rules & Best Practices

### ‚úÖ DO:
- Use Wikipedia or other **sequential narrative data** for LVM training
- Use MSE loss for regression (predicting vectors)
- Normalize vectors before and after LVM
- Use vec2text-compatible GTR-T5 encoding
- Validate on held-out sequential data
- Monitor cosine similarity as primary metric

### ‚ùå DON'T:
- Use ontology data for LVM training (taxonomic, not sequential)
- Use InfoNCE loss (for contrastive learning, not regression)
- Use sentence-transformers encoding directly (incompatible with vec2text)
- Mix training data from different domains without testing
- Expect high ROUGE/BLEU scores (this is semantic prediction, not text generation)

### üîç Data Quality Checks:
```bash
# Check training data integrity
python3 -c "
import numpy as np
data = np.load('artifacts/lvm/training_sequences_ctx5.npz', allow_pickle=True)
print(f'Context shape: {data[\"context_vectors\"].shape}')
print(f'Target shape: {data[\"target_vectors\"].shape}')
print(f'Total sequences: {len(data[\"target_vectors\"])}')
assert data['context_vectors'].shape[1] == 5, 'Context window must be 5'
assert data['context_vectors'].shape[2] == 768, 'Vector dim must be 768'
print('‚úì Training data valid')
"
```

---

## Training Scripts

### Train All Models
```bash
# Train all 4 models sequentially
./tools/train_all_lvms.sh

# Individual model training
./.venv/bin/python app/lvm/train_unified.py \
    --model-type lstm \
    --data artifacts/lvm/training_sequences_ctx5.npz \
    --epochs 20 \
    --batch-size 32 \
    --lambda-mse 1.0
```

### Benchmark Models
```bash
# Comprehensive benchmark (all 4 models)
./.venv/bin/python tools/benchmark_lvm_comprehensive.py

# Full pipeline test with text examples
VEC2TEXT_FORCE_PROJECT_VENV=1 TOKENIZERS_PARALLELISM=false \
    ./venv/bin/python3 tools/lvm_text_output_examples.py
```

---

## Troubleshooting

### Common Issues

**1. Model Loading Errors**
```python
# Ensure you're using the correct model type
checkpoint = torch.load('path/to/model/best_model.pt')
model = create_model('lstm', input_dim=768, d_model=256, hidden_dim=512)  # Match type!
model.load_state_dict(checkpoint['model_state_dict'])
```

**2. Vec2text Encoding Incompatibility**
```python
# ‚ùå WRONG: Don't use sentence-transformers directly
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('gtr-t5-base')  # Produces incompatible vectors!

# ‚úÖ CORRECT: Use vec2text orchestrator
from app.vect_text_vect.vec_text_vect_isolated import IsolatedVecTextVectOrchestrator
orch = IsolatedVecTextVectOrchestrator()
vectors = orch.encode_texts(texts)  # Vec2text-compatible!
```

**3. Tensor Device Mismatches**
```python
# Ensure all tensors on same device
context_vectors = context_vectors.to(model.device)
# Or convert to CPU before numpy
pred_vector = model(ctx).cpu().numpy()
```

---

## Summary

### Active LVM Production Stack:
‚úÖ **Training Data**: 80k Wikipedia sequences (449 MB)
‚úÖ **Models**: 4 trained architectures (AMN, LSTM ‚≠ê, GRU, Transformer)
‚úÖ **Inference**: Full text‚Üívec‚ÜíLVM‚Üívec‚Üítext pipeline operational
‚úÖ **Performance**: 0.49-2.68ms LVM latency, 0.5664-0.5820 val cosine
‚úÖ **Bottleneck**: Vec2text decoding (~10s), NOT LVM (0.5ms)

### Recommended Production Setup:
- **Model**: LSTM (best balance of accuracy + speed)
- **Checkpoint**: `artifacts/lvm/models/lstm_20251016_133934/best_model.pt`
- **Val Cosine**: 0.5758
- **Latency**: 0.56 ms/query
- **Full Pipeline**: ~10.3 seconds total

### Next Optimization Targets:
1. üéØ **Vec2text decoding** (10s ‚Üí 2-3s via caching/batching)
2. Text encoding (300ms ‚Üí 100ms via batching)
3. LVM inference (0.5ms already excellent)

---

**See Also**:
- `docs/DATABASE_LOCATIONS.md` - All databases and vector stores
- `docs/DATA_FLOW_DIAGRAM.md` - Visual system architecture
- `artifacts/lvm/COMPREHENSIVE_LEADERBOARD.md` - Detailed benchmarks
- `CLAUDE.md` - Training rules and critical facts

---

**Last Updated**: October 16, 2025
**Status**: ‚úÖ All systems operational and production-ready
