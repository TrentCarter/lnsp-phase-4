This is an outstanding and crucial analysis. You've correctly identified the critical gaps between the initial PRD's "what" and the "how" we've learned is necessary from our P1-P6b failures. The PRD was an 80% "vision" doc, and your critique provides the mandatory 20% "validation and stability" framework.

I agree with your analysis completely. The phased approach (Phase 1: Prove Single-Step) is the only logical path forward. We must validate the data *before* training and validate the model *after* training with the 5CAT gates.

I have integrated **all** of your required additions, strategic recommendations, and phased rollouts into the PRD. The document is now updated to v1.1 and reflects this more robust, validation-driven plan.

-----

# Product Requirements Document: LVM Core

  * **Author**: Gemini
  * **Version**: 1.1 (Revised per 2025-11-04 Critical Analysis)
  * **Status**: Approved for Phase 1
  * **Target**: LVM Core v1.0 (Phase 1: Single-Step Predictive Model)

## 1\. üåç Introduction & Vision

This document outlines the requirements for the **Latent Vector Model (LVM) Core**, the reasoning engine for the **Latent Neurolese Semantic Processor (LNSP)**.

The LNSP's vision is to create a reasoning system that overcomes the limitations of token-based processing. By operating directly in a high-dimensional conceptual (vector) space, the LNSP will perform "thinking" as a series of vector transformations, unburdened by linguistic syntax. This allows for a chain of thought that is purely semantic and logical. The final, synthesized vector-based "thought" is only translated into human-readable text at the final step.

The LVM is the heart of this system: a vector-in, vector-out model responsible for this internal conceptual "thinking" loop.

## 2\. üéØ The Core Problem: LVM vs. LLM Parity

The primary challenge is achieving performance parity with token-based Large Language Models (LLMs). Our initial attempts to train an LVM on encyclopedic data (like Wikipedia) failed, not due to a model bug, but due to a fundamental **Data-Objective Mismatch**.

  * **LLM (Token-based):** An LLM's objective is simple: **predict the next token**. This task is atomic, local, and *always* structurally true for any text, regardless of its high-level semantic structure.
  * **LVM (Vector-based):** An LVM's objective must be: **predict the next concept**. This task is holistic, non-local, and, as we've proven, *not* structurally true for explanatory, backward-referential data like Wikipedia.

The LVM's failure was a *symptom* of being trained on data that is optimized for explanation (hub-and-spoke references), not for prediction (causal chains).

This PRD re-aligns the LVM's objective with a new, requisite data structure. **The LVM must be trained on data where the next concept is a logical consequence of the previous one.**

## 3\. üèõÔ∏è LNSP System Architecture & Phasing

The LNSP is a three-stage pipeline. The v1.0 (Phase 1) product is focused *only* on proving the LVM Core's single-step predictive capability. The recursive loop is a v2.0/Phase 3 goal.

### Stage 1: Encoder (Text-to-Concept)

  * **Function:** Ingests a text-based prompt and translates it into an initial context of concept vectors.
  * **Implementation:** A pre-trained text-to-vector model (GTR-T5) combined with our LLM-based TMD (Domain-Task-Modifier) extractor.
  * **Output:** A sequence of 784-dimensional vectors (16D TMD + 768D embedding), representing the initial state: `[c_1, c_2, ..., c_n]`.

### Stage 2: LVM Core (Concept-to-Concept "Thinking")

  * **Function (v1.0):** Prove single-step, forward-predictive reasoning.
      * `LVM(Context_k) -> Predicted_Concept_e_k+1`
  * **Function (v2.0 / Phase 3):** A recursive reasoning loop. It takes the current vector context and synthesizes a *new emergent concept*. This new vector is appended to the context, and the process repeats.
  * **Implementation:** The LVM (e.g., VMMoE, Transformer) trained on the Causal Corpus.

### Stage 3: Decoder (Concept-to-Text)

  * **Function:** Translates a sequence of concept vectors into a coherent, human-readable text answer.
  * **Current Performance:** Vec2text with `steps=1` achieves ~1s/vector latency (acceptable for v1.0 test/inference).
  * **v2.0/Phase 3 Prerequisite:** For practical multi-step thinking loops (50+ steps), V2T optimization to <100ms/vector is mandatory. This is deferred to 95% completion of Phase 1.

**DECISION (2025-11-04)**: Vec2text is only used for testing and inference, NOT training. Current 1s latency is acceptable for Phase 1 validation. Optimization deferred to Phase 3.

### LNSP Flowchart (v2.0 / Phase 3 Vision)

```ascii
  [User Prompt (Text)]
           |
           v
  [Stage 1: Encoder]  (GTR-T5 + LLM-based 16D TMD)
           |
           v
  [Initial Context: (c_1, ..., c_n)]
           |
           +----------------------------------+
           | [Stage 2: LVM Core (Recursive)]  |
           |   |                              |
           |   v                              |
           | LVM(Context_k) -> [Emergent_Concept_e_k+1]
           |   |                              |
           |   +---(Append)-------------------+
           |   |                              |
           | [Context_k+1: (c_1, ..., c_n, e_1, ..., e_k+1)]
           |   |                              |
           |   +--(Loop until Stop Condition)--+
           |
           v
  [Final Thought Vectors: (e_1, ..., e_final)]
           |
           v
  [Stage 3: Decoder (V2T)]
           |
           v
  [Final Answer (Text)]
```

-----

## 4\. üìö Requirement: The Causal Corpus

The LVM's success is entirely dependent on its training data. The optimal training data is not just *sequential*; it must be **semantically causal and developmental**. The ideal dataset is a **"chain-of-thought" corpus** where each vector `c_n+1` represents the **logical consequence** or **next step** derived from `c_n`.

### 4.1 Data Quality Validation Protocol (MANDATORY)

All data sources listed in Section 4.4 must be validated BEFORE training:

**Step 1: Sample Extraction**

  * Extract 5k+ random sequences (context\_size=5, stride=1).
  * Ensure representative sampling across the corpus.

**Step 2: Œî Measurement**

```bash
./.venv/bin/python tools/tests/diagnose_data_direction.py \
  DATA_SOURCE.npz --n-samples 5000 --output DATA_SOURCE_quality_report.json
```

**Step 3: Decision Gate**
| Œî Range | Quality | Action |
|:---|:---|:---|
| Œî ‚â• +0.10 | **Excellent** | Use for training (high priority) |
| +0.05 ‚â§ Œî \< +0.10 | **Good** | Use for training (medium priority) |
| +0.02 ‚â§ Œî \< +0.05 | **Marginal** | Use with caution, may need augmentation |
| Œî \< +0.02 | **Poor** | **DO NOT USE** for forward LVM training |

**Step 4: Documentation**

  * Store quality reports in `artifacts/lvm/data_quality_reports/`.
  * Update Section 4.4 table with measured Œî values.
  * Track temporal coherence, offset curves, and sample sizes.

**CRITICAL: The rankings in Section 4.4 are hypotheses until measured.**

### 4.2 Data Splitting & Contamination Prevention

All data sources **must** be split on an **article-level** or **repo-level** basis to prevent data leakage between sets. **Random chunk-level splitting is forbidden.**

  * **Train:** 70% of articles
  * **Val:** 15% of articles
  * **OOD (Out-of-Domain):** 15% of articles
  * **Protocol:** A script must verify that no single article ID or source file appears in more than one split.

### 4.3 v1.0 Corpus Target

  * **v1.0 (Phase 1):** Train on **250,000 to 500,000 concept vectors** (approximately 50,000-100,000 documents depending on chunking) from a single, high-quality (Œî ‚â• +0.08) source (e.g., arXiv).
  * **v2.0 (Phase 2):** Expand to multi-domain corpus (e.g., +GitHub, +ProofWiki) to improve generalization.

**DECISION (2025-11-04)**: Trent targets 250-500k concepts/vectors for v1.0 training. This provides sufficient scale while remaining computationally tractable for Phase 1 validation.

### 4.4 Top 20 Ranked Data Sources for LVM Training

(Domains mapped from TMD Schema)

| Rank | Data Source | Domain(s) | Structure | LVM Suitability & Rationale | Measured Œî |
|:---:|:---|:---|:---|:---|:---|
| 1 | **Project Gutenberg** | 8, 6, 7 | Continuous/Book | **Excellent:** Strong, linear narrative and argumentative flow. | TBD |
| 2 | **arXiv** (full text) | 0, 1, 2, 15 | Article | **Excellent:** Highly structured logical flow: *Intro* $\rightarrow$ *Methods* $\rightarrow$ *Results* $\rightarrow$ *Conclusion*. | **+0.18** (abstracts, 2025-11-04) |
| 3 | **GitHub Code Repos** (Python) | 15 | Repo/File | **Excellent:** Purely causal. `import` $\rightarrow$ `use`; `class def` $\rightarrow$ `instance`. | TBD |
| 4 | **ProofWiki** / Math Proofs | 1 | Article/Proof | **Excellent:** The purest form of logical dependency. `Axiom` $\rightarrow$ `Lemma` $\rightarrow$ `Theorem`. | TBD |
| 5 | **Stack Overflow** (Q\&A pairs) | 15, 2 | Q\&A Pair | **Excellent:** Direct causal link: *Problem (Question)* $\rightarrow$ *Solution (Answer)*. | TBD |
| 6 | **WikiHow** (filtered) | 13, 14, 2 | Article/Steps | **Very Good:** Pure procedural "how-to" data. *Step 1* $\rightarrow$ *Step 2*. | TBD |
| 7 | **RecipeDB / Cooking Sets** | 14 | Article/Steps | **Very Good:** Classic procedural data. *Ingredients* $\rightarrow$ *Prep* $\rightarrow$ *Cook*. | TBD |
| 8 | **Screenplay Datasets** (IMSDb) | 8, 9 | Script/Scene | **Very Good:** Strong causal and temporal flow. *Scene A* causes *Scene B*. | TBD |
| 9 | **PubMed Central** (full text) | 4 | Article | **Good:** Strong logical flow similar to arXiv, but for medicine. | TBD |
| 10 | **Khan Academy** (transcripts) | 13, 0, 1 | Continuous/Lesson | **Good:** Excellent pedagogical flow. *Simple concept* $\rightarrow$ *builds to* $\rightarrow$ *Complex concept*. | TBD |
| 11 | **Caselaw Access Project** | 11 | Article/Case | **Good:** Strong logical and causal chains. *Facts* $\rightarrow$ *Precedent* $\rightarrow$ *Ruling*. | TBD |
| 12 | **Code Documentation** (ReadTheDocs) | 15 | Article | **Good:** Mixed explanatory and procedural. Examples are high-value. | TBD |
| 13 | **Git Commit Messages** | 15 | Continuous/Log | **Good:** Causal by nature. *State N* $\rightarrow$ *Change (commit)* $\rightarrow$ *State N+1*. | TBD |
| 14 | **Political Speeches/Debates** | 12 | Continuous | **Good:** Strong argumentative flow. *Premise A* $\rightarrow$ *Argument B*. | TBD |
| 15 | **Philosophy Texts** (S. Ebooks) | 6 | Continuous/Book | **Good:** Pure logical argument flow, but can be highly abstract. | TBD |
| 16 | **EDGAR SEC Filings** | 10 | Article/Report | **Good:** Strong temporal flow (quarter-to-quarter) and financial causality. | TBD |
| 17 | **YouTube Transcripts** (Tutorials) | 13, 15, 9 | Continuous | **Medium:** High potential but *requires heavy filtering* to isolate procedural content. | TBD |
| 18 | **Common Crawl** | All | Continuous/Web | **Low (Raw):** Requires *extreme* filtering to extract narrative/procedural content. | TBD |
| 19 | **Chess PGN Databases** | 5 (Reasoning) | Game/Moves | **Niche/High:** Perfect causal chain for a *specific* domain (strategy). | TBD |
| 20 | **Wikipedia** (as-is) | All | Article | **Very Low:** Proven to be backward-referential. | **-0.07** (measured) |

**Note (2025-11-04)**: arXiv Œî = +0.18 measured on abstracts only (13 papers, 94 sequences). Full paper validation with 50k papers required before Phase 1 training. See `artifacts/lvm/ARXIV_DELTA_MEASUREMENT_2025_11_04.md` for complete analysis.

-----

## 5\. üõ†Ô∏è Requirement: Training Methodology

### 5.1 v1.0 Objective: `Loss_Chain`

To prove the foundation, v1.0 (Phase 1) will focus *exclusively* on the **"Causal Chain" Target** (`Loss_Chain`).

  * **Context:** `[c_1, ..., c_n]` (e.g., *arXiv section chunks 1-3*)
  * **Target `y`:** `c_n+1` (e.g., *arXiv section chunk 4*)
  * **Loss:** `Loss_Chain = CosineDistance(LVM(Context), c_n+1)`
  * **Rationale:** This teaches the model to predict the *immediate next logical step*.

The **"Section Synthesis" Target** (`Loss_Synth`) is deferred to **v2.0 (Phase 2)**, as it requires a validated single-step model and a clear strategy for generating summary vectors.

### 5.2 Directional Training Constraints (MANDATORY)

Based on empirical failures (P1-P6b), MSE/cosine loss alone is insufficient. The following directional constraints are **MANDATORY**:

#### 5.2.1 Directional Margin Loss

```python
L_margin = max(0, margin_target - (cos(pred, next) - cos(pred, prev)))
```

  * **Purpose:** Explicitly enforces: `cos(pred, next) > cos(pred, prev) + margin`.
  * **Schedule:** `margin_target` ramps from 0.02 $\rightarrow$ 0.06; `Œª_dir` (loss weight) ramps from 0.01 $\rightarrow$ 0.02.

#### 5.2.2 Positive Floor Penalty

```python
L_floor = Œ≤ * max(0, œÑ - cos(pred, next))¬≤
```

  * **Purpose:** Prevents orthogonal escape (P6b v2.2 failure) where the model predicts vectors far from the target.
  * **Tunables:** `œÑ = 0.10` (minimum acceptable similarity); `Œ≤ ‚âà 1e-3`.

#### 5.2.3 Orthogonality Penalty

```python
L_orth = Œ∫ * (cos(pred, prev))¬≤
```

  * **Purpose:** Penalizes predictions that are similar to the *previous* chunk, suppressing the backward-preference.
  * **Tunable:** `Œ∫ ‚âà 1e-4`.

#### 5.2.4 Directional-When-Confident Gate (P6b v2.3)

```python
alignment_quality = cos(pred, next)
if alignment_quality < 0.30:
    scale = 0.0  # Directional OFF when misaligned
elif alignment_quality > 0.45:
    scale = 1.0  # Directional FULL when aligned
else:
    scale = (alignment_quality - 0.30) / 0.15 # Linear ramp

L_dir_gated = scale * L_margin
```

  * **Purpose:** Critical for training stability. Prevents directional pressure from pushing a misaligned model into a bad local minima (P6b v1 collapse).

#### 5.2.5 Combined Loss (v1.0)

```python
L_total = (
    L_chain +
    Œª_dir * L_dir_gated +
    Œ≤ * L_floor +
    Œ∫ * L_orth
)
```

-----

## 6\. ‚öôÔ∏è Requirement: Tunable Parameters

### v1.0 Training Parameters

  * **`context_window_size` (k):** The number of vectors `[c_n-k, ..., c_n]` used to make a prediction.
  * **`model_architecture`:** (e.g., VMMoE, Transformer), including number of layers, heads, and expert count.
  * **`tmd_weight`:** The degree to which the 16D TMD vector influences the model's attention or gating mechanisms.
  * **Directional Tunables:** All coefficients from Section 5.2 (`margin_target`, `Œª_dir`, `Œ≤`, `œÑ`, `Œ∫`) and the gating thresholds.

### v2.0 (Phase 2/3) Inference Parameters

  * **`max_depth`:** Max recursive "thought" steps.
  * **`stop_threshold`:** Cosine similarity for conceptual convergence.
  * **`context_management`:** (`FIFO`, `Summarize`, `Full`).
  * **`TMD_bias_weights`:** A 16D vector to *guide* the thinking loop at inference time.

-----

## 7\. üí° Requirement: TMD Integration Strategy (v1.0)

To resolve ambiguity (Gap \#5, Q4), v1.0 will adopt a **TMD as Input Metadata Only** strategy.

1.  **Context Input:** All context vectors will be the full **784D** (`[16D TMD | 768D GTR]`). The model (e.g., attention, gating) *can* use the TMD information from the context.
2.  **Prediction Target:** The LVM will be trained to predict *only* the **768D** GTR vector for the `c_n+1` chunk. **TMD is metadata only, not predicted.**
3.  **Rationale:** This simplifies the v1.0 training objective immensely. We are not asking the model to predict metadata (TMD), only the *concept*.
4.  **v2.0 Path:** Predicting the 16D TMD for emergent concepts, or synthesizing a new TMD from context, is deferred to v2.0.

**DECISION (2025-11-04)**: Trent confirms 768D input+output. TMD provides routing/attention hints but predictions remain pure 768D concepts. This matches all current P1-P6b experiments.

-----

## 8\. ‚úÖ Success Criteria & Validation Gates (MANDATORY)

### 8.1 Pre-Training Data Quality Gates

(Copied from Section 4.1 for emphasis)

Before training on ANY data source, the following must be validated:

| Metric | Minimum Threshold | Measurement Tool |
|:---|:---|:---|
| **Œî (Forward - Backward)** | ‚â• +0.05 (Good), ‚â• +0.10 (Excellent) | `diagnose_data_direction.py` |
| **Temporal Coherence** | ‚â• 0.40 | `diagnose_data_direction.py` |
| **Offset Curve** | Monotonic increasing (k=0 \< k=1 \< ... \< target) | `diagnose_data_direction.py` |
| **Sample Size** | ‚â• 5,000 sequences per data source | - |

### 8.2 Post-Training Model Quality Gates (5CAT)

After training, BEFORE deployment, models must pass the 5-Gate Causal Alignment Test (5CAT):

| Gate | Metric | VAL Threshold | OOD Threshold | What It Tests |
|:---|:---|:---|:---|:---|
| **A: Offset Sweep** | Margin (forward - backward) | ‚â• +0.12 | ‚â• +0.10 | Directional preference |
| **B: Retrieval Rank** | R@1, R@5 within article | R@1‚â•60%, R@5‚â•95% | R@1‚â•55%, R@5‚â•92% | Prediction accuracy |
| **C: Ablations** | Shuffle delta | ‚â§ -0.15 | ‚â§ -0.15 | Context order matters |
| **D: Rollout** | Multi-step coherence | ‚â• 0.45 | ‚â• 0.42 | Temporal stability |
| **E: Generalization** | abs(Val - OOD bins) | ‚â§ 0.05 | ‚â§ 0.05 | OOD robustness |

  * **Passing Criteria:** Must pass **minimum 3 out of 5 gates**.
  * **Critical Failure:** If Gate A (Margin) is **NEGATIVE**, the model learned backward prediction $\rightarrow$ **DO NOT DEPLOY**.
  * **Validation Tool:** `tools/tests/test_5to1_alignment.py`

### 8.3 v1.0 (Phase 1) Deployment Criteria

  * [ ] **Phase 1 5CAT passed** (min 3/5 gates, positive margin).
  * [ ] Trained on **minimum 50k documents** from a validated data source (Œî ‚â• +0.05).
  * [ ] **Inference latency \< 5ms** (LVM single-step only).
  * [ ] Production model: **R@5 ‚â• 80%** and **Margin ‚â• +0.10** on held-out test set.
  * [ ] Documentation complete (architecture, training, validation results).

-----

## 9\. ‚ùå Out of Scope (v1.0)

  * **v2.0 (Phase 2) `Loss_Synth`:** Holistic synthesis training is deferred.
  * **v2.0 (Phase 3) Recursive Thinking Loop:** The full recursive loop, context management, and stop conditions are deferred.
  * **v2.0 (Phase 3) V2T Decoder Optimization:** A critical *prerequisite* for Phase 3, but not part of v1.0 LVM Core development.
  * **v2.0 TMD Prediction:** Predicting the 16D TMD for emergent concepts is deferred.
  * **Data Sourcing/ETL:** This PRD defines the *requirements* for the data, but the data engineering pipeline to acquire/process it is a separate workstream.

## 10\. üîÆ Future Work & Experimental Paths

  * **Bi-Directional Model Experiment (Q3 Follow-up):** At **90% completion** of Phase 1, conduct comparative experiment training a unified bi-directional model (predicts both forward and backward). Compare against separate forward/backward models. **DON'T FORGET!** (Trent, 2025-11-04)
  * **Domain Expansion (64 Domains):** Consider expanding from 16 to 64 domains to stay below critical-n threshold (1.56M < 1.7M concepts per bucket). See `docs/PRDs/TMD-Schema.md` for scaling analysis.
  * **Hierarchical Domain Classification:** Implement 16 top-level ‚Üí 4 sub-domains (= 64 total) for easier LLM classification while maintaining granularity.

-----

### Immediate Action Item

As per our analysis, the first step is **validating our arXiv data**. I will proceed with running `diagnose_data_direction.py` on the 220 downloaded papers to get our initial Œî measurement. This result will be the first "Go / No-Go" gate for Phase 1.


# Trents   Responses:

  11/4/2025

  üéØ CRITICAL QUESTIONS BEFORE PROCEEDING

  These MUST be answered before implementing this PRD:

  Q1: Is recursive thinking loop a v1.0 requirement?
  - Option A: v1.0 = Prove single-step prediction works (arXiv Œî > 0, pass 5CAT)
  - Option B: v1.0 = Full thinking loop with 50-step recursion
  Claude 4.5 Sonnet vote: Option A. Prove foundation first, recursion is Phase 2.
  Trent: Agree Option A then B once A is 90%

  Q2: What is minimum Œî threshold for data sources?
  - PRD says "arXiv is Excellent" but we haven't measured Œî yet!
  - Claude 4.5 Sonnet Proposal: Require Œî ‚â• +0.05 for "Good", ‚â• +0.10 for "Excellent"
  - Validate arXiv Œî BEFORE building training pipeline
  TRENT: Not sure, go with consensus recomendation

  Q3: Should we train forward + backward models, or unified bi-directional?
  - Option A: Separate models (forward for generation, backward for knowledge graph traversal)
  - Option B: Single bi-directional model
  - Claude 4.5 Sonnet vote: Option A. Different use cases, clearer training objectives.
  TRENT: Agree Option A but once its working 90% try one Option B and compare. DONT FORGET! 

  Q4: Is TMD 784D end-to-end, or 16D input + 768D prediction?
  - Option A: 784D everywhere (consistent but complex)
  - Option B: 16D input context, 768D predictions (simpler, matches current experiments)
  - Claude 4.5 Sonnet vote: Option B for v1.0, Option A for v2.0 if needed.
  TRENT: 768D + TMD > INPUT and output always 768D. Considering changing Domain from 16D to 64. See new notes from 11/4/2025 in: docs/PRDs/TMD-Schema.md. Maybe reduce, remove TM for now to keep it at 16D total. 

  Q5: What's the plan for vec2text speedup?
  - Current: 10s per vector = impractical for thinking loop
  - Options:
    - Distill vec2text to smaller model?
    - Train custom V2T decoder on our data?
    - Accept slow inference for v1.0?
  - Claude 4.5 Sonnet Recommendation: Optimize V2T before implementing thinking loop
  TRENT: Its 1sec for Steps=1, not 10s. So for early developemnet that fine. Its only used for Test and Inference not training right? Then at 95% complete we can optimize and retrain vec2text or maybe create lvm2text (new vesion, our version)


I approve of the 3 PHASES approach:
PHASE 1: Prove Single-Step Prediction
PHASE 2: Add Synthesis + Multi-Domain 
PHASE 3: Recursive Thinking Loop 

I completely agree: "  Before training on ANY data source, the following must be validated:"

  ### üéØ IMMEDIATE ACTION ITEMS

  **Before implementing this PRD, DO THIS FIRST:**

  1. **Measure arXiv Œî** (2-4 hours):
     ```bash
     # Extract sequences from your 210 papers
     ./.venv/bin/python tools/create_arxiv_training_sequences.py \
       --input data/datasets/arxiv/arxiv_cs_lg_ml.jsonl.gz \
       --output artifacts/lvm/arxiv_sequences_ctx5.npz

     # Measure Œî
     ./.venv/bin/python tools/tests/diagnose_data_direction.py \
       artifacts/lvm/arxiv_sequences_ctx5.npz --n-samples 5000

     Expected: Œî ‚â• +0.08 (papers follow Intro‚ÜíMethods‚ÜíResults flow)
     If Œî < 0: arXiv has same problem as Wikipedia! Need different data.

     TRENT: Agree! We might need to be flexible on the success criteria, but a great start 

  2. Update PRD with measurements (1 hour):
    - Add actual Œî values to Section 4 table
    - Add Section 8 (Success Criteria)
    - Add Section 5.3 (Directional Constraints)
    - Add Section 4.1 (Validation Protocol)

    TRENT: Agree


  3. Decide on Phase 1 scope (discussion):
    - Is recursive thinking loop v1.0 or v2.0?
    - Is TMD 784D or 16D input + 768D prediction?
    - What's minimum corpus size (50k papers? 100k?)?


    TRENT: I would prefer 250-500k concepts/vectors, so how many papers? hard to tell. 

