# PRD: Tiny Recursion in Large Vector Models (LVMs)

## Overview

This Product Requirements Document (PRD) defines the purpose, implementation strategy, and design structure for introducing **Tiny Recursion (TR)** into Large Vector Model (LVM) inference pipelines. Tiny Recursion is a lightweight feedback mechanism that enables vector-native models to refine or validate their outputs via a single additional pass.

---

## Objective

Introduce a minimal, low-cost recursion loop that operates **after initial vector generation** but **before final output consumption**, allowing the model to verify or refine the inferred vector for higher confidence, reduced drift, or improved semantic stability.

---

## Motivation

Traditional LVM pipelines rely on single-pass vector generation and immediate downstream consumption (e.g., retrieval, vec2text decoding). This approach:

* Lacks a verification mechanism
* May propagate small semantic drifts
* Has no built-in â€œconfidence recheckâ€ before emitting outputs

Tiny Recursion solves this by:

* Refeeding the output vector into the model
* Checking for stabilization or refinement
* Allowing rejection or re-routing of unstable outputs

---

## Architecture Diagram

```
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  User Query Vector â”‚ â—„â”€â”€â”€â”
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                                â–¼                 â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
                      â”‚   TMD Pre-routing   â”‚      â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                â–¼                 â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                 â”‚  Vector Search (FAISS etc.) â”‚  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â–¼                       â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
             â”‚ Top-K Concept Candidates    â”‚      â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚
                      â–¼               â–¼           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
            â”‚ Vecâ†’Vec Decode â”‚   â”‚ Graph RAG  â”‚    â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                   â–¼                  â–¼            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                â”‚    LVM Core Vector Inference  â”‚â—„â”€â”˜
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ ğŸ” Tiny Recursion (TR) Step â”‚   â—„â”€â”€â”€â”€ Optional recheck
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Final Vector Refinement   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Final Decision (Text or Vec)â”‚
            â”‚  â†’ vec2text, or return vec   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Design

### Where TR fits:

* Location: **After initial LVM vector output**, before final consumption (e.g. ranking, decoding, echo loop)
* Format: **Same architecture**, reuses the model for a single-pass re-evaluation

### What is passed:

* Original output vector (768D or 784D fused)
* Original mission/prompt context
* Optionally: Top-K nearby concepts (as supporting memory)

### How it operates:

* Refeeds the vector into the LVM as a pseudo-input
* Compares output vector (V2) to initial (V1)
* Measures cosine delta between V1 and V2
* If delta < threshold (e.g., 0.05), accept V2; else, flag or reroute

---

## Benefits

* âœ… Improves semantic stability of vector outputs
* âœ… Enables lightweight self-validation inside the LVM
* âœ… Reduces hallucination or vector drift without costly multi-pass systems

---

## Optional Enhancements

* Echo Score Integration: Use post-TR vector in final cosine check
* Graph Expansion Retry: If TR fails, expand graph neighborhood and rerun
* Temperature Annealing: Apply slightly different attention mask or temp in TR step for diversity check

---

## Metrics

| Metric                 | Baseline | With TR | Goal      |
| ---------------------- | -------- | ------- | --------- |
| Output Drift (Î”Cosine) | â€”        | < 0.05  | â‰¤ 0.03    |
| Rejection Rate         | â€”        | 5â€“15%   | Adaptable |
| Echo Match Rate        | â€”        | +3â€“7%   | â‰¥ +5%     |

---

## Implementation Notes

* TR is implemented as a modular hook in the LVM pipeline
* Can be toggled per TMD-lane or per batch
* Compatible with batched inference (e.g., vLLM)
* Adds ~1 extra forward pass per inference, no additional training cost

---

## Next Steps

* [ ] Integrate TR module into `lnsp_lvm_infer.py`
* [ ] Add config flag: `--use_tiny_recursion`
* [ ] Log vector deltas (pre vs post TR)
* [ ] Evaluate across 1000 samples: with and without TR
* [ ] Deploy into full CPESH-RAG+MoE pipeline
