# PRD: LLM Task Interface (Conversational AI Page)

**Version**: 1.2
**Date**: 2025-11-12
**Status**: Ready for Implementation (Critical Review Complete)
**Owner**: LNSP Core Team

**Revision History**:
- **v1.0** (2025-11-12): Initial draft with core features
- **v1.1** (2025-11-12): **PRD Review Improvements** - Added agent routing parameters, context isolation, role indicator UI, token finalization timing, dynamic routing documentation
- **v1.2** (2025-11-12): **Critical Review Implementation** - Fixed API inconsistencies, removed agent_port from frontend, clarified SQLite/Postgres strategy, added comprehensive SSE event schema, enhanced security (CSRF, CSP), realistic SLAs, cancellation semantics, agent/model registry endpoints

---

## Executive Summary

**LLM Task Interface** is a conversational AI page in the HMI that provides a rich, chat-based interface for submitting Prime Directives to the PAS hierarchy. It enables natural language task submission with the full feature set of modern LLM interfaces (streaming responses, conversation history, model selection, agent selection, etc.) while maintaining the PAS hierarchy's safety guardrails and communication patterns.

**What It Is**:
- New HMI page at `/llm` (alongside Tree, Sequencer, Dashboard, Actions)
- Rich chat interface with streaming LLM responses
- **Agent selector dropdown**: "SSH-like" connection to any agent in hierarchy (Architect, Directors, Managers, Programmers)
- **Default agent**: Architect (Port 6110) - LLM-powered top-level coordinator
- Model selection dropdown (shared with Settings page)
- Real-time token tracking (count, context window %, cost)
- Conversation history, export, and session management
- Optional voice features (Whisper STT + open-source TTS)

**What It Is NOT**:
- A replacement for Verdict CLI (complements CLI with visual interface)
- A direct LLM API wrapper (maintains PAS hierarchy communication pattern)
- An independent agent (all communication flows through selected agent in hierarchy)

**Core Innovation**: Brings modern chat-based UX to the PAS hierarchy with **SSH-like agent selection**, allowing users to "connect" to any agent (Architect, Directors, Managers, Programmers) and communicate as if they were that agent's parent. Includes real-time token/cost tracking and optional voice interface.

---

## 1. Problem Statement

### Current State
- Users can submit Prime Directives via:
  - **Verdict CLI** (`./bin/verdict send`) - Text-based, scripting-friendly
  - **`/pas-task` slash command** (Claude Code DirEng) - Conversational but external to HMI
- No visual, chat-based interface within the HMI for task submission
- No way to have back-and-forth conversation with **Architect** or other agents
- No visibility into LLM reasoning, plan generation, or iterative improvements
- No agent selection (can't "SSH into" a specific Director or Manager to debug/instruct)

### Pain Points
1. **No conversational refinement**: Users must fully specify task upfront (no iteration)
2. **Limited visibility**: Can't see Architect's planning, questions, or reasoning process
3. **No agent access**: Can't "SSH into" a specific Director/Manager to debug or give direct instructions
4. **No token tracking**: Can't see real-time token usage, context window %, or costs during conversation
5. **No rich UX features**: Missing streaming responses, syntax highlighting, code blocks, export
6. **Model lock-in**: Can't easily switch LLM models for different task types
7. **Disconnected workflows**: Chat interface (Claude Code) is separate from HMI monitoring (Tree/Sequencer)

### Why Now?
- PAS P0 stack is production-ready (Gateway â†’ PAS Root â†’ Aider-LCO working end-to-end)
- HMI infrastructure mature (Flask @ 6101, WebSocket/SSE streaming @ 6102)
- Users requesting "ChatGPT-like interface" for task submission
- Need bridge between conversational task intake and visual progress monitoring

---

## 2. Goals & Success Criteria

### Goals
1. **Rich Chat UX**: Modern LLM interface with streaming, markdown, code blocks, syntax highlighting
2. **Agent Selection**: "SSH-like" dropdown to connect to any agent (Architect, Directors, Managers, Programmers)
3. **Model Selection**: Dropdown to choose LLM model (same list as Settings page)
4. **Real-time Token Tracking**: Display tokens used, context window %, cost per message and cumulative
5. **Conversational Refinement**: Multi-turn conversations to clarify, plan, and approve tasks
6. **PAS Integration**: All messages route through selected agent in hierarchy
7. **Session Management**: Conversation history, export to markdown/JSON, clear/reset
8. **Voice Interface** (V2): Whisper STT + open-source TTS for hands-free operation
9. **Seamless Monitoring**: One-click navigation from chat to Tree/Sequencer to watch execution

### Success Criteria (V1 MVP)
- [ ] Chat interface live at `/llm` route
- [ ] **Agent selector dropdown** with all agents (Architect, Dir-*, Mgr-*, Programmer-*)
- [ ] Default agent: Architect (Port 6110)
- [ ] Streaming responses with typing indicators
- [ ] Markdown rendering with syntax highlighting (code blocks)
- [ ] **Model selection dropdown** (populated from Settings â†’ LLM Models)
- [ ] **Real-time token tracking**: tokens/message, cumulative, context %, cost
- [ ] **Token meter** visual gauge (progress bar showing context window usage)
- [ ] Conversation history persisted (SQLite or JSON files)
- [ ] Export conversation to markdown/JSON
- [ ] Clear conversation and start new session
- [ ] Submit task â†’ selected agent flow working
- [ ] Display task status inline (submitted â†’ planning â†’ executing â†’ complete)
- [ ] Link to Tree/Sequencer views for active task
- [ ] Mobile-responsive (works on tablet/phone)

### Non-Goals (V1)
- Voice input/output (Whisper STT + TTS) â†’ V2 (architecture planned, defer implementation)
- Multi-modal inputs (images, PDFs, diagrams) â†’ V2
- Branching conversations (tree of dialogue paths) â†’ V2
- Real-time collaboration (multiple users in same chat) â†’ V2
- Agentic loop visualization (show Director/Manager sub-conversations inline) â†’ V2

---

## 3. User Experience

### 3.1 Page Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Header: PAS Agent Swarm (OK) | Dashboard | Tree | Sequencer | ... â”‚
â”‚                                                            [LLM] â† â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Toolbar                                                         â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚
â”‚ â”‚ â”‚Agent: [Architect â–¼]â”‚ â”‚Model: [Sonnet 4 â–¼] â”‚ â”‚Exportâ”‚Clearâ”‚â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚
â”‚ â”‚ Tokens: 2,458/200k (1.2%) | Cost: $0.03 | Window: [â–“â–‘â–‘â–‘â–‘] 1% â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Conversation Area (Scrollable)                                  â”‚ â”‚
â”‚ â”‚                                                                  â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚ â”‚ â”‚ ğŸ‘¤ User                                        [10:23 AM]    â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ Please implement a new feature to track user login times    â”‚ â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚ â”‚                                                                  â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚ â”‚ â”‚ ğŸ›ï¸ Architect (Claude Sonnet 4.5)              [10:23 AM]    â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ I'll help you implement user login tracking. Let me gather  â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ some requirements:                                           â”‚ â”‚ â”‚
â”‚ â”‚ â”‚                                                              â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ 1. Where should login times be stored? (DB, logs, both?)    â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ 2. Do you need timezone support?                            â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ 3. Should we track login/logout or just login?              â”‚ â”‚ â”‚
â”‚ â”‚ â”‚                                                              â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ [View in Tree] [View in Sequencer]                          â”‚ â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚ â”‚                                                                  â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚ â”‚ â”‚ ğŸ‘¤ User                                        [10:24 AM]    â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ Store in PostgreSQL, yes to timezone, login only is fine    â”‚ â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚ â”‚                                                                  â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚ â”‚ â”‚ ğŸ›ï¸ Architect (Claude Sonnet 4.5)              [10:24 AM]    â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ Tokens: 1,234 | Context: 0.6% | Cost: $0.015         â”‚   â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ Perfect! Here's my implementation plan:                      â”‚ â”‚ â”‚
â”‚ â”‚ â”‚                                                              â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ **Phase 1: Database Schema**                                â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ - Add `user_logins` table with columns:                     â”‚ â”‚ â”‚
â”‚ â”‚ â”‚   - `user_id` (FK to users)                                 â”‚ â”‚ â”‚
â”‚ â”‚ â”‚   - `login_timestamp` (TIMESTAMPTZ)                         â”‚ â”‚ â”‚
â”‚ â”‚ â”‚   - `ip_address`, `user_agent`                              â”‚ â”‚ â”‚
â”‚ â”‚ â”‚                                                              â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ **Phase 2: Backend Logic**                                  â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ - Middleware to capture login events                        â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ - Insert login records on successful auth                   â”‚ â”‚ â”‚
â”‚ â”‚ â”‚                                                              â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ **Phase 3: API Endpoint**                                   â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ - GET /api/users/{id}/login-history                         â”‚ â”‚ â”‚
â”‚ â”‚ â”‚                                                              â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ Estimated tokens: ~15,000 | Time: ~8 minutes                â”‚ â”‚ â”‚
â”‚ â”‚ â”‚                                                              â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ [âœ“ Approve & Execute] [âœ Refine Plan] [âœ— Cancel]           â”‚ â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚ â”‚                                                                  â”‚ â”‚
â”‚ â”‚ [Typing indicator: PAS Root is thinking...]                    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Input Area                                                      â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚ â”‚ â”‚ Type your message... (Shift+Enter for new line)          â”‚ â”‚ â”‚
â”‚ â”‚ â”‚                                                            â”‚ â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚ â”‚ [ğŸ“ Attach] [ğŸ¤ Voice (V2)] [ğŸ’¾ Templates (V2)]    [Send â¤]  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Key Features

#### Agent Selection Dropdown ("SSH Mode")
- **Location**: Top toolbar, left side
- **Concept**: "SSH into" any agent in the PAS hierarchy
- **Content**: All active agents from Registry (6121)
  - **Tier 2 - Architect**: `Architect` (Port 6110) - **DEFAULT**
  - **Tier 3 - Directors**: `Dir-Code`, `Dir-Models`, `Dir-Data`, `Dir-DevSecOps`, `Dir-Docs` (6111-6115)
  - **Tier 4 - Managers**: `Mgr-Code-01`, `Mgr-Code-02`, etc. (6141-6150)
  - **Tier 5 - Programmers**: `Programmer-Qwen-001`, `Programmer-Claude-001`, etc. (6151-6199)
- **Behavior**:
  - Shows currently selected agent (e.g., "ğŸ›ï¸ Architect", "ğŸ’» Dir-Code", "ğŸ‘¨â€ğŸ’» Programmer-Claude-001")
  - Dropdown organized by tier with visual separators
  - Role icons displayed next to agent names (ğŸ›ï¸ Architect, ğŸ’» Director, âš™ï¸ Manager, ğŸ‘¨â€ğŸ’» Programmer)
  - Port numbers shown in dropdown (e.g., "Architect (6110)")
  - Agent selection persists per conversation
  - When switched, next message goes to new agent (can switch mid-conversation)
- **Context Isolation on Agent Switch** (CRITICAL):
  - **Rule**: Switching agents starts a **new, isolated conversational context** with the new agent
  - **Rationale**: A low-level agent (e.g., Programmer) does not have access to high-level directives given to the Architect. Context bleed would violate the parent simulation model.
  - **UI Behavior**: When user switches agents mid-conversation:
    1. Display confirmation dialog: "Switch to [New Agent]? This will start a new conversation thread with [New Agent]. Previous messages will not be shared."
    2. On confirmation, visually branch the conversation (indented thread or tab system)
    3. New agent sees only messages sent after the switch (clean slate)
  - **Database Design**: Each `session_id` is tied to a single `agent_id`. Switching agents creates a new `session_id`.
  - **Visual Representation**: Threaded/nested conversations (like Slack threads or Gmail conversations)
    - Parent conversation: Messages with Architect
    - Child thread: User switches to Dir-Code â†’ New indented thread below
    - Return to parent: User can navigate back to Architect thread
  - **Exception**: User can manually "share context" by copy/pasting previous messages into new thread (explicit action)
- **Communication Pattern**: User communicates as if they are the agent's **parent**
  - Chat with Architect â†’ You are PAS Root (the Architect's parent)
  - Chat with Dir-Code â†’ You are Architect (the Director's parent)
  - Chat with Mgr-Code-01 â†’ You are Dir-Code (the Manager's parent)
  - Chat with Programmer-Claude-001 â†’ You are Mgr-Code-01 (the Programmer's parent)

#### Role Indicator (Status Bar)
- **Location**: Top toolbar, directly below agent selector, full width
- **Purpose**: Explicitly show the user's assumed role when chatting with an agent
- **Display Format**: `Your Role: [Parent Agent Name] â†’ Directing: [Selected Agent Name]`
- **Examples**:
  - When Architect selected: `Your Role: PAS Root â†’ Directing: Architect`
  - When Dir-Code selected: `Your Role: Architect â†’ Directing: Dir-Code`
  - When Programmer-Claude-001 selected: `Your Role: Mgr-Code-01 â†’ Directing: Programmer-Claude-001`
- **Visual Design**:
  - Subtle background color (light blue/gray)
  - Small font size (12-14px)
  - Icon: ğŸ‘¤ (user role) â†’ ğŸ¯ (target agent)
  - Auto-updates when agent selection changes
- **Rationale**: The "parent simulation" is a novel concept that might confuse users. This status bar makes the mental model explicit: "You are acting as X when talking to Y."

#### Model Selection Dropdown
- **Location**: Top toolbar, center
- **Content**: Same model list as Settings â†’ LLM Models
- **Behavior**:
  - Shows currently selected model (e.g., "Claude Sonnet 4.5", "GPT-4o", "Llama 3.1:8b")
  - Dropdown lists all configured models with metadata:
    - Model name
    - Provider (Anthropic, OpenAI, Ollama)
    - Context window size
    - Cost per 1M tokens (if applicable)
  - Model selection persists per conversation (can switch mid-conversation)
  - Displays model name in each AI response bubble

#### Real-Time Token Tracking
- **Location**: Top toolbar, below dropdowns (second row)
- **Display**:
  - **Cumulative tokens**: `2,458 / 200,000` (current conversation total / model context window)
  - **Context percentage**: `(1.2%)` - visual indicator of how full the context window is
  - **Cumulative cost**: `$0.03` - total cost for current conversation
  - **Visual gauge**: Progress bar `[â–“â–‘â–‘â–‘â–‘]` showing context window fill percentage
- **Per-Message Tracking**: Each AI message bubble shows:
  - Tokens used in that response: `Tokens: 1,234`
  - Context percentage after that message: `Context: 0.6%`
  - Cost for that message: `Cost: $0.015`
- **Token Count Finalization** (IMPORTANT):
  - **During Streaming**: Token count shows estimated/incremental value (e.g., "~1,200 tokens" with spinning indicator)
  - **After Streaming Completes**: Final token count is calculated and displayed (e.g., "1,234 tokens" - no indicator)
  - **Timing**: Token metadata is finalized at the end of the streaming response (SSE stream sends `[DONE]` event with final token count)
  - **Visual Indicator**: Use spinner or "..." next to token count during streaming to set correct user expectations
  - **Rationale**: Accurate token counting requires full response completion. Incremental counts during streaming are estimates.
- **Color Coding** (Context Window %):
  - Green: 0-50% (plenty of room)
  - Yellow: 50-75% (getting full)
  - Orange: 75-90% (consider clearing)
  - Red: 90-100% (almost full, clear soon)

#### Streaming Responses
- **Typing Indicator**: "ğŸ¤– PAS Root is thinking..." while waiting
- **Token-by-Token Streaming**: Text appears word-by-word (via SSE/WebSocket)
- **Markdown Rendering**: Real-time markdown parsing as text streams
- **Code Block Highlighting**: Syntax highlighting applied after code block completes

#### Message Bubbles
- **User Messages**: Right-aligned, blue background, white text
- **AI Messages**: Left-aligned, dark gray background, white text
- **Metadata**: Timestamp, model name (for AI), role icon (ğŸ‘¤ user, ğŸ¤– AI)
- **Status Badges**: For task-related messages:
  - ğŸŸ¡ Planning
  - ğŸ”µ Executing
  - ğŸŸ¢ Complete
  - ğŸ”´ Error
  - ğŸŸ£ Awaiting Approval

#### Conversation History
- **Persistence**: SQLite table `llm_conversations`
  - Columns: `id`, `session_id`, `role` (user/assistant), `content`, `model`, `timestamp`, `metadata` (JSON)
- **Session Management**:
  - Each conversation has unique `session_id`
  - Sessions listed in sidebar (V2) or accessible via "History" dropdown
  - Can resume previous sessions
- **Search**: Full-text search across conversation history (V2)

#### Export Functionality
- **Formats**:
  - **Markdown**: Human-readable, formatted with headers/code blocks
  - **JSON**: Machine-readable, includes all metadata (timestamps, models, status)
  - **PDF**: Formatted conversation with syntax highlighting (V2)
- **Filename**: `pas_conversation_{session_id}_{timestamp}.{ext}`

#### Clear Conversation
- **Behavior**: Clears current conversation, starts new session
- **Confirmation**: "Are you sure? This will clear the current conversation."
- **Preservation**: Old conversation saved in history, not deleted

---

## 4. Architecture & Technical Design

### 4.1 Communication Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Types Message in HMI Chat Interface                       â”‚
â”‚ - Selects target agent from dropdown (agent_id, agent_port)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ HMI Flask Route                         â”‚
         â”‚ POST /api/chat/message                  â”‚
         â”‚ (Port 6101)                             â”‚
         â”‚ - Includes agent_id + agent_port        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼ POST /prime_directives (with agent routing)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Gateway (Port 6120)                     â”‚
         â”‚ - Idempotency key                       â”‚
         â”‚ - DYNAMIC ROUTING based on agent_port   â”‚
         â”‚ - Routes to Architect, Directors, or    â”‚
         â”‚   Managers based on selection           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼ POST /chat (to selected agent)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Selected Agent (dynamic port)           â”‚
         â”‚ - Architect (6110): LLM coordinator     â”‚
         â”‚ - Dir-Code (6111): Code lane director   â”‚
         â”‚ - Dir-Data (6112): Data lane director   â”‚
         â”‚ - Mgr-*: Manager agents                 â”‚
         â”‚ - Programmer-*: Code executors          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼ Streaming response (SSE)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Event Stream (Port 6102)                â”‚
         â”‚ - Token-by-token streaming              â”‚
         â”‚ - Status updates                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ HMI Frontend (JavaScript)               â”‚
         â”‚ - Markdown rendering                    â”‚
         â”‚ - Syntax highlighting                   â”‚
         â”‚ - Status badges                         â”‚
         â”‚ - Shows agent_id in response bubble     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Communication Pattern: Human â†” PAS Hierarchy

**Key Insight**: Each agent thinks their **parent** is the human. The actual human can "SSH into" any agent and communicate as that agent's parent.

**Example 1: Chat with Architect (Default)**
```
Human (HMI Chat) - Selects "Architect" from dropdown
  â”‚
  â”œâ”€ "Implement login tracking"
  â”‚  [Human acts as PAS Root, Architect's parent]
  â”‚
  â–¼
Architect (Tier 2 - LLM Coordinator)
  â”‚
  â”œâ”€ [Thinks PAS Root is talking to it]
  â”œâ”€ [Generates Plan]
  â”‚  "I'll delegate schema to Dir-Data, API to Dir-Code"
  â”‚
  â”œâ”€ â†’ Dir-Data (Director)
  â”‚     â”‚
  â”‚     â”œâ”€ [Receives] "Create user_logins table schema"
  â”‚     â”‚
  â”‚     â”œâ”€ â†’ Mgr-Schema (Manager)
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ [Receives] "Write migration for user_logins table"
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€ â†’ DB-Programmer (Worker)
  â”‚     â”‚     â”‚     â”‚
  â”‚     â”‚     â”‚     â””â”€ [Thinks Mgr-Schema is human]
  â”‚     â”‚     â”‚         "Here's the migration SQL..."
  â”‚     â”‚     â”‚
  â”‚     â”‚     â””â”€ [Aggregates] "Migration ready"
  â”‚     â”‚
  â”‚     â””â”€ [Reports to PAS Root] "Schema complete"
  â”‚
  â”œâ”€ â†’ Dir-Code (Director)
  â”‚     â”‚
  â”‚     â””â”€ [Similar delegation pattern]
  â”‚
  â””â”€ [Aggregates All Results]
     "Login tracking implemented! Here's the summary..."
     â”‚
     â–¼
Human (HMI Chat)
  â””â”€ [Sees final result + all intermediate steps (optional)]
```

**Example 2: Chat with Director (Direct Access)**
```
Human (HMI Chat) - Selects "Dir-Code" from dropdown
  â”‚
  â”œâ”€ "Review the API implementation for security issues"
  â”‚  [Human acts as Architect, Dir-Code's parent]
  â”‚
  â–¼
Dir-Code (Tier 3 - Code Lane Director)
  â”‚
  â”œâ”€ [Thinks Architect is talking to it]
  â”œâ”€ [Analyzes API code]
  â”‚  "Found 3 potential SQL injection vulnerabilities"
  â”‚
  â”œâ”€ â†’ Mgr-Code-01 (Manager)
  â”‚     "Fix SQL injection in user_search endpoint"
```

**Example 3: Chat with Programmer (Deep Dive)**
```
Human (HMI Chat) - Selects "Programmer-Claude-001" from dropdown
  â”‚
  â”œâ”€ "Why did you use a list comprehension here instead of a loop?"
  â”‚  [Human acts as Mgr-Code-01, Programmer's parent]
  â”‚
  â–¼
Programmer-Claude-001 (Tier 5 - Code Executor)
  â”‚
  â”œâ”€ [Thinks Mgr-Code-01 is asking]
  â””â”€ "I used a list comprehension because it's more Pythonic and
      has better performance for this dataset size (N < 10,000)..."
```

**Implementation Notes**:
- **Agent Selection**: User can "SSH into" any agent in the hierarchy
- **Parent Simulation**: HMI impersonates the agent's parent when communicating
- **Direct Access**: Allows debugging, clarification, or direct instruction of any agent
- **No Hierarchy Bypass**: Selected agent still follows normal delegation patterns
- **Visibility**: All messages in current conversation go to/from selected agent only

### 4.3 Data Models

#### Conversation Session
```python
class ConversationSession:
    session_id: str           # UUID (Primary Key)
    user_id: str              # User identifier
    agent_id: str             # Target agent (architect, dir-code, etc.) - CRITICAL for context isolation
    agent_name: str           # Display name (Architect, Dir-Code, etc.)
    parent_role: str          # User's assumed role (PAS Root, Architect, etc.)
    model_id: str             # Model identifier (for Settings lookup)
    model_name: str           # Display name (Claude Sonnet 4.5, etc.)
    created_at: datetime      # Session start time
    updated_at: datetime      # Last message time
    status: str               # active, archived, deleted
    archived_at: datetime     # Nullable, timestamp when archived
    title: str                # Optional, auto-generated from first user message
    metadata: dict            # Custom data (tags, project_id, etc.)

    # CONSTRAINT: One agent per session (enforces context isolation)
    # Switching agents â†’ new session_id
```

#### Message
```python
class Message:
    id: str                   # UUID (Primary Key)
    session_id: str           # FK to ConversationSession
    message_type: str         # user, assistant, system, status (for inline status updates)
    role: str                 # user, assistant, system (deprecated, use message_type)
    content: str              # Message text (markdown)
    agent_id: str             # Agent that sent/received this message
    model_name: str           # Model used for response (if assistant)
    timestamp: datetime       # Message time
    status: str               # Nullable: planning, executing, complete, error, awaiting_approval
    usage: dict               # Nullable: {"prompt_tokens": 123, "completion_tokens": 456, "total_tokens": 579, "cost_usd": 0.015}
    metadata: dict            # {
                              #   "task_id": "...",          # If message spawned task
                              #   "elapsed_seconds": 5.2,    # Response time
                              # }

    # INDEX on (session_id, timestamp) for fast history queries
    # INDEX on (session_id, id) for message lookup
```

#### Model Configuration (Shared with Settings)
```python
class ModelConfig:
    model_id: str             # Unique identifier
    model_name: str           # Display name
    provider: str             # anthropic, openai, ollama
    endpoint: str             # API endpoint URL
    context_window: int       # Max tokens
    cost_per_1m_input: float  # Cost (USD)
    cost_per_1m_output: float # Cost (USD)
    capabilities: list        # ["streaming", "function_calling", "vision"]
    is_active: bool           # Enabled/disabled
```

#### Persistence Strategy (V1 MVP)

**V1 (MVP)**: SQLite + SQLAlchemy ORM
- **Why**: Portable, zero-config, sufficient for single-user/dev testing
- **Location**: `services/webui/data/llm_chat.db`
- **Implementation**:
  - Use SQLAlchemy models for `ConversationSession` and `Message`
  - Emulate JSON fields with `TEXT` columns + `json.dumps()`/`json.loads()`
  - UUIDs generated client-side (Python `uuid.uuid4()`)
  - Indices on `(session_id, timestamp)` and `(session_id, id)`

**V2 (Production)**: PostgreSQL with JSONB
- **Why**: Multi-user, concurrent access, JSONB query performance, production-ready
- **Migration Path**: SQLAlchemy migrations (Alembic) from SQLite schema to Postgres
- **Implementation**:
  - Use `gen_random_uuid()` for UUIDs
  - Use `JSONB` for `metadata` and `usage` columns
  - Same SQLAlchemy models with conditional column types

**Decision**: V1 uses SQLite exclusively. No Postgres-specific features in V1 code.

### 4.4 API Endpoints

#### HMI Backend (Flask @ 6101)
```python
# Render chat page
GET /llm
  â†’ Returns: HTML template with chat interface

# Send message
POST /api/chat/message
  Body: {
    "session_id": "uuid",      # Optional, creates new if omitted
    "message": "...",          # User message text
    "agent_id": "architect",   # Target agent (architect, dir-code, mgr-code-01, etc.)
                               # Gateway maps agent_id â†’ port internally
    "model": "claude-4-sonnet" # Optional, uses default if omitted
  }
  â†’ Returns: {
    "session_id": "uuid",
    "message_id": "uuid",
    "agent_id": "architect",
    "status": "streaming"  # Message queued for streaming
  }

# Stream response (SSE)
GET /api/chat/stream/{session_id}
  â†’ SSE stream proxied from Gateway
  â†’ Event types: token, status_update, usage, done (see SSE Event Schema below)

# Get conversation history
GET /api/chat/sessions/{session_id}
  â†’ Returns: {
    "session_id": "uuid",
    "messages": [
      {"id": "...", "role": "user", "content": "...", ...},
      {"id": "...", "role": "assistant", "content": "...", ...}
    ],
    "metadata": {...}
  }

# List all sessions
GET /api/chat/sessions
  Query: ?limit=20&offset=0
  â†’ Returns: {
    "sessions": [
      {"session_id": "...", "created_at": "...", "message_count": 5},
      ...
    ],
    "total": 42
  }

# Export conversation
GET /api/chat/sessions/{session_id}/export?format=markdown
GET /api/chat/sessions/{session_id}/export?format=json
  â†’ Returns: File download (Content-Disposition: attachment)

# Clear conversation (archive)
POST /api/chat/sessions/{session_id}/clear
  â†’ Returns: {"status": "archived", "new_session_id": "uuid"}

# Delete conversation
DELETE /api/chat/sessions/{session_id}
  â†’ Returns: {"status": "deleted"}

# Cancel streaming response
POST /api/chat/sessions/{session_id}/cancel
  â†’ Propagates cancel to Gateway/PAS Root, closes SSE stream
  â†’ Returns: {"status": "cancelled"}

# Get available models (from Settings)
GET /api/models
  â†’ Returns: {
    "models": [
      {"model_id": "...", "model_name": "...", "provider": "...", "context_window": ..., "cost_per_1m_input": ..., ...},
      ...
    ]
  }

# Get available agents (from Registry @ 6121)
GET /api/agents
  â†’ Returns: {
    "agents": [
      {"agent_id": "architect", "agent_name": "Architect", "port": 6110, "tier": 2, "parent_role": "PAS Root", "status": "active"},
      {"agent_id": "dir-code", "agent_name": "Dir-Code", "port": 6111, "tier": 3, "parent_role": "Architect", "status": "active"},
      ...
    ]
  }
```

#### Gateway (FastAPI @ 6120)
```python
# Forward chat message to selected agent
POST /chat
  Body: {
    "session_id": "uuid",
    "message": "...",
    "agent_id": "architect",   # Gateway maps agent_id â†’ port internally
    "model": "...",
    "idempotency_key": "..."
  }
  â†’ Gateway resolves agent_id to port (architect â†’ 6110, dir-code â†’ 6111, etc.)
  â†’ Forwards to selected agent's /chat endpoint

# Stream response (SSE)
GET /chat/stream/{session_id}
  â†’ SSE stream of tokens, status updates, usage
  â†’ Event types: token, status_update, usage, done
  â†’ Includes heartbeats (:keep-alive\n\n) every 15s

# Cancel streaming
POST /chat/{session_id}/cancel
  â†’ Propagates cancel to agent, terminates SSE
```

#### PAS Root (FastAPI @ 6100)
```python
# Process chat message
POST /pas/chat
  Body: {
    "session_id": "uuid",
    "message": "...",
    "model": "..."
  }
  â†’ Returns: {
    "message_id": "uuid",
    "status": "processing"  # Will stream via SSE
  }

# Stream response tokens
GET /pas/chat/stream/{session_id}
  â†’ SSE stream (see SSE Event Schema below)
```

#### SSE Event Schema (Standardized)

All SSE events follow this structure:

```javascript
// Event Type 1: Token (streaming text)
data: {"type": "token", "content": "I'll"}
data: {"type": "token", "content": " help"}

// Event Type 2: Status Update (task progress)
data: {
  "type": "status_update",
  "status": "planning",  // planning | executing | complete | error | awaiting_approval
  "task_id": "uuid",
  "detail": "Analyzing requirements..."
}

// Event Type 3: Usage (token/cost tracking)
data: {
  "type": "usage",
  "usage": {
    "prompt_tokens": 123,
    "completion_tokens": 456,
    "total_tokens": 579
  },
  "cost_usd": 0.015
}

// Event Type 4: Done (stream complete)
data: {"type": "done"}

// Heartbeat (keep-alive, every 15s)
:keep-alive

```

**Timing**:
- `token` events: Emitted as LLM generates response
- `status_update` events: Emitted when agent transitions state
- `usage` event: Emitted **once**, right before `done` event (finalized token count)
- `done` event: Signals end of stream, close connection
- Heartbeats: Every 15s to prevent proxy timeouts

**Frontend Handling**:
```javascript
const eventSource = new EventSource(`/api/chat/stream/${sessionId}`);

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);

  switch (data.type) {
    case 'token':
      appendToMessageBubble(data.content);
      break;
    case 'status_update':
      updateStatusBadge(data.status, data.detail);
      break;
    case 'usage':
      updateTokenCounter(data.usage, data.cost_usd);
      break;
    case 'done':
      finalizeMessage();
      eventSource.close();
      break;
  }
};

eventSource.onerror = (error) => {
  // Implement exponential backoff for reconnection
  reconnectWithBackoff();
};
```

### 4.5 Frontend Components (JavaScript)

#### ChatInterface.js
```javascript
class ChatInterface {
  constructor(containerId, apiBaseUrl) {
    this.container = document.getElementById(containerId);
    this.apiUrl = apiBaseUrl;
    this.sessionId = null;
    this.model = null;
    this.eventSource = null;
    this.init();
  }

  async init() {
    // Load model list
    this.models = await this.fetchModels();
    // Render UI
    this.render();
    // Setup event listeners
    this.bindEvents();
    // Restore last session (from localStorage)
    this.restoreSession();
  }

  async sendMessage(message) {
    // Append user message to UI
    this.appendMessage('user', message);

    // Send to backend
    const response = await fetch(`${this.apiUrl}/api/chat/message`, {
      method: 'POST',
      headers: {'Content-Type': 'application/json'},
      body: JSON.stringify({
        session_id: this.sessionId,
        message: message,
        model: this.model
      })
    });

    const data = await response.json();
    this.sessionId = data.session_id;

    // Start streaming response
    this.streamResponse(data.message_id);
  }

  streamResponse(messageId) {
    // Create SSE connection
    this.eventSource = new EventSource(
      `${this.apiUrl}/api/chat/stream/${this.sessionId}`
    );

    let currentMessage = '';
    let messageDiv = this.appendMessage('assistant', '', {streaming: true});

    this.eventSource.onmessage = (event) => {
      const data = JSON.parse(event.data);

      if (data.type === 'token') {
        currentMessage += data.content;
        this.updateMessage(messageDiv, currentMessage);
      } else if (data.type === 'done') {
        this.eventSource.close();
        this.finalizeMessage(messageDiv, data.metadata);
      }
    };
  }

  appendMessage(role, content, options = {}) {
    // Create message bubble with markdown rendering
    const messageDiv = document.createElement('div');
    messageDiv.className = `message message-${role}`;

    // Add timestamp, model name, status badge
    // Render markdown with syntax highlighting
    // Return div for streaming updates

    this.container.appendChild(messageDiv);
    this.scrollToBottom();
    return messageDiv;
  }

  // ... more methods for export, clear, model selection, etc.
}
```

#### MarkdownRenderer.js
```javascript
class MarkdownRenderer {
  constructor() {
    // Use marked.js for markdown parsing
    this.marked = marked;
    // Use Prism.js or Highlight.js for syntax highlighting
    this.highlighter = Prism;
  }

  render(markdown) {
    // Convert markdown to HTML
    const html = this.marked.parse(markdown);
    // Apply syntax highlighting to code blocks
    return this.highlightCode(html);
  }

  renderStreaming(partialMarkdown) {
    // Handle incomplete code blocks gracefully
    // Don't highlight until code block is complete
  }
}
```

### 4.6 Database Schema

```sql
-- Conversation sessions
CREATE TABLE llm_conversations (
  session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id TEXT NOT NULL,
  model_name TEXT NOT NULL,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  status TEXT NOT NULL DEFAULT 'active',  -- active, archived, deleted
  metadata JSONB DEFAULT '{}'
);

CREATE INDEX idx_conversations_user ON llm_conversations(user_id);
CREATE INDEX idx_conversations_status ON llm_conversations(status);
CREATE INDEX idx_conversations_created ON llm_conversations(created_at DESC);

-- Messages
CREATE TABLE llm_messages (
  message_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id UUID NOT NULL REFERENCES llm_conversations(session_id) ON DELETE CASCADE,
  role TEXT NOT NULL,  -- user, assistant, system
  content TEXT NOT NULL,
  model_name TEXT,
  timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  metadata JSONB DEFAULT '{}'
);

CREATE INDEX idx_messages_session ON llm_messages(session_id, timestamp);
CREATE INDEX idx_messages_timestamp ON llm_messages(timestamp DESC);

-- Full-text search on message content (V2)
CREATE INDEX idx_messages_content_fts ON llm_messages USING gin(to_tsvector('english', content));
```

---

## 5. User Workflows

### 5.1 Simple Task Submission

1. User navigates to HMI â†’ LLM tab
2. Types: "Add a health check endpoint to the API"
3. Clicks Send (or presses Enter)
4. PAS Root responds with clarifying questions:
   - "Which API? (Gateway, PAS Root, Aider-LCO?)"
   - "What should the health check return?"
5. User answers: "Gateway API, return status and uptime"
6. PAS Root generates plan, shows estimation, asks for approval
7. User clicks "Approve & Execute"
8. PAS Root delegates to Dir-Code â†’ Mgr-API â†’ Programmer
9. Progress updates stream to chat interface
10. Final result: "Health check endpoint added to Gateway at `/health`"
11. User can click "View in Tree" to see execution hierarchy

### 5.2 Iterative Plan Refinement

1. User: "Optimize the database queries"
2. PAS Root: "I found 12 slow queries. Here's my optimization plan..."
   - [Shows detailed plan with 3 phases]
   - Estimated tokens: 50,000 | Time: 25 minutes
3. User: "That's too much. Just optimize the top 3 slowest queries."
4. PAS Root: "Updated plan for top 3 queries only..."
   - Estimated tokens: 12,000 | Time: 8 minutes
5. User: "Perfect, go ahead"
6. PAS Root executes, streams progress updates
7. User sees completion notification with performance metrics

### 5.3 Model Switching Mid-Conversation

1. User starts conversation with "Claude Sonnet 4.5"
2. Asks complex architectural question
3. PAS Root provides detailed answer
4. User switches model to "GPT-4o" (dropdown)
5. Asks follow-up question
6. GPT-4o responds in same conversation thread
7. Message bubbles show which model generated each response

### 5.4 Export and Share

1. User completes complex task through conversation
2. Clicks "Export" button â†’ selects "Markdown"
3. Downloads `pas_conversation_2025-11-12_10-23-45.md`
4. File contains:
   - Full conversation history
   - Syntax-highlighted code blocks
   - Timestamps and model names
   - Task status updates
5. User shares exported file with team for review

---

## 6. UI/UX Design Specifications

### 6.1 Color Scheme (Consistent with HMI)

```css
/* Match existing HMI dark theme */
--bg-primary: #0f1423;
--bg-secondary: #1a1f3a;
--bg-tertiary: #2a3558;
--border-color: #3a4568;

--text-primary: #ffffff;
--text-secondary: #a0aec0;
--text-muted: #6b7280;

--accent-blue: #3b82f6;
--accent-green: #10b981;
--accent-yellow: #f59e0b;
--accent-red: #ef4444;
--accent-purple: #8b5cf6;

/* Message bubbles */
--user-bg: #3b82f6;       /* Blue */
--user-text: #ffffff;
--assistant-bg: #2a3558;  /* Dark gray */
--assistant-text: #ffffff;

/* Status badges */
--status-planning: #f59e0b;   /* Yellow */
--status-executing: #3b82f6;  /* Blue */
--status-complete: #10b981;   /* Green */
--status-error: #ef4444;      /* Red */
--status-approval: #8b5cf6;   /* Purple */
```

### 6.2 Typography

```css
/* Message content */
.message-content {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
  font-size: 15px;
  line-height: 1.6;
  color: var(--text-primary);
}

/* Code blocks */
.message-content code {
  font-family: 'SF Mono', Monaco, Menlo, 'Courier New', monospace;
  font-size: 13px;
  background: rgba(0, 0, 0, 0.3);
  padding: 2px 6px;
  border-radius: 4px;
}

/* Code blocks (multi-line) */
.message-content pre {
  background: #1e1e1e;
  padding: 16px;
  border-radius: 8px;
  overflow-x: auto;
  border: 1px solid var(--border-color);
}

/* Timestamps */
.message-timestamp {
  font-size: 12px;
  color: var(--text-muted);
  font-weight: 500;
}
```

### 6.3 Responsive Design

```css
/* Desktop (>= 1024px) */
@media (min-width: 1024px) {
  .chat-container {
    max-width: 900px;
    margin: 0 auto;
  }

  .message {
    max-width: 75%;
  }
}

/* Tablet (768px - 1023px) */
@media (min-width: 768px) and (max-width: 1023px) {
  .chat-container {
    padding: 1rem;
  }

  .message {
    max-width: 85%;
  }
}

/* Mobile (< 768px) */
@media (max-width: 767px) {
  .toolbar {
    flex-direction: column;
    gap: 0.5rem;
  }

  .message {
    max-width: 95%;
  }

  .input-area {
    padding: 0.75rem;
  }
}
```

### 6.4 Animations

```css
/* Typing indicator animation */
@keyframes typing {
  0%, 60%, 100% { opacity: 0.3; }
  30% { opacity: 1; }
}

.typing-indicator span {
  animation: typing 1.4s infinite;
}

.typing-indicator span:nth-child(2) {
  animation-delay: 0.2s;
}

.typing-indicator span:nth-child(3) {
  animation-delay: 0.4s;
}

/* Message fade-in */
@keyframes fadeInUp {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.message {
  animation: fadeInUp 0.3s ease-out;
}

/* Streaming text cursor */
@keyframes blink {
  0%, 49% { opacity: 1; }
  50%, 100% { opacity: 0; }
}

.streaming-cursor {
  display: inline-block;
  width: 2px;
  height: 1em;
  background: var(--accent-blue);
  animation: blink 1s step-start infinite;
}
```

---

## 7. Security & Safety

### 7.1 Authentication & Authorization
- **User sessions**: Validated via Flask session cookies with `secure=True`, `httponly=True`, `samesite='Lax'`
- **Per-user isolation**: Each user gets isolated conversation history (via `user_id`)
- **No cross-user access**: Server-side queries enforce `WHERE user_id = current_user.id`
- **CSRF protection**: Add CSRF tokens to all POST/DELETE routes (use `flask-wtf` or manual token generation)
  - CSRF token included in forms and validated on submission
  - AJAX requests include `X-CSRF-Token` header
- **Content Security Policy (CSP)**: Set CSP headers to restrict script/style sources and reduce XSS risk

### 7.2 Input Validation
- **Message length limit**: 50,000 characters (prevent abuse)
- **Rate limiting**: 10 messages per minute per user (use `flask-limiter`)
- **XSS prevention**: All user input sanitized before rendering (use `bleach` or `markdown-it` with sanitization)
- **SQL injection prevention**: Parameterized queries only (SQLAlchemy ORM enforces this)
- **Agent ID validation**: Ensure `agent_id` exists in Registry before routing
- **Session validation**: Verify `session_id` belongs to current user before operations

### 7.3 Model Access Control
- **Model whitelist**: Only models marked `is_active=true` appear in dropdown
- **Credentials security**: Model endpoint credentials stored securely (env vars, not in DB)
- **API key protection**: API keys never exposed to frontend
- **Log redaction**: Ensure logs redact sensitive keys/tokens

### 7.4 Content Safety
- Profanity filter optional (configurable in Settings)
- Prompt injection detection (basic heuristics)
- Task approval gates for high-risk operations (file deletion, git push --force, etc.)

### 7.5 Audit Trail
- All messages logged with timestamps, user_id, model, tokens used
- Task submissions linked to conversation message (via `metadata.task_id`)
- Full auditability: Who requested what, when, using which model

---

## 8. Performance & Scalability

### 8.1 Response Time Targets (Realistic SLAs)
- **Message submission** (user â†’ server acceptance): < 500ms P95 (Gateway â†’ PAS Root â†’ LLM chain has latency)
- **Time-to-first-token (TTFT)**: < 1s P50, < 1.5s P95 (primary UX metric for "responsiveness")
- **Median token latency**: < 50ms (streaming fluency)
- **Page load time**: < 1s (initial page render)
- **Note**: 200ms for full submission is optimistic under load; 500ms P95 is reasonable with server-side buffering/queues

### 8.2 Streaming Optimization
- **SSE with heartbeats**: Server-Sent Events (SSE) for low-latency streaming + `:keep-alive\n\n` every 15s
- **Reconnection strategy**: EventSource default + exponential backoff on disconnect
- **Backpressure handling**: Pause streaming if client falls behind
- **Compression**: Gzip/brotli for SSE stream (test with proxies to avoid chunk coalescing issues)

### 8.3 Database Performance
- Conversation history paginated (20 messages per load)
- Infinite scroll with lazy loading
- Indexed queries (session_id, timestamp)
- Periodic archival of old conversations (>90 days â†’ cold storage)

### 8.4 Caching
- Model list cached (5 minute TTL)
- User session cached in Redis (optional, for multi-instance deployments)

---

## 9. Testing Strategy

### 9.1 Unit Tests
- Message rendering (markdown â†’ HTML)
- Syntax highlighting (code blocks)
- Input validation (XSS, length limits)
- Model selection logic

### 9.2 Integration Tests
- End-to-end message flow (HMI â†’ Gateway â†’ PAS Root â†’ HMI)
- Streaming response handling
- Conversation persistence (SQLite)
- Export functionality (markdown, JSON)

### 9.3 Load Tests
- 10 concurrent users sending messages
- 100 messages per minute across all users
- Verify no dropped SSE connections
- Verify database write performance

### 9.4 UI/UX Tests
- Mobile responsiveness (tablet, phone)
- Keyboard shortcuts (Enter to send, Shift+Enter for newline)
- Accessibility (screen reader compatibility, ARIA labels)

---

## 10. Rollout Plan

### Phase 1: Core Chat Interface (Week 1)
- [ ] Basic chat UI (message bubbles, input area)
- [ ] Model selection dropdown
- [ ] Message persistence (SQLite)
- [ ] Markdown rendering

### Phase 2: Streaming & Integration (Week 2)
- [ ] SSE streaming implementation
- [ ] Integration with Gateway/PAS Root
- [ ] Task status badges
- [ ] Typing indicators

### Phase 3: History & Export (Week 3)
- [ ] Conversation history view
- [ ] Export to markdown/JSON
- [ ] Session management (clear, archive)
- [ ] Search functionality (V2 stretch goal)

### Phase 4: Polish & Optimization (Week 4)
- [ ] Syntax highlighting (Prism.js)
- [ ] Mobile responsiveness
- [ ] Keyboard shortcuts
- [ ] Performance optimization
- [ ] Load testing
- [ ] Documentation

---

## 11. Open Questions

1. **Visibility Toggle**: Should users see only PAS Root messages (simple), or all Director/Manager/Programmer messages (detailed)?
   - **Recommendation**: Default to simple, add "Show Details" toggle

2. **Voice Input/Output**: Priority for V1 or defer to V2?
   - **Recommendation**: Defer to V2 (requires audio service integration)

3. **Task Approval in Chat**: Should approvals happen inline (chat buttons) or redirect to separate approval page?
   - **Recommendation**: Inline approval buttons ("âœ“ Approve", "âœ Refine", "âœ— Cancel")

4. **Model Switching Mid-Task**: If user switches model while task is executing, should it:
   - Cancel current task and restart with new model?
   - Queue new model for next message only?
   - **Recommendation**: Queue for next message, don't interrupt running tasks

5. **Cost Tracking**: Show running token cost in chat interface?
   - **Recommendation**: Yes, show cumulative cost in toolbar ("Tokens: 12,456 | Cost: $0.18")

---

## 12. Success Metrics (Post-Launch)

### Usage Metrics
- Daily active users (DAU) on LLM page
- Messages per user per session (engagement)
- Average conversation length (messages per session)
- Model selection distribution (which models are most popular)

### Performance Metrics
- P50/P95/P99 latency for message submission
- P50/P95/P99 time-to-first-token (streaming)
- SSE connection success rate
- Export success rate

### Quality Metrics
- Task success rate (submitted via chat vs. completed successfully)
- User satisfaction score (optional feedback widget)
- Error rate (failed API calls, dropped streams)

### Business Metrics
- Reduction in support tickets (easier task submission)
- Increase in PAS usage (more accessible interface)
- Token cost per conversation (cost efficiency)

---

## 13. Future Enhancements (V2+)

### V2: Advanced Features

#### Voice Interface (Whisper STT + Open-Source TTS)
- [ ] **Speech-to-Text (STT)**: OpenAI Whisper (local model via Faster-Whisper or whisper.cpp)
  - Push-to-talk button (ğŸ¤) or voice activity detection (VAD)
  - Real-time transcription displayed in input area before sending
  - Support for multiple languages (English, Spanish, French, etc.)
  - Local processing (no cloud API calls for privacy)
- [ ] **Text-to-Speech (TTS)**: Open-source TTS engines
  - **Option 1**: Piper TTS (fast, low-latency, high-quality)
  - **Option 2**: Coqui TTS (formerly Mozilla TTS, voice cloning support)
  - **Option 3**: Festival TTS (classic, lightweight)
- [ ] **Voice Settings**:
  - Enable/disable voice input/output independently
  - Voice speed control (0.5x - 2.0x)
  - Voice selection (male/female, different accents)
  - Auto-play responses (on/off)
- [ ] **Integration**:
  - Microphone access via WebRTC (browser permission required)
  - Audio playback via Web Audio API
  - Background audio service (Port 6103) handles STT/TTS processing
  - Streaming audio playback (start playing TTS before full response completes)

#### Other V2 Features
- [ ] Multi-modal inputs (upload images, PDFs, diagrams)
- [ ] Branching conversations (explore multiple solutions)
- [ ] Conversation templates (pre-filled prompts for common tasks)
- [ ] Real-time collaboration (multiple users in same chat)

### V3: Agentic Features
- [ ] Agentic loop visualization (show Director/Manager sub-conversations in tree view)
- [ ] Interactive plan approval (edit plan steps before execution)
- [ ] What-if analysis ("What if I change this requirement?")
- [ ] Side-by-side comparison (compare outputs from different models)

### V4: Enterprise Features
- [ ] Team workspaces (shared conversation history)
- [ ] Role-based access control (read-only vs. execute)
- [ ] Conversation analytics dashboard (usage patterns, cost breakdown)
- [ ] Integration with external tools (Jira, GitHub, Slack)

---

## 14. Appendices

### A. Markdown Support

#### Supported Markdown Syntax
- **Headers**: `# H1`, `## H2`, `### H3`, etc.
- **Bold/Italic**: `**bold**`, `*italic*`, `***both***`
- **Lists**: Unordered (`-`, `*`, `+`), Ordered (`1.`, `2.`)
- **Links**: `[text](url)`
- **Images**: `![alt](url)` (V2: inline preview)
- **Code**: Inline `` `code` ``, Blocks ` ```lang ... ``` `
- **Blockquotes**: `> quote`
- **Tables**: Markdown tables with alignment
- **Task Lists**: `- [ ]`, `- [x]` (V2: interactive checkboxes)

#### Syntax Highlighting Languages
- Python, JavaScript, TypeScript, Go, Rust, Java, C++, C#
- SQL, JSON, YAML, TOML, Markdown, HTML, CSS
- Bash/Shell, Dockerfile, Makefile
- (Full list via Prism.js supported languages)

### B. Keyboard Shortcuts

| Shortcut          | Action                          |
|-------------------|---------------------------------|
| `Enter`           | Send message                    |
| `Shift + Enter`   | New line (don't send)           |
| `Ctrl + K`        | Clear conversation              |
| `Ctrl + E`        | Export conversation (markdown)  |
| `Ctrl + /`        | Focus model dropdown            |
| `Esc`             | Cancel streaming response       |
| `Ctrl + â†‘`        | Load previous message (history) |
| `Ctrl + â†“`        | Load next message (history)     |

### C. API Rate Limits

| Endpoint                  | Rate Limit           |
|---------------------------|----------------------|
| `POST /api/chat/message`  | 10 req/min per user  |
| `GET /api/chat/sessions`  | 60 req/min per user  |
| `GET /api/chat/stream/*`  | 5 concurrent streams |
| `GET /api/models`         | 120 req/min per user |

### D. Error Messages

| Error Code | Message                                  | User Action                     |
|------------|------------------------------------------|---------------------------------|
| 400        | "Message too long (max 50,000 chars)"   | Shorten message                 |
| 401        | "Unauthorized. Please log in."          | Log in                          |
| 429        | "Rate limit exceeded. Wait 1 minute."   | Wait before sending more        |
| 500        | "Server error. Please try again."       | Retry or contact support        |
| 503        | "PAS Root unavailable. Check status."   | Check service health (Dashboard)|

---

**End of PRD**

---

## Review Checklist

- [ ] User workflows cover common use cases?
- [ ] API endpoints clearly defined?
- [ ] Security considerations addressed?
- [ ] Performance targets realistic?
- [ ] UI/UX mockups clear?
- [ ] Integration with existing HMI feasible?
- [ ] Testing strategy comprehensive?
- [ ] Rollout plan achievable in 4 weeks?

**Ready for implementation!** ğŸš€

---

## Appendix: v1.1 Revision Notes (2025-11-12)

The following improvements were made based on PRD review feedback to strengthen the design before implementation:

### 1. API Routing Ambiguity â†’ FIXED
**Problem**: `POST /api/chat/message` endpoint was missing parameters to specify the target agent. The backend wouldn't know where to route messages.

**Solution**:
- Added `agent_id` field (string, e.g., "architect", "dir-code")
- Added `agent_port` field (integer, e.g., 6110, 6111) for Gateway routing
- Updated API spec in Section 4.4 (lines 470-471)
- Return `agent_id` in response to confirm routing target

**Impact**: Gateway can now perform dynamic routing to any agent in the hierarchy based on user selection.

### 2. Dynamic Routing Documentation â†’ ADDED
**Problem**: Architecture diagram showed static routing to PAS Root only. Didn't reflect the "SSH-like" agent selection feature.

**Solution**:
- Updated Communication Flow diagram (Section 4.1, lines 281-327) to show dynamic routing
- Gateway now routes to selected agent (Architect, Directors, Managers, Programmers)
- Added explicit port mapping in diagram (Architect 6110, Dir-Code 6111, etc.)
- Clarified that agent_port drives routing logic

**Impact**: Architecture now accurately represents the multi-agent chat capability.

### 3. User Role Confusion â†’ UI ROLE INDICATOR ADDED
**Problem**: The "parent simulation" concept (user acts as agent's parent) is novel and could confuse users. Not immediately obvious that when talking to Dir-Code, the user is implicitly acting as Architect.

**Solution**:
- Added **Role Indicator (Status Bar)** UI component (Section 3.2, lines 206-219)
- Display format: `Your Role: [Parent] â†’ Directing: [Selected Agent]`
- Examples: "Your Role: PAS Root â†’ Directing: Architect", "Your Role: Architect â†’ Directing: Dir-Code"
- Visual design: Subtle background, small font, icons (ğŸ‘¤ user role â†’ ğŸ¯ target agent)
- Auto-updates when agent selection changes

**Impact**: Makes the mental model explicit, reducing user confusion about their role in the conversation.

### 4. Context Isolation on Agent Switch â†’ CRITICAL POLICY DEFINED
**Problem**: PRD didn't specify how conversational context is handled when switching agents mid-conversation. Risk of context bleed (e.g., Programmer seeing high-level Architect directives).

**Solution**:
- Added **Context Isolation on Agent Switch** policy (Section 3.2, lines 200-212)
- **Rule**: Switching agents starts a new, isolated conversational context
- **Rationale**: Low-level agents don't have access to high-level directives (violates parent simulation)
- **UI Behavior**: Confirmation dialog, visually branched conversation (threaded/nested like Slack)
- **Database Design**: Each `session_id` tied to single `agent_id`. Agent switch creates new `session_id`.
- **Visual Representation**: Threaded conversations (parent thread + child threads)
- **Exception**: User can manually share context by copy/pasting (explicit action)

**Impact**: Prevents context bleed, aligns with parent simulation model, improves conversation clarity.

### 5. Token Count Finalization Timing â†’ SPECIFIED
**Problem**: Unclear when token counts are finalized during streaming responses. Users might expect real-time accuracy, but accurate counting requires full response completion.

**Solution**:
- Added **Token Count Finalization** specification (Section 3.2, lines 258-263)
- **During Streaming**: Show estimated/incremental value with spinner (e.g., "~1,200 tokens...")
- **After Streaming**: Display final accurate count (e.g., "1,234 tokens")
- **Timing**: SSE stream sends `[DONE]` event with final token count at end of response
- **Visual Indicator**: Spinner or "..." during streaming to set correct user expectations

**Impact**: Sets correct user expectations, reduces confusion about token count accuracy during streaming.

---

### Summary of Changes

| Issue | Section | Lines | Status |
|-------|---------|-------|--------|
| API routing ambiguity | 4.4 API Endpoints | 470-471 | âœ… Fixed |
| Dynamic routing documentation | 4.1 Communication Flow | 281-327 | âœ… Added |
| User role confusion | 3.2 Key Features | 206-219 | âœ… UI Added |
| Context isolation policy | 3.2 Key Features | 200-212 | âœ… Defined |
| Token count finalization | 3.2 Key Features | 258-263 | âœ… Specified |

**Result**: PRD is now more robust, reduces implementation risks, and provides clearer guidance for developers. Status upgraded from "Draft" to "Ready for Implementation".

## Appendix: v1.2 Critical Review Implementation (2025-11-12)

The following critical fixes were implemented based on comprehensive technical review to resolve blockers and ambiguities before implementation:

---

### 1. Streaming API Inconsistency â†’ RESOLVED âœ…

**Problem**: UI referenced `GET /api/chat/stream/{session_id}`, but HMI endpoints omitted this route. Backend defined both Gateway and PAS Root streaming endpoints, causing confusion.

**Solution**:
- Added single HMI route: `GET /api/chat/stream/{session_id}` (Section 4.4, line 526)
- HMI proxies to Gateway, Gateway handles internal routing
- Removed PAS Root streaming from HMI's concern
- Added comprehensive SSE event schema with 4 event types

**Impact**: Single, clear streaming path. No frontend confusion about which endpoint to use.

---

### 2. Agent Routing Simplified â†’ RESOLVED âœ…

**Problem**: v1.1 exposed `agent_port` to frontend, creating CORS/security surface. Frontend shouldn't know internal port mappings.

**Solution**:
- Removed `agent_port` from `POST /api/chat/message` payload (Section 4.4, line 515)
- Frontend only sends `agent_id` (e.g., "architect", "dir-code")
- Gateway maps `agent_id` â†’ port internally (Section 4.4, line 601)
- Added `GET /api/agents` endpoint for agent discovery (Section 4.4, line 580)

**Impact**: Cleaner API contract, reduced attack surface, Gateway handles port resolution.

---

### 3. Persistence Store Clarified â†’ RESOLVED âœ…

**Problem**: PRD said "SQLite or JSON files" but used Postgres-specific features (`gen_random_uuid()`, `JSONB`), blocking implementation.

**Solution**:
- Added **Persistence Strategy** section (Section 4.3, lines 514-533)
- **V1 MVP**: SQLite + SQLAlchemy ORM exclusively
  - JSON fields emulated with `TEXT` + `json.dumps()`/`json.loads()`
  - UUIDs generated client-side (`uuid.uuid4()`)
- **V2 Production**: PostgreSQL with JSONB (migration path via Alembic)
- No Postgres-specific features in V1 code

**Impact**: Clear implementation path. No mixed SQLite/Postgres code in V1.

---

### 4. Session Schema Enhanced â†’ RESOLVED âœ…

**Problem**: `ConversationSession` lacked `agent_id`, `agent_name`, `parent_role`. Could not enforce "one agent per session" rule for context isolation.

**Solution**:
- Added fields to `ConversationSession` (Section 4.3, lines 462-476):
  - `agent_id`: Target agent (architect, dir-code, etc.)
  - `agent_name`: Display name (Architect, Dir-Code, etc.)
  - `parent_role`: User's assumed role (PAS Root, Architect, etc.)
  - `model_id`: Model identifier (for Settings lookup)
  - `archived_at`: Nullable timestamp for archival
  - `title`: Optional, auto-generated from first user message
- Added constraint comment: "One agent per session (enforces context isolation)"

**Impact**: Database schema enforces context isolation. Agent switching â†’ new `session_id`.

---

### 5. Message Schema Enhanced â†’ RESOLVED âœ…

**Problem**: `Message` model lacked fields for status updates, token tracking, and agent attribution.

**Solution**:
- Added fields to `Message` (Section 4.3, lines 483-498):
  - `message_type`: user | assistant | system | status (replaces `role`)
  - `agent_id`: Agent that sent/received message
  - `status`: Nullable (planning, executing, complete, error, awaiting_approval)
  - `usage`: Nullable dict (`{"prompt_tokens": ..., "completion_tokens": ..., "total_tokens": ..., "cost_usd": ...}`)
- Added indices: `(session_id, timestamp)` and `(session_id, id)`

**Impact**: Full token tracking, status updates, agent attribution. Database supports all UI features.

---

### 6. Cancellation Semantics Added â†’ RESOLVED âœ…

**Problem**: Keyboard shortcut (Esc) to cancel streaming had no backend endpoint or behavior.

**Solution**:
- Added `POST /api/chat/sessions/{session_id}/cancel` endpoint (Section 4.4, line 566)
- Propagates cancel to Gateway/PAS Root, closes SSE stream
- Returns `{"status": "cancelled"}`

**Impact**: Users can cancel long-running LLM responses. Full UX control.

---

### 7. Security Enhanced â†’ RESOLVED âœ…

**Problem**: No CSRF protection, missing CSP headers, vague auth/session handling.

**Solution**:
- **CSRF Protection** (Section 7.1, lines 1120-1122):
  - Added CSRF tokens to all POST/DELETE routes
  - AJAX requests include `X-CSRF-Token` header
- **Session Security** (Section 7.1, line 1117):
  - Flask session cookies with `secure=True`, `httponly=True`, `samesite='Lax'`
- **CSP Headers** (Section 7.1, line 1123):
  - Set Content-Security-Policy to restrict script/style sources
- **Enhanced Input Validation** (Section 7.2, lines 1130-1131):
  - Agent ID validation (ensure exists in Registry)
  - Session validation (verify belongs to current user)
- **Log Redaction** (Section 7.3, line 1137):
  - Ensure logs redact sensitive keys/tokens

**Impact**: Production-grade security. CSRF protection, secure cookies, CSP, validation.

---

### 8. SSE Event Schema Standardized â†’ RESOLVED âœ…

**Problem**: PRD promised status badges and token tracking over SSE, but event spec was vague. Only showed `token` and `done` events.

**Solution**:
- Added **SSE Event Schema** section (Section 4.4, lines 668-739)
- Defined 4 standardized event types:
  1. **token**: `{"type": "token", "content": "..."}`
  2. **status_update**: `{"type": "status_update", "status": "planning|executing|complete|error|awaiting_approval", "task_id": "...", "detail": "..."}`
  3. **usage**: `{"type": "usage", "usage": {"prompt_tokens": ..., "completion_tokens": ..., "total_tokens": ...}, "cost_usd": ...}`
  4. **done**: `{"type": "done"}`
- Added **heartbeats**: `:keep-alive\n\n` every 15s
- Specified timing: `usage` event emitted **once**, right before `done`
- Included frontend handling code example

**Impact**: Clear SSE contract. Status badges, token tracking, heartbeats all specified.

---

### 9. SLA Realism Adjusted â†’ RESOLVED âœ…

**Problem**: "Message submission < 200ms" was optimistic for Gateway â†’ PAS Root â†’ LLM chain under load.

**Solution**:
- Updated **Response Time Targets** (Section 8.1, lines 1153-1158):
  - Message submission: < 500ms P95 (was 200ms) - "Gateway â†’ PAS Root â†’ LLM chain has latency"
  - TTFT: < 1s P50, < 1.5s P95 (primary UX metric)
  - Added note: "200ms for full submission is optimistic under load; 500ms P95 is reasonable with server-side buffering/queues"
- Updated **Streaming Optimization** (Section 8.2, lines 1160-1164):
  - Added heartbeats (`:keep-alive\n\n` every 15s)
  - Added reconnection strategy (EventSource + exponential backoff)
  - Added compression note (test with proxies to avoid coalescing)

**Impact**: Realistic performance targets. Heartbeats prevent proxy timeouts.

---

### 10. Agent/Model Discovery Endpoints Added â†’ RESOLVED âœ…

**Problem**: PRD referenced Registry (6121) and Settings â†’ Models, but didn't specify API sources.

**Solution**:
- Added `GET /api/agents` endpoint (Section 4.4, lines 580-587):
  - Returns agents from Registry @ 6121
  - Schema: `{"agent_id": "...", "agent_name": "...", "port": ..., "tier": ..., "parent_role": "...", "status": "active"}`
- Enhanced `GET /api/models` endpoint (Section 4.4, lines 571-577):
  - Added context_window, cost_per_1m_input to response schema

**Impact**: Clear data sources for agent selector and model selector dropdowns.

---

### Summary of v1.2 Changes

| Issue | Section | Status |
|-------|---------|--------|
| Streaming API inconsistency | 4.4 API Endpoints | âœ… Fixed |
| Agent routing (remove agent_port) | 4.4 API Endpoints | âœ… Simplified |
| Persistence (SQLite vs Postgres) | 4.3 Data Models | âœ… Clarified |
| Session schema (agent_id, parent_role) | 4.3 Data Models | âœ… Enhanced |
| Message schema (status, usage) | 4.3 Data Models | âœ… Enhanced |
| Cancellation semantics | 4.4 API Endpoints | âœ… Added |
| Security (CSRF, CSP, session) | 7.1-7.3 Security | âœ… Enhanced |
| SSE event schema (4 event types) | 4.4 SSE Event Schema | âœ… Standardized |
| SLA realism (500ms P95, TTFT) | 8.1 Performance | âœ… Adjusted |
| Agent/model discovery endpoints | 4.4 API Endpoints | âœ… Added |

**Result**: All critical blockers resolved. PRD is now **production-ready** with zero ambiguities. API contracts are consistent, security is robust, persistence strategy is clear, and SSE event schema is fully specified. Status: **Ready for Implementation**.

---

### Implementation Checklist (Based on v1.2 Fixes)

Before starting implementation, verify:

- [ ] HMI has single SSE route: `GET /api/chat/stream/{session_id}`
- [ ] Frontend sends `agent_id` only (no `agent_port`)
- [ ] Gateway maps `agent_id` â†’ port internally
- [ ] V1 uses SQLite exclusively (no Postgres features)
- [ ] `ConversationSession` has `agent_id`, `agent_name`, `parent_role`
- [ ] `Message` has `message_type`, `status`, `usage`, `agent_id`
- [ ] Cancel endpoint exists: `POST /api/chat/sessions/{session_id}/cancel`
- [ ] CSRF tokens on all POST/DELETE routes
- [ ] Flask session cookies: `secure=True`, `httponly=True`, `samesite='Lax'`
- [ ] SSE heartbeats (`:keep-alive\n\n`) every 15s
- [ ] SSE event types: `token`, `status_update`, `usage`, `done`
- [ ] `usage` event emitted **once**, right before `done`
- [ ] `GET /api/agents` endpoint for agent discovery
- [ ] SLA targets: < 500ms P95 (submission), < 1s P50 TTFT

**All items above are critical for a clean, production-ready implementation.** ğŸš€
