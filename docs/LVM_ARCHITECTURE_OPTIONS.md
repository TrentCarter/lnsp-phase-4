# LVM Architecture Options for Vector-Native Training

**Date**: October 12, 2025
**Purpose**: Evaluate 12+ architectures for tokenless LVM training on 10K Wikipedia concepts
**Goal**: Train 10-100 models with different architectures to find optimal next-vector prediction

---

## üéØ Requirements

All architectures must support:
- ‚úÖ **Input**: 768D vectors (GTR-T5 embeddings) or 784D (768D + 16D TMD)
- ‚úÖ **Output**: 768D predicted vector (next concept)
- ‚úÖ **NO token embeddings** (nn.Embedding removed)
- ‚úÖ **NO vocabulary head** (lm_head removed)
- ‚úÖ **Training**: Autoregressive next-vector prediction
- ‚úÖ **Inference**: Generate vector ‚Üí vec2text ‚Üí LLM smoothing

---

## üìä Architecture Comparison Table

| # | Architecture | Params | Complexity | Speed | Open Source | Difficulty | Success Score |
|---|--------------|---------|------------|-------|-------------|------------|---------------|
| 1 | **LSTM Baseline** | 10M | O(n) | ‚ö°‚ö°‚ö° | ‚úÖ | ‚≠ê Easy | 70% |
| 2 | **Mamba-2** | 130M | O(n) | ‚ö°‚ö°‚ö° | ‚úÖ | ‚≠ê‚≠ê Medium | 95% |
| 3 | **Meta LCM** | 1.6B | O(n) | ‚ö°‚ö° | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê Hard | 90% |
| 4 | **RWKV** | 169M | O(n) | ‚ö°‚ö°‚ö° | ‚úÖ | ‚≠ê‚≠ê‚≠ê Medium-Hard | 85% |
| 5 | **Hyena** | 125M | O(n log n) | ‚ö°‚ö° | ‚úÖ | ‚≠ê‚≠ê‚≠ê Medium-Hard | 80% |
| 6 | **RetNet** | 125M | O(1) inference | ‚ö°‚ö°‚ö° | ‚úÖ | ‚≠ê‚≠ê‚≠ê Medium-Hard | 88% |
| 7 | **S4** | 100M | O(n log n) | ‚ö°‚ö° | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê Hard | 75% |
| 8 | **DistilGPT-2** | 82M | O(n¬≤) | ‚ö° | ‚úÖ | ‚≠ê‚≠ê Medium | 82% |
| 9 | **Linformer** | 100M | O(n) | ‚ö°‚ö° | ‚úÖ | ‚≠ê‚≠ê‚≠ê Medium-Hard | 77% |
| 10 | **Performer** | 100M | O(n) | ‚ö°‚ö° | ‚úÖ | ‚≠ê‚≠ê‚≠ê Medium-Hard | 78% |
| 11 | **GRU Stacked** | 12M | O(n) | ‚ö°‚ö°‚ö° | ‚úÖ | ‚≠ê Easy | 72% |
| 12 | **Hybrid Mamba-Attn** | 150M | O(n) + O(n¬≤) | ‚ö°‚ö° | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê Hard | 92% |

**Success Score** = (Efficiency √ó Performance √ó Ease of Implementation) / 3

---

## 1Ô∏è‚É£ LSTM Baseline (Simple, Fast, Proven)

### Why Start Here
- ‚úÖ **Simplest tokenless architecture** - already implemented in `src/lvm/models.py`
- ‚úÖ **Fast training** - 5 min per epoch on 10K concepts
- ‚úÖ **Proven for sequences** - 30+ years of research
- ‚ö†Ô∏è **Lower ceiling** - won't match state-of-the-art

### Architecture
```python
class LSTMBaseline(nn.Module):
    def __init__(self, d_input=768, d_hidden=512, n_layers=2):
        self.input_proj = nn.Linear(d_input, d_hidden)
        self.lstm = nn.LSTM(d_hidden, d_hidden, n_layers, batch_first=True)
        self.output_head = nn.Linear(d_hidden, d_input)

    def forward(self, x):
        h = self.input_proj(x)
        lstm_out, _ = self.lstm(h)
        return self.output_head(lstm_out[:, -1, :])
```

### Conversion Steps
- ‚úÖ Already done! This is our current implementation

### Novel Enhancement Ideas
1. **Bidirectional LSTM** - Use forward + backward pass for context
2. **Residual connections** - Add skip connections every 2 layers
3. **Attention over hidden states** - Weighted sum instead of last hidden state

### Recommended Experiments (10 runs)
- Vary `d_hidden`: [256, 512, 1024]
- Vary `n_layers`: [2, 4, 6]
- Vary learning rate: [1e-4, 1e-3, 1e-2]

---

## 2Ô∏è‚É£ Mamba-2 (State-of-the-Art SSM) ‚≠ê **HIGHEST PRIORITY**

### Why This Will Work
- ‚úÖ **Linear complexity** O(n) - efficient on long sequences
- ‚úÖ **No attention** - pure SSM, naturally vector-native
- ‚úÖ **Proven at scale** - 2.7B parameters trained on 300B tokens
- ‚úÖ **Small sizes available** - 130M, 370M, 780M variants
- ‚úÖ **Active development** - Used in IBM Granite 4.0

### Architecture
```python
from mamba_ssm import Mamba2

class Mamba2LVM(nn.Module):
    def __init__(self, d_input=768, d_model=512, n_layers=12):
        self.input_proj = nn.Linear(d_input, d_model)
        self.layers = nn.ModuleList([
            Mamba2(
                d_model=d_model,
                d_state=64,
                d_conv=4,
                expand=2
            ) for _ in range(n_layers)
        ])
        self.output_head = nn.Linear(d_model, d_input)
```

### Conversion Steps
1. Install: `pip install mamba-ssm causal-conv1d>=1.2.0`
2. Remove embedding layer from pretrained checkpoint
3. Replace lm_head with 768D projection
4. Fine-tune on vector sequences

### Novel Enhancement Ideas
1. **TMD-Conditional SSM** - Use 16D TMD codes to modulate SSM parameters
2. **Multi-Scale State** - Different state sizes for different semantic levels
3. **Vector Quantization** - Discretize vector space for SSM transitions

### Recommended Experiments (20 runs)
- Vary `d_model`: [256, 512, 768, 1024]
- Vary `n_layers`: [6, 12, 18, 24]
- Vary `d_state`: [16, 32, 64, 128]
- Vary learning rate: [1e-5, 5e-5, 1e-4, 5e-4]
- Try hybrid: Mamba + attention every 6 layers

---

## 3Ô∏è‚É£ Meta Large Concept Models (LCM) ‚≠ê **MOST SIMILAR TO OUR APPROACH**

### Why This is Perfect
- ‚úÖ **Already concept-level** - operates on sentence embeddings (like our chunks!)
- ‚úÖ **SONAR embeddings** - 1024D sentence vectors (we use 768D GTR-T5)
- ‚úÖ **Autoregressive** - predicts next concept in sequence
- ‚úÖ **Open source** - GitHub: facebookresearch/large_concept_model
- ‚úÖ **Diffusion variant** - alternative to MSE regression

### Architecture
```python
class LCM_Style(nn.Module):
    def __init__(self, d_input=768, d_model=1024, n_layers=16):
        # One-Tower: single decoder handles context + denoising
        self.encoder = nn.TransformerEncoder(...)
        self.denoiser = DiffusionHead(d_model, d_input)

    # Two-Tower: separate context encoding and denoising
    def forward(self, context_vecs, target_vec_noisy):
        context = self.encoder(context_vecs)
        denoised = self.denoiser(target_vec_noisy, context)
        return denoised
```

### Conversion Steps
1. Clone: `git clone https://github.com/facebookresearch/large_concept_model`
2. Replace SONAR (1024D) with GTR-T5 (768D)
3. Adapt from sentence-level to chunk-level
4. Train with MSE or diffusion objective

### Novel Enhancement Ideas
1. **Hybrid Loss** - Combine MSE + cosine + diffusion
2. **Hierarchical Concepts** - Chunk ‚Üí Sentence ‚Üí Paragraph levels
3. **TMD-Guided Diffusion** - Use TMD codes to guide denoising

### Recommended Experiments (15 runs)
- Compare One-Tower vs Two-Tower
- MSE vs Diffusion vs Hybrid loss
- Vary diffusion steps: [1, 5, 10, 20]
- Vary model size: [1.6B ‚Üí 700M ‚Üí 350M]

---

## 4Ô∏è‚É£ RWKV (RNN with Transformer-Level Performance)

### Why This is Interesting
- ‚úÖ **Linear complexity** O(n) - RNN formulation
- ‚úÖ **Parallelizable training** - can be trained like transformer
- ‚úÖ **Constant inference** - O(1) memory (no KV cache)
- ‚úÖ **Proven scalable** - 14B parameters trained
- ‚úÖ **Open source** - Multiple implementations available

### Architecture
```python
class RWKV_LVM(nn.Module):
    def __init__(self, d_input=768, d_model=512, n_layers=12):
        self.input_proj = nn.Linear(d_input, d_model)
        self.layers = nn.ModuleList([
            RWKVBlock(d_model) for _ in range(n_layers)
        ])
        self.output_head = nn.Linear(d_model, d_input)

    def forward(self, x):
        h = self.input_proj(x)
        for layer in self.layers:
            h = layer(h)  # Time-mixing + channel-mixing
        return self.output_head(h[:, -1, :])
```

### Conversion Steps
1. Install: `pip install rwkv`
2. Load pretrained RWKV checkpoint
3. Remove embedding + lm_head layers
4. Add 768D input/output projections

### Novel Enhancement Ideas
1. **Vector Time-Mixing** - Adapt time-mixing for continuous vectors
2. **Multi-Resolution** - Different mixing scales for different semantics
3. **Ensemble RWKV** - Multiple RWKV streams, late fusion

### Recommended Experiments (12 runs)
- Vary `d_model`: [512, 768, 1024]
- Vary `n_layers`: [8, 12, 16, 20]
- Vary learning rate: [1e-5, 5e-5, 1e-4]

---

## 5Ô∏è‚É£ Hyena Hierarchy (Long Convolution)

### Why This is Fast
- ‚úÖ **Subquadratic** - O(n log n) complexity
- ‚úÖ **100x faster** than attention at 64K length
- ‚úÖ **Long context** - handles 100K+ tokens
- ‚úÖ **Proven quality** - Matches transformer on WikiText103

### Architecture
```python
from safari import HyenaOperator

class HyenaLVM(nn.Module):
    def __init__(self, d_input=768, d_model=512, n_layers=12):
        self.input_proj = nn.Linear(d_input, d_model)
        self.layers = nn.ModuleList([
            HyenaOperator(d_model, l_max=1024)
            for _ in range(n_layers)
        ])
        self.output_head = nn.Linear(d_model, d_input)
```

### Conversion Steps
1. Install: `pip install safari-ssm`
2. Adapt HyenaOperator to accept continuous vectors
3. Train from scratch (no pretrained vector models)

### Novel Enhancement Ideas
1. **Learnable Convolution Filters** - Adapt filters to concept semantics
2. **Multi-Scale Hyena** - Different filter lengths per layer
3. **Hybrid Hyena-Attention** - Use attention every N layers

### Recommended Experiments (10 runs)
- Vary `l_max`: [512, 1024, 2048]
- Vary filter order: [2, 3, 4]
- Vary `d_model`: [512, 768, 1024]

---

## 6Ô∏è‚É£ RetNet (Microsoft's Transformer Successor)

### Why This is Promising
- ‚úÖ **O(1) inference** - constant memory, no KV cache
- ‚úÖ **8.4x faster** than transformers at decoding
- ‚úÖ **70% memory savings** vs attention
- ‚úÖ **Microsoft Research** - Well-documented

### Architecture
```python
class RetNetLVM(nn.Module):
    def __init__(self, d_input=768, d_model=512, n_layers=12):
        self.input_proj = nn.Linear(d_input, d_model)
        self.layers = nn.ModuleList([
            MultiScaleRetention(d_model)
            for _ in range(n_layers)
        ])
        self.output_head = nn.Linear(d_model, d_input)
```

### Conversion Steps
1. Clone: `git clone https://github.com/Jamie-Stirling/RetNet`
2. Remove token embeddings
3. Train in parallel mode, infer in recurrent mode

### Novel Enhancement Ideas
1. **Vector Retention** - Adapt retention for continuous space
2. **TMD-Conditional Retention** - Modulate retention by TMD
3. **Chunkwise Processing** - Optimize for 100-1000 concept sequences

### Recommended Experiments (12 runs)
- Vary retention heads: [4, 8, 16]
- Vary `d_model`: [512, 768, 1024]
- Compare parallel vs chunkwise training

---

## 7Ô∏è‚É£ S4 (Structured State Space)

### Why This is Foundational
- ‚úÖ **30x faster** than previous SSMs
- ‚úÖ **400x less memory** than LSSL
- ‚úÖ **Long Range Arena** - state-of-the-art on all tasks
- ‚úÖ **Path-X solved** - 16K sequence reasoning

### Architecture
```python
from s4 import S4Block

class S4_LVM(nn.Module):
    def __init__(self, d_input=768, d_model=512, n_layers=6):
        self.input_proj = nn.Linear(d_input, d_model)
        self.layers = nn.ModuleList([
            S4Block(d_model, l_max=1024)
            for _ in range(n_layers)
        ])
        self.output_head = nn.Linear(d_model, d_input)
```

### Conversion Steps
1. Install: `pip install state-spaces`
2. Use S4 blocks directly (already vector-compatible)
3. Train from scratch

### Novel Enhancement Ideas
1. **Learnable State Matrices** - Adapt HiPPO initialization
2. **Multi-Resolution S4** - Different timescales per layer
3. **S4 + Attention Hybrid** - Combine for best of both

### Recommended Experiments (10 runs)
- Vary `d_state`: [16, 32, 64, 128]
- Vary `n_layers`: [4, 6, 8, 12]
- Compare HiPPO vs random initialization

---

## 8Ô∏è‚É£ DistilGPT-2 (Small, Fast Transformer)

### Why This is Practical
- ‚úÖ **82M parameters** - small enough for rapid iteration
- ‚úÖ **2x faster** than GPT-2 base
- ‚úÖ **Open source** - Hugging Face Transformers
- ‚úÖ **Proven quality** - Close to GPT-2 performance

### Architecture
```python
from transformers import GPT2Model

class DistilGPT2_LVM(nn.Module):
    def __init__(self, d_input=768):
        config = GPT2Config(
            vocab_size=1,  # Dummy, will be removed
            n_embd=768,
            n_layer=6,
            n_head=12
        )
        self.gpt = GPT2Model(config)
        # Remove wte (token embeddings)
        del self.gpt.wte
        self.input_proj = nn.Identity()  # Already 768D
        self.output_head = nn.Linear(768, 768)
```

### Conversion Steps
1. Load: `GPT2LMHeadModel.from_pretrained("distilgpt2")`
2. Remove `transformer.wte` (token embeddings)
3. Remove `lm_head` (vocabulary projection)
4. Feed 768D vectors directly to transformer blocks

### Novel Enhancement Ideas
1. **Vector Position Encoding** - Replace sinusoidal with learned vector PE
2. **Sparse Attention** - Only attend to semantically similar concepts
3. **LoRA Fine-Tuning** - Low-rank adaptation for efficiency

### Recommended Experiments (15 runs)
- Vary `n_layer`: [4, 6, 8, 10]
- Vary `n_head`: [6, 8, 12, 16]
- Compare full fine-tuning vs LoRA
- Try flash-attention for speed

---

## 9Ô∏è‚É£ Linformer (Linear Complexity Transformer)

### Why This is Efficient
- ‚úÖ **O(n) complexity** - linear self-attention
- ‚úÖ **Low-rank approximation** - mathematically grounded
- ‚úÖ **Comparable quality** - close to full attention

### Architecture
```python
class LinformerLVM(nn.Module):
    def __init__(self, d_input=768, d_model=512, n_layers=8, k=256):
        self.input_proj = nn.Linear(d_input, d_model)
        self.layers = nn.ModuleList([
            LinformerBlock(d_model, k=k)  # k = low-rank projection
            for _ in range(n_layers)
        ])
        self.output_head = nn.Linear(d_model, d_input)
```

### Conversion Steps
1. Install: `pip install linformer`
2. Use Linformer attention in place of standard attention
3. Train from scratch

### Novel Enhancement Ideas
1. **Adaptive k** - Learn projection rank per layer
2. **Concept-Aware Projection** - Use TMD to guide low-rank projection
3. **Progressive Training** - Start with low k, increase gradually

### Recommended Experiments (10 runs)
- Vary `k`: [64, 128, 256, 512]
- Vary `n_layers`: [6, 8, 12, 16]
- Compare fixed vs learned projection matrices

---

## üîü Performer (FAVOR+ Fast Attention)

### Why This is Novel
- ‚úÖ **O(n) complexity** - kernel approximation
- ‚úÖ **Unbiased estimator** - theoretically sound
- ‚úÖ **Positive features** - guaranteed positive attention

### Architecture
```python
from performer_pytorch import Performer

class PerformerLVM(nn.Module):
    def __init__(self, d_input=768, d_model=512, n_layers=8):
        self.input_proj = nn.Linear(d_input, d_model)
        self.performer = Performer(
            dim=d_model,
            depth=n_layers,
            heads=8,
            causal=True,
            nb_features=256  # FAVOR+ random features
        )
        self.output_head = nn.Linear(d_model, d_input)
```

### Conversion Steps
1. Install: `pip install performer-pytorch`
2. Configure for autoregressive (causal=True)
3. Train from scratch

### Novel Enhancement Ideas
1. **Learned Random Features** - Train feature projection
2. **Multi-Scale Features** - Different feature dimensions per head
3. **Concept Kernel** - Use GTR-T5 cosine as kernel

### Recommended Experiments (10 runs)
- Vary `nb_features`: [128, 256, 512]
- Vary `heads`: [4, 8, 16]
- Compare orthogonal vs random features

---

## 1Ô∏è‚É£1Ô∏è‚É£ Stacked GRU (Simple RNN Variant)

### Why This is Fast
- ‚úÖ **Simpler than LSTM** - fewer parameters, faster training
- ‚úÖ **Often better** - empirically matches or beats LSTM
- ‚úÖ **Lightweight** - 12M parameters for 6 layers

### Architecture
```python
class StackedGRU_LVM(nn.Module):
    def __init__(self, d_input=768, d_hidden=512, n_layers=6):
        self.input_proj = nn.Linear(d_input, d_hidden)
        self.gru = nn.GRU(d_hidden, d_hidden, n_layers, batch_first=True)
        self.output_head = nn.Linear(d_hidden, d_input)
```

### Conversion Steps
- ‚úÖ Already simple to implement

### Novel Enhancement Ideas
1. **Residual GRU** - Add skip connections
2. **Bidirectional GRU** - Forward + backward pass
3. **GRU with Attention** - Attend over all hidden states

### Recommended Experiments (8 runs)
- Vary `d_hidden`: [256, 512, 768]
- Vary `n_layers`: [3, 6, 9]
- Compare unidirectional vs bidirectional

---

## 1Ô∏è‚É£2Ô∏è‚É£ Hybrid Mamba-Attention ‚≠ê **BEST OF BOTH WORLDS**

### Why This Will Dominate
- ‚úÖ **Proven by NVIDIA** - validated in 2024 research
- ‚úÖ **Used in production** - AI2 Jamba, IBM Granite 4.0
- ‚úÖ **Efficiency + Quality** - Mamba speed, attention quality
- ‚úÖ **Flexible ratio** - Tune Mamba:Attention layers

### Architecture
```python
class HybridMambaAttention_LVM(nn.Module):
    def __init__(self, d_input=768, d_model=512, n_layers=24, attn_every=6):
        self.input_proj = nn.Linear(d_input, d_model)
        self.layers = nn.ModuleList([
            Mamba2Block(d_model) if (i+1) % attn_every != 0
            else AttentionBlock(d_model)
            for i in range(n_layers)
        ])
        self.output_head = nn.Linear(d_model, d_input)
```

### Conversion Steps
1. Combine Mamba-2 + standard attention
2. Interleave: 5 Mamba layers ‚Üí 1 Attention layer (repeat)
3. Train from scratch or adapt from hybrid checkpoint

### Novel Enhancement Ideas
1. **Adaptive Layer Selection** - Learn which layers need attention
2. **TMD-Routed Attention** - Use attention only for complex TMD codes
3. **Progressive Replacement** - Start all-Mamba, add attention gradually

### Recommended Experiments (20 runs)
- Vary `attn_every`: [4, 6, 8, 12]
- Vary total layers: [12, 18, 24]
- Vary attention type: [Full, Linformer, Performer]
- Compare sparse vs dense attention

---

## üöÄ Novel Success-Boosting Strategies

### Strategy 1: Multi-Stage Progressive Training
```
Stage 1 (1K concepts, 5 epochs): Learn basic vector transitions
Stage 2 (5K concepts, 10 epochs): Expand to diverse semantics
Stage 3 (10K concepts, 20 epochs): Full training with regularization
```

### Strategy 2: Ensemble Voting
Train 3-5 small models (LSTM + GRU + Mamba) and ensemble predictions:
```python
def ensemble_predict(models, context):
    preds = [model(context) for model in models]
    return torch.mean(torch.stack(preds), dim=0)  # Average
```

### Strategy 3: Curriculum Learning
```
Easy ‚Üí Hard based on cosine similarity:
Week 1: High similarity sequences (cosine > 0.8)
Week 2: Medium similarity (0.6-0.8)
Week 3: Low similarity (0.4-0.6)
Week 4: Mixed (all ranges)
```

### Strategy 4: Contrastive Pre-Training
Pre-train with CPESH contrastive loss before autoregressive training:
```python
# Phase 1: Contrastive (align probe ‚Üí concept)
loss = contrastive_loss(probe_vec, concept_vec, negatives)
# Phase 2: Autoregressive (predict next concept)
loss = mse_loss(pred_vec, target_vec)
```

### Strategy 5: TMD-Conditional Generation
Use 16D TMD codes to modulate model behavior:
```python
class TMD_ConditionalLVM(nn.Module):
    def forward(self, x, tmd_codes):
        h = self.input_proj(x)
        # Modulate hidden states by TMD
        h = h * self.tmd_scale(tmd_codes).unsqueeze(1)
        return self.core(h)
```

### Strategy 6: Multi-Resolution Vectors
Train on multiple granularities simultaneously:
```
768D: Full semantic (GTR-T5)
384D: Compressed (PCA)
192D: Ultra-compressed (PCA)
Output: Predict all 3 scales, enforce consistency
```

### Strategy 7: Vec2Text in the Loop
During training, occasionally decode vectors to text and re-encode:
```python
# Every 100 steps:
text = vec2text(pred_vec)
re_encoded = gtr_t5(text)
loss += consistency_loss(pred_vec, re_encoded)
```

---

## üéØ Recommended Training Order (100 Iterations)

### Week 1: Baselines (15 iterations)
1. LSTM (5 runs) - Establish baseline
2. GRU (5 runs) - Compare to LSTM
3. Stacked Deeper (5 runs) - Test capacity

### Week 2: State Space Models (30 iterations) ‚≠ê PRIORITY
1. Mamba-2 (10 runs) - Vary d_model, n_layers
2. S4 (8 runs) - Vary d_state
3. RWKV (8 runs) - Vary architecture
4. Hybrid Mamba-Attention (4 runs) - Test best ratios

### Week 3: Transformers (20 iterations)
1. DistilGPT-2 (8 runs) - Vary layers, heads
2. Linformer (6 runs) - Vary low-rank k
3. Performer (6 runs) - Vary FAVOR+ features

### Week 4: Advanced (20 iterations)
1. Hyena (6 runs) - Long convolution experiments
2. RetNet (8 runs) - Retention experiments
3. Meta LCM (6 runs) - Diffusion vs MSE

### Week 5: Ensemble & Refinement (15 iterations)
1. Best 3 architectures (3 runs each = 9)
2. Ensemble models (3 runs)
3. Final validation (3 runs)

---

## üìà Evaluation Metrics

### Training Metrics
- **MSE Loss**: Mean squared error between predicted and target vectors
- **Cosine Similarity**: Semantic alignment (target: >0.85)
- **Training Time**: Minutes per epoch on 10K concepts

### Inference Metrics
- **Vec2Text Accuracy**: % of vectors decoded to correct concept text
- **Smoothed Output Quality**: LLM-smoothed response coherence
- **Latency**: Time to generate next concept vector

### Comparison Baselines
- **Random Baseline**: Random 768D vector (cosine ~0.0)
- **Nearest Neighbor**: Closest training vector (cosine ~0.7)
- **Linear Projection**: Simple Wx + b model (cosine ~0.6)

---

## üîç Success Criteria

### Minimum Viable LVM
- ‚úÖ Cosine similarity > 0.70 (better than nearest neighbor)
- ‚úÖ Vec2Text accuracy > 50% (half of predictions are correct concept)
- ‚úÖ Training time < 1 hour per model (enable 100 iterations)

### Production-Ready LVM
- ‚úÖ Cosine similarity > 0.85 (high semantic alignment)
- ‚úÖ Vec2Text accuracy > 75% (most predictions correct)
- ‚úÖ Smoothed output quality > 80% human preference
- ‚úÖ Inference latency < 100ms (fast enough for real-time)

---

## üìö Implementation Resources

### Key Repositories
1. **Mamba**: `pip install mamba-ssm` | [GitHub](https://github.com/state-spaces/mamba)
2. **Meta LCM**: [GitHub](https://github.com/facebookresearch/large_concept_model)
3. **RWKV**: `pip install rwkv` | [GitHub](https://github.com/BlinkDL/RWKV-LM)
4. **Hyena**: `pip install safari-ssm` | [GitHub](https://github.com/HazyResearch/safari)
5. **RetNet**: [GitHub](https://github.com/Jamie-Stirling/RetNet)
6. **S4**: `pip install state-spaces` | [GitHub](https://github.com/state-spaces/s4)
7. **Performer**: `pip install performer-pytorch`
8. **Linformer**: `pip install linformer`

### Papers
- Mamba: [arXiv:2312.00752](https://arxiv.org/abs/2312.00752)
- Meta LCM: [arXiv:2412.08821](https://arxiv.org/abs/2412.08821)
- RWKV: [arXiv:2305.13048](https://arxiv.org/abs/2305.13048)
- Hyena: [arXiv:2302.10866](https://arxiv.org/abs/2302.10866)
- RetNet: [arXiv:2307.08621](https://arxiv.org/abs/2307.08621)
- S4: [arXiv:2111.00396](https://arxiv.org/abs/2111.00396)

---

## ‚úÖ Next Steps

1. **Verify Data Ready** (In Progress)
   - Wait for 870 articles ‚Üí ~10K concepts ingestion
   - Build FAISS index for retrieval
   - Create training sequences (context ‚Üí target)

2. **Start with Baselines** (Hours 0-8)
   - Run LSTM (5 experiments)
   - Run GRU (5 experiments)
   - Establish performance floor

3. **Test Mamba-2** (Hours 8-24) ‚≠ê PRIORITY
   - Small: 130M params, 6 layers
   - Medium: 370M params, 12 layers
   - Find optimal configuration

4. **Scale to 100 Iterations** (Week 1-5)
   - Follow recommended training order above
   - Track all metrics in wandb/tensorboard
   - Identify best architecture + hyperparameters

5. **Production Model** (Week 6+)
   - Train final model on full dataset
   - Deploy with vec2text + LLM smoothing
   - A/B test vs baseline systems

---

**Status**: Ready for 10-100 training iterations on 10K Wikipedia concepts!
**Next**: Wait for ingestion to complete, then start LSTM baseline experiments.
