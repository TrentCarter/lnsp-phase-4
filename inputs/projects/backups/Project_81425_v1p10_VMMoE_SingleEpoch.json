{
  "project": {
    "metadata": {
      "id": "VMMoE_Production_V1p10_SingleEpoch",
      "name": "VMMoE Production Training v1.10 - Single Epoch Strategy",
      "description": "Ultra-minimal training: Stop at Epoch 0 based on V1.9 analysis showing COS 0.9831 peak",
      "version": "1.10.0",
      "created_date": "2025-08-16",
      "training_date": "2025-08-16T11:xx",
      "notes": "Hard-cap at 1 epoch, checkpoint every 500 steps to capture optimal point"
    },
    "model": "sentence-transformers/gtr-t5-base",
    "local_model_path": "data/teacher_models/gtr-t5-base"
  },
  "training": {
    "architecture": {
      "mamba_config": {
        "d_model": 768,
        "n_layers": 5,
        "d_state": 24,
        "d_conv": 6,
        "expand": 1.75,
        "dt_rank": "auto",
        "dt_min": 0.001,
        "dt_max": 0.1,
        "dt_init": "xavier",
        "dt_scale": 0.9,
        "bias": false,
        "conv_bias": true,
        "dropout": 0.15,
        "layer_norm_epsilon": 1e-5
      },
      "positional_config": {
        "mode": "hybrid",
        "learned_weight": 0.7,
        "sinusoidal_weight": 0.3,
        "max_len": 512,
        "dropout": 0.1,
        "learned_config": {
          "embedding_dim": 768,
          "max_position_embeddings": 512
        },
        "sinusoidal_config": {
          "embedding_dim": 768,
          "temperature": 10000.0
        }
      },
      "peft_config": {
        "enabled": true,
        "adapter_type": "lora",
        "lora_rank": 12,
        "lora_alpha": 24,
        "lora_dropout": 0.05,
        "target_modules": [
          "in_proj",
          "x_proj",
          "dt_proj",
          "A_log",
          "D",
          "out_proj"
        ],
        "modules_to_save": [
          "layer_norm"
        ]
      }
    },
    "loss_config": {
      "loss_type": "triplet",
      "margin_start": 0.05,
      "margin_end": 0.08,
      "margin_schedule": "constant",
      "cosine_weight": 0.65,
      "mse_weight": 0.125,
      "contrastive_weight": 0.2,
      "orthogonality_weight": 0.0015,
      "temperature": 0.05,
      "label_smoothing": 0.05,
      "normalization_enforcement": {
        "enabled": true,
        "l2_penalty_weight": 0.0003,
        "cosine_similarity_threshold": 0.05,
        "normalize_embeddings": true,
        "stability_epsilon": 1e-8
      }
    },
    "optimization": {
      "optimizer": "AdamW",
      "learning_rate": 4e-05,
      "weight_decay": 0.02,
      "gradient_clipping": 1.0,
      "warmup_steps": 300,
      "scheduler": "constant_with_warmup",
      "betas": [0.9, 0.98],
      "eps": 1e-08
    },
    "data": {
      "batch_size": 24,
      "sequence_length": 16,
      "num_workers": 2,
      "shuffle": true,
      "drop_last": true,
      "concept_databases": [
        "data/databases/concept_vector_db_768_training",
        "data/databases/qa_training_triplets_768d"
      ],
      "augmentation": {
        "enabled": true,
        "noise_std": 0.006,
        "dropout_prob": 0.03,
        "mixup_alpha": 0.15
      }
    },
    "training_params": {
      "epochs": 1,
      "steps_per_epoch": 2325,
      "validation_frequency": 1,
      "checkpoint_frequency": 500,
      "device_priority": [
        "mps",
        "cuda",
        "cpu"
      ],
      "seed": 42,
      "deterministic": false
    },
    "early_stopping": {
      "enabled": true,
      "patience": 0,
      "min_delta": 0.0005,
      "monitor": "val/anchor_positive_distance",
      "mode": "min",
      "min_epochs": 0,
      "loss_target": 0.35,
      "loss_target_enabled": false,
      "verbose": true,
      "restore_best_weights": true,
      "additional_criteria": {
        "max_d_ap": 0.10,
        "min_d_an": 0.14,
        "max_overfit_ratio": 1.5,
        "min_ap_cos": 0.05,
        "max_epochs": 1,
        "stop_on_ap_degradation": true,
        "ap_degradation_threshold": 0.002
      }
    },
    "regularization": {
      "dropout": 0.10,
      "attention_dropout": 0.04,
      "weight_noise": 0.0008,
      "gradient_noise": 0.00008,
      "ema_decay": 0.999,
      "stochastic_depth": 0.06
    }
  },
  "evaluation": {
    "validation_split": 0.07,
    "metrics": [
      "retrieval_at_1",
      "retrieval_at_5",
      "mean_cosine_similarity",
      "triplet_accuracy",
      "semantic_coherence"
    ],
    "semantic_tests": {
      "enabled": true,
      "test_analogies": true,
      "test_clustering": true
    }
  },
  "mlflow": {
    "tracking_uri": "http://localhost:5006",
    "experiment_name": "vmmoe_stable_v1p10",
    "run_name_prefix": "single_epoch_v1p10",
    "log_models": true,
    "log_metrics_frequency": 1,
    "log_artifacts": true,
    "log_visualizations": true
  },
  "checkpointing": {
    "checkpoint_dir": "output/vmmoe_stable_v1p10",
    "save_best": true,
    "save_last": true,
    "save_on_early_stop": true,
    "embed_config": true,
    "save_optimizer_state": false,
    "save_every_n_steps": 500,
    "save_every_n_epochs": 1
  },
  "strategy": {
    "philosophy": "V1.10 Single Epoch - Capture peak performance at Epoch 0",
    "key_insights": {
      "optimal_point": "Steps 2000-2325 (end of Epoch 0) shows COS 0.9831",
      "early_stopping": "Ultra-aggressive with immediate stop on AP degradation",
      "margin_schedule": "Constant low margin (0.05-0.08) to avoid over-separation",
      "learning_rate": "Even gentler 4e-05 with shorter warmup",
      "checkpoint_frequency": "Every 500 steps to capture exact optimal point"
    },
    "improvements_from_v1_9": {
      "epochs": {
        "max_epochs": "3 → 1 (hard cap at single epoch)",
        "min_epochs": "2 → 0 (allow partial epoch if optimal)",
        "checkpoint_frequency": "775 → 500 (more granular capture)"
      },
      "early_stopping": {
        "min_ap_cos": "Added 0.05 threshold as hard stop",
        "ap_degradation": "New: stop if AP increases by 0.002",
        "loss_target": "Disabled (was causing premature stops)"
      },
      "learning": {
        "learning_rate": "5e-05 → 4e-05 (even gentler)",
        "warmup_steps": "500 → 300 (faster warmup for single epoch)",
        "scheduler": "cosine_with_restarts → constant_with_warmup"
      },
      "margin": {
        "margin_end": "0.15 → 0.08 (minimal separation)",
        "schedule": "linear → constant (no progression needed)"
      },
      "augmentation": {
        "noise_std": "0.008 → 0.006 (less noise)",
        "dropout_prob": "0.05 → 0.03 (less dropout)",
        "mixup_alpha": "0.2 → 0.15 (less mixing)"
      }
    },
    "expected_results": {
      "convergence": "Single epoch or less (2000-2325 steps)",
      "final_loss": "~0.35-0.37 (Epoch 0 range)",
      "d_ap": "~0.016-0.10 (maintain low distance)",
      "d_an": "~0.14-0.15 (sufficient separation)",
      "training_time": "~20-25 minutes (1 epoch)",
      "quality": "COS 0.98+ with clean semantic preservation"
    },
    "monitoring": {
      "critical_metrics": [
        "val/anchor_positive_distance (must stay < 0.10)",
        "train/anchor_positive_distance (watch for increases > 0.002)",
        "Checkpoint every 500 steps (5 checkpoints total)",
        "Stop immediately on AP degradation"
      ],
      "watch_for": [
        "Peak performance around steps 2000-2325",
        "AP distance staying below 0.10",
        "Clean semantic output (no 'invulgable' artifacts)",
        "COS similarity above 0.98"
      ]
    },
    "warm_start_option": {
      "enabled": false,
      "checkpoint_path": "output/vmmoe_stable_v1p9/20250815T182317_SN000115_VMamba_epoch0.pth",
      "note": "Can use V1.9 Epoch 0 as initialization if needed"
    }
  }
}