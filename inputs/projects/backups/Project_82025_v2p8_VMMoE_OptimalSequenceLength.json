{
  "project": {
    "name": "VMMoE_v2p8_OptimalSequenceLength",
    "version": "2.8",
    "description": "SEQUENCE LENGTH FIX: Reduce sequence_length from 32 to 12 to match actual data (8.6 avg concepts/seq) and eliminate 73% padding waste",
    "timestamp": "2025-08-25T14:00:00",
    "author": "AI Assistant + User",
    "tags": ["VMMoE", "concept_sequences", "optimal_length", "data_driven", "padding_fix"],
    "metadata": {
      "id": "v2p8_optimal_sequence_length",
      "experiment_type": "data_optimization_training",
      "baseline": "v2p7_strong_preservation",
      "critical_problem": "v2p7_73_percent_padding_waste_only_28_steps_per_epoch",
      "key_innovations": [
        "sequence_length_12_covers_88_percent_of_sequences",
        "reduced_padding_from_73_to_12_percent",
        "increased_steps_from_28_to_90plus_per_epoch", 
        "maintained_strong_preservation_loss_weights",
        "data_driven_sequence_length_optimization"
      ],
      "data_analysis": {
        "current_issue": "32 length vs 8.6 avg concepts = 73% padding",
        "optimal_length": "12 covers 1739/1986 sequences (88%)",
        "padding_reduction": "73% -> 12% padding waste",
        "training_steps_increase": "28 -> 90+ steps per epoch",
        "token_equivalent": "12 concepts Ã— 17 tokens = 204 tokens context"
      },
      "expected_outcomes": [
        "increased_training_steps_90plus_per_epoch",
        "reduced_padding_waste_to_12_percent",
        "better_signal_to_noise_ratio_in_learning",
        "maintained_preservation_loss_balance",
        "realistic_training_duration_2_to_4_hours"
      ],
      "data_stats": {
        "sequences": 1986,
        "total_concepts": 16982,
        "vectors": 43488,
        "faiss_index": "20250822T114404_SN002588_concept_sequences_768.idx",
        "avg_concepts_per_seq": 8.6,
        "sequence_length_coverage": {
          "length_10": "85%",
          "length_12": "88%", 
          "length_16": "99%"
        }
      }
    }
  },
  "training": {
    "patience": 5,
    "min_delta": 0.0005,
    "save_every_n_epochs": 1,
    "early_stopping_metric": "val/total_loss",
    "early_stopping_mode": "min", 
    "early_stopping_patience": 7,
    "force_full_epochs": true,
    "training_params": {
      "batch_size": 16,
      "num_workers": 4,
      "epochs": 25,
      "steps_per_epoch": null,
      "validation_frequency": 1,
      "checkpoint_frequency": 100,
      "device_priority": ["mps", "cuda", "cpu"],
      "mixed_precision": false,
      "gradient_checkpointing": false,
      "gradient_accumulation_steps": 1,
      "max_grad_norm": 1.0,
      "warmup_steps": 150,
      "log_every_n_steps": 10,
      "progress_bar": true,
      "enable_profiling": false,
      "deterministic": false,
      "benchmark": true,
      "optimizer": "AdamW"
    },
    "optimizer": {
      "type": "AdamW",
      "optimizer": "AdamW", 
      "learning_rate": 3e-5,
      "lr": 3e-5,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01,
      "amsgrad": false,
      "foreach": true,
      "maximize": false,
      "capturable": false,
      "differentiable": false,
      "fused": false
    },
    "scheduler": {
      "type": "CosineAnnealingLR",
      "T_max": 800,
      "eta_min": 1e-6,
      "last_epoch": -1,
      "verbose": false
    },
    "loss_config": {
      "loss_type": "next_concept_prediction",
      "type": "sequence_prediction",
      "prediction_loss_weight": 0.4,
      "reconstruction_loss_weight": 0.4,
      "cosine_similarity_weight": 0.2,
      "cross_entropy_weight": 0.4,
      "coherence_weight": 0.05,
      "label_smoothing": 0.1,
      "ignore_index": -100,
      "reduction": "mean",
      "temperature": 0.3,
      "margin": 0.05,
      "cosine_similarity_target": 0.5,
      "cosine_similarity_tolerance": 0.3,
      "normalization_enforcement": {
        "enabled": true,
        "unit_norm_final": true,
        "l2_penalty_weight": 0.015,
        "cosine_similarity_threshold": 0.01,
        "normalize_embeddings": true,
        "stability_epsilon": 1e-8
      },
      "anti_collapse_config": {
        "enabled": true,
        "reconstruction_weight": 0.4,
        "cosine_preservation_weight": 0.2,
        "similarity_target": 0.5,
        "similarity_tolerance": 0.3
      }
    },
    "optimization": {
      "optimizer": "AdamW",
      "learning_rate": 3e-5,
      "weight_decay": 0.01,
      "gradient_clipping": 1.0,
      "warmup_steps": 150,
      "scheduler": "cosine",
      "eta_min": 1e-6,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "gradient_accumulation_steps": 1
    },
    "data": {
      "sequence_mode": true,
      "sequence_length": 12,
      "batch_size": 16,
      "num_workers": 4,
      "shuffle": true,
      "drop_last": true,
      "data_type": "concept_sequences",
      "max_vectors": null,
      "load_all_sequences": true,
      "sequence_sampling": "full",
      "concept_sequence_databases": [
        "data/databases/concept_sequences_768_vectors"
      ],
      "sequence_metadata_db": "data/databases/concept_sequences_metadata.db",
      "faiss_index_path": "data/databases/concept_sequences_768_vectors/20250822T114404_SN002588_concept_sequences_768.idx",
      "database_config": {
        "path": "data/databases/concept_sequences_768_vectors",
        "max_sequences": null,
        "load_all": true
      },
      "sequence_packing": {
        "enabled": true,
        "max_sequences_per_batch": 16,
        "padding_strategy": "longest",
        "truncation_strategy": "longest_first",
        "pack_efficiency_target": 0.8
      },
      "validation_split": 0.1,
      "test_split": 0.05
    },
    "architecture": {
      "model_type": "MambaLMHeadModel",
      "mamba_config": {
        "d_model": 768,
        "n_layers": 8,
        "d_state": 16,
        "d_conv": 4,
        "expand": 1.0,
        "bidirectional": false,
        "norm_type": "rmsnorm",
        "norm_eps": 1e-5,
        "ssm_activation": "silu",
        "gated": true
      },
      "output_projection": {
        "enabled": true,
        "hidden_dim": 768,
        "activation": "gelu",
        "dropout": 0.1,
        "normalize_output": true,
        "preserve_similarity": true
      },
      "base_model": "state-spaces/mamba-130m",
      "hidden_size": 768,
      "intermediate_size": 1536,
      "num_hidden_layers": 24,
      "vocab_size": 50280,
      "state_size": 16,
      "num_heads": 1,
      "head_dim": null,
      "use_bias": false,
      "use_conv_bias": true,
      "conv_kernel": 4,
      "expand_factor": 2,
      "dt_rank": "auto",
      "dt_min": 0.001,
      "dt_max": 0.1,
      "dt_init": "random",
      "dt_scale": 1.0,
      "dt_init_floor": 1e-4,
      "rms_norm": true,
      "norm_epsilon": 1e-5,
      "tie_word_embeddings": true,
      "pad_token_id": 50256,
      "bos_token_id": 50256,
      "eos_token_id": 50256,
      "prediction_head": {
        "type": "linear",
        "input_dim": 768,
        "output_dim": 768,
        "bias": false,
        "activation": "gelu",
        "dropout": 0.1,
        "layer_norm": true,
        "residual": true,
        "normalize_output": true,
        "preserve_similarity": true
      },
      "peft_config": null,
      "freeze_base": false,
      "positional_config": {
        "type": "learned",
        "mode": "learned", 
        "max_length": 512,
        "embedding_dim": 768
      },
      "sequence_config": {
        "max_length": 12,
        "sequence_length": 12
      }
    },
    "checkpoint_dir": "output/vmmoe_optimal_sequence_length_v2p8",
    "experiment_name": "vmmoe_optimal_sequence_length_v2p8", 
    "run_name": "v2p8_12_sequence_length_optimal"
  },
  "mlflow": {
    "enabled": true,
    "tracking_uri": "sqlite:///mlflow.db",
    "experiment_name": "vmmoe_optimal_sequence_length_v2p8",
    "run_name": "v2p8_12_sequence_length_optimal",
    "run_name_prefix": "v2p8_optimal",
    "log_models": true,
    "log_metrics": true,
    "log_params": true,
    "log_artifacts": true,
    "log_system_metrics": true,
    "log_input_examples": false,
    "log_metrics_frequency": 1,
    "registered_model_name": null,
    "tags": {
      "version": "v2p8",
      "type": "sequence_length_optimization",
      "epochs": "25",
      "purpose": "fix_padding_waste_increase_training_steps",
      "vectors": "43488",
      "critical_fix": "sequence_length_12_vs_32_reduces_padding_73_to_12_percent",
      "sequence_length": "12",
      "data_coverage": "88_percent"
    }
  },
  "evaluation": {
    "validation_split": 0.1
  },
  "checkpointing": {
    "checkpoint_dir": "output/vmmoe_optimal_sequence_length_v2p8",
    "save_best": true,
    "embed_config": true
  },
  "model": {
    "vmmoe": {
      "enabled": true,
      "num_experts": 8,
      "expert_dim": 768,
      "gating": {
        "type": "top_k",
        "k": 2,
        "noise_epsilon": 0.01,
        "capacity_factor": 1.25,
        "drop_tokens": false,
        "use_rts": false,
        "use_tutel": false,
        "moe_eval_capacity_factor": 1.0,
        "moe_min_capacity": 4,
        "gate_logits_norm": false,
        "gating_temperature": 1.0
      }
    },
    "normalization_enforcement": {
      "enabled": true,
      "unit_norm_final": true,
      "target_norm": 1.0,
      "norm_penalty_weight": 0.015,
      "norm_epsilon": 1e-6,
      "enforce_during_training": true,
      "enforce_during_inference": true,
      "norm_type": "l2",
      "clamp_norm": true,
      "clamp_min": 0.95,
      "clamp_max": 1.05
    }
  }
}