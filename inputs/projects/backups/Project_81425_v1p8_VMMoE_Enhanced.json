{
  "project": {
    "metadata": {
      "id": "VMMoE_Production_V1p8_Enhanced",
      "name": "VMMoE Production Training v1.8 - Enhanced with Critical Features",
      "description": "V1.8 enhanced with normalization enforcement, hybrid positional encoding, and LoRA adapters based on contractor analysis",
      "version": "1.8.0",
      "created_date": "2025-08-15",
      "training_date": "2025-08-15T13:xx",
      "notes": "Three critical features added: Normalization enforcement (L2 penalty 0.001), hybrid pos encoding (learned+sinusoidal), LoRA adapters (rank=8)"
    },
    "model": "sentence-transformers/gtr-t5-base",
    "local_model_path": "data/teacher_models/gtr-t5-base"
  },
  "training": {
    "architecture": {
      "mamba_config": {
        "d_model": 768,
        "n_layers": 5,
        "d_state": 24,
        "d_conv": 6,
        "expand": 1.75,
        "dt_rank": "auto",
        "dt_min": 0.001,
        "dt_max": 0.1,
        "dt_init": "xavier",
        "dt_scale": 0.9,
        "bias": false,
        "conv_bias": true,
        "dropout": 0.15,
        "layer_norm_epsilon": 1e-5
      },
      "positional_config": {
        "mode": "hybrid",
        "learned_weight": 0.7,
        "sinusoidal_weight": 0.3,
        "max_len": 512,
        "dropout": 0.1,
        "learned_config": {
          "embedding_dim": 768,
          "max_position_embeddings": 512
        },
        "sinusoidal_config": {
          "embedding_dim": 768,
          "temperature": 10000.0
        }
      },
      "peft_config": {
        "enabled": true,
        "adapter_type": "lora",
        "lora_rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.1,
        "target_modules": [
          "in_proj",
          "x_proj",
          "dt_proj",
          "A_log",
          "D",
          "out_proj"
        ],
        "modules_to_save": [
          "layer_norm"
        ]
      }
    },
    "loss_config": {
      "loss_type": "triplet",
      "margin_start": 0.06,
      "margin_end": 0.25,
      "margin_schedule": "cosine",
      "cosine_weight": 0.65,
      "mse_weight": 0.125,
      "contrastive_weight": 0.2,
      "orthogonality_weight": 0.0015,
      "temperature": 0.05,
      "label_smoothing": 0.05,
      "normalization_enforcement": {
        "enabled": true,
        "l2_penalty_weight": 0.001,
        "cosine_similarity_threshold": 0.05,
        "normalize_embeddings": true,
        "stability_epsilon": 1e-8
      }
    },
    "optimization": {
      "optimizer": "AdamW",
      "learning_rate": 7e-05,
      "weight_decay": 0.02,
      "gradient_clipping": 1.0,
      "warmup_steps": 200,
      "scheduler": "cosine_with_restarts",
      "restart_interval": 500,
      "restart_multiplier": 1.0,
      "betas": [0.9, 0.98],
      "eps": 1e-08
    },
    "data": {
      "batch_size": 24,
      "sequence_length": 16,
      "num_workers": 2,
      "shuffle": true,
      "drop_last": true,
      "concept_databases": [
        "data/databases/concept_vector_db_768_training",
        "data/databases/qa_training_triplets_768d"
      ],
      "augmentation": {
        "enabled": true,
        "noise_std": 0.008,
        "dropout_prob": 0.05,
        "mixup_alpha": 0.2
      }
    },
    "training_params": {
      "epochs": 10,
      "steps_per_epoch": 350,
      "validation_frequency": 1,
      "checkpoint_frequency": 50,
      "device_priority": [
        "mps",
        "cuda",
        "cpu"
      ],
      "seed": 42,
      "deterministic": false
    },
    "early_stopping": {
      "enabled": true,
      "patience": 1,
      "min_delta": 0.0003,
      "monitor": "val/loss",
      "mode": "min",
      "min_epochs": 2,
      "loss_target": 0.007,
      "loss_target_enabled": true,
      "verbose": true,
      "restore_best_weights": true,
      "additional_criteria": {
        "max_d_ap": 0.03,
        "min_d_an": 0.18,
        "max_overfit_ratio": 2.5,
        "min_ap_cos": 0.05
      }
    },
    "regularization": {
      "dropout": 0.12,
      "attention_dropout": 0.05,
      "weight_noise": 0.001,
      "gradient_noise": 0.0001,
      "ema_decay": 0.999,
      "stochastic_depth": 0.08
    }
  },
  "evaluation": {
    "validation_split": 0.07,
    "metrics": [
      "retrieval_at_1",
      "retrieval_at_5",
      "mean_cosine_similarity",
      "triplet_accuracy",
      "semantic_coherence"
    ],
    "semantic_tests": {
      "enabled": true,
      "test_analogies": true,
      "test_clustering": true
    }
  },
  "mlflow": {
    "tracking_uri": "http://localhost:5006",
    "experiment_name": "vmmoe_stable_v1p8",
    "run_name_prefix": "enhanced_v1p8",
    "log_models": true,
    "log_metrics_frequency": 1,
    "log_artifacts": true,
    "log_visualizations": true
  },
  "checkpointing": {
    "checkpoint_dir": "output/vmmoe_stable_v1p8",
    "save_best": true,
    "save_last": true,
    "save_on_early_stop": true,
    "embed_config": true,
    "save_optimizer_state": false
  },
  "strategy": {
    "philosophy": "V1.8 Enhanced - Critical features restoration based on contractor analysis",
    "key_features": {
      "normalization_enforcement": "L2 penalty (weight 0.001) to stabilize cosines and prevent negative AP similarities",
      "hybrid_positional_encoding": "70% learned + 30% sinusoidal for trajectory smoothness and semantic neighborhood preservation",
      "lora_adapters": "Rank=8 PEFT for parameter efficiency, preventing overfitting on 11.7M param base",
      "training_strategy": "Expert V1.7 base with 3 critical enhancements from removed features analysis"
    },
    "technical_rationale": {
      "normalization_enforcement": {
        "problem": "V1.6 curves showed negative AP similarities, likely from unstable vector norms",
        "solution": "Light L2 penalty (0.001) stabilizes cosines above 0.05 threshold",
        "benefit": "Aligns with Phase 1 nuclear diversity (mean cosine ~0.5166), prevents collapse"
      },
      "hybrid_positional_encoding": {
        "problem": "Full 'learned' mode may have flattened semantic neighborhood trajectories",
        "solution": "Hybrid approach: 70% learned + 30% sinusoidal for smoothness",
        "benefit": "Restores 81% trajectory improvement, supports Mamba temporal modeling"
      },
      "lora_adapters": {
        "problem": "Full parameter updates on 11.7M params may contribute to overfitting",
        "solution": "LoRA rank=8 enables PEFT-like fine-tuning with selective updates",
        "benefit": "Improves generalization on mixed datasets while maintaining 15-25min training"
      }
    },
    "improvements_from_v1_7": {
      "architecture": {
        "positional_encoding": "learned â†’ hybrid (70% learned + 30% sinusoidal)",
        "peft_integration": "Added LoRA adapters (rank=8, alpha=16) on key Mamba modules",
        "target_modules": "in_proj, x_proj, dt_proj, A_log, D, out_proj for selective fine-tuning"
      },
      "loss": {
        "normalization_enforcement": "New: L2 penalty (0.001) + cosine threshold (0.05)",
        "embedding_normalization": "Forced normalization with stability epsilon (1e-8)",
        "min_ap_cos_criterion": "Added to early stopping (0.05 threshold)"
      },
      "training": {
        "parameter_efficiency": "LoRA reduces trainable params while maintaining capacity",
        "stability": "Normalization prevents vector drift and negative similarities",
        "generalization": "Hybrid encoding improves multi-dataset performance"
      }
    },
    "expected_results": {
      "convergence": "1-2 epochs (maintained from V1.7 with enhanced stability)",
      "final_loss": "0.004-0.007 (expert-validated range with better stability)",
      "d_ap": "< 0.03 with guaranteed positive cosines (>0.05)",
      "d_an": "> 0.18 (enhanced separation with normalization)",
      "semantic_quality": "Improved trajectory smoothness and neighborhood coherence",
      "training_time": "~15-25 minutes (LoRA efficiency maintains speed)",
      "generalization": "Superior multi-dataset performance with PEFT fine-tuning",
      "stability": "Eliminated negative AP risk through normalization enforcement"
    },
    "monitoring": {
      "critical_metrics": [
        "AP cosine similarities (must stay > 0.05)",
        "Vector norms (should stabilize around 1.0)",
        "LoRA adapter utilization",
        "Hybrid position encoding weights",
        "Normalization penalty contribution"
      ],
      "watch_for": [
        "Negative AP prevention (normalization should fix)",
        "Position encoding balance (70/30 learned/sinusoidal)",
        "LoRA rank saturation",
        "Loss < 0.004 (overfitting risk)",
        "d_ap rising above 0.03",
        "d_an falling below 0.18",
        "Val/train ratio > 2.5"
      ]
    }
  }
}