{
  "project": {
    "metadata": {
      "id": "VMMoE_Production_V3_Optimized",
      "name": "VMMoE Production Training v1.2 - Optimized Architecture",
      "description": "Enhanced architecture with deeper Mamba layers, improved regularization, and refined hyperparameters based on v1.1 results",
      "version": "1.2.0",
      "created_date": "2025-08-14",
      "training_date": "2025-08-14T15:xx",
      "notes": "Deeper network (6 layers), stronger regularization, cosine warmup restart, mixed precision training"
    },
    "model": "sentence-transformers/gtr-t5-base",
    "local_model_path": "data/teacher_models/gtr-t5-base"
  },
  "training": {
    "architecture": {
      "mamba_config": {
        "d_model": 768,
        "n_layers": 6,
        "d_state": 24,
        "d_conv": 6,
        "expand": 2.0,
        "dt_rank": "auto",
        "dt_min": 0.0001,
        "dt_max": 0.05,
        "dt_init": "xavier",
        "dt_scale": 0.8,
        "bias": true,
        "conv_bias": true,
        "use_layer_norm": true,
        "dropout": 0.15,
        "attention_dropout": 0.1
      },
      "positional_config": {
        "mode": "learned",
        "max_len": 1024
      },
      "mixture_of_experts": {
        "enabled": false,
        "num_experts": 4,
        "expert_capacity": 1.25,
        "top_k": 2,
        "load_balancing_weight": 0.01
      }
    },
    "loss_config": {
      "loss_type": "triplet",
      "margin_start": 0.1,
      "margin_end": 0.3,
      "margin_schedule": "cosine",
      "cosine_weight": 0.6,
      "mse_weight": 0.15,
      "contrastive_weight": 0.2,
      "orthogonality_weight": 0.005,
      "diversity_weight": 0.01,
      "temperature": 0.07,
      "label_smoothing": 0.1,
      "focal_loss_gamma": 2.0
    },
    "optimization": {
      "optimizer": "AdamW",
      "learning_rate": 2e-04,
      "weight_decay": 0.05,
      "gradient_clipping": 1.0,
      "warmup_steps": 500,
      "scheduler": "cosine_with_restarts",
      "t_mult": 2,
      "eta_min": 1e-06,
      "betas": [0.9, 0.999],
      "eps": 1e-08,
      "mixed_precision": true,
      "gradient_accumulation_steps": 2
    },
    "data": {
      "batch_size": 32,
      "sequence_length": 32,
      "num_workers": 4,
      "shuffle": true,
      "drop_last": true,
      "concept_databases": [
        "data/databases/concept_vector_db_768_training"
      ],
      "augmentation": {
        "enabled": true,
        "noise_std": 0.01,
        "dropout_prob": 0.1,
        "mixup_alpha": 0.2
      }
    },
    "training_params": {
      "epochs": 15,
      "steps_per_epoch": 200,
      "validation_frequency": 5,
      "checkpoint_frequency": 50,
      "device_priority": [
        "mps",
        "cuda",
        "cpu"
      ],
      "seed": 42,
      "deterministic": true
    },
    "early_stopping": {
      "enabled": true,
      "patience": 8,
      "min_delta": 0.000001,
      "monitor": "val/loss",
      "mode": "min",
      "min_epochs": 5,
      "loss_target": 0.001,
      "loss_target_enabled": true,
      "restore_best_weights": true,
      "verbose": true
    },
    "regularization": {
      "dropout": 0.15,
      "attention_dropout": 0.1,
      "hidden_dropout": 0.1,
      "weight_noise": 0.01,
      "spectral_norm": true,
      "max_norm": 1.0
    }
  },
  "evaluation": {
    "validation_split": 0.1,
    "metrics": [
      "retrieval_at_1",
      "retrieval_at_5",
      "retrieval_at_10",
      "mean_cosine_similarity",
      "triplet_accuracy",
      "mean_reciprocal_rank",
      "normalized_discounted_cumulative_gain"
    ],
    "test_augmentation": false,
    "compute_embeddings_similarity_matrix": true
  },
  "mlflow": {
    "tracking_uri": "http://localhost:5006",
    "experiment_name": "vmmoe_stable_v1p2",
    "run_name_prefix": "production_v1p2_optimized",
    "log_models": true,
    "log_metrics_frequency": 1,
    "log_gradients": true,
    "log_parameters": true,
    "log_artifacts": true
  },
  "checkpointing": {
    "checkpoint_dir": "output/vmmoe_stable_v1p2",
    "save_best": true,
    "save_last": true,
    "save_top_k": 3,
    "embed_config": true,
    "save_optimizer_state": true
  },
  "improvements": {
    "from_v1.1": {
      "architecture": {
        "mamba_layers": "4 → 6 (deeper network)",
        "d_state": "16 → 24 (more state capacity)",
        "d_conv": "4 → 6 (larger receptive field)",
        "expand": "1.5 → 2.0 (more hidden units)",
        "positional": "learned → RoPE (better position encoding)",
        "dropout": "none → 0.15 (regularization)"
      },
      "optimization": {
        "learning_rate": "5e-05 → 2e-04 (4x, faster initial learning)",
        "scheduler": "cosine → cosine_with_restarts (avoid plateaus)",
        "batch_size": "16 → 32 (2x, better gradients)",
        "gradient_accumulation": "1 → 2 (effective batch 64)",
        "mixed_precision": "false → true (faster training)",
        "weight_decay": "0.01 → 0.05 (stronger regularization)"
      },
      "training": {
        "epochs": "50 → 15 (focused training)",
        "steps_per_epoch": "500 → 200 (3000 total steps)",
        "data_augmentation": "none → mixup + noise",
        "hard_negatives": "none → enabled (better discrimination)",
        "validation_split": "5% → 10% (better generalization tracking)"
      },
      "loss": {
        "margin_end": "0.2 → 0.3 (larger final margin)",
        "margin_schedule": "linear → cosine (smoother progression)",
        "hard_negatives": "false → true (mine difficult examples)",
        "temperature_scaling": "none → 0.07 (contrastive focus)"
      },
      "expected_improvements": {
        "convergence_speed": "3-4 epochs → 2-3 epochs",
        "final_loss": "~0.001 → <0.0005",
        "validation_accuracy": "better generalization",
        "training_time": "~7 hours → ~1 hour"
      }
    }
  }
}